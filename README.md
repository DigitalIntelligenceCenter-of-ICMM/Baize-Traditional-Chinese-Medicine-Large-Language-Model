# ç™½æ³½ä¸­åŒ»è¯å¤§è¯­è¨€æ¨¡å‹  
**Baize-Traditional-Chinese-Medicine-Large-Language-Model**

ğŸ©º ä¸“æ³¨ä¸­åŒ»è¯é¢†åŸŸçš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆTCM-LLMï¼‰  
ğŸ“š æ”¯æŒå¤šç‰ˆæœ¬è¯­æ–™è®­ç»ƒ | ğŸ§  åŒ…å« 0.6B ä¸ 8B ç³»åˆ—æ¨¡å‹ | ğŸ”§ æ”¯æŒ LoRA å¾®è°ƒä¸ 4bit/16bit æ¨ç†

> ğŸ‰ â€œç™½æ³½â€å–è‡ªä¸­å›½å¤ä»£é€šæ™“ä¸‡ç‰©çš„ç¥å…½ï¼Œå¯“æ„â€œé€šæ™“ä¸­åŒ»ï¼Œæ™ºå¯æœªæ¥â€ã€‚æœ¬é¡¹ç›®è‡´åŠ›äºæ„å»ºé¢å‘ä¸­åŒ»è¯é¢†åŸŸçš„æ™ºèƒ½é—®ç­”ä¸çŸ¥è¯†ç†è§£å¤§æ¨¡å‹ã€‚

---

## ğŸ“š é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å‘å¸ƒä¸€ç³»åˆ—åŸºäºå¼€æºå¤§æ¨¡å‹å¾®è°ƒçš„**ä¸­åŒ»è¯é¢†åŸŸä¸“ç”¨è¯­è¨€æ¨¡å‹**ï¼Œæ¶µç›–ä»å°å‚æ•°åˆ°å¤§æ¨¡å‹ã€ä»é‡åŒ–åˆ°å…¨ç²¾åº¦çš„å¤šç§é…ç½®ï¼Œæ”¯æŒç ”ç©¶ã€æ•™å­¦ä¸éå•†ä¸šåº”ç”¨ã€‚æ¨¡å‹åŸºäº [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) æ¡†æ¶è¿›è¡Œé«˜æ•ˆå¾®è°ƒï¼Œä½¿ç”¨è‡ªç ”çš„ **Baize-TCM-Corpus** ç³»åˆ—è¯­æ–™åº“è®­ç»ƒï¼Œå…·å¤‡è‰¯å¥½çš„ä¸­åŒ»çŸ¥è¯†ç†è§£ä¸é—®ç­”èƒ½åŠ›ã€‚

---

## ğŸ—ï¸ æ¨¡å‹æ¦‚è§ˆ

| æ¨¡å‹åç§° | åŸºç¡€æ¨¡å‹ | å‚æ•°é‡ | è®­ç»ƒè¯­æ–™ | é‡åŒ–æ–¹å¼ | æ¨ç†ç²¾åº¦ | ä¸‹è½½é“¾æ¥ |
|--------|----------|--------|----------|----------|----------|----------|
| `Baize-Traditional-Chinese-Medicine-Large-Language-Model` | Qwen3-0.6B | ~0.6B | V3 | æœªé‡åŒ– | FP16 | [Hugging Face](https://huggingface.co/DigitalIntelligenceCenter-of-ICMM/Baize-Traditional-Chinese-Medicine-Large-Language-Model) |
| `[Baize-Traditional-Chinese-Medicine-Large-Language-Model-V1-4bit]` | Qwen3-8B | ~8B | V1 | 4bit (NF4) | Int4 æ¨ç† | [Hugging Face](https://huggingface.co/DigitalIntelligenceCenter-of-ICMM/Baize-Traditional-Chinese-Medicine-Large-Language-Model-V1-4bit) |
| `Baize-Traditional-Chinese-Medicine-Large-Language-Model-V1-16bit` | Qwen3-8B  | ~8B | V1 | æœªé‡åŒ– | FP16 | [Hugging Face](https://huggingface.co/DigitalIntelligenceCenter-of-ICMM/Baize-Traditional-Chinese-Medicine-Large-Language-Model-V1-16bit)) |
| `Baize-Traditional-Chinese-Medicine-Large-Language-Model-V2-4bit` | Qwen3-8B  | ~8B | V2 | 4bit (NF4) | Int4 æ¨ç† | [Hugging Face](https://huggingface.co/DigitalIntelligenceCenter-of-ICMM/Baize-Traditional-Chinese-Medicine-Large-Language-Model-V2-4bit)) |
| `Baize-Traditional-Chinese-Medicine-Large-Language-Model-V2-16bit` | Qwen3-8B  | ~8B | V2 | æœªé‡åŒ– | FP16 | [Hugging Face](https://huggingface.co/DigitalIntelligenceCenter-of-ICMM/Baize-Traditional-Chinese-Medicine-Large-Language-Model-V2-16bit) |
| `Baize-Traditional-Chinese-Medicine-Large-Language-Model-V3-4bit` | Qwen3-8B  | ~8B | V3 | 4bit (NF4) | Int4 æ¨ç† | [Hugging Face](https://huggingface.co/DigitalIntelligenceCenter-of-ICMM/Baize-Traditional-Chinese-Medicine-Large-Language-Model-V3-16bit) |
| `Baize-Traditional-Chinese-Medicine-Large-Language-Model-V3-16bit` | Qwen3-8B  | ~8B | V3 | æœªé‡åŒ– | FP16 | [Hugging Face](https://huggingface.co/DigitalIntelligenceCenter-of-ICMM/Baize-Traditional-Chinese-Medicine-Large-Language-Model-V3-16bit) |

> âœ… æ‰€æœ‰ 8B æ¨¡å‹å‡é‡‡ç”¨ **LoRA å¾®è°ƒ**ï¼ŒåŸå§‹ä¸»å¹²å†»ç»“ï¼Œé€‚é…å™¨æƒé‡ç‹¬ç«‹å‘å¸ƒã€‚

---

## ğŸ§¾ è®­ç»ƒè¯¦æƒ…

- **å¾®è°ƒæ¡†æ¶**ï¼š[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
- **è®­ç»ƒæ–¹å¼**ï¼šLoRAï¼ˆ`r=64, lora_alpha=128, target_modules=q_proj,v_proj`ï¼‰
- **ä¸Šä¸‹æ–‡é•¿åº¦**ï¼š2048 tokens
- **è¯­æ–™ç‰ˆæœ¬è¯´æ˜**ï¼š
  - **V1**ï¼šåˆç‰ˆè¯­æ–™ï¼Œ4,000+ QA å¯¹
  - **V2**ï¼šæ‰©å±•è‡³ 10,578 æ¡ï¼Œä¼˜åŒ–æœ¯è¯­ä¸€è‡´æ€§
  - **V3**ï¼šè¿›ä¸€æ­¥æ¸…æ´—ä¸æ‰©å±•(157,438å¯¹)ï¼Œè¦†ç›–æ›´å¤šä¸´åºŠä¸ç»å…¸æ–‡çŒ®åœºæ™¯

---

## ğŸš€ å¿«é€Ÿä½¿ç”¨

### 1. åŠ è½½ 4bit é‡åŒ–æ¨¡å‹ï¼ˆä½æ˜¾å­˜ï¼‰

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    "your-username/baize-tcm-8b-v3-4bit",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained("your-username/baize-tcm-8b-v3-4bit", trust_remote_code=True)


### 2. åŠ è½½ 16bit æ¨¡å‹ï¼ˆé«˜ç²¾åº¦æ¨ç†ï¼‰
python
model = AutoModelForCausalLM.from_pretrained(
    "your-username/baize-tcm-8b-v3-16bit",
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained("your-username/baize-tcm-8b-v3-16bit", trust_remote_code=True)
