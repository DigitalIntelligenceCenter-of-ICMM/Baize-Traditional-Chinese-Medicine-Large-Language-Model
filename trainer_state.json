{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.0,
  "eval_steps": 500,
  "global_step": 6687,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0013458950201884253,
      "grad_norm": 0.6331137418746948,
      "learning_rate": 0.0,
      "loss": 2.2619,
      "step": 1
    },
    {
      "epoch": 0.0026917900403768506,
      "grad_norm": 0.5771821737289429,
      "learning_rate": 4e-05,
      "loss": 2.4295,
      "step": 2
    },
    {
      "epoch": 0.004037685060565276,
      "grad_norm": 0.5667468309402466,
      "learning_rate": 8e-05,
      "loss": 2.0916,
      "step": 3
    },
    {
      "epoch": 0.005383580080753701,
      "grad_norm": 0.7270931601524353,
      "learning_rate": 0.00012,
      "loss": 2.1628,
      "step": 4
    },
    {
      "epoch": 0.006729475100942127,
      "grad_norm": 0.6907123327255249,
      "learning_rate": 0.00016,
      "loss": 2.2011,
      "step": 5
    },
    {
      "epoch": 0.008075370121130552,
      "grad_norm": 0.7242401838302612,
      "learning_rate": 0.0002,
      "loss": 1.983,
      "step": 6
    },
    {
      "epoch": 0.009421265141318977,
      "grad_norm": 0.6059079170227051,
      "learning_rate": 0.0001999700688416642,
      "loss": 2.0471,
      "step": 7
    },
    {
      "epoch": 0.010767160161507403,
      "grad_norm": 0.5192564129829407,
      "learning_rate": 0.00019994013768332835,
      "loss": 2.0371,
      "step": 8
    },
    {
      "epoch": 0.012113055181695828,
      "grad_norm": 0.48665887117385864,
      "learning_rate": 0.00019991020652499253,
      "loss": 2.078,
      "step": 9
    },
    {
      "epoch": 0.013458950201884253,
      "grad_norm": 0.43057167530059814,
      "learning_rate": 0.00019988027536665669,
      "loss": 1.7477,
      "step": 10
    },
    {
      "epoch": 0.014804845222072678,
      "grad_norm": 0.477268785238266,
      "learning_rate": 0.00019985034420832087,
      "loss": 1.662,
      "step": 11
    },
    {
      "epoch": 0.016150740242261104,
      "grad_norm": 0.35192716121673584,
      "learning_rate": 0.00019982041304998505,
      "loss": 1.7547,
      "step": 12
    },
    {
      "epoch": 0.017496635262449527,
      "grad_norm": 0.3691205084323883,
      "learning_rate": 0.0001997904818916492,
      "loss": 1.7951,
      "step": 13
    },
    {
      "epoch": 0.018842530282637954,
      "grad_norm": 0.4220775365829468,
      "learning_rate": 0.0001997605507333134,
      "loss": 1.6614,
      "step": 14
    },
    {
      "epoch": 0.020188425302826378,
      "grad_norm": 0.38720911741256714,
      "learning_rate": 0.00019973061957497757,
      "loss": 1.7048,
      "step": 15
    },
    {
      "epoch": 0.021534320323014805,
      "grad_norm": 0.4589530825614929,
      "learning_rate": 0.00019970068841664173,
      "loss": 1.9298,
      "step": 16
    },
    {
      "epoch": 0.02288021534320323,
      "grad_norm": 0.4917554557323456,
      "learning_rate": 0.0001996707572583059,
      "loss": 1.6908,
      "step": 17
    },
    {
      "epoch": 0.024226110363391656,
      "grad_norm": 0.44583624601364136,
      "learning_rate": 0.00019964082609997007,
      "loss": 1.4792,
      "step": 18
    },
    {
      "epoch": 0.02557200538358008,
      "grad_norm": 0.4176270663738251,
      "learning_rate": 0.00019961089494163425,
      "loss": 1.4511,
      "step": 19
    },
    {
      "epoch": 0.026917900403768506,
      "grad_norm": 0.3964238166809082,
      "learning_rate": 0.00019958096378329843,
      "loss": 1.677,
      "step": 20
    },
    {
      "epoch": 0.02826379542395693,
      "grad_norm": 0.4791238605976105,
      "learning_rate": 0.0001995510326249626,
      "loss": 1.6379,
      "step": 21
    },
    {
      "epoch": 0.029609690444145357,
      "grad_norm": 0.45319995284080505,
      "learning_rate": 0.00019952110146662677,
      "loss": 1.7843,
      "step": 22
    },
    {
      "epoch": 0.03095558546433378,
      "grad_norm": 0.4280042052268982,
      "learning_rate": 0.00019949117030829095,
      "loss": 1.6075,
      "step": 23
    },
    {
      "epoch": 0.03230148048452221,
      "grad_norm": 0.546413242816925,
      "learning_rate": 0.0001994612391499551,
      "loss": 1.3208,
      "step": 24
    },
    {
      "epoch": 0.033647375504710635,
      "grad_norm": 0.4363706111907959,
      "learning_rate": 0.0001994313079916193,
      "loss": 1.8539,
      "step": 25
    },
    {
      "epoch": 0.034993270524899055,
      "grad_norm": 0.38727909326553345,
      "learning_rate": 0.00019940137683328345,
      "loss": 1.5812,
      "step": 26
    },
    {
      "epoch": 0.03633916554508748,
      "grad_norm": 0.3942873179912567,
      "learning_rate": 0.00019937144567494763,
      "loss": 1.5009,
      "step": 27
    },
    {
      "epoch": 0.03768506056527591,
      "grad_norm": 0.4387199580669403,
      "learning_rate": 0.0001993415145166118,
      "loss": 1.7524,
      "step": 28
    },
    {
      "epoch": 0.039030955585464336,
      "grad_norm": 0.37331467866897583,
      "learning_rate": 0.00019931158335827597,
      "loss": 1.5616,
      "step": 29
    },
    {
      "epoch": 0.040376850605652756,
      "grad_norm": 0.4031124413013458,
      "learning_rate": 0.00019928165219994015,
      "loss": 1.8215,
      "step": 30
    },
    {
      "epoch": 0.04172274562584118,
      "grad_norm": 0.3036375939846039,
      "learning_rate": 0.00019925172104160433,
      "loss": 1.5246,
      "step": 31
    },
    {
      "epoch": 0.04306864064602961,
      "grad_norm": 0.36452293395996094,
      "learning_rate": 0.0001992217898832685,
      "loss": 1.6225,
      "step": 32
    },
    {
      "epoch": 0.04441453566621804,
      "grad_norm": 0.44182324409484863,
      "learning_rate": 0.00019919185872493267,
      "loss": 1.7409,
      "step": 33
    },
    {
      "epoch": 0.04576043068640646,
      "grad_norm": 0.381164014339447,
      "learning_rate": 0.00019916192756659683,
      "loss": 1.5962,
      "step": 34
    },
    {
      "epoch": 0.047106325706594884,
      "grad_norm": 0.41734662652015686,
      "learning_rate": 0.000199131996408261,
      "loss": 1.4411,
      "step": 35
    },
    {
      "epoch": 0.04845222072678331,
      "grad_norm": 0.4574701189994812,
      "learning_rate": 0.0001991020652499252,
      "loss": 1.7742,
      "step": 36
    },
    {
      "epoch": 0.04979811574697174,
      "grad_norm": 0.4209780693054199,
      "learning_rate": 0.00019907213409158935,
      "loss": 1.3222,
      "step": 37
    },
    {
      "epoch": 0.05114401076716016,
      "grad_norm": 0.413492888212204,
      "learning_rate": 0.00019904220293325353,
      "loss": 1.5381,
      "step": 38
    },
    {
      "epoch": 0.052489905787348586,
      "grad_norm": 0.38612574338912964,
      "learning_rate": 0.0001990122717749177,
      "loss": 1.6355,
      "step": 39
    },
    {
      "epoch": 0.05383580080753701,
      "grad_norm": 0.33559083938598633,
      "learning_rate": 0.00019898234061658187,
      "loss": 1.8111,
      "step": 40
    },
    {
      "epoch": 0.05518169582772544,
      "grad_norm": 0.38693222403526306,
      "learning_rate": 0.00019895240945824605,
      "loss": 1.6336,
      "step": 41
    },
    {
      "epoch": 0.05652759084791386,
      "grad_norm": 0.3171728849411011,
      "learning_rate": 0.0001989224782999102,
      "loss": 1.4855,
      "step": 42
    },
    {
      "epoch": 0.05787348586810229,
      "grad_norm": 0.46706050634384155,
      "learning_rate": 0.0001988925471415744,
      "loss": 1.5393,
      "step": 43
    },
    {
      "epoch": 0.059219380888290714,
      "grad_norm": 0.3266170024871826,
      "learning_rate": 0.00019886261598323857,
      "loss": 1.576,
      "step": 44
    },
    {
      "epoch": 0.06056527590847914,
      "grad_norm": 0.32221290469169617,
      "learning_rate": 0.00019883268482490273,
      "loss": 1.6629,
      "step": 45
    },
    {
      "epoch": 0.06191117092866756,
      "grad_norm": 0.37127652764320374,
      "learning_rate": 0.0001988027536665669,
      "loss": 1.4947,
      "step": 46
    },
    {
      "epoch": 0.063257065948856,
      "grad_norm": 0.4001587927341461,
      "learning_rate": 0.0001987728225082311,
      "loss": 1.6498,
      "step": 47
    },
    {
      "epoch": 0.06460296096904442,
      "grad_norm": 0.41890600323677063,
      "learning_rate": 0.00019874289134989525,
      "loss": 1.7009,
      "step": 48
    },
    {
      "epoch": 0.06594885598923284,
      "grad_norm": 0.4138035476207733,
      "learning_rate": 0.00019871296019155943,
      "loss": 1.5429,
      "step": 49
    },
    {
      "epoch": 0.06729475100942127,
      "grad_norm": 0.3285013735294342,
      "learning_rate": 0.0001986830290332236,
      "loss": 1.4293,
      "step": 50
    },
    {
      "epoch": 0.06864064602960969,
      "grad_norm": 0.377492219209671,
      "learning_rate": 0.00019865309787488777,
      "loss": 1.6436,
      "step": 51
    },
    {
      "epoch": 0.06998654104979811,
      "grad_norm": 0.40500110387802124,
      "learning_rate": 0.00019862316671655195,
      "loss": 1.5143,
      "step": 52
    },
    {
      "epoch": 0.07133243606998654,
      "grad_norm": 0.374634712934494,
      "learning_rate": 0.0001985932355582161,
      "loss": 1.57,
      "step": 53
    },
    {
      "epoch": 0.07267833109017496,
      "grad_norm": 0.37493351101875305,
      "learning_rate": 0.0001985633043998803,
      "loss": 1.4201,
      "step": 54
    },
    {
      "epoch": 0.0740242261103634,
      "grad_norm": 0.4249369204044342,
      "learning_rate": 0.00019853337324154447,
      "loss": 1.5647,
      "step": 55
    },
    {
      "epoch": 0.07537012113055182,
      "grad_norm": 0.4239809513092041,
      "learning_rate": 0.00019850344208320863,
      "loss": 1.6073,
      "step": 56
    },
    {
      "epoch": 0.07671601615074024,
      "grad_norm": 0.43505775928497314,
      "learning_rate": 0.0001984735109248728,
      "loss": 1.4632,
      "step": 57
    },
    {
      "epoch": 0.07806191117092867,
      "grad_norm": 0.42029252648353577,
      "learning_rate": 0.00019844357976653697,
      "loss": 1.6405,
      "step": 58
    },
    {
      "epoch": 0.07940780619111709,
      "grad_norm": 0.35187211632728577,
      "learning_rate": 0.00019841364860820115,
      "loss": 1.4728,
      "step": 59
    },
    {
      "epoch": 0.08075370121130551,
      "grad_norm": 0.3820531368255615,
      "learning_rate": 0.00019838371744986533,
      "loss": 1.8498,
      "step": 60
    },
    {
      "epoch": 0.08209959623149395,
      "grad_norm": 0.36241552233695984,
      "learning_rate": 0.0001983537862915295,
      "loss": 1.4717,
      "step": 61
    },
    {
      "epoch": 0.08344549125168237,
      "grad_norm": 0.3983094394207001,
      "learning_rate": 0.00019832385513319367,
      "loss": 1.4437,
      "step": 62
    },
    {
      "epoch": 0.0847913862718708,
      "grad_norm": 0.3486325442790985,
      "learning_rate": 0.00019829392397485785,
      "loss": 1.699,
      "step": 63
    },
    {
      "epoch": 0.08613728129205922,
      "grad_norm": 0.43411388993263245,
      "learning_rate": 0.000198263992816522,
      "loss": 1.7498,
      "step": 64
    },
    {
      "epoch": 0.08748317631224764,
      "grad_norm": 0.4564831554889679,
      "learning_rate": 0.0001982340616581862,
      "loss": 1.7468,
      "step": 65
    },
    {
      "epoch": 0.08882907133243607,
      "grad_norm": 0.4456576108932495,
      "learning_rate": 0.00019820413049985035,
      "loss": 1.6188,
      "step": 66
    },
    {
      "epoch": 0.0901749663526245,
      "grad_norm": 0.4078943729400635,
      "learning_rate": 0.00019817419934151453,
      "loss": 1.4379,
      "step": 67
    },
    {
      "epoch": 0.09152086137281291,
      "grad_norm": 0.42457330226898193,
      "learning_rate": 0.00019814426818317871,
      "loss": 1.601,
      "step": 68
    },
    {
      "epoch": 0.09286675639300135,
      "grad_norm": 0.537818193435669,
      "learning_rate": 0.00019811433702484287,
      "loss": 1.6019,
      "step": 69
    },
    {
      "epoch": 0.09421265141318977,
      "grad_norm": 0.486696720123291,
      "learning_rate": 0.00019808440586650705,
      "loss": 1.3613,
      "step": 70
    },
    {
      "epoch": 0.0955585464333782,
      "grad_norm": 0.4536153972148895,
      "learning_rate": 0.00019805447470817123,
      "loss": 1.4615,
      "step": 71
    },
    {
      "epoch": 0.09690444145356662,
      "grad_norm": 0.3901122808456421,
      "learning_rate": 0.0001980245435498354,
      "loss": 1.5568,
      "step": 72
    },
    {
      "epoch": 0.09825033647375504,
      "grad_norm": 0.3457449972629547,
      "learning_rate": 0.00019799461239149957,
      "loss": 1.4444,
      "step": 73
    },
    {
      "epoch": 0.09959623149394348,
      "grad_norm": 0.4090017080307007,
      "learning_rate": 0.00019796468123316373,
      "loss": 1.5685,
      "step": 74
    },
    {
      "epoch": 0.1009421265141319,
      "grad_norm": 0.4485475718975067,
      "learning_rate": 0.0001979347500748279,
      "loss": 1.6511,
      "step": 75
    },
    {
      "epoch": 0.10228802153432032,
      "grad_norm": 0.528103768825531,
      "learning_rate": 0.0001979048189164921,
      "loss": 1.6162,
      "step": 76
    },
    {
      "epoch": 0.10363391655450875,
      "grad_norm": 0.4342747628688812,
      "learning_rate": 0.00019787488775815625,
      "loss": 1.4834,
      "step": 77
    },
    {
      "epoch": 0.10497981157469717,
      "grad_norm": 0.43274837732315063,
      "learning_rate": 0.00019784495659982043,
      "loss": 1.5945,
      "step": 78
    },
    {
      "epoch": 0.1063257065948856,
      "grad_norm": 0.38515108823776245,
      "learning_rate": 0.00019781502544148461,
      "loss": 1.3776,
      "step": 79
    },
    {
      "epoch": 0.10767160161507403,
      "grad_norm": 0.3971472978591919,
      "learning_rate": 0.00019778509428314877,
      "loss": 1.3572,
      "step": 80
    },
    {
      "epoch": 0.10901749663526245,
      "grad_norm": 0.45689767599105835,
      "learning_rate": 0.00019775516312481295,
      "loss": 2.1396,
      "step": 81
    },
    {
      "epoch": 0.11036339165545088,
      "grad_norm": 0.39676979184150696,
      "learning_rate": 0.0001977252319664771,
      "loss": 1.6091,
      "step": 82
    },
    {
      "epoch": 0.1117092866756393,
      "grad_norm": 0.3890925347805023,
      "learning_rate": 0.0001976953008081413,
      "loss": 1.4877,
      "step": 83
    },
    {
      "epoch": 0.11305518169582772,
      "grad_norm": 0.4282072186470032,
      "learning_rate": 0.00019766536964980547,
      "loss": 1.3812,
      "step": 84
    },
    {
      "epoch": 0.11440107671601615,
      "grad_norm": 0.4052659273147583,
      "learning_rate": 0.00019763543849146963,
      "loss": 1.6952,
      "step": 85
    },
    {
      "epoch": 0.11574697173620457,
      "grad_norm": 0.40783730149269104,
      "learning_rate": 0.0001976055073331338,
      "loss": 1.4433,
      "step": 86
    },
    {
      "epoch": 0.11709286675639301,
      "grad_norm": 0.39481669664382935,
      "learning_rate": 0.000197575576174798,
      "loss": 1.5893,
      "step": 87
    },
    {
      "epoch": 0.11843876177658143,
      "grad_norm": 0.43898117542266846,
      "learning_rate": 0.00019754564501646215,
      "loss": 1.4936,
      "step": 88
    },
    {
      "epoch": 0.11978465679676985,
      "grad_norm": 0.42417600750923157,
      "learning_rate": 0.00019751571385812633,
      "loss": 1.5677,
      "step": 89
    },
    {
      "epoch": 0.12113055181695828,
      "grad_norm": 0.4393293857574463,
      "learning_rate": 0.0001974857826997905,
      "loss": 1.5296,
      "step": 90
    },
    {
      "epoch": 0.1224764468371467,
      "grad_norm": 0.3817737400531769,
      "learning_rate": 0.00019745585154145467,
      "loss": 1.6955,
      "step": 91
    },
    {
      "epoch": 0.12382234185733512,
      "grad_norm": 0.3622349798679352,
      "learning_rate": 0.00019742592038311885,
      "loss": 1.6331,
      "step": 92
    },
    {
      "epoch": 0.12516823687752354,
      "grad_norm": 0.44163960218429565,
      "learning_rate": 0.000197395989224783,
      "loss": 1.4625,
      "step": 93
    },
    {
      "epoch": 0.126514131897712,
      "grad_norm": 0.3737627863883972,
      "learning_rate": 0.0001973660580664472,
      "loss": 1.6437,
      "step": 94
    },
    {
      "epoch": 0.1278600269179004,
      "grad_norm": 0.4400952458381653,
      "learning_rate": 0.00019733612690811138,
      "loss": 1.7862,
      "step": 95
    },
    {
      "epoch": 0.12920592193808883,
      "grad_norm": 0.509540855884552,
      "learning_rate": 0.00019730619574977553,
      "loss": 1.8865,
      "step": 96
    },
    {
      "epoch": 0.13055181695827725,
      "grad_norm": 0.5624663829803467,
      "learning_rate": 0.00019727626459143971,
      "loss": 1.2987,
      "step": 97
    },
    {
      "epoch": 0.13189771197846567,
      "grad_norm": 0.41098901629447937,
      "learning_rate": 0.00019724633343310387,
      "loss": 1.6267,
      "step": 98
    },
    {
      "epoch": 0.13324360699865412,
      "grad_norm": 0.443421334028244,
      "learning_rate": 0.00019721640227476805,
      "loss": 1.3626,
      "step": 99
    },
    {
      "epoch": 0.13458950201884254,
      "grad_norm": 0.39046457409858704,
      "learning_rate": 0.00019718647111643223,
      "loss": 1.7649,
      "step": 100
    },
    {
      "epoch": 0.13593539703903096,
      "grad_norm": 0.46480998396873474,
      "learning_rate": 0.0001971565399580964,
      "loss": 1.7111,
      "step": 101
    },
    {
      "epoch": 0.13728129205921938,
      "grad_norm": 0.4011566936969757,
      "learning_rate": 0.00019712660879976057,
      "loss": 1.6753,
      "step": 102
    },
    {
      "epoch": 0.1386271870794078,
      "grad_norm": 0.37489527463912964,
      "learning_rate": 0.00019709667764142476,
      "loss": 1.6887,
      "step": 103
    },
    {
      "epoch": 0.13997308209959622,
      "grad_norm": 0.40640902519226074,
      "learning_rate": 0.0001970667464830889,
      "loss": 1.8397,
      "step": 104
    },
    {
      "epoch": 0.14131897711978467,
      "grad_norm": 0.4466208219528198,
      "learning_rate": 0.0001970368153247531,
      "loss": 1.7874,
      "step": 105
    },
    {
      "epoch": 0.1426648721399731,
      "grad_norm": 0.403050035238266,
      "learning_rate": 0.00019700688416641725,
      "loss": 1.3834,
      "step": 106
    },
    {
      "epoch": 0.1440107671601615,
      "grad_norm": 0.41965579986572266,
      "learning_rate": 0.00019697695300808143,
      "loss": 1.401,
      "step": 107
    },
    {
      "epoch": 0.14535666218034993,
      "grad_norm": 0.4109238386154175,
      "learning_rate": 0.00019694702184974562,
      "loss": 1.1945,
      "step": 108
    },
    {
      "epoch": 0.14670255720053835,
      "grad_norm": 0.379739373922348,
      "learning_rate": 0.00019691709069140977,
      "loss": 1.2926,
      "step": 109
    },
    {
      "epoch": 0.1480484522207268,
      "grad_norm": 0.5421762466430664,
      "learning_rate": 0.00019688715953307395,
      "loss": 1.3591,
      "step": 110
    },
    {
      "epoch": 0.14939434724091522,
      "grad_norm": 0.37746426463127136,
      "learning_rate": 0.0001968572283747381,
      "loss": 1.4597,
      "step": 111
    },
    {
      "epoch": 0.15074024226110364,
      "grad_norm": 0.39208486676216125,
      "learning_rate": 0.00019682729721640226,
      "loss": 1.6417,
      "step": 112
    },
    {
      "epoch": 0.15208613728129206,
      "grad_norm": 0.5070697665214539,
      "learning_rate": 0.00019679736605806645,
      "loss": 1.5951,
      "step": 113
    },
    {
      "epoch": 0.15343203230148048,
      "grad_norm": 0.5069887042045593,
      "learning_rate": 0.00019676743489973063,
      "loss": 1.6743,
      "step": 114
    },
    {
      "epoch": 0.15477792732166892,
      "grad_norm": 0.43133458495140076,
      "learning_rate": 0.00019673750374139479,
      "loss": 1.9676,
      "step": 115
    },
    {
      "epoch": 0.15612382234185734,
      "grad_norm": 0.3689342439174652,
      "learning_rate": 0.00019670757258305897,
      "loss": 1.6082,
      "step": 116
    },
    {
      "epoch": 0.15746971736204576,
      "grad_norm": 0.43124863505363464,
      "learning_rate": 0.00019667764142472312,
      "loss": 1.5671,
      "step": 117
    },
    {
      "epoch": 0.15881561238223418,
      "grad_norm": 0.4896514117717743,
      "learning_rate": 0.0001966477102663873,
      "loss": 1.476,
      "step": 118
    },
    {
      "epoch": 0.1601615074024226,
      "grad_norm": 0.4288739562034607,
      "learning_rate": 0.0001966177791080515,
      "loss": 1.3787,
      "step": 119
    },
    {
      "epoch": 0.16150740242261102,
      "grad_norm": 0.4196672737598419,
      "learning_rate": 0.00019658784794971564,
      "loss": 1.5459,
      "step": 120
    },
    {
      "epoch": 0.16285329744279947,
      "grad_norm": 0.42750629782676697,
      "learning_rate": 0.00019655791679137983,
      "loss": 1.5475,
      "step": 121
    },
    {
      "epoch": 0.1641991924629879,
      "grad_norm": 0.4268766939640045,
      "learning_rate": 0.000196527985633044,
      "loss": 1.7352,
      "step": 122
    },
    {
      "epoch": 0.1655450874831763,
      "grad_norm": 0.39081791043281555,
      "learning_rate": 0.00019649805447470817,
      "loss": 1.6481,
      "step": 123
    },
    {
      "epoch": 0.16689098250336473,
      "grad_norm": 0.46328678727149963,
      "learning_rate": 0.00019646812331637235,
      "loss": 1.516,
      "step": 124
    },
    {
      "epoch": 0.16823687752355315,
      "grad_norm": 0.4722370505332947,
      "learning_rate": 0.0001964381921580365,
      "loss": 1.4618,
      "step": 125
    },
    {
      "epoch": 0.1695827725437416,
      "grad_norm": 0.4286503195762634,
      "learning_rate": 0.0001964082609997007,
      "loss": 1.4559,
      "step": 126
    },
    {
      "epoch": 0.17092866756393002,
      "grad_norm": 0.4019536077976227,
      "learning_rate": 0.00019637832984136487,
      "loss": 1.3672,
      "step": 127
    },
    {
      "epoch": 0.17227456258411844,
      "grad_norm": 0.40248697996139526,
      "learning_rate": 0.00019634839868302903,
      "loss": 1.3571,
      "step": 128
    },
    {
      "epoch": 0.17362045760430686,
      "grad_norm": 0.46810445189476013,
      "learning_rate": 0.0001963184675246932,
      "loss": 1.6105,
      "step": 129
    },
    {
      "epoch": 0.17496635262449528,
      "grad_norm": 0.41831427812576294,
      "learning_rate": 0.0001962885363663574,
      "loss": 1.2926,
      "step": 130
    },
    {
      "epoch": 0.17631224764468373,
      "grad_norm": 0.4834308922290802,
      "learning_rate": 0.00019625860520802155,
      "loss": 1.7246,
      "step": 131
    },
    {
      "epoch": 0.17765814266487215,
      "grad_norm": 0.5390082001686096,
      "learning_rate": 0.00019622867404968573,
      "loss": 1.7106,
      "step": 132
    },
    {
      "epoch": 0.17900403768506057,
      "grad_norm": 0.4356420338153839,
      "learning_rate": 0.00019619874289134988,
      "loss": 1.5604,
      "step": 133
    },
    {
      "epoch": 0.180349932705249,
      "grad_norm": 0.4660649597644806,
      "learning_rate": 0.00019616881173301407,
      "loss": 1.8641,
      "step": 134
    },
    {
      "epoch": 0.1816958277254374,
      "grad_norm": 0.3566037714481354,
      "learning_rate": 0.00019613888057467825,
      "loss": 1.7158,
      "step": 135
    },
    {
      "epoch": 0.18304172274562583,
      "grad_norm": 0.42522186040878296,
      "learning_rate": 0.0001961089494163424,
      "loss": 1.556,
      "step": 136
    },
    {
      "epoch": 0.18438761776581428,
      "grad_norm": 0.4237120449542999,
      "learning_rate": 0.0001960790182580066,
      "loss": 1.6161,
      "step": 137
    },
    {
      "epoch": 0.1857335127860027,
      "grad_norm": 0.4443824291229248,
      "learning_rate": 0.00019604908709967077,
      "loss": 1.7562,
      "step": 138
    },
    {
      "epoch": 0.18707940780619112,
      "grad_norm": 0.3791770339012146,
      "learning_rate": 0.00019601915594133493,
      "loss": 1.6427,
      "step": 139
    },
    {
      "epoch": 0.18842530282637954,
      "grad_norm": 0.3727545440196991,
      "learning_rate": 0.0001959892247829991,
      "loss": 1.2613,
      "step": 140
    },
    {
      "epoch": 0.18977119784656796,
      "grad_norm": 0.39042016863822937,
      "learning_rate": 0.00019595929362466326,
      "loss": 1.3628,
      "step": 141
    },
    {
      "epoch": 0.1911170928667564,
      "grad_norm": 0.4054146409034729,
      "learning_rate": 0.00019592936246632745,
      "loss": 1.5926,
      "step": 142
    },
    {
      "epoch": 0.19246298788694483,
      "grad_norm": 0.4905805289745331,
      "learning_rate": 0.00019589943130799163,
      "loss": 1.3675,
      "step": 143
    },
    {
      "epoch": 0.19380888290713325,
      "grad_norm": 0.4533534049987793,
      "learning_rate": 0.00019586950014965579,
      "loss": 1.388,
      "step": 144
    },
    {
      "epoch": 0.19515477792732167,
      "grad_norm": 0.45286592841148376,
      "learning_rate": 0.00019583956899131997,
      "loss": 1.3987,
      "step": 145
    },
    {
      "epoch": 0.19650067294751009,
      "grad_norm": 0.5145238041877747,
      "learning_rate": 0.00019580963783298415,
      "loss": 1.5807,
      "step": 146
    },
    {
      "epoch": 0.19784656796769853,
      "grad_norm": 0.49729278683662415,
      "learning_rate": 0.0001957797066746483,
      "loss": 1.5831,
      "step": 147
    },
    {
      "epoch": 0.19919246298788695,
      "grad_norm": 0.45549705624580383,
      "learning_rate": 0.0001957497755163125,
      "loss": 1.3237,
      "step": 148
    },
    {
      "epoch": 0.20053835800807537,
      "grad_norm": 0.5804128646850586,
      "learning_rate": 0.00019571984435797665,
      "loss": 1.5508,
      "step": 149
    },
    {
      "epoch": 0.2018842530282638,
      "grad_norm": 0.4224434792995453,
      "learning_rate": 0.00019568991319964083,
      "loss": 1.4315,
      "step": 150
    },
    {
      "epoch": 0.2032301480484522,
      "grad_norm": 0.36007529497146606,
      "learning_rate": 0.000195659982041305,
      "loss": 1.7647,
      "step": 151
    },
    {
      "epoch": 0.20457604306864063,
      "grad_norm": 0.4304380714893341,
      "learning_rate": 0.00019563005088296917,
      "loss": 1.7948,
      "step": 152
    },
    {
      "epoch": 0.20592193808882908,
      "grad_norm": 0.4207760989665985,
      "learning_rate": 0.00019560011972463335,
      "loss": 1.5448,
      "step": 153
    },
    {
      "epoch": 0.2072678331090175,
      "grad_norm": 0.4294090270996094,
      "learning_rate": 0.00019557018856629753,
      "loss": 1.5532,
      "step": 154
    },
    {
      "epoch": 0.20861372812920592,
      "grad_norm": 0.438914030790329,
      "learning_rate": 0.0001955402574079617,
      "loss": 1.4501,
      "step": 155
    },
    {
      "epoch": 0.20995962314939434,
      "grad_norm": 0.3628188371658325,
      "learning_rate": 0.00019551032624962587,
      "loss": 1.4005,
      "step": 156
    },
    {
      "epoch": 0.21130551816958276,
      "grad_norm": 0.3910064995288849,
      "learning_rate": 0.00019548039509129003,
      "loss": 1.5278,
      "step": 157
    },
    {
      "epoch": 0.2126514131897712,
      "grad_norm": 0.4621768891811371,
      "learning_rate": 0.0001954504639329542,
      "loss": 1.3701,
      "step": 158
    },
    {
      "epoch": 0.21399730820995963,
      "grad_norm": 0.35832691192626953,
      "learning_rate": 0.0001954205327746184,
      "loss": 1.5201,
      "step": 159
    },
    {
      "epoch": 0.21534320323014805,
      "grad_norm": 0.4175947308540344,
      "learning_rate": 0.00019539060161628255,
      "loss": 1.91,
      "step": 160
    },
    {
      "epoch": 0.21668909825033647,
      "grad_norm": 0.539040744304657,
      "learning_rate": 0.00019536067045794673,
      "loss": 1.7276,
      "step": 161
    },
    {
      "epoch": 0.2180349932705249,
      "grad_norm": 0.4951575696468353,
      "learning_rate": 0.0001953307392996109,
      "loss": 1.4898,
      "step": 162
    },
    {
      "epoch": 0.2193808882907133,
      "grad_norm": 0.5471806526184082,
      "learning_rate": 0.00019530080814127507,
      "loss": 1.4832,
      "step": 163
    },
    {
      "epoch": 0.22072678331090176,
      "grad_norm": 0.50568026304245,
      "learning_rate": 0.00019527087698293925,
      "loss": 1.5819,
      "step": 164
    },
    {
      "epoch": 0.22207267833109018,
      "grad_norm": 0.46545329689979553,
      "learning_rate": 0.0001952409458246034,
      "loss": 1.3919,
      "step": 165
    },
    {
      "epoch": 0.2234185733512786,
      "grad_norm": 0.48999884724617004,
      "learning_rate": 0.0001952110146662676,
      "loss": 1.8097,
      "step": 166
    },
    {
      "epoch": 0.22476446837146702,
      "grad_norm": 0.395100474357605,
      "learning_rate": 0.00019518108350793177,
      "loss": 1.3257,
      "step": 167
    },
    {
      "epoch": 0.22611036339165544,
      "grad_norm": 0.44834989309310913,
      "learning_rate": 0.00019515115234959593,
      "loss": 1.5627,
      "step": 168
    },
    {
      "epoch": 0.2274562584118439,
      "grad_norm": 0.44601038098335266,
      "learning_rate": 0.0001951212211912601,
      "loss": 1.4136,
      "step": 169
    },
    {
      "epoch": 0.2288021534320323,
      "grad_norm": 0.5723384022712708,
      "learning_rate": 0.0001950912900329243,
      "loss": 1.6715,
      "step": 170
    },
    {
      "epoch": 0.23014804845222073,
      "grad_norm": 0.3367037773132324,
      "learning_rate": 0.00019506135887458845,
      "loss": 1.3464,
      "step": 171
    },
    {
      "epoch": 0.23149394347240915,
      "grad_norm": 0.4698930084705353,
      "learning_rate": 0.00019503142771625263,
      "loss": 1.5496,
      "step": 172
    },
    {
      "epoch": 0.23283983849259757,
      "grad_norm": 0.40855029225349426,
      "learning_rate": 0.00019500149655791679,
      "loss": 1.608,
      "step": 173
    },
    {
      "epoch": 0.23418573351278602,
      "grad_norm": 0.409900963306427,
      "learning_rate": 0.00019497156539958097,
      "loss": 1.4244,
      "step": 174
    },
    {
      "epoch": 0.23553162853297444,
      "grad_norm": 0.4560488760471344,
      "learning_rate": 0.00019494163424124515,
      "loss": 1.6332,
      "step": 175
    },
    {
      "epoch": 0.23687752355316286,
      "grad_norm": 0.4858758747577667,
      "learning_rate": 0.0001949117030829093,
      "loss": 1.2847,
      "step": 176
    },
    {
      "epoch": 0.23822341857335128,
      "grad_norm": 0.45006194710731506,
      "learning_rate": 0.0001948817719245735,
      "loss": 1.3861,
      "step": 177
    },
    {
      "epoch": 0.2395693135935397,
      "grad_norm": 0.4624711275100708,
      "learning_rate": 0.00019485184076623765,
      "loss": 1.6031,
      "step": 178
    },
    {
      "epoch": 0.24091520861372812,
      "grad_norm": 0.61151123046875,
      "learning_rate": 0.00019482190960790183,
      "loss": 1.4792,
      "step": 179
    },
    {
      "epoch": 0.24226110363391656,
      "grad_norm": 0.4420463442802429,
      "learning_rate": 0.000194791978449566,
      "loss": 1.4517,
      "step": 180
    },
    {
      "epoch": 0.24360699865410498,
      "grad_norm": 0.4012461006641388,
      "learning_rate": 0.00019476204729123017,
      "loss": 1.5797,
      "step": 181
    },
    {
      "epoch": 0.2449528936742934,
      "grad_norm": 0.5816612243652344,
      "learning_rate": 0.00019473211613289435,
      "loss": 1.4705,
      "step": 182
    },
    {
      "epoch": 0.24629878869448182,
      "grad_norm": 0.5111032724380493,
      "learning_rate": 0.00019470218497455853,
      "loss": 1.4773,
      "step": 183
    },
    {
      "epoch": 0.24764468371467024,
      "grad_norm": 0.4303726851940155,
      "learning_rate": 0.0001946722538162227,
      "loss": 1.4886,
      "step": 184
    },
    {
      "epoch": 0.2489905787348587,
      "grad_norm": 0.46440669894218445,
      "learning_rate": 0.00019464232265788687,
      "loss": 1.7709,
      "step": 185
    },
    {
      "epoch": 0.2503364737550471,
      "grad_norm": 0.45738485455513,
      "learning_rate": 0.00019461239149955103,
      "loss": 1.3864,
      "step": 186
    },
    {
      "epoch": 0.25168236877523553,
      "grad_norm": 0.4825378358364105,
      "learning_rate": 0.0001945824603412152,
      "loss": 1.2567,
      "step": 187
    },
    {
      "epoch": 0.253028263795424,
      "grad_norm": 0.48585134744644165,
      "learning_rate": 0.0001945525291828794,
      "loss": 1.5431,
      "step": 188
    },
    {
      "epoch": 0.2543741588156124,
      "grad_norm": 0.5264890789985657,
      "learning_rate": 0.00019452259802454355,
      "loss": 1.6415,
      "step": 189
    },
    {
      "epoch": 0.2557200538358008,
      "grad_norm": 0.40344446897506714,
      "learning_rate": 0.00019449266686620773,
      "loss": 1.9884,
      "step": 190
    },
    {
      "epoch": 0.2570659488559892,
      "grad_norm": 0.5275837779045105,
      "learning_rate": 0.0001944627357078719,
      "loss": 1.6903,
      "step": 191
    },
    {
      "epoch": 0.25841184387617766,
      "grad_norm": 0.4708457589149475,
      "learning_rate": 0.00019443280454953607,
      "loss": 1.4659,
      "step": 192
    },
    {
      "epoch": 0.2597577388963661,
      "grad_norm": 0.529093861579895,
      "learning_rate": 0.00019440287339120025,
      "loss": 1.3872,
      "step": 193
    },
    {
      "epoch": 0.2611036339165545,
      "grad_norm": 0.5148000717163086,
      "learning_rate": 0.0001943729422328644,
      "loss": 1.7431,
      "step": 194
    },
    {
      "epoch": 0.26244952893674295,
      "grad_norm": 0.44865983724594116,
      "learning_rate": 0.0001943430110745286,
      "loss": 1.4997,
      "step": 195
    },
    {
      "epoch": 0.26379542395693134,
      "grad_norm": 0.4515587389469147,
      "learning_rate": 0.00019431307991619277,
      "loss": 1.7963,
      "step": 196
    },
    {
      "epoch": 0.2651413189771198,
      "grad_norm": 0.4285007119178772,
      "learning_rate": 0.00019428314875785693,
      "loss": 1.531,
      "step": 197
    },
    {
      "epoch": 0.26648721399730824,
      "grad_norm": 0.4564701020717621,
      "learning_rate": 0.0001942532175995211,
      "loss": 1.7458,
      "step": 198
    },
    {
      "epoch": 0.26783310901749663,
      "grad_norm": 0.4382796585559845,
      "learning_rate": 0.0001942232864411853,
      "loss": 1.5325,
      "step": 199
    },
    {
      "epoch": 0.2691790040376851,
      "grad_norm": 0.4909805655479431,
      "learning_rate": 0.00019419335528284945,
      "loss": 1.6802,
      "step": 200
    },
    {
      "epoch": 0.27052489905787347,
      "grad_norm": 0.40810564160346985,
      "learning_rate": 0.00019416342412451363,
      "loss": 1.4562,
      "step": 201
    },
    {
      "epoch": 0.2718707940780619,
      "grad_norm": 0.39414459466934204,
      "learning_rate": 0.00019413349296617779,
      "loss": 1.4607,
      "step": 202
    },
    {
      "epoch": 0.2732166890982503,
      "grad_norm": 0.4595535099506378,
      "learning_rate": 0.00019410356180784197,
      "loss": 1.5517,
      "step": 203
    },
    {
      "epoch": 0.27456258411843876,
      "grad_norm": 0.46467891335487366,
      "learning_rate": 0.00019407363064950615,
      "loss": 1.657,
      "step": 204
    },
    {
      "epoch": 0.2759084791386272,
      "grad_norm": 0.5982556939125061,
      "learning_rate": 0.0001940436994911703,
      "loss": 1.5234,
      "step": 205
    },
    {
      "epoch": 0.2772543741588156,
      "grad_norm": 0.4622337520122528,
      "learning_rate": 0.0001940137683328345,
      "loss": 1.4028,
      "step": 206
    },
    {
      "epoch": 0.27860026917900405,
      "grad_norm": 0.4506739377975464,
      "learning_rate": 0.00019398383717449867,
      "loss": 1.6264,
      "step": 207
    },
    {
      "epoch": 0.27994616419919244,
      "grad_norm": 0.39559632539749146,
      "learning_rate": 0.00019395390601616283,
      "loss": 1.4371,
      "step": 208
    },
    {
      "epoch": 0.2812920592193809,
      "grad_norm": 0.40988773107528687,
      "learning_rate": 0.000193923974857827,
      "loss": 1.4975,
      "step": 209
    },
    {
      "epoch": 0.28263795423956933,
      "grad_norm": 0.49815863370895386,
      "learning_rate": 0.00019389404369949117,
      "loss": 1.5761,
      "step": 210
    },
    {
      "epoch": 0.2839838492597577,
      "grad_norm": 0.5598440766334534,
      "learning_rate": 0.00019386411254115535,
      "loss": 1.5976,
      "step": 211
    },
    {
      "epoch": 0.2853297442799462,
      "grad_norm": 0.5860545039176941,
      "learning_rate": 0.00019383418138281953,
      "loss": 1.3096,
      "step": 212
    },
    {
      "epoch": 0.28667563930013457,
      "grad_norm": 0.4950152039527893,
      "learning_rate": 0.0001938042502244837,
      "loss": 1.586,
      "step": 213
    },
    {
      "epoch": 0.288021534320323,
      "grad_norm": 0.42872354388237,
      "learning_rate": 0.00019377431906614787,
      "loss": 1.2565,
      "step": 214
    },
    {
      "epoch": 0.28936742934051146,
      "grad_norm": 0.5131812691688538,
      "learning_rate": 0.00019374438790781205,
      "loss": 1.6742,
      "step": 215
    },
    {
      "epoch": 0.29071332436069985,
      "grad_norm": 0.4640704095363617,
      "learning_rate": 0.0001937144567494762,
      "loss": 2.0577,
      "step": 216
    },
    {
      "epoch": 0.2920592193808883,
      "grad_norm": 0.3785565197467804,
      "learning_rate": 0.0001936845255911404,
      "loss": 1.3017,
      "step": 217
    },
    {
      "epoch": 0.2934051144010767,
      "grad_norm": 0.40346527099609375,
      "learning_rate": 0.00019365459443280455,
      "loss": 1.5277,
      "step": 218
    },
    {
      "epoch": 0.29475100942126514,
      "grad_norm": 0.5495027899742126,
      "learning_rate": 0.00019362466327446873,
      "loss": 1.767,
      "step": 219
    },
    {
      "epoch": 0.2960969044414536,
      "grad_norm": 0.4634105861186981,
      "learning_rate": 0.0001935947321161329,
      "loss": 1.5622,
      "step": 220
    },
    {
      "epoch": 0.297442799461642,
      "grad_norm": 0.415061354637146,
      "learning_rate": 0.00019356480095779707,
      "loss": 1.8126,
      "step": 221
    },
    {
      "epoch": 0.29878869448183043,
      "grad_norm": 0.47698768973350525,
      "learning_rate": 0.00019353486979946125,
      "loss": 1.5785,
      "step": 222
    },
    {
      "epoch": 0.3001345895020188,
      "grad_norm": 0.4094924330711365,
      "learning_rate": 0.00019350493864112543,
      "loss": 1.3139,
      "step": 223
    },
    {
      "epoch": 0.30148048452220727,
      "grad_norm": 0.46532198786735535,
      "learning_rate": 0.0001934750074827896,
      "loss": 1.2834,
      "step": 224
    },
    {
      "epoch": 0.3028263795423957,
      "grad_norm": 0.5071805119514465,
      "learning_rate": 0.00019344507632445377,
      "loss": 1.1895,
      "step": 225
    },
    {
      "epoch": 0.3041722745625841,
      "grad_norm": 0.46388646960258484,
      "learning_rate": 0.00019341514516611793,
      "loss": 1.5274,
      "step": 226
    },
    {
      "epoch": 0.30551816958277256,
      "grad_norm": 0.603579580783844,
      "learning_rate": 0.0001933852140077821,
      "loss": 1.7224,
      "step": 227
    },
    {
      "epoch": 0.30686406460296095,
      "grad_norm": 0.5098437070846558,
      "learning_rate": 0.0001933552828494463,
      "loss": 1.8132,
      "step": 228
    },
    {
      "epoch": 0.3082099596231494,
      "grad_norm": 0.4898987114429474,
      "learning_rate": 0.00019332535169111045,
      "loss": 1.5669,
      "step": 229
    },
    {
      "epoch": 0.30955585464333785,
      "grad_norm": 0.3612382411956787,
      "learning_rate": 0.00019329542053277463,
      "loss": 1.5383,
      "step": 230
    },
    {
      "epoch": 0.31090174966352624,
      "grad_norm": 0.4676969647407532,
      "learning_rate": 0.0001932654893744388,
      "loss": 1.5321,
      "step": 231
    },
    {
      "epoch": 0.3122476446837147,
      "grad_norm": 0.4371066093444824,
      "learning_rate": 0.00019323555821610297,
      "loss": 1.4185,
      "step": 232
    },
    {
      "epoch": 0.3135935397039031,
      "grad_norm": 0.525248110294342,
      "learning_rate": 0.00019320562705776715,
      "loss": 1.4887,
      "step": 233
    },
    {
      "epoch": 0.3149394347240915,
      "grad_norm": 0.4738536477088928,
      "learning_rate": 0.0001931756958994313,
      "loss": 1.5034,
      "step": 234
    },
    {
      "epoch": 0.3162853297442799,
      "grad_norm": 0.4605509340763092,
      "learning_rate": 0.0001931457647410955,
      "loss": 1.4767,
      "step": 235
    },
    {
      "epoch": 0.31763122476446837,
      "grad_norm": 0.4862232208251953,
      "learning_rate": 0.00019311583358275967,
      "loss": 1.4035,
      "step": 236
    },
    {
      "epoch": 0.3189771197846568,
      "grad_norm": 0.48931124806404114,
      "learning_rate": 0.00019308590242442383,
      "loss": 1.3353,
      "step": 237
    },
    {
      "epoch": 0.3203230148048452,
      "grad_norm": 0.4586960971355438,
      "learning_rate": 0.000193055971266088,
      "loss": 1.5437,
      "step": 238
    },
    {
      "epoch": 0.32166890982503366,
      "grad_norm": 0.6008138656616211,
      "learning_rate": 0.0001930260401077522,
      "loss": 1.409,
      "step": 239
    },
    {
      "epoch": 0.32301480484522205,
      "grad_norm": 0.44041377305984497,
      "learning_rate": 0.00019299610894941635,
      "loss": 1.3149,
      "step": 240
    },
    {
      "epoch": 0.3243606998654105,
      "grad_norm": 0.5644239783287048,
      "learning_rate": 0.00019296617779108053,
      "loss": 1.8258,
      "step": 241
    },
    {
      "epoch": 0.32570659488559894,
      "grad_norm": 0.49344688653945923,
      "learning_rate": 0.0001929362466327447,
      "loss": 1.4467,
      "step": 242
    },
    {
      "epoch": 0.32705248990578734,
      "grad_norm": 0.4639555811882019,
      "learning_rate": 0.00019290631547440887,
      "loss": 1.7277,
      "step": 243
    },
    {
      "epoch": 0.3283983849259758,
      "grad_norm": 0.4451589584350586,
      "learning_rate": 0.00019287638431607305,
      "loss": 1.7307,
      "step": 244
    },
    {
      "epoch": 0.3297442799461642,
      "grad_norm": 0.5167214870452881,
      "learning_rate": 0.0001928464531577372,
      "loss": 1.3982,
      "step": 245
    },
    {
      "epoch": 0.3310901749663526,
      "grad_norm": 0.5041912198066711,
      "learning_rate": 0.0001928165219994014,
      "loss": 1.187,
      "step": 246
    },
    {
      "epoch": 0.33243606998654107,
      "grad_norm": 0.47471916675567627,
      "learning_rate": 0.00019278659084106557,
      "loss": 1.3003,
      "step": 247
    },
    {
      "epoch": 0.33378196500672946,
      "grad_norm": 0.5360086560249329,
      "learning_rate": 0.00019275665968272973,
      "loss": 1.6752,
      "step": 248
    },
    {
      "epoch": 0.3351278600269179,
      "grad_norm": 0.4514797329902649,
      "learning_rate": 0.0001927267285243939,
      "loss": 1.2182,
      "step": 249
    },
    {
      "epoch": 0.3364737550471063,
      "grad_norm": 0.4504340589046478,
      "learning_rate": 0.00019269679736605807,
      "loss": 1.5946,
      "step": 250
    },
    {
      "epoch": 0.33781965006729475,
      "grad_norm": 0.3989861309528351,
      "learning_rate": 0.00019266686620772225,
      "loss": 1.3433,
      "step": 251
    },
    {
      "epoch": 0.3391655450874832,
      "grad_norm": 0.5113117098808289,
      "learning_rate": 0.00019263693504938643,
      "loss": 1.4652,
      "step": 252
    },
    {
      "epoch": 0.3405114401076716,
      "grad_norm": 0.5567252039909363,
      "learning_rate": 0.0001926070038910506,
      "loss": 1.5656,
      "step": 253
    },
    {
      "epoch": 0.34185733512786004,
      "grad_norm": 0.3591674268245697,
      "learning_rate": 0.00019257707273271477,
      "loss": 1.3397,
      "step": 254
    },
    {
      "epoch": 0.34320323014804843,
      "grad_norm": 0.43158358335494995,
      "learning_rate": 0.00019254714157437895,
      "loss": 1.6104,
      "step": 255
    },
    {
      "epoch": 0.3445491251682369,
      "grad_norm": 0.4279532730579376,
      "learning_rate": 0.0001925172104160431,
      "loss": 1.6735,
      "step": 256
    },
    {
      "epoch": 0.34589502018842533,
      "grad_norm": 0.4369085729122162,
      "learning_rate": 0.0001924872792577073,
      "loss": 1.2056,
      "step": 257
    },
    {
      "epoch": 0.3472409152086137,
      "grad_norm": 0.5199142694473267,
      "learning_rate": 0.00019245734809937145,
      "loss": 1.5555,
      "step": 258
    },
    {
      "epoch": 0.34858681022880217,
      "grad_norm": 0.4664379060268402,
      "learning_rate": 0.00019242741694103563,
      "loss": 1.6156,
      "step": 259
    },
    {
      "epoch": 0.34993270524899056,
      "grad_norm": 0.3627687394618988,
      "learning_rate": 0.00019239748578269981,
      "loss": 1.5936,
      "step": 260
    },
    {
      "epoch": 0.351278600269179,
      "grad_norm": 0.4269024729728699,
      "learning_rate": 0.00019236755462436397,
      "loss": 1.6541,
      "step": 261
    },
    {
      "epoch": 0.35262449528936746,
      "grad_norm": 0.40662312507629395,
      "learning_rate": 0.00019233762346602815,
      "loss": 1.3711,
      "step": 262
    },
    {
      "epoch": 0.35397039030955585,
      "grad_norm": 0.5161728262901306,
      "learning_rate": 0.00019230769230769233,
      "loss": 1.2438,
      "step": 263
    },
    {
      "epoch": 0.3553162853297443,
      "grad_norm": 0.5530118942260742,
      "learning_rate": 0.0001922777611493565,
      "loss": 1.4421,
      "step": 264
    },
    {
      "epoch": 0.3566621803499327,
      "grad_norm": 0.44823747873306274,
      "learning_rate": 0.00019224782999102067,
      "loss": 1.4004,
      "step": 265
    },
    {
      "epoch": 0.35800807537012114,
      "grad_norm": 0.4432414472103119,
      "learning_rate": 0.00019221789883268483,
      "loss": 1.4443,
      "step": 266
    },
    {
      "epoch": 0.35935397039030953,
      "grad_norm": 0.3331683278083801,
      "learning_rate": 0.000192187967674349,
      "loss": 1.0359,
      "step": 267
    },
    {
      "epoch": 0.360699865410498,
      "grad_norm": 0.41179612278938293,
      "learning_rate": 0.0001921580365160132,
      "loss": 1.5635,
      "step": 268
    },
    {
      "epoch": 0.3620457604306864,
      "grad_norm": 0.4034177362918854,
      "learning_rate": 0.00019212810535767735,
      "loss": 1.2808,
      "step": 269
    },
    {
      "epoch": 0.3633916554508748,
      "grad_norm": 0.4952380955219269,
      "learning_rate": 0.00019209817419934153,
      "loss": 1.8371,
      "step": 270
    },
    {
      "epoch": 0.36473755047106327,
      "grad_norm": 0.4314601719379425,
      "learning_rate": 0.00019206824304100571,
      "loss": 1.2504,
      "step": 271
    },
    {
      "epoch": 0.36608344549125166,
      "grad_norm": 0.4860560894012451,
      "learning_rate": 0.00019203831188266987,
      "loss": 1.3789,
      "step": 272
    },
    {
      "epoch": 0.3674293405114401,
      "grad_norm": 0.5817725658416748,
      "learning_rate": 0.00019200838072433405,
      "loss": 1.3809,
      "step": 273
    },
    {
      "epoch": 0.36877523553162855,
      "grad_norm": 0.5159561038017273,
      "learning_rate": 0.0001919784495659982,
      "loss": 1.4749,
      "step": 274
    },
    {
      "epoch": 0.37012113055181695,
      "grad_norm": 0.4596201181411743,
      "learning_rate": 0.0001919485184076624,
      "loss": 1.3291,
      "step": 275
    },
    {
      "epoch": 0.3714670255720054,
      "grad_norm": 0.5096549391746521,
      "learning_rate": 0.00019191858724932657,
      "loss": 1.3754,
      "step": 276
    },
    {
      "epoch": 0.3728129205921938,
      "grad_norm": 0.4547315537929535,
      "learning_rate": 0.00019188865609099073,
      "loss": 1.44,
      "step": 277
    },
    {
      "epoch": 0.37415881561238223,
      "grad_norm": 0.3869415521621704,
      "learning_rate": 0.0001918587249326549,
      "loss": 1.4467,
      "step": 278
    },
    {
      "epoch": 0.3755047106325707,
      "grad_norm": 0.4892759919166565,
      "learning_rate": 0.0001918287937743191,
      "loss": 1.485,
      "step": 279
    },
    {
      "epoch": 0.3768506056527591,
      "grad_norm": 0.4046100974082947,
      "learning_rate": 0.00019179886261598325,
      "loss": 1.6716,
      "step": 280
    },
    {
      "epoch": 0.3781965006729475,
      "grad_norm": 0.36155039072036743,
      "learning_rate": 0.00019176893145764743,
      "loss": 1.4517,
      "step": 281
    },
    {
      "epoch": 0.3795423956931359,
      "grad_norm": 0.4726179242134094,
      "learning_rate": 0.0001917390002993116,
      "loss": 1.2486,
      "step": 282
    },
    {
      "epoch": 0.38088829071332436,
      "grad_norm": 0.5265393257141113,
      "learning_rate": 0.00019170906914097577,
      "loss": 1.5279,
      "step": 283
    },
    {
      "epoch": 0.3822341857335128,
      "grad_norm": 0.47786521911621094,
      "learning_rate": 0.00019167913798263995,
      "loss": 1.4226,
      "step": 284
    },
    {
      "epoch": 0.3835800807537012,
      "grad_norm": 0.4485851526260376,
      "learning_rate": 0.0001916492068243041,
      "loss": 1.4529,
      "step": 285
    },
    {
      "epoch": 0.38492597577388965,
      "grad_norm": 0.4146694242954254,
      "learning_rate": 0.0001916192756659683,
      "loss": 1.6188,
      "step": 286
    },
    {
      "epoch": 0.38627187079407804,
      "grad_norm": 0.4156564772129059,
      "learning_rate": 0.00019158934450763248,
      "loss": 1.839,
      "step": 287
    },
    {
      "epoch": 0.3876177658142665,
      "grad_norm": 0.41906285285949707,
      "learning_rate": 0.00019155941334929663,
      "loss": 1.7008,
      "step": 288
    },
    {
      "epoch": 0.38896366083445494,
      "grad_norm": 0.5344641804695129,
      "learning_rate": 0.00019152948219096081,
      "loss": 1.3029,
      "step": 289
    },
    {
      "epoch": 0.39030955585464333,
      "grad_norm": 0.4238939583301544,
      "learning_rate": 0.00019149955103262497,
      "loss": 1.5781,
      "step": 290
    },
    {
      "epoch": 0.3916554508748318,
      "grad_norm": 0.5728873610496521,
      "learning_rate": 0.00019146961987428915,
      "loss": 1.9345,
      "step": 291
    },
    {
      "epoch": 0.39300134589502017,
      "grad_norm": 0.4156350791454315,
      "learning_rate": 0.00019143968871595333,
      "loss": 1.7064,
      "step": 292
    },
    {
      "epoch": 0.3943472409152086,
      "grad_norm": 0.37366220355033875,
      "learning_rate": 0.0001914097575576175,
      "loss": 1.352,
      "step": 293
    },
    {
      "epoch": 0.39569313593539707,
      "grad_norm": 0.43273958563804626,
      "learning_rate": 0.00019137982639928167,
      "loss": 1.5656,
      "step": 294
    },
    {
      "epoch": 0.39703903095558546,
      "grad_norm": 0.4020211696624756,
      "learning_rate": 0.00019134989524094586,
      "loss": 1.515,
      "step": 295
    },
    {
      "epoch": 0.3983849259757739,
      "grad_norm": 0.5462031960487366,
      "learning_rate": 0.00019131996408261,
      "loss": 1.3261,
      "step": 296
    },
    {
      "epoch": 0.3997308209959623,
      "grad_norm": 0.515134334564209,
      "learning_rate": 0.0001912900329242742,
      "loss": 1.9432,
      "step": 297
    },
    {
      "epoch": 0.40107671601615075,
      "grad_norm": 0.39480170607566833,
      "learning_rate": 0.00019126010176593835,
      "loss": 1.4163,
      "step": 298
    },
    {
      "epoch": 0.40242261103633914,
      "grad_norm": 0.4476102888584137,
      "learning_rate": 0.00019123017060760253,
      "loss": 1.3822,
      "step": 299
    },
    {
      "epoch": 0.4037685060565276,
      "grad_norm": 0.4717882573604584,
      "learning_rate": 0.00019120023944926672,
      "loss": 1.6648,
      "step": 300
    },
    {
      "epoch": 0.40511440107671604,
      "grad_norm": 0.4740871489048004,
      "learning_rate": 0.00019117030829093087,
      "loss": 1.4691,
      "step": 301
    },
    {
      "epoch": 0.4064602960969044,
      "grad_norm": 0.44294074177742004,
      "learning_rate": 0.00019114037713259505,
      "loss": 1.2771,
      "step": 302
    },
    {
      "epoch": 0.4078061911170929,
      "grad_norm": 0.42846184968948364,
      "learning_rate": 0.0001911104459742592,
      "loss": 1.5132,
      "step": 303
    },
    {
      "epoch": 0.40915208613728127,
      "grad_norm": 0.4643179476261139,
      "learning_rate": 0.0001910805148159234,
      "loss": 1.5857,
      "step": 304
    },
    {
      "epoch": 0.4104979811574697,
      "grad_norm": 0.44565892219543457,
      "learning_rate": 0.00019105058365758757,
      "loss": 1.5857,
      "step": 305
    },
    {
      "epoch": 0.41184387617765816,
      "grad_norm": 0.6671677827835083,
      "learning_rate": 0.00019102065249925173,
      "loss": 1.5331,
      "step": 306
    },
    {
      "epoch": 0.41318977119784656,
      "grad_norm": 0.589820146560669,
      "learning_rate": 0.0001909907213409159,
      "loss": 2.024,
      "step": 307
    },
    {
      "epoch": 0.414535666218035,
      "grad_norm": 0.49123451113700867,
      "learning_rate": 0.0001909607901825801,
      "loss": 1.2333,
      "step": 308
    },
    {
      "epoch": 0.4158815612382234,
      "grad_norm": 0.4705427885055542,
      "learning_rate": 0.00019093085902424425,
      "loss": 1.4159,
      "step": 309
    },
    {
      "epoch": 0.41722745625841184,
      "grad_norm": 0.5186806917190552,
      "learning_rate": 0.00019090092786590843,
      "loss": 1.5009,
      "step": 310
    },
    {
      "epoch": 0.4185733512786003,
      "grad_norm": 0.5350711941719055,
      "learning_rate": 0.0001908709967075726,
      "loss": 1.4274,
      "step": 311
    },
    {
      "epoch": 0.4199192462987887,
      "grad_norm": 0.4203391671180725,
      "learning_rate": 0.00019084106554923677,
      "loss": 1.1364,
      "step": 312
    },
    {
      "epoch": 0.42126514131897713,
      "grad_norm": 0.4461205005645752,
      "learning_rate": 0.00019081113439090095,
      "loss": 1.5774,
      "step": 313
    },
    {
      "epoch": 0.4226110363391655,
      "grad_norm": 0.47680097818374634,
      "learning_rate": 0.0001907812032325651,
      "loss": 1.3723,
      "step": 314
    },
    {
      "epoch": 0.423956931359354,
      "grad_norm": 0.433506041765213,
      "learning_rate": 0.0001907512720742293,
      "loss": 1.4784,
      "step": 315
    },
    {
      "epoch": 0.4253028263795424,
      "grad_norm": 0.4565995931625366,
      "learning_rate": 0.00019072134091589348,
      "loss": 1.7089,
      "step": 316
    },
    {
      "epoch": 0.4266487213997308,
      "grad_norm": 0.46399492025375366,
      "learning_rate": 0.00019069140975755763,
      "loss": 1.4205,
      "step": 317
    },
    {
      "epoch": 0.42799461641991926,
      "grad_norm": 0.5128790736198425,
      "learning_rate": 0.00019066147859922181,
      "loss": 1.2894,
      "step": 318
    },
    {
      "epoch": 0.42934051144010765,
      "grad_norm": 0.5110344886779785,
      "learning_rate": 0.00019063154744088597,
      "loss": 1.2026,
      "step": 319
    },
    {
      "epoch": 0.4306864064602961,
      "grad_norm": 0.49751022458076477,
      "learning_rate": 0.00019060161628255013,
      "loss": 1.3157,
      "step": 320
    },
    {
      "epoch": 0.43203230148048455,
      "grad_norm": 0.5076984763145447,
      "learning_rate": 0.0001905716851242143,
      "loss": 1.1847,
      "step": 321
    },
    {
      "epoch": 0.43337819650067294,
      "grad_norm": 0.4318643808364868,
      "learning_rate": 0.0001905417539658785,
      "loss": 1.4002,
      "step": 322
    },
    {
      "epoch": 0.4347240915208614,
      "grad_norm": 0.47191497683525085,
      "learning_rate": 0.00019051182280754265,
      "loss": 1.2827,
      "step": 323
    },
    {
      "epoch": 0.4360699865410498,
      "grad_norm": 0.4109831750392914,
      "learning_rate": 0.00019048189164920683,
      "loss": 1.369,
      "step": 324
    },
    {
      "epoch": 0.43741588156123823,
      "grad_norm": 0.4882808029651642,
      "learning_rate": 0.00019045196049087098,
      "loss": 1.3083,
      "step": 325
    },
    {
      "epoch": 0.4387617765814266,
      "grad_norm": 0.5028949975967407,
      "learning_rate": 0.00019042202933253517,
      "loss": 1.5989,
      "step": 326
    },
    {
      "epoch": 0.44010767160161507,
      "grad_norm": 0.43528807163238525,
      "learning_rate": 0.00019039209817419935,
      "loss": 1.386,
      "step": 327
    },
    {
      "epoch": 0.4414535666218035,
      "grad_norm": 0.5021744966506958,
      "learning_rate": 0.0001903621670158635,
      "loss": 1.4179,
      "step": 328
    },
    {
      "epoch": 0.4427994616419919,
      "grad_norm": 0.4227435290813446,
      "learning_rate": 0.0001903322358575277,
      "loss": 1.4964,
      "step": 329
    },
    {
      "epoch": 0.44414535666218036,
      "grad_norm": 0.4157199263572693,
      "learning_rate": 0.00019030230469919187,
      "loss": 1.6062,
      "step": 330
    },
    {
      "epoch": 0.44549125168236875,
      "grad_norm": 0.37977614998817444,
      "learning_rate": 0.00019027237354085603,
      "loss": 1.6347,
      "step": 331
    },
    {
      "epoch": 0.4468371467025572,
      "grad_norm": 0.4536263644695282,
      "learning_rate": 0.0001902424423825202,
      "loss": 1.469,
      "step": 332
    },
    {
      "epoch": 0.44818304172274565,
      "grad_norm": 0.47187161445617676,
      "learning_rate": 0.00019021251122418436,
      "loss": 1.3811,
      "step": 333
    },
    {
      "epoch": 0.44952893674293404,
      "grad_norm": 0.4893442988395691,
      "learning_rate": 0.00019018258006584855,
      "loss": 1.3406,
      "step": 334
    },
    {
      "epoch": 0.4508748317631225,
      "grad_norm": 0.4538818299770355,
      "learning_rate": 0.00019015264890751273,
      "loss": 1.6064,
      "step": 335
    },
    {
      "epoch": 0.4522207267833109,
      "grad_norm": 0.4250541925430298,
      "learning_rate": 0.00019012271774917689,
      "loss": 1.6945,
      "step": 336
    },
    {
      "epoch": 0.4535666218034993,
      "grad_norm": 0.39315521717071533,
      "learning_rate": 0.00019009278659084107,
      "loss": 1.5046,
      "step": 337
    },
    {
      "epoch": 0.4549125168236878,
      "grad_norm": 0.3926957845687866,
      "learning_rate": 0.00019006285543250525,
      "loss": 1.4258,
      "step": 338
    },
    {
      "epoch": 0.45625841184387617,
      "grad_norm": 0.44945192337036133,
      "learning_rate": 0.0001900329242741694,
      "loss": 1.388,
      "step": 339
    },
    {
      "epoch": 0.4576043068640646,
      "grad_norm": 0.38810694217681885,
      "learning_rate": 0.0001900029931158336,
      "loss": 1.3374,
      "step": 340
    },
    {
      "epoch": 0.458950201884253,
      "grad_norm": 0.478705495595932,
      "learning_rate": 0.00018997306195749775,
      "loss": 1.4867,
      "step": 341
    },
    {
      "epoch": 0.46029609690444145,
      "grad_norm": 0.4666513204574585,
      "learning_rate": 0.00018994313079916193,
      "loss": 1.5493,
      "step": 342
    },
    {
      "epoch": 0.4616419919246299,
      "grad_norm": 0.461978018283844,
      "learning_rate": 0.0001899131996408261,
      "loss": 1.4068,
      "step": 343
    },
    {
      "epoch": 0.4629878869448183,
      "grad_norm": 0.5089786648750305,
      "learning_rate": 0.00018988326848249027,
      "loss": 1.5802,
      "step": 344
    },
    {
      "epoch": 0.46433378196500674,
      "grad_norm": 0.5878669619560242,
      "learning_rate": 0.00018985333732415445,
      "loss": 1.5218,
      "step": 345
    },
    {
      "epoch": 0.46567967698519513,
      "grad_norm": 0.5297355055809021,
      "learning_rate": 0.00018982340616581863,
      "loss": 1.4596,
      "step": 346
    },
    {
      "epoch": 0.4670255720053836,
      "grad_norm": 0.37427854537963867,
      "learning_rate": 0.0001897934750074828,
      "loss": 1.6585,
      "step": 347
    },
    {
      "epoch": 0.46837146702557203,
      "grad_norm": 0.4785940945148468,
      "learning_rate": 0.00018976354384914697,
      "loss": 1.4216,
      "step": 348
    },
    {
      "epoch": 0.4697173620457604,
      "grad_norm": 0.45127949118614197,
      "learning_rate": 0.00018973361269081113,
      "loss": 1.3742,
      "step": 349
    },
    {
      "epoch": 0.47106325706594887,
      "grad_norm": 0.3956683576107025,
      "learning_rate": 0.0001897036815324753,
      "loss": 1.4886,
      "step": 350
    },
    {
      "epoch": 0.47240915208613726,
      "grad_norm": 0.506726086139679,
      "learning_rate": 0.0001896737503741395,
      "loss": 1.7209,
      "step": 351
    },
    {
      "epoch": 0.4737550471063257,
      "grad_norm": 0.4281995892524719,
      "learning_rate": 0.00018964381921580365,
      "loss": 1.6501,
      "step": 352
    },
    {
      "epoch": 0.47510094212651416,
      "grad_norm": 0.4176883399486542,
      "learning_rate": 0.00018961388805746783,
      "loss": 1.6089,
      "step": 353
    },
    {
      "epoch": 0.47644683714670255,
      "grad_norm": 0.46936652064323425,
      "learning_rate": 0.00018958395689913198,
      "loss": 1.8603,
      "step": 354
    },
    {
      "epoch": 0.477792732166891,
      "grad_norm": 0.39557790756225586,
      "learning_rate": 0.00018955402574079617,
      "loss": 1.5588,
      "step": 355
    },
    {
      "epoch": 0.4791386271870794,
      "grad_norm": 0.4626084268093109,
      "learning_rate": 0.00018952409458246035,
      "loss": 1.4078,
      "step": 356
    },
    {
      "epoch": 0.48048452220726784,
      "grad_norm": 0.39597803354263306,
      "learning_rate": 0.0001894941634241245,
      "loss": 1.6255,
      "step": 357
    },
    {
      "epoch": 0.48183041722745623,
      "grad_norm": 0.3589905798435211,
      "learning_rate": 0.0001894642322657887,
      "loss": 1.5743,
      "step": 358
    },
    {
      "epoch": 0.4831763122476447,
      "grad_norm": 0.4154353141784668,
      "learning_rate": 0.00018943430110745287,
      "loss": 1.6271,
      "step": 359
    },
    {
      "epoch": 0.4845222072678331,
      "grad_norm": 0.5457944273948669,
      "learning_rate": 0.00018940436994911703,
      "loss": 1.3475,
      "step": 360
    },
    {
      "epoch": 0.4858681022880215,
      "grad_norm": 0.4591892957687378,
      "learning_rate": 0.0001893744387907812,
      "loss": 1.545,
      "step": 361
    },
    {
      "epoch": 0.48721399730820997,
      "grad_norm": 0.5087862014770508,
      "learning_rate": 0.00018934450763244536,
      "loss": 1.2291,
      "step": 362
    },
    {
      "epoch": 0.48855989232839836,
      "grad_norm": 0.5403264164924622,
      "learning_rate": 0.00018931457647410955,
      "loss": 1.7865,
      "step": 363
    },
    {
      "epoch": 0.4899057873485868,
      "grad_norm": 0.4715443551540375,
      "learning_rate": 0.00018928464531577373,
      "loss": 1.1229,
      "step": 364
    },
    {
      "epoch": 0.49125168236877526,
      "grad_norm": 0.5009695291519165,
      "learning_rate": 0.00018925471415743789,
      "loss": 1.3018,
      "step": 365
    },
    {
      "epoch": 0.49259757738896365,
      "grad_norm": 0.6009246110916138,
      "learning_rate": 0.00018922478299910207,
      "loss": 1.472,
      "step": 366
    },
    {
      "epoch": 0.4939434724091521,
      "grad_norm": 0.5123151540756226,
      "learning_rate": 0.00018919485184076625,
      "loss": 1.4615,
      "step": 367
    },
    {
      "epoch": 0.4952893674293405,
      "grad_norm": 0.3925535976886749,
      "learning_rate": 0.0001891649206824304,
      "loss": 1.3077,
      "step": 368
    },
    {
      "epoch": 0.49663526244952894,
      "grad_norm": 0.5631649494171143,
      "learning_rate": 0.0001891349895240946,
      "loss": 1.5031,
      "step": 369
    },
    {
      "epoch": 0.4979811574697174,
      "grad_norm": 0.41553795337677,
      "learning_rate": 0.00018910505836575875,
      "loss": 1.4474,
      "step": 370
    },
    {
      "epoch": 0.4993270524899058,
      "grad_norm": 0.6132914423942566,
      "learning_rate": 0.00018907512720742293,
      "loss": 1.1435,
      "step": 371
    },
    {
      "epoch": 0.5006729475100942,
      "grad_norm": 0.5028371810913086,
      "learning_rate": 0.0001890451960490871,
      "loss": 1.4325,
      "step": 372
    },
    {
      "epoch": 0.5020188425302826,
      "grad_norm": 0.47313401103019714,
      "learning_rate": 0.00018901526489075127,
      "loss": 1.708,
      "step": 373
    },
    {
      "epoch": 0.5033647375504711,
      "grad_norm": 0.5329447388648987,
      "learning_rate": 0.00018898533373241545,
      "loss": 1.4652,
      "step": 374
    },
    {
      "epoch": 0.5047106325706595,
      "grad_norm": 0.4438988268375397,
      "learning_rate": 0.00018895540257407963,
      "loss": 1.3016,
      "step": 375
    },
    {
      "epoch": 0.506056527590848,
      "grad_norm": 0.5998989343643188,
      "learning_rate": 0.0001889254714157438,
      "loss": 1.5156,
      "step": 376
    },
    {
      "epoch": 0.5074024226110363,
      "grad_norm": 0.5115354061126709,
      "learning_rate": 0.00018889554025740797,
      "loss": 1.4289,
      "step": 377
    },
    {
      "epoch": 0.5087483176312247,
      "grad_norm": 0.44387564063072205,
      "learning_rate": 0.00018886560909907213,
      "loss": 1.2122,
      "step": 378
    },
    {
      "epoch": 0.5100942126514132,
      "grad_norm": 0.39141178131103516,
      "learning_rate": 0.0001888356779407363,
      "loss": 1.5053,
      "step": 379
    },
    {
      "epoch": 0.5114401076716016,
      "grad_norm": 0.5296815633773804,
      "learning_rate": 0.0001888057467824005,
      "loss": 1.8229,
      "step": 380
    },
    {
      "epoch": 0.5127860026917901,
      "grad_norm": 0.4057481586933136,
      "learning_rate": 0.00018877581562406465,
      "loss": 1.4516,
      "step": 381
    },
    {
      "epoch": 0.5141318977119784,
      "grad_norm": 0.44006067514419556,
      "learning_rate": 0.00018874588446572883,
      "loss": 1.1611,
      "step": 382
    },
    {
      "epoch": 0.5154777927321669,
      "grad_norm": 0.5060757994651794,
      "learning_rate": 0.000188715953307393,
      "loss": 1.4011,
      "step": 383
    },
    {
      "epoch": 0.5168236877523553,
      "grad_norm": 0.40790238976478577,
      "learning_rate": 0.00018868602214905717,
      "loss": 1.6645,
      "step": 384
    },
    {
      "epoch": 0.5181695827725438,
      "grad_norm": 0.4289325773715973,
      "learning_rate": 0.00018865609099072135,
      "loss": 1.8702,
      "step": 385
    },
    {
      "epoch": 0.5195154777927322,
      "grad_norm": 0.4688355326652527,
      "learning_rate": 0.0001886261598323855,
      "loss": 1.5396,
      "step": 386
    },
    {
      "epoch": 0.5208613728129206,
      "grad_norm": 0.47207921743392944,
      "learning_rate": 0.0001885962286740497,
      "loss": 1.5027,
      "step": 387
    },
    {
      "epoch": 0.522207267833109,
      "grad_norm": 0.4133245646953583,
      "learning_rate": 0.00018856629751571387,
      "loss": 1.3277,
      "step": 388
    },
    {
      "epoch": 0.5235531628532974,
      "grad_norm": 0.4344273805618286,
      "learning_rate": 0.00018853636635737803,
      "loss": 1.7768,
      "step": 389
    },
    {
      "epoch": 0.5248990578734859,
      "grad_norm": 0.4587644040584564,
      "learning_rate": 0.0001885064351990422,
      "loss": 1.4416,
      "step": 390
    },
    {
      "epoch": 0.5262449528936743,
      "grad_norm": 0.43503573536872864,
      "learning_rate": 0.0001884765040407064,
      "loss": 1.5742,
      "step": 391
    },
    {
      "epoch": 0.5275908479138627,
      "grad_norm": 0.49237313866615295,
      "learning_rate": 0.00018844657288237055,
      "loss": 1.4206,
      "step": 392
    },
    {
      "epoch": 0.5289367429340511,
      "grad_norm": 0.4154742956161499,
      "learning_rate": 0.00018841664172403473,
      "loss": 1.5504,
      "step": 393
    },
    {
      "epoch": 0.5302826379542396,
      "grad_norm": 0.5765494704246521,
      "learning_rate": 0.00018838671056569889,
      "loss": 1.5649,
      "step": 394
    },
    {
      "epoch": 0.531628532974428,
      "grad_norm": 0.4260021150112152,
      "learning_rate": 0.00018835677940736307,
      "loss": 1.3826,
      "step": 395
    },
    {
      "epoch": 0.5329744279946165,
      "grad_norm": 0.39513248205184937,
      "learning_rate": 0.00018832684824902725,
      "loss": 1.3707,
      "step": 396
    },
    {
      "epoch": 0.5343203230148048,
      "grad_norm": 0.48422104120254517,
      "learning_rate": 0.0001882969170906914,
      "loss": 1.4162,
      "step": 397
    },
    {
      "epoch": 0.5356662180349933,
      "grad_norm": 0.46934759616851807,
      "learning_rate": 0.0001882669859323556,
      "loss": 1.5319,
      "step": 398
    },
    {
      "epoch": 0.5370121130551817,
      "grad_norm": 0.4572007656097412,
      "learning_rate": 0.00018823705477401977,
      "loss": 1.1881,
      "step": 399
    },
    {
      "epoch": 0.5383580080753702,
      "grad_norm": 0.3251186013221741,
      "learning_rate": 0.00018820712361568393,
      "loss": 1.6325,
      "step": 400
    },
    {
      "epoch": 0.5397039030955586,
      "grad_norm": 0.4727834165096283,
      "learning_rate": 0.0001881771924573481,
      "loss": 1.6496,
      "step": 401
    },
    {
      "epoch": 0.5410497981157469,
      "grad_norm": 0.46044424176216125,
      "learning_rate": 0.00018814726129901227,
      "loss": 1.3894,
      "step": 402
    },
    {
      "epoch": 0.5423956931359354,
      "grad_norm": 0.382127583026886,
      "learning_rate": 0.00018811733014067645,
      "loss": 1.5069,
      "step": 403
    },
    {
      "epoch": 0.5437415881561238,
      "grad_norm": 0.40806326270103455,
      "learning_rate": 0.00018808739898234063,
      "loss": 1.2702,
      "step": 404
    },
    {
      "epoch": 0.5450874831763123,
      "grad_norm": 0.36793839931488037,
      "learning_rate": 0.0001880574678240048,
      "loss": 1.3116,
      "step": 405
    },
    {
      "epoch": 0.5464333781965006,
      "grad_norm": 0.445284903049469,
      "learning_rate": 0.00018802753666566897,
      "loss": 1.6102,
      "step": 406
    },
    {
      "epoch": 0.5477792732166891,
      "grad_norm": 0.5413986444473267,
      "learning_rate": 0.00018799760550733315,
      "loss": 1.4391,
      "step": 407
    },
    {
      "epoch": 0.5491251682368775,
      "grad_norm": 0.43060871958732605,
      "learning_rate": 0.0001879676743489973,
      "loss": 1.6345,
      "step": 408
    },
    {
      "epoch": 0.550471063257066,
      "grad_norm": 0.36216461658477783,
      "learning_rate": 0.0001879377431906615,
      "loss": 1.126,
      "step": 409
    },
    {
      "epoch": 0.5518169582772544,
      "grad_norm": 0.5059884786605835,
      "learning_rate": 0.00018790781203232565,
      "loss": 1.2856,
      "step": 410
    },
    {
      "epoch": 0.5531628532974427,
      "grad_norm": 0.3599752187728882,
      "learning_rate": 0.00018787788087398983,
      "loss": 1.699,
      "step": 411
    },
    {
      "epoch": 0.5545087483176312,
      "grad_norm": 0.5095322132110596,
      "learning_rate": 0.000187847949715654,
      "loss": 1.4348,
      "step": 412
    },
    {
      "epoch": 0.5558546433378196,
      "grad_norm": 0.4535653293132782,
      "learning_rate": 0.00018781801855731817,
      "loss": 1.4932,
      "step": 413
    },
    {
      "epoch": 0.5572005383580081,
      "grad_norm": 0.3827337920665741,
      "learning_rate": 0.00018778808739898235,
      "loss": 1.5766,
      "step": 414
    },
    {
      "epoch": 0.5585464333781965,
      "grad_norm": 0.5107067227363586,
      "learning_rate": 0.00018775815624064653,
      "loss": 1.3874,
      "step": 415
    },
    {
      "epoch": 0.5598923283983849,
      "grad_norm": 0.4401836097240448,
      "learning_rate": 0.0001877282250823107,
      "loss": 1.3494,
      "step": 416
    },
    {
      "epoch": 0.5612382234185733,
      "grad_norm": 0.4219941794872284,
      "learning_rate": 0.00018769829392397487,
      "loss": 1.7249,
      "step": 417
    },
    {
      "epoch": 0.5625841184387618,
      "grad_norm": 0.46729782223701477,
      "learning_rate": 0.00018766836276563903,
      "loss": 1.544,
      "step": 418
    },
    {
      "epoch": 0.5639300134589502,
      "grad_norm": 0.5841214060783386,
      "learning_rate": 0.0001876384316073032,
      "loss": 1.5439,
      "step": 419
    },
    {
      "epoch": 0.5652759084791387,
      "grad_norm": 0.47453561425209045,
      "learning_rate": 0.0001876085004489674,
      "loss": 1.321,
      "step": 420
    },
    {
      "epoch": 0.566621803499327,
      "grad_norm": 0.541898250579834,
      "learning_rate": 0.00018757856929063155,
      "loss": 1.6666,
      "step": 421
    },
    {
      "epoch": 0.5679676985195155,
      "grad_norm": 0.37695181369781494,
      "learning_rate": 0.00018754863813229573,
      "loss": 1.3944,
      "step": 422
    },
    {
      "epoch": 0.5693135935397039,
      "grad_norm": 0.433668315410614,
      "learning_rate": 0.0001875187069739599,
      "loss": 1.267,
      "step": 423
    },
    {
      "epoch": 0.5706594885598923,
      "grad_norm": 0.5032244324684143,
      "learning_rate": 0.00018748877581562407,
      "loss": 1.5162,
      "step": 424
    },
    {
      "epoch": 0.5720053835800808,
      "grad_norm": 0.4932514429092407,
      "learning_rate": 0.00018745884465728825,
      "loss": 1.6542,
      "step": 425
    },
    {
      "epoch": 0.5733512786002691,
      "grad_norm": 0.4010337293148041,
      "learning_rate": 0.0001874289134989524,
      "loss": 1.6053,
      "step": 426
    },
    {
      "epoch": 0.5746971736204576,
      "grad_norm": 0.5228455662727356,
      "learning_rate": 0.0001873989823406166,
      "loss": 1.4106,
      "step": 427
    },
    {
      "epoch": 0.576043068640646,
      "grad_norm": 0.4373619556427002,
      "learning_rate": 0.00018736905118228077,
      "loss": 1.3858,
      "step": 428
    },
    {
      "epoch": 0.5773889636608345,
      "grad_norm": 0.5179089307785034,
      "learning_rate": 0.00018733912002394493,
      "loss": 1.3445,
      "step": 429
    },
    {
      "epoch": 0.5787348586810229,
      "grad_norm": 0.5583253502845764,
      "learning_rate": 0.0001873091888656091,
      "loss": 1.4083,
      "step": 430
    },
    {
      "epoch": 0.5800807537012113,
      "grad_norm": 0.6077472567558289,
      "learning_rate": 0.0001872792577072733,
      "loss": 1.6694,
      "step": 431
    },
    {
      "epoch": 0.5814266487213997,
      "grad_norm": 0.40994927287101746,
      "learning_rate": 0.00018724932654893745,
      "loss": 1.3154,
      "step": 432
    },
    {
      "epoch": 0.5827725437415882,
      "grad_norm": 0.442055881023407,
      "learning_rate": 0.00018721939539060163,
      "loss": 1.4865,
      "step": 433
    },
    {
      "epoch": 0.5841184387617766,
      "grad_norm": 0.42842671275138855,
      "learning_rate": 0.0001871894642322658,
      "loss": 1.8087,
      "step": 434
    },
    {
      "epoch": 0.585464333781965,
      "grad_norm": 0.529995322227478,
      "learning_rate": 0.00018715953307392997,
      "loss": 1.4947,
      "step": 435
    },
    {
      "epoch": 0.5868102288021534,
      "grad_norm": 0.47611939907073975,
      "learning_rate": 0.00018712960191559415,
      "loss": 1.4678,
      "step": 436
    },
    {
      "epoch": 0.5881561238223418,
      "grad_norm": 0.472938597202301,
      "learning_rate": 0.0001870996707572583,
      "loss": 1.5408,
      "step": 437
    },
    {
      "epoch": 0.5895020188425303,
      "grad_norm": 0.3701465427875519,
      "learning_rate": 0.0001870697395989225,
      "loss": 1.5028,
      "step": 438
    },
    {
      "epoch": 0.5908479138627187,
      "grad_norm": 0.36045849323272705,
      "learning_rate": 0.00018703980844058667,
      "loss": 1.4246,
      "step": 439
    },
    {
      "epoch": 0.5921938088829072,
      "grad_norm": 0.3880743980407715,
      "learning_rate": 0.00018700987728225083,
      "loss": 1.4006,
      "step": 440
    },
    {
      "epoch": 0.5935397039030955,
      "grad_norm": 0.3998335599899292,
      "learning_rate": 0.000186979946123915,
      "loss": 1.6166,
      "step": 441
    },
    {
      "epoch": 0.594885598923284,
      "grad_norm": 0.4139010012149811,
      "learning_rate": 0.00018695001496557917,
      "loss": 1.5854,
      "step": 442
    },
    {
      "epoch": 0.5962314939434724,
      "grad_norm": 0.4453850984573364,
      "learning_rate": 0.00018692008380724335,
      "loss": 1.7184,
      "step": 443
    },
    {
      "epoch": 0.5975773889636609,
      "grad_norm": 0.4283633828163147,
      "learning_rate": 0.00018689015264890753,
      "loss": 1.4244,
      "step": 444
    },
    {
      "epoch": 0.5989232839838493,
      "grad_norm": 0.4533657431602478,
      "learning_rate": 0.0001868602214905717,
      "loss": 1.4764,
      "step": 445
    },
    {
      "epoch": 0.6002691790040376,
      "grad_norm": 0.48810863494873047,
      "learning_rate": 0.00018683029033223587,
      "loss": 1.4555,
      "step": 446
    },
    {
      "epoch": 0.6016150740242261,
      "grad_norm": 0.37967121601104736,
      "learning_rate": 0.00018680035917390005,
      "loss": 1.3188,
      "step": 447
    },
    {
      "epoch": 0.6029609690444145,
      "grad_norm": 0.4726078510284424,
      "learning_rate": 0.0001867704280155642,
      "loss": 1.4235,
      "step": 448
    },
    {
      "epoch": 0.604306864064603,
      "grad_norm": 0.40545448660850525,
      "learning_rate": 0.0001867404968572284,
      "loss": 1.3253,
      "step": 449
    },
    {
      "epoch": 0.6056527590847914,
      "grad_norm": 0.4983719289302826,
      "learning_rate": 0.00018671056569889255,
      "loss": 1.1381,
      "step": 450
    },
    {
      "epoch": 0.6069986541049798,
      "grad_norm": 0.39880087971687317,
      "learning_rate": 0.00018668063454055673,
      "loss": 1.3348,
      "step": 451
    },
    {
      "epoch": 0.6083445491251682,
      "grad_norm": 0.4788765013217926,
      "learning_rate": 0.00018665070338222091,
      "loss": 1.552,
      "step": 452
    },
    {
      "epoch": 0.6096904441453567,
      "grad_norm": 0.4306666851043701,
      "learning_rate": 0.00018662077222388507,
      "loss": 1.3228,
      "step": 453
    },
    {
      "epoch": 0.6110363391655451,
      "grad_norm": 0.5097247362136841,
      "learning_rate": 0.00018659084106554925,
      "loss": 1.7333,
      "step": 454
    },
    {
      "epoch": 0.6123822341857336,
      "grad_norm": 0.4845592677593231,
      "learning_rate": 0.00018656090990721343,
      "loss": 1.5271,
      "step": 455
    },
    {
      "epoch": 0.6137281292059219,
      "grad_norm": 0.5141584277153015,
      "learning_rate": 0.0001865309787488776,
      "loss": 1.3927,
      "step": 456
    },
    {
      "epoch": 0.6150740242261103,
      "grad_norm": 0.4940685033798218,
      "learning_rate": 0.00018650104759054177,
      "loss": 1.5028,
      "step": 457
    },
    {
      "epoch": 0.6164199192462988,
      "grad_norm": 0.6319875121116638,
      "learning_rate": 0.00018647111643220593,
      "loss": 1.695,
      "step": 458
    },
    {
      "epoch": 0.6177658142664872,
      "grad_norm": 0.4590522050857544,
      "learning_rate": 0.0001864411852738701,
      "loss": 1.402,
      "step": 459
    },
    {
      "epoch": 0.6191117092866757,
      "grad_norm": 0.4801904261112213,
      "learning_rate": 0.0001864112541155343,
      "loss": 1.3168,
      "step": 460
    },
    {
      "epoch": 0.620457604306864,
      "grad_norm": 0.43207642436027527,
      "learning_rate": 0.00018638132295719845,
      "loss": 1.3833,
      "step": 461
    },
    {
      "epoch": 0.6218034993270525,
      "grad_norm": 0.5050480365753174,
      "learning_rate": 0.00018635139179886263,
      "loss": 1.5143,
      "step": 462
    },
    {
      "epoch": 0.6231493943472409,
      "grad_norm": 0.6723858714103699,
      "learning_rate": 0.00018632146064052681,
      "loss": 1.2317,
      "step": 463
    },
    {
      "epoch": 0.6244952893674294,
      "grad_norm": 0.4649413228034973,
      "learning_rate": 0.00018629152948219097,
      "loss": 1.4169,
      "step": 464
    },
    {
      "epoch": 0.6258411843876177,
      "grad_norm": 0.48775216937065125,
      "learning_rate": 0.00018626159832385515,
      "loss": 1.5297,
      "step": 465
    },
    {
      "epoch": 0.6271870794078062,
      "grad_norm": 0.47670501470565796,
      "learning_rate": 0.0001862316671655193,
      "loss": 1.2736,
      "step": 466
    },
    {
      "epoch": 0.6285329744279946,
      "grad_norm": 0.4610641598701477,
      "learning_rate": 0.0001862017360071835,
      "loss": 1.4599,
      "step": 467
    },
    {
      "epoch": 0.629878869448183,
      "grad_norm": 0.49908968806266785,
      "learning_rate": 0.00018617180484884767,
      "loss": 1.3691,
      "step": 468
    },
    {
      "epoch": 0.6312247644683715,
      "grad_norm": 0.47592976689338684,
      "learning_rate": 0.00018614187369051183,
      "loss": 1.645,
      "step": 469
    },
    {
      "epoch": 0.6325706594885598,
      "grad_norm": 0.4360126554965973,
      "learning_rate": 0.000186111942532176,
      "loss": 1.2024,
      "step": 470
    },
    {
      "epoch": 0.6339165545087483,
      "grad_norm": 0.43520811200141907,
      "learning_rate": 0.00018608201137384017,
      "loss": 1.47,
      "step": 471
    },
    {
      "epoch": 0.6352624495289367,
      "grad_norm": 0.5606088042259216,
      "learning_rate": 0.00018605208021550435,
      "loss": 1.3507,
      "step": 472
    },
    {
      "epoch": 0.6366083445491252,
      "grad_norm": 0.4913078546524048,
      "learning_rate": 0.00018602214905716853,
      "loss": 1.7626,
      "step": 473
    },
    {
      "epoch": 0.6379542395693136,
      "grad_norm": 0.4591622054576874,
      "learning_rate": 0.0001859922178988327,
      "loss": 1.4223,
      "step": 474
    },
    {
      "epoch": 0.639300134589502,
      "grad_norm": 0.42917919158935547,
      "learning_rate": 0.00018596228674049687,
      "loss": 1.4513,
      "step": 475
    },
    {
      "epoch": 0.6406460296096904,
      "grad_norm": 0.40534690022468567,
      "learning_rate": 0.00018593235558216105,
      "loss": 1.4421,
      "step": 476
    },
    {
      "epoch": 0.6419919246298789,
      "grad_norm": 0.4566594064235687,
      "learning_rate": 0.0001859024244238252,
      "loss": 1.3499,
      "step": 477
    },
    {
      "epoch": 0.6433378196500673,
      "grad_norm": 0.360037624835968,
      "learning_rate": 0.0001858724932654894,
      "loss": 1.743,
      "step": 478
    },
    {
      "epoch": 0.6446837146702558,
      "grad_norm": 0.392039954662323,
      "learning_rate": 0.00018584256210715355,
      "loss": 1.535,
      "step": 479
    },
    {
      "epoch": 0.6460296096904441,
      "grad_norm": 0.5278076529502869,
      "learning_rate": 0.00018581263094881773,
      "loss": 1.597,
      "step": 480
    },
    {
      "epoch": 0.6473755047106325,
      "grad_norm": 0.4530724883079529,
      "learning_rate": 0.00018578269979048191,
      "loss": 1.4316,
      "step": 481
    },
    {
      "epoch": 0.648721399730821,
      "grad_norm": 0.4577494263648987,
      "learning_rate": 0.00018575276863214607,
      "loss": 1.4813,
      "step": 482
    },
    {
      "epoch": 0.6500672947510094,
      "grad_norm": 0.44852396845817566,
      "learning_rate": 0.00018572283747381025,
      "loss": 1.5047,
      "step": 483
    },
    {
      "epoch": 0.6514131897711979,
      "grad_norm": 0.45030513405799866,
      "learning_rate": 0.00018569290631547443,
      "loss": 1.8297,
      "step": 484
    },
    {
      "epoch": 0.6527590847913862,
      "grad_norm": 0.47395890951156616,
      "learning_rate": 0.0001856629751571386,
      "loss": 1.514,
      "step": 485
    },
    {
      "epoch": 0.6541049798115747,
      "grad_norm": 0.4658500552177429,
      "learning_rate": 0.00018563304399880277,
      "loss": 1.5206,
      "step": 486
    },
    {
      "epoch": 0.6554508748317631,
      "grad_norm": 0.47607412934303284,
      "learning_rate": 0.00018560311284046693,
      "loss": 1.448,
      "step": 487
    },
    {
      "epoch": 0.6567967698519516,
      "grad_norm": 0.439117431640625,
      "learning_rate": 0.0001855731816821311,
      "loss": 1.5233,
      "step": 488
    },
    {
      "epoch": 0.65814266487214,
      "grad_norm": 0.48609569668769836,
      "learning_rate": 0.0001855432505237953,
      "loss": 1.3861,
      "step": 489
    },
    {
      "epoch": 0.6594885598923284,
      "grad_norm": 0.4505992531776428,
      "learning_rate": 0.00018551331936545945,
      "loss": 1.5389,
      "step": 490
    },
    {
      "epoch": 0.6608344549125168,
      "grad_norm": 0.5423902273178101,
      "learning_rate": 0.00018548338820712363,
      "loss": 1.4068,
      "step": 491
    },
    {
      "epoch": 0.6621803499327052,
      "grad_norm": 0.4630659818649292,
      "learning_rate": 0.00018545345704878782,
      "loss": 1.5537,
      "step": 492
    },
    {
      "epoch": 0.6635262449528937,
      "grad_norm": 0.45201197266578674,
      "learning_rate": 0.00018542352589045197,
      "loss": 1.2816,
      "step": 493
    },
    {
      "epoch": 0.6648721399730821,
      "grad_norm": 0.5283516645431519,
      "learning_rate": 0.00018539359473211615,
      "loss": 1.6388,
      "step": 494
    },
    {
      "epoch": 0.6662180349932705,
      "grad_norm": 0.45738986134529114,
      "learning_rate": 0.0001853636635737803,
      "loss": 1.3769,
      "step": 495
    },
    {
      "epoch": 0.6675639300134589,
      "grad_norm": 0.4424607455730438,
      "learning_rate": 0.0001853337324154445,
      "loss": 1.2641,
      "step": 496
    },
    {
      "epoch": 0.6689098250336474,
      "grad_norm": 0.4192241430282593,
      "learning_rate": 0.00018530380125710867,
      "loss": 1.2096,
      "step": 497
    },
    {
      "epoch": 0.6702557200538358,
      "grad_norm": 0.333326131105423,
      "learning_rate": 0.00018527387009877283,
      "loss": 1.6811,
      "step": 498
    },
    {
      "epoch": 0.6716016150740243,
      "grad_norm": 0.41642072796821594,
      "learning_rate": 0.000185243938940437,
      "loss": 1.4389,
      "step": 499
    },
    {
      "epoch": 0.6729475100942126,
      "grad_norm": 0.4631737768650055,
      "learning_rate": 0.0001852140077821012,
      "loss": 1.4069,
      "step": 500
    },
    {
      "epoch": 0.6742934051144011,
      "grad_norm": 0.4564732611179352,
      "learning_rate": 0.00018518407662376535,
      "loss": 1.4674,
      "step": 501
    },
    {
      "epoch": 0.6756393001345895,
      "grad_norm": 0.5674712061882019,
      "learning_rate": 0.00018515414546542953,
      "loss": 1.6969,
      "step": 502
    },
    {
      "epoch": 0.676985195154778,
      "grad_norm": 0.5077922940254211,
      "learning_rate": 0.0001851242143070937,
      "loss": 1.6098,
      "step": 503
    },
    {
      "epoch": 0.6783310901749664,
      "grad_norm": 0.4279720187187195,
      "learning_rate": 0.00018509428314875787,
      "loss": 1.4311,
      "step": 504
    },
    {
      "epoch": 0.6796769851951547,
      "grad_norm": 0.5300177931785583,
      "learning_rate": 0.00018506435199042205,
      "loss": 1.3303,
      "step": 505
    },
    {
      "epoch": 0.6810228802153432,
      "grad_norm": 0.37736260890960693,
      "learning_rate": 0.0001850344208320862,
      "loss": 1.234,
      "step": 506
    },
    {
      "epoch": 0.6823687752355316,
      "grad_norm": 0.3883439898490906,
      "learning_rate": 0.0001850044896737504,
      "loss": 1.3985,
      "step": 507
    },
    {
      "epoch": 0.6837146702557201,
      "grad_norm": 0.5029048919677734,
      "learning_rate": 0.00018497455851541458,
      "loss": 1.7194,
      "step": 508
    },
    {
      "epoch": 0.6850605652759085,
      "grad_norm": 0.5060417652130127,
      "learning_rate": 0.00018494462735707873,
      "loss": 1.4676,
      "step": 509
    },
    {
      "epoch": 0.6864064602960969,
      "grad_norm": 0.40727394819259644,
      "learning_rate": 0.00018491469619874291,
      "loss": 1.1997,
      "step": 510
    },
    {
      "epoch": 0.6877523553162853,
      "grad_norm": 0.4397975206375122,
      "learning_rate": 0.00018488476504040707,
      "loss": 1.5276,
      "step": 511
    },
    {
      "epoch": 0.6890982503364738,
      "grad_norm": 0.4405744969844818,
      "learning_rate": 0.00018485483388207125,
      "loss": 1.2035,
      "step": 512
    },
    {
      "epoch": 0.6904441453566622,
      "grad_norm": 0.5460373759269714,
      "learning_rate": 0.00018482490272373543,
      "loss": 1.3708,
      "step": 513
    },
    {
      "epoch": 0.6917900403768507,
      "grad_norm": 0.44524362683296204,
      "learning_rate": 0.0001847949715653996,
      "loss": 1.2712,
      "step": 514
    },
    {
      "epoch": 0.693135935397039,
      "grad_norm": 0.44441094994544983,
      "learning_rate": 0.00018476504040706377,
      "loss": 1.2457,
      "step": 515
    },
    {
      "epoch": 0.6944818304172274,
      "grad_norm": 0.4765458405017853,
      "learning_rate": 0.00018473510924872796,
      "loss": 1.519,
      "step": 516
    },
    {
      "epoch": 0.6958277254374159,
      "grad_norm": 0.4405894875526428,
      "learning_rate": 0.0001847051780903921,
      "loss": 1.5605,
      "step": 517
    },
    {
      "epoch": 0.6971736204576043,
      "grad_norm": 0.49320363998413086,
      "learning_rate": 0.0001846752469320563,
      "loss": 1.594,
      "step": 518
    },
    {
      "epoch": 0.6985195154777928,
      "grad_norm": 0.4115109443664551,
      "learning_rate": 0.00018464531577372045,
      "loss": 1.3451,
      "step": 519
    },
    {
      "epoch": 0.6998654104979811,
      "grad_norm": 0.5573132634162903,
      "learning_rate": 0.00018461538461538463,
      "loss": 1.2785,
      "step": 520
    },
    {
      "epoch": 0.7012113055181696,
      "grad_norm": 0.4024668037891388,
      "learning_rate": 0.00018458545345704882,
      "loss": 1.6263,
      "step": 521
    },
    {
      "epoch": 0.702557200538358,
      "grad_norm": 0.33385348320007324,
      "learning_rate": 0.00018455552229871297,
      "loss": 1.4502,
      "step": 522
    },
    {
      "epoch": 0.7039030955585465,
      "grad_norm": 0.4440448582172394,
      "learning_rate": 0.00018452559114037715,
      "loss": 1.6067,
      "step": 523
    },
    {
      "epoch": 0.7052489905787349,
      "grad_norm": 0.4236752390861511,
      "learning_rate": 0.00018449565998204134,
      "loss": 1.3115,
      "step": 524
    },
    {
      "epoch": 0.7065948855989233,
      "grad_norm": 0.521457850933075,
      "learning_rate": 0.0001844657288237055,
      "loss": 1.7658,
      "step": 525
    },
    {
      "epoch": 0.7079407806191117,
      "grad_norm": 0.5565138459205627,
      "learning_rate": 0.00018443579766536967,
      "loss": 1.5205,
      "step": 526
    },
    {
      "epoch": 0.7092866756393001,
      "grad_norm": 0.49203410744667053,
      "learning_rate": 0.00018440586650703383,
      "loss": 1.3549,
      "step": 527
    },
    {
      "epoch": 0.7106325706594886,
      "grad_norm": 0.4901714324951172,
      "learning_rate": 0.000184375935348698,
      "loss": 1.2527,
      "step": 528
    },
    {
      "epoch": 0.7119784656796769,
      "grad_norm": 0.46717289090156555,
      "learning_rate": 0.00018434600419036217,
      "loss": 1.5171,
      "step": 529
    },
    {
      "epoch": 0.7133243606998654,
      "grad_norm": 0.4491562247276306,
      "learning_rate": 0.00018431607303202632,
      "loss": 1.5643,
      "step": 530
    },
    {
      "epoch": 0.7146702557200538,
      "grad_norm": 0.46044161915779114,
      "learning_rate": 0.0001842861418736905,
      "loss": 1.2728,
      "step": 531
    },
    {
      "epoch": 0.7160161507402423,
      "grad_norm": 0.5139637589454651,
      "learning_rate": 0.0001842562107153547,
      "loss": 1.4018,
      "step": 532
    },
    {
      "epoch": 0.7173620457604307,
      "grad_norm": 0.499616414308548,
      "learning_rate": 0.00018422627955701885,
      "loss": 1.5885,
      "step": 533
    },
    {
      "epoch": 0.7187079407806191,
      "grad_norm": 0.5564426779747009,
      "learning_rate": 0.00018419634839868303,
      "loss": 1.497,
      "step": 534
    },
    {
      "epoch": 0.7200538358008075,
      "grad_norm": 0.4835631251335144,
      "learning_rate": 0.0001841664172403472,
      "loss": 1.5397,
      "step": 535
    },
    {
      "epoch": 0.721399730820996,
      "grad_norm": 0.4518422484397888,
      "learning_rate": 0.00018413648608201137,
      "loss": 1.329,
      "step": 536
    },
    {
      "epoch": 0.7227456258411844,
      "grad_norm": 0.42270198464393616,
      "learning_rate": 0.00018410655492367555,
      "loss": 1.2959,
      "step": 537
    },
    {
      "epoch": 0.7240915208613729,
      "grad_norm": 0.3913941979408264,
      "learning_rate": 0.0001840766237653397,
      "loss": 1.3696,
      "step": 538
    },
    {
      "epoch": 0.7254374158815612,
      "grad_norm": 0.41780102252960205,
      "learning_rate": 0.0001840466926070039,
      "loss": 1.366,
      "step": 539
    },
    {
      "epoch": 0.7267833109017496,
      "grad_norm": 0.4245999753475189,
      "learning_rate": 0.00018401676144866807,
      "loss": 1.3218,
      "step": 540
    },
    {
      "epoch": 0.7281292059219381,
      "grad_norm": 0.47496312856674194,
      "learning_rate": 0.00018398683029033223,
      "loss": 1.4198,
      "step": 541
    },
    {
      "epoch": 0.7294751009421265,
      "grad_norm": 0.5407896041870117,
      "learning_rate": 0.0001839568991319964,
      "loss": 1.3756,
      "step": 542
    },
    {
      "epoch": 0.730820995962315,
      "grad_norm": 0.5500769019126892,
      "learning_rate": 0.0001839269679736606,
      "loss": 1.4052,
      "step": 543
    },
    {
      "epoch": 0.7321668909825033,
      "grad_norm": 0.524797260761261,
      "learning_rate": 0.00018389703681532475,
      "loss": 1.4557,
      "step": 544
    },
    {
      "epoch": 0.7335127860026918,
      "grad_norm": 0.5780274271965027,
      "learning_rate": 0.00018386710565698893,
      "loss": 1.4159,
      "step": 545
    },
    {
      "epoch": 0.7348586810228802,
      "grad_norm": 0.4235246777534485,
      "learning_rate": 0.00018383717449865308,
      "loss": 1.2855,
      "step": 546
    },
    {
      "epoch": 0.7362045760430687,
      "grad_norm": 0.47682783007621765,
      "learning_rate": 0.00018380724334031727,
      "loss": 1.2788,
      "step": 547
    },
    {
      "epoch": 0.7375504710632571,
      "grad_norm": 0.35894066095352173,
      "learning_rate": 0.00018377731218198145,
      "loss": 1.6281,
      "step": 548
    },
    {
      "epoch": 0.7388963660834454,
      "grad_norm": 0.5047533512115479,
      "learning_rate": 0.0001837473810236456,
      "loss": 1.4703,
      "step": 549
    },
    {
      "epoch": 0.7402422611036339,
      "grad_norm": 0.47202685475349426,
      "learning_rate": 0.0001837174498653098,
      "loss": 1.3889,
      "step": 550
    },
    {
      "epoch": 0.7415881561238223,
      "grad_norm": 0.4387233257293701,
      "learning_rate": 0.00018368751870697397,
      "loss": 2.0295,
      "step": 551
    },
    {
      "epoch": 0.7429340511440108,
      "grad_norm": 0.5104343891143799,
      "learning_rate": 0.00018365758754863813,
      "loss": 1.6041,
      "step": 552
    },
    {
      "epoch": 0.7442799461641992,
      "grad_norm": 0.4362710416316986,
      "learning_rate": 0.0001836276563903023,
      "loss": 1.432,
      "step": 553
    },
    {
      "epoch": 0.7456258411843876,
      "grad_norm": 0.42451080679893494,
      "learning_rate": 0.00018359772523196646,
      "loss": 1.3617,
      "step": 554
    },
    {
      "epoch": 0.746971736204576,
      "grad_norm": 0.465589314699173,
      "learning_rate": 0.00018356779407363065,
      "loss": 1.4022,
      "step": 555
    },
    {
      "epoch": 0.7483176312247645,
      "grad_norm": 0.5633970499038696,
      "learning_rate": 0.00018353786291529483,
      "loss": 1.5787,
      "step": 556
    },
    {
      "epoch": 0.7496635262449529,
      "grad_norm": 0.3650865852832794,
      "learning_rate": 0.00018350793175695899,
      "loss": 1.5678,
      "step": 557
    },
    {
      "epoch": 0.7510094212651414,
      "grad_norm": 0.41819000244140625,
      "learning_rate": 0.00018347800059862317,
      "loss": 1.3518,
      "step": 558
    },
    {
      "epoch": 0.7523553162853297,
      "grad_norm": 0.5541055798530579,
      "learning_rate": 0.00018344806944028735,
      "loss": 1.4089,
      "step": 559
    },
    {
      "epoch": 0.7537012113055181,
      "grad_norm": 0.5924892425537109,
      "learning_rate": 0.0001834181382819515,
      "loss": 1.5388,
      "step": 560
    },
    {
      "epoch": 0.7550471063257066,
      "grad_norm": 0.5475035309791565,
      "learning_rate": 0.0001833882071236157,
      "loss": 1.4341,
      "step": 561
    },
    {
      "epoch": 0.756393001345895,
      "grad_norm": 0.41767585277557373,
      "learning_rate": 0.00018335827596527985,
      "loss": 1.5055,
      "step": 562
    },
    {
      "epoch": 0.7577388963660835,
      "grad_norm": 0.4818255305290222,
      "learning_rate": 0.00018332834480694403,
      "loss": 1.458,
      "step": 563
    },
    {
      "epoch": 0.7590847913862718,
      "grad_norm": 0.46043533086776733,
      "learning_rate": 0.0001832984136486082,
      "loss": 1.4446,
      "step": 564
    },
    {
      "epoch": 0.7604306864064603,
      "grad_norm": 0.5161400437355042,
      "learning_rate": 0.00018326848249027237,
      "loss": 1.3645,
      "step": 565
    },
    {
      "epoch": 0.7617765814266487,
      "grad_norm": 0.6058946847915649,
      "learning_rate": 0.00018323855133193655,
      "loss": 1.4828,
      "step": 566
    },
    {
      "epoch": 0.7631224764468372,
      "grad_norm": 0.435458242893219,
      "learning_rate": 0.00018320862017360073,
      "loss": 1.6326,
      "step": 567
    },
    {
      "epoch": 0.7644683714670256,
      "grad_norm": 0.39970049262046814,
      "learning_rate": 0.0001831786890152649,
      "loss": 1.2456,
      "step": 568
    },
    {
      "epoch": 0.765814266487214,
      "grad_norm": 0.43221548199653625,
      "learning_rate": 0.00018314875785692907,
      "loss": 1.3578,
      "step": 569
    },
    {
      "epoch": 0.7671601615074024,
      "grad_norm": 0.4835311770439148,
      "learning_rate": 0.00018311882669859323,
      "loss": 1.4663,
      "step": 570
    },
    {
      "epoch": 0.7685060565275909,
      "grad_norm": 0.5757130980491638,
      "learning_rate": 0.0001830888955402574,
      "loss": 1.2102,
      "step": 571
    },
    {
      "epoch": 0.7698519515477793,
      "grad_norm": 0.3742407560348511,
      "learning_rate": 0.0001830589643819216,
      "loss": 1.2406,
      "step": 572
    },
    {
      "epoch": 0.7711978465679677,
      "grad_norm": 0.5070323944091797,
      "learning_rate": 0.00018302903322358575,
      "loss": 1.5271,
      "step": 573
    },
    {
      "epoch": 0.7725437415881561,
      "grad_norm": 0.40378206968307495,
      "learning_rate": 0.00018299910206524993,
      "loss": 1.4947,
      "step": 574
    },
    {
      "epoch": 0.7738896366083445,
      "grad_norm": 0.47312068939208984,
      "learning_rate": 0.0001829691709069141,
      "loss": 1.6063,
      "step": 575
    },
    {
      "epoch": 0.775235531628533,
      "grad_norm": 0.42852890491485596,
      "learning_rate": 0.00018293923974857827,
      "loss": 1.5053,
      "step": 576
    },
    {
      "epoch": 0.7765814266487214,
      "grad_norm": 0.3738015294075012,
      "learning_rate": 0.00018290930859024245,
      "loss": 1.3563,
      "step": 577
    },
    {
      "epoch": 0.7779273216689099,
      "grad_norm": 0.45047727227211,
      "learning_rate": 0.0001828793774319066,
      "loss": 1.3783,
      "step": 578
    },
    {
      "epoch": 0.7792732166890982,
      "grad_norm": 0.39881250262260437,
      "learning_rate": 0.0001828494462735708,
      "loss": 1.4756,
      "step": 579
    },
    {
      "epoch": 0.7806191117092867,
      "grad_norm": 0.41502252221107483,
      "learning_rate": 0.00018281951511523497,
      "loss": 1.3912,
      "step": 580
    },
    {
      "epoch": 0.7819650067294751,
      "grad_norm": 0.35330232977867126,
      "learning_rate": 0.00018278958395689913,
      "loss": 1.6615,
      "step": 581
    },
    {
      "epoch": 0.7833109017496636,
      "grad_norm": 0.4296402335166931,
      "learning_rate": 0.0001827596527985633,
      "loss": 1.2065,
      "step": 582
    },
    {
      "epoch": 0.784656796769852,
      "grad_norm": 0.5051036477088928,
      "learning_rate": 0.0001827297216402275,
      "loss": 1.5372,
      "step": 583
    },
    {
      "epoch": 0.7860026917900403,
      "grad_norm": 0.4662574827671051,
      "learning_rate": 0.00018269979048189165,
      "loss": 1.5373,
      "step": 584
    },
    {
      "epoch": 0.7873485868102288,
      "grad_norm": 0.7023072242736816,
      "learning_rate": 0.00018266985932355583,
      "loss": 1.7487,
      "step": 585
    },
    {
      "epoch": 0.7886944818304172,
      "grad_norm": 0.43994924426078796,
      "learning_rate": 0.00018263992816521999,
      "loss": 1.3473,
      "step": 586
    },
    {
      "epoch": 0.7900403768506057,
      "grad_norm": 0.4496094286441803,
      "learning_rate": 0.00018260999700688417,
      "loss": 1.4545,
      "step": 587
    },
    {
      "epoch": 0.7913862718707941,
      "grad_norm": 0.5407683849334717,
      "learning_rate": 0.00018258006584854835,
      "loss": 1.3674,
      "step": 588
    },
    {
      "epoch": 0.7927321668909825,
      "grad_norm": 0.4390406012535095,
      "learning_rate": 0.0001825501346902125,
      "loss": 1.6276,
      "step": 589
    },
    {
      "epoch": 0.7940780619111709,
      "grad_norm": 0.3708310127258301,
      "learning_rate": 0.0001825202035318767,
      "loss": 1.4971,
      "step": 590
    },
    {
      "epoch": 0.7954239569313594,
      "grad_norm": 0.4320962429046631,
      "learning_rate": 0.00018249027237354087,
      "loss": 1.6114,
      "step": 591
    },
    {
      "epoch": 0.7967698519515478,
      "grad_norm": 0.48134857416152954,
      "learning_rate": 0.00018246034121520503,
      "loss": 1.3754,
      "step": 592
    },
    {
      "epoch": 0.7981157469717362,
      "grad_norm": 0.5861306190490723,
      "learning_rate": 0.0001824304100568692,
      "loss": 1.4295,
      "step": 593
    },
    {
      "epoch": 0.7994616419919246,
      "grad_norm": 0.5587659478187561,
      "learning_rate": 0.00018240047889853337,
      "loss": 1.2306,
      "step": 594
    },
    {
      "epoch": 0.800807537012113,
      "grad_norm": 0.5254026055335999,
      "learning_rate": 0.00018237054774019755,
      "loss": 1.319,
      "step": 595
    },
    {
      "epoch": 0.8021534320323015,
      "grad_norm": 0.4930957853794098,
      "learning_rate": 0.00018234061658186173,
      "loss": 1.7918,
      "step": 596
    },
    {
      "epoch": 0.8034993270524899,
      "grad_norm": 0.5323616862297058,
      "learning_rate": 0.0001823106854235259,
      "loss": 1.319,
      "step": 597
    },
    {
      "epoch": 0.8048452220726783,
      "grad_norm": 0.4447184205055237,
      "learning_rate": 0.00018228075426519007,
      "loss": 1.4795,
      "step": 598
    },
    {
      "epoch": 0.8061911170928667,
      "grad_norm": 0.5879616141319275,
      "learning_rate": 0.00018225082310685425,
      "loss": 1.5143,
      "step": 599
    },
    {
      "epoch": 0.8075370121130552,
      "grad_norm": 0.5105277895927429,
      "learning_rate": 0.0001822208919485184,
      "loss": 1.4191,
      "step": 600
    },
    {
      "epoch": 0.8088829071332436,
      "grad_norm": 0.5439625978469849,
      "learning_rate": 0.0001821909607901826,
      "loss": 1.4307,
      "step": 601
    },
    {
      "epoch": 0.8102288021534321,
      "grad_norm": 0.5285142660140991,
      "learning_rate": 0.00018216102963184675,
      "loss": 1.4976,
      "step": 602
    },
    {
      "epoch": 0.8115746971736204,
      "grad_norm": 0.4307524263858795,
      "learning_rate": 0.00018213109847351093,
      "loss": 1.6169,
      "step": 603
    },
    {
      "epoch": 0.8129205921938089,
      "grad_norm": 0.5383846163749695,
      "learning_rate": 0.0001821011673151751,
      "loss": 1.5122,
      "step": 604
    },
    {
      "epoch": 0.8142664872139973,
      "grad_norm": 0.42746615409851074,
      "learning_rate": 0.00018207123615683927,
      "loss": 1.4091,
      "step": 605
    },
    {
      "epoch": 0.8156123822341858,
      "grad_norm": 0.5257524251937866,
      "learning_rate": 0.00018204130499850345,
      "loss": 1.6259,
      "step": 606
    },
    {
      "epoch": 0.8169582772543742,
      "grad_norm": 0.4419781565666199,
      "learning_rate": 0.00018201137384016763,
      "loss": 1.5021,
      "step": 607
    },
    {
      "epoch": 0.8183041722745625,
      "grad_norm": 0.4905708432197571,
      "learning_rate": 0.0001819814426818318,
      "loss": 1.4364,
      "step": 608
    },
    {
      "epoch": 0.819650067294751,
      "grad_norm": 0.5101691484451294,
      "learning_rate": 0.00018195151152349597,
      "loss": 1.391,
      "step": 609
    },
    {
      "epoch": 0.8209959623149394,
      "grad_norm": 0.4110160171985626,
      "learning_rate": 0.00018192158036516013,
      "loss": 1.266,
      "step": 610
    },
    {
      "epoch": 0.8223418573351279,
      "grad_norm": 0.4904564321041107,
      "learning_rate": 0.0001818916492068243,
      "loss": 1.5232,
      "step": 611
    },
    {
      "epoch": 0.8236877523553163,
      "grad_norm": 0.4367629289627075,
      "learning_rate": 0.0001818617180484885,
      "loss": 1.4496,
      "step": 612
    },
    {
      "epoch": 0.8250336473755047,
      "grad_norm": 0.45887094736099243,
      "learning_rate": 0.00018183178689015265,
      "loss": 1.2311,
      "step": 613
    },
    {
      "epoch": 0.8263795423956931,
      "grad_norm": 0.41560864448547363,
      "learning_rate": 0.00018180185573181683,
      "loss": 1.3299,
      "step": 614
    },
    {
      "epoch": 0.8277254374158816,
      "grad_norm": 0.44986510276794434,
      "learning_rate": 0.000181771924573481,
      "loss": 1.4085,
      "step": 615
    },
    {
      "epoch": 0.82907133243607,
      "grad_norm": 0.39232251048088074,
      "learning_rate": 0.00018174199341514517,
      "loss": 1.6206,
      "step": 616
    },
    {
      "epoch": 0.8304172274562585,
      "grad_norm": 0.4203210771083832,
      "learning_rate": 0.00018171206225680935,
      "loss": 1.6299,
      "step": 617
    },
    {
      "epoch": 0.8317631224764468,
      "grad_norm": 0.47057071328163147,
      "learning_rate": 0.0001816821310984735,
      "loss": 1.3475,
      "step": 618
    },
    {
      "epoch": 0.8331090174966352,
      "grad_norm": 0.4111063778400421,
      "learning_rate": 0.0001816521999401377,
      "loss": 1.3267,
      "step": 619
    },
    {
      "epoch": 0.8344549125168237,
      "grad_norm": 0.4630427956581116,
      "learning_rate": 0.00018162226878180187,
      "loss": 1.5661,
      "step": 620
    },
    {
      "epoch": 0.8358008075370121,
      "grad_norm": 0.4168062210083008,
      "learning_rate": 0.00018159233762346603,
      "loss": 1.4524,
      "step": 621
    },
    {
      "epoch": 0.8371467025572006,
      "grad_norm": 0.5438057780265808,
      "learning_rate": 0.0001815624064651302,
      "loss": 1.4047,
      "step": 622
    },
    {
      "epoch": 0.8384925975773889,
      "grad_norm": 0.37481337785720825,
      "learning_rate": 0.0001815324753067944,
      "loss": 1.4766,
      "step": 623
    },
    {
      "epoch": 0.8398384925975774,
      "grad_norm": 0.4800478518009186,
      "learning_rate": 0.00018150254414845855,
      "loss": 1.6237,
      "step": 624
    },
    {
      "epoch": 0.8411843876177658,
      "grad_norm": 0.40694913268089294,
      "learning_rate": 0.00018147261299012273,
      "loss": 1.5894,
      "step": 625
    },
    {
      "epoch": 0.8425302826379543,
      "grad_norm": 0.4286954402923584,
      "learning_rate": 0.0001814426818317869,
      "loss": 1.5378,
      "step": 626
    },
    {
      "epoch": 0.8438761776581427,
      "grad_norm": 0.5640167593955994,
      "learning_rate": 0.00018141275067345107,
      "loss": 1.5063,
      "step": 627
    },
    {
      "epoch": 0.845222072678331,
      "grad_norm": 0.5479482412338257,
      "learning_rate": 0.00018138281951511525,
      "loss": 1.6555,
      "step": 628
    },
    {
      "epoch": 0.8465679676985195,
      "grad_norm": 0.5026683807373047,
      "learning_rate": 0.0001813528883567794,
      "loss": 1.5926,
      "step": 629
    },
    {
      "epoch": 0.847913862718708,
      "grad_norm": 0.4500994086265564,
      "learning_rate": 0.0001813229571984436,
      "loss": 1.4825,
      "step": 630
    },
    {
      "epoch": 0.8492597577388964,
      "grad_norm": 0.3842369318008423,
      "learning_rate": 0.00018129302604010777,
      "loss": 1.2016,
      "step": 631
    },
    {
      "epoch": 0.8506056527590848,
      "grad_norm": 0.5369496941566467,
      "learning_rate": 0.00018126309488177193,
      "loss": 1.585,
      "step": 632
    },
    {
      "epoch": 0.8519515477792732,
      "grad_norm": 0.500618577003479,
      "learning_rate": 0.0001812331637234361,
      "loss": 1.3724,
      "step": 633
    },
    {
      "epoch": 0.8532974427994616,
      "grad_norm": 0.45382219552993774,
      "learning_rate": 0.00018120323256510027,
      "loss": 1.4103,
      "step": 634
    },
    {
      "epoch": 0.8546433378196501,
      "grad_norm": 0.39967167377471924,
      "learning_rate": 0.00018117330140676445,
      "loss": 1.9041,
      "step": 635
    },
    {
      "epoch": 0.8559892328398385,
      "grad_norm": 0.5006618499755859,
      "learning_rate": 0.00018114337024842863,
      "loss": 1.2822,
      "step": 636
    },
    {
      "epoch": 0.857335127860027,
      "grad_norm": 0.5972815752029419,
      "learning_rate": 0.0001811134390900928,
      "loss": 1.4257,
      "step": 637
    },
    {
      "epoch": 0.8586810228802153,
      "grad_norm": 0.45843297243118286,
      "learning_rate": 0.00018108350793175697,
      "loss": 1.5516,
      "step": 638
    },
    {
      "epoch": 0.8600269179004038,
      "grad_norm": 0.5922388434410095,
      "learning_rate": 0.00018105357677342115,
      "loss": 1.2979,
      "step": 639
    },
    {
      "epoch": 0.8613728129205922,
      "grad_norm": 0.5490004420280457,
      "learning_rate": 0.0001810236456150853,
      "loss": 1.6785,
      "step": 640
    },
    {
      "epoch": 0.8627187079407806,
      "grad_norm": 0.5350499749183655,
      "learning_rate": 0.0001809937144567495,
      "loss": 1.4728,
      "step": 641
    },
    {
      "epoch": 0.8640646029609691,
      "grad_norm": 0.4933207631111145,
      "learning_rate": 0.00018096378329841365,
      "loss": 1.4596,
      "step": 642
    },
    {
      "epoch": 0.8654104979811574,
      "grad_norm": 0.5204578042030334,
      "learning_rate": 0.00018093385214007783,
      "loss": 1.5873,
      "step": 643
    },
    {
      "epoch": 0.8667563930013459,
      "grad_norm": 0.39974913001060486,
      "learning_rate": 0.00018090392098174201,
      "loss": 1.3597,
      "step": 644
    },
    {
      "epoch": 0.8681022880215343,
      "grad_norm": 0.49575576186180115,
      "learning_rate": 0.00018087398982340617,
      "loss": 1.3223,
      "step": 645
    },
    {
      "epoch": 0.8694481830417228,
      "grad_norm": 0.4586371183395386,
      "learning_rate": 0.00018084405866507035,
      "loss": 1.652,
      "step": 646
    },
    {
      "epoch": 0.8707940780619112,
      "grad_norm": 0.41737642884254456,
      "learning_rate": 0.0001808141275067345,
      "loss": 1.383,
      "step": 647
    },
    {
      "epoch": 0.8721399730820996,
      "grad_norm": 0.36707666516304016,
      "learning_rate": 0.0001807841963483987,
      "loss": 1.2659,
      "step": 648
    },
    {
      "epoch": 0.873485868102288,
      "grad_norm": 0.4135816693305969,
      "learning_rate": 0.00018075426519006287,
      "loss": 1.5872,
      "step": 649
    },
    {
      "epoch": 0.8748317631224765,
      "grad_norm": 0.49417635798454285,
      "learning_rate": 0.00018072433403172703,
      "loss": 1.3187,
      "step": 650
    },
    {
      "epoch": 0.8761776581426649,
      "grad_norm": 0.46527472138404846,
      "learning_rate": 0.0001806944028733912,
      "loss": 1.1668,
      "step": 651
    },
    {
      "epoch": 0.8775235531628532,
      "grad_norm": 0.4604685306549072,
      "learning_rate": 0.0001806644717150554,
      "loss": 1.6727,
      "step": 652
    },
    {
      "epoch": 0.8788694481830417,
      "grad_norm": 0.4818871021270752,
      "learning_rate": 0.00018063454055671955,
      "loss": 1.6298,
      "step": 653
    },
    {
      "epoch": 0.8802153432032301,
      "grad_norm": 0.48489657044410706,
      "learning_rate": 0.00018060460939838373,
      "loss": 1.3691,
      "step": 654
    },
    {
      "epoch": 0.8815612382234186,
      "grad_norm": 0.4169621169567108,
      "learning_rate": 0.0001805746782400479,
      "loss": 1.3101,
      "step": 655
    },
    {
      "epoch": 0.882907133243607,
      "grad_norm": 0.4925619661808014,
      "learning_rate": 0.00018054474708171207,
      "loss": 1.5571,
      "step": 656
    },
    {
      "epoch": 0.8842530282637954,
      "grad_norm": 0.5916274189949036,
      "learning_rate": 0.00018051481592337625,
      "loss": 1.4914,
      "step": 657
    },
    {
      "epoch": 0.8855989232839838,
      "grad_norm": 0.5345675945281982,
      "learning_rate": 0.0001804848847650404,
      "loss": 1.5495,
      "step": 658
    },
    {
      "epoch": 0.8869448183041723,
      "grad_norm": 0.4328306317329407,
      "learning_rate": 0.0001804549536067046,
      "loss": 1.5135,
      "step": 659
    },
    {
      "epoch": 0.8882907133243607,
      "grad_norm": 0.39282160997390747,
      "learning_rate": 0.00018042502244836877,
      "loss": 1.6142,
      "step": 660
    },
    {
      "epoch": 0.8896366083445492,
      "grad_norm": 0.5431445837020874,
      "learning_rate": 0.00018039509129003293,
      "loss": 1.1773,
      "step": 661
    },
    {
      "epoch": 0.8909825033647375,
      "grad_norm": 0.43568429350852966,
      "learning_rate": 0.0001803651601316971,
      "loss": 1.4358,
      "step": 662
    },
    {
      "epoch": 0.892328398384926,
      "grad_norm": 0.4574878215789795,
      "learning_rate": 0.00018033522897336127,
      "loss": 1.499,
      "step": 663
    },
    {
      "epoch": 0.8936742934051144,
      "grad_norm": 0.5012984871864319,
      "learning_rate": 0.00018030529781502545,
      "loss": 1.7015,
      "step": 664
    },
    {
      "epoch": 0.8950201884253028,
      "grad_norm": 0.43911802768707275,
      "learning_rate": 0.00018027536665668963,
      "loss": 1.4373,
      "step": 665
    },
    {
      "epoch": 0.8963660834454913,
      "grad_norm": 0.4758835732936859,
      "learning_rate": 0.0001802454354983538,
      "loss": 1.6012,
      "step": 666
    },
    {
      "epoch": 0.8977119784656796,
      "grad_norm": 0.3692780137062073,
      "learning_rate": 0.00018021550434001797,
      "loss": 1.6653,
      "step": 667
    },
    {
      "epoch": 0.8990578734858681,
      "grad_norm": 0.4894292652606964,
      "learning_rate": 0.00018018557318168215,
      "loss": 1.4431,
      "step": 668
    },
    {
      "epoch": 0.9004037685060565,
      "grad_norm": 0.4863475561141968,
      "learning_rate": 0.0001801556420233463,
      "loss": 1.169,
      "step": 669
    },
    {
      "epoch": 0.901749663526245,
      "grad_norm": 0.4378502368927002,
      "learning_rate": 0.0001801257108650105,
      "loss": 1.336,
      "step": 670
    },
    {
      "epoch": 0.9030955585464334,
      "grad_norm": 0.47859257459640503,
      "learning_rate": 0.00018009577970667465,
      "loss": 1.7389,
      "step": 671
    },
    {
      "epoch": 0.9044414535666218,
      "grad_norm": 0.5246309041976929,
      "learning_rate": 0.00018006584854833883,
      "loss": 1.4688,
      "step": 672
    },
    {
      "epoch": 0.9057873485868102,
      "grad_norm": 0.4420144259929657,
      "learning_rate": 0.00018003591739000301,
      "loss": 1.3998,
      "step": 673
    },
    {
      "epoch": 0.9071332436069987,
      "grad_norm": 0.5764909982681274,
      "learning_rate": 0.00018000598623166717,
      "loss": 1.308,
      "step": 674
    },
    {
      "epoch": 0.9084791386271871,
      "grad_norm": 0.5281131863594055,
      "learning_rate": 0.00017997605507333135,
      "loss": 1.282,
      "step": 675
    },
    {
      "epoch": 0.9098250336473755,
      "grad_norm": 0.5853833556175232,
      "learning_rate": 0.00017994612391499553,
      "loss": 1.2669,
      "step": 676
    },
    {
      "epoch": 0.9111709286675639,
      "grad_norm": 0.5367790460586548,
      "learning_rate": 0.0001799161927566597,
      "loss": 1.6696,
      "step": 677
    },
    {
      "epoch": 0.9125168236877523,
      "grad_norm": 0.434390127658844,
      "learning_rate": 0.00017988626159832387,
      "loss": 1.2771,
      "step": 678
    },
    {
      "epoch": 0.9138627187079408,
      "grad_norm": 0.4495984613895416,
      "learning_rate": 0.00017985633043998803,
      "loss": 1.5907,
      "step": 679
    },
    {
      "epoch": 0.9152086137281292,
      "grad_norm": 0.5734472274780273,
      "learning_rate": 0.0001798263992816522,
      "loss": 1.4898,
      "step": 680
    },
    {
      "epoch": 0.9165545087483177,
      "grad_norm": 0.5170580148696899,
      "learning_rate": 0.0001797964681233164,
      "loss": 1.2353,
      "step": 681
    },
    {
      "epoch": 0.917900403768506,
      "grad_norm": 0.43634504079818726,
      "learning_rate": 0.00017976653696498055,
      "loss": 1.5514,
      "step": 682
    },
    {
      "epoch": 0.9192462987886945,
      "grad_norm": 0.5829743146896362,
      "learning_rate": 0.00017973660580664473,
      "loss": 1.4056,
      "step": 683
    },
    {
      "epoch": 0.9205921938088829,
      "grad_norm": 0.4463706910610199,
      "learning_rate": 0.00017970667464830892,
      "loss": 1.7249,
      "step": 684
    },
    {
      "epoch": 0.9219380888290714,
      "grad_norm": 0.5616116523742676,
      "learning_rate": 0.00017967674348997307,
      "loss": 1.2736,
      "step": 685
    },
    {
      "epoch": 0.9232839838492598,
      "grad_norm": 0.4899437129497528,
      "learning_rate": 0.00017964681233163725,
      "loss": 1.3506,
      "step": 686
    },
    {
      "epoch": 0.9246298788694481,
      "grad_norm": 0.4451247453689575,
      "learning_rate": 0.0001796168811733014,
      "loss": 1.2949,
      "step": 687
    },
    {
      "epoch": 0.9259757738896366,
      "grad_norm": 0.46106410026550293,
      "learning_rate": 0.0001795869500149656,
      "loss": 1.3975,
      "step": 688
    },
    {
      "epoch": 0.927321668909825,
      "grad_norm": 0.46043384075164795,
      "learning_rate": 0.00017955701885662977,
      "loss": 1.3832,
      "step": 689
    },
    {
      "epoch": 0.9286675639300135,
      "grad_norm": 0.43289870023727417,
      "learning_rate": 0.00017952708769829393,
      "loss": 1.7107,
      "step": 690
    },
    {
      "epoch": 0.9300134589502019,
      "grad_norm": 0.4864335358142853,
      "learning_rate": 0.0001794971565399581,
      "loss": 1.2756,
      "step": 691
    },
    {
      "epoch": 0.9313593539703903,
      "grad_norm": 0.6950482726097107,
      "learning_rate": 0.0001794672253816223,
      "loss": 1.1968,
      "step": 692
    },
    {
      "epoch": 0.9327052489905787,
      "grad_norm": 0.40570884943008423,
      "learning_rate": 0.00017943729422328645,
      "loss": 1.4049,
      "step": 693
    },
    {
      "epoch": 0.9340511440107672,
      "grad_norm": 0.5183216333389282,
      "learning_rate": 0.00017940736306495063,
      "loss": 1.3584,
      "step": 694
    },
    {
      "epoch": 0.9353970390309556,
      "grad_norm": 0.5654231905937195,
      "learning_rate": 0.0001793774319066148,
      "loss": 1.4523,
      "step": 695
    },
    {
      "epoch": 0.9367429340511441,
      "grad_norm": 0.44605541229248047,
      "learning_rate": 0.00017934750074827897,
      "loss": 1.3575,
      "step": 696
    },
    {
      "epoch": 0.9380888290713324,
      "grad_norm": 0.47505807876586914,
      "learning_rate": 0.00017931756958994315,
      "loss": 1.6387,
      "step": 697
    },
    {
      "epoch": 0.9394347240915208,
      "grad_norm": 0.4488275349140167,
      "learning_rate": 0.0001792876384316073,
      "loss": 1.3686,
      "step": 698
    },
    {
      "epoch": 0.9407806191117093,
      "grad_norm": 0.43443140387535095,
      "learning_rate": 0.0001792577072732715,
      "loss": 1.5744,
      "step": 699
    },
    {
      "epoch": 0.9421265141318977,
      "grad_norm": 0.4680863618850708,
      "learning_rate": 0.00017922777611493568,
      "loss": 1.7237,
      "step": 700
    },
    {
      "epoch": 0.9434724091520862,
      "grad_norm": 0.49813470244407654,
      "learning_rate": 0.00017919784495659983,
      "loss": 1.5607,
      "step": 701
    },
    {
      "epoch": 0.9448183041722745,
      "grad_norm": 0.5098064541816711,
      "learning_rate": 0.00017916791379826401,
      "loss": 1.5221,
      "step": 702
    },
    {
      "epoch": 0.946164199192463,
      "grad_norm": 0.5080711245536804,
      "learning_rate": 0.00017913798263992817,
      "loss": 1.3352,
      "step": 703
    },
    {
      "epoch": 0.9475100942126514,
      "grad_norm": 0.48359835147857666,
      "learning_rate": 0.00017910805148159235,
      "loss": 1.4944,
      "step": 704
    },
    {
      "epoch": 0.9488559892328399,
      "grad_norm": 0.523952305316925,
      "learning_rate": 0.00017907812032325653,
      "loss": 1.2537,
      "step": 705
    },
    {
      "epoch": 0.9502018842530283,
      "grad_norm": 0.44025087356567383,
      "learning_rate": 0.0001790481891649207,
      "loss": 1.3836,
      "step": 706
    },
    {
      "epoch": 0.9515477792732167,
      "grad_norm": 0.3681611716747284,
      "learning_rate": 0.00017901825800658487,
      "loss": 1.954,
      "step": 707
    },
    {
      "epoch": 0.9528936742934051,
      "grad_norm": 0.4256681501865387,
      "learning_rate": 0.00017898832684824906,
      "loss": 1.2425,
      "step": 708
    },
    {
      "epoch": 0.9542395693135935,
      "grad_norm": 0.5458763837814331,
      "learning_rate": 0.0001789583956899132,
      "loss": 1.4806,
      "step": 709
    },
    {
      "epoch": 0.955585464333782,
      "grad_norm": 0.4272420108318329,
      "learning_rate": 0.0001789284645315774,
      "loss": 1.6599,
      "step": 710
    },
    {
      "epoch": 0.9569313593539704,
      "grad_norm": 0.49962857365608215,
      "learning_rate": 0.00017889853337324155,
      "loss": 1.4516,
      "step": 711
    },
    {
      "epoch": 0.9582772543741588,
      "grad_norm": 0.39092525839805603,
      "learning_rate": 0.00017886860221490573,
      "loss": 1.6046,
      "step": 712
    },
    {
      "epoch": 0.9596231493943472,
      "grad_norm": 0.42909613251686096,
      "learning_rate": 0.00017883867105656992,
      "loss": 1.4129,
      "step": 713
    },
    {
      "epoch": 0.9609690444145357,
      "grad_norm": 0.4870665967464447,
      "learning_rate": 0.00017880873989823407,
      "loss": 1.7517,
      "step": 714
    },
    {
      "epoch": 0.9623149394347241,
      "grad_norm": 0.44375860691070557,
      "learning_rate": 0.00017877880873989825,
      "loss": 1.4516,
      "step": 715
    },
    {
      "epoch": 0.9636608344549125,
      "grad_norm": 0.45294174551963806,
      "learning_rate": 0.00017874887758156244,
      "loss": 1.1383,
      "step": 716
    },
    {
      "epoch": 0.9650067294751009,
      "grad_norm": 0.4544730484485626,
      "learning_rate": 0.0001787189464232266,
      "loss": 1.2909,
      "step": 717
    },
    {
      "epoch": 0.9663526244952894,
      "grad_norm": 0.5035930275917053,
      "learning_rate": 0.00017868901526489077,
      "loss": 1.4655,
      "step": 718
    },
    {
      "epoch": 0.9676985195154778,
      "grad_norm": 0.4792298376560211,
      "learning_rate": 0.00017865908410655493,
      "loss": 1.5858,
      "step": 719
    },
    {
      "epoch": 0.9690444145356663,
      "grad_norm": 0.39034420251846313,
      "learning_rate": 0.0001786291529482191,
      "loss": 1.9424,
      "step": 720
    },
    {
      "epoch": 0.9703903095558546,
      "grad_norm": 0.4636358618736267,
      "learning_rate": 0.0001785992217898833,
      "loss": 1.5504,
      "step": 721
    },
    {
      "epoch": 0.971736204576043,
      "grad_norm": 0.42716264724731445,
      "learning_rate": 0.00017856929063154745,
      "loss": 1.3979,
      "step": 722
    },
    {
      "epoch": 0.9730820995962315,
      "grad_norm": 0.49607357382774353,
      "learning_rate": 0.00017853935947321163,
      "loss": 1.428,
      "step": 723
    },
    {
      "epoch": 0.9744279946164199,
      "grad_norm": 0.5159328579902649,
      "learning_rate": 0.00017850942831487582,
      "loss": 1.3005,
      "step": 724
    },
    {
      "epoch": 0.9757738896366084,
      "grad_norm": 0.34367644786834717,
      "learning_rate": 0.00017847949715653997,
      "loss": 1.3062,
      "step": 725
    },
    {
      "epoch": 0.9771197846567967,
      "grad_norm": 0.47954246401786804,
      "learning_rate": 0.00017844956599820415,
      "loss": 1.4871,
      "step": 726
    },
    {
      "epoch": 0.9784656796769852,
      "grad_norm": 0.49504515528678894,
      "learning_rate": 0.0001784196348398683,
      "loss": 1.3725,
      "step": 727
    },
    {
      "epoch": 0.9798115746971736,
      "grad_norm": 0.4093569219112396,
      "learning_rate": 0.0001783897036815325,
      "loss": 1.3079,
      "step": 728
    },
    {
      "epoch": 0.9811574697173621,
      "grad_norm": 0.4748086631298065,
      "learning_rate": 0.00017835977252319668,
      "loss": 1.4502,
      "step": 729
    },
    {
      "epoch": 0.9825033647375505,
      "grad_norm": 0.4437590539455414,
      "learning_rate": 0.00017832984136486083,
      "loss": 1.5763,
      "step": 730
    },
    {
      "epoch": 0.9838492597577388,
      "grad_norm": 0.48980191349983215,
      "learning_rate": 0.00017829991020652501,
      "loss": 1.2788,
      "step": 731
    },
    {
      "epoch": 0.9851951547779273,
      "grad_norm": 0.44685783982276917,
      "learning_rate": 0.0001782699790481892,
      "loss": 1.5213,
      "step": 732
    },
    {
      "epoch": 0.9865410497981157,
      "grad_norm": 0.49555742740631104,
      "learning_rate": 0.00017824004788985335,
      "loss": 1.3336,
      "step": 733
    },
    {
      "epoch": 0.9878869448183042,
      "grad_norm": 0.4104944169521332,
      "learning_rate": 0.00017821011673151754,
      "loss": 1.2963,
      "step": 734
    },
    {
      "epoch": 0.9892328398384926,
      "grad_norm": 0.6089917421340942,
      "learning_rate": 0.0001781801855731817,
      "loss": 1.2097,
      "step": 735
    },
    {
      "epoch": 0.990578734858681,
      "grad_norm": 0.4667544662952423,
      "learning_rate": 0.00017815025441484587,
      "loss": 1.3966,
      "step": 736
    },
    {
      "epoch": 0.9919246298788694,
      "grad_norm": 0.4014158844947815,
      "learning_rate": 0.00017812032325651003,
      "loss": 1.4755,
      "step": 737
    },
    {
      "epoch": 0.9932705248990579,
      "grad_norm": 0.4196886122226715,
      "learning_rate": 0.00017809039209817418,
      "loss": 1.2498,
      "step": 738
    },
    {
      "epoch": 0.9946164199192463,
      "grad_norm": 0.46307310461997986,
      "learning_rate": 0.00017806046093983837,
      "loss": 1.6982,
      "step": 739
    },
    {
      "epoch": 0.9959623149394348,
      "grad_norm": 0.5075513124465942,
      "learning_rate": 0.00017803052978150255,
      "loss": 1.8731,
      "step": 740
    },
    {
      "epoch": 0.9973082099596231,
      "grad_norm": 0.439258873462677,
      "learning_rate": 0.0001780005986231667,
      "loss": 1.3893,
      "step": 741
    },
    {
      "epoch": 0.9986541049798116,
      "grad_norm": 0.45417505502700806,
      "learning_rate": 0.0001779706674648309,
      "loss": 1.5489,
      "step": 742
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.451631098985672,
      "learning_rate": 0.00017794073630649507,
      "loss": 1.4542,
      "step": 743
    },
    {
      "epoch": 1.0013458950201883,
      "grad_norm": 0.40290939807891846,
      "learning_rate": 0.00017791080514815923,
      "loss": 1.4675,
      "step": 744
    },
    {
      "epoch": 1.002691790040377,
      "grad_norm": 0.5743503570556641,
      "learning_rate": 0.0001778808739898234,
      "loss": 1.2877,
      "step": 745
    },
    {
      "epoch": 1.0040376850605652,
      "grad_norm": 0.39423537254333496,
      "learning_rate": 0.00017785094283148756,
      "loss": 1.4216,
      "step": 746
    },
    {
      "epoch": 1.0053835800807538,
      "grad_norm": 0.5101011395454407,
      "learning_rate": 0.00017782101167315175,
      "loss": 1.3311,
      "step": 747
    },
    {
      "epoch": 1.0067294751009421,
      "grad_norm": 0.48270702362060547,
      "learning_rate": 0.00017779108051481593,
      "loss": 1.1345,
      "step": 748
    },
    {
      "epoch": 1.0080753701211305,
      "grad_norm": 0.3954140543937683,
      "learning_rate": 0.00017776114935648009,
      "loss": 1.1287,
      "step": 749
    },
    {
      "epoch": 1.009421265141319,
      "grad_norm": 0.5452514290809631,
      "learning_rate": 0.00017773121819814427,
      "loss": 1.1837,
      "step": 750
    },
    {
      "epoch": 1.0107671601615074,
      "grad_norm": 0.3841758668422699,
      "learning_rate": 0.00017770128703980845,
      "loss": 1.2011,
      "step": 751
    },
    {
      "epoch": 1.012113055181696,
      "grad_norm": 0.5079122185707092,
      "learning_rate": 0.0001776713558814726,
      "loss": 1.3424,
      "step": 752
    },
    {
      "epoch": 1.0134589502018843,
      "grad_norm": 0.41855672001838684,
      "learning_rate": 0.0001776414247231368,
      "loss": 1.2397,
      "step": 753
    },
    {
      "epoch": 1.0148048452220726,
      "grad_norm": 0.5569015741348267,
      "learning_rate": 0.00017761149356480095,
      "loss": 1.1397,
      "step": 754
    },
    {
      "epoch": 1.0161507402422612,
      "grad_norm": 0.5208772420883179,
      "learning_rate": 0.00017758156240646513,
      "loss": 1.3231,
      "step": 755
    },
    {
      "epoch": 1.0174966352624495,
      "grad_norm": 0.4810003936290741,
      "learning_rate": 0.0001775516312481293,
      "loss": 1.0605,
      "step": 756
    },
    {
      "epoch": 1.018842530282638,
      "grad_norm": 0.4740264117717743,
      "learning_rate": 0.00017752170008979347,
      "loss": 1.4385,
      "step": 757
    },
    {
      "epoch": 1.0201884253028264,
      "grad_norm": 0.545818030834198,
      "learning_rate": 0.00017749176893145765,
      "loss": 1.2032,
      "step": 758
    },
    {
      "epoch": 1.0215343203230147,
      "grad_norm": 0.4648980498313904,
      "learning_rate": 0.00017746183777312183,
      "loss": 1.2942,
      "step": 759
    },
    {
      "epoch": 1.0228802153432033,
      "grad_norm": 0.5538685917854309,
      "learning_rate": 0.000177431906614786,
      "loss": 1.3548,
      "step": 760
    },
    {
      "epoch": 1.0242261103633916,
      "grad_norm": 0.582592248916626,
      "learning_rate": 0.00017740197545645017,
      "loss": 1.3531,
      "step": 761
    },
    {
      "epoch": 1.0255720053835802,
      "grad_norm": 0.5134252905845642,
      "learning_rate": 0.00017737204429811433,
      "loss": 1.2186,
      "step": 762
    },
    {
      "epoch": 1.0269179004037685,
      "grad_norm": 0.45787596702575684,
      "learning_rate": 0.0001773421131397785,
      "loss": 1.2383,
      "step": 763
    },
    {
      "epoch": 1.0282637954239569,
      "grad_norm": 0.6655726432800293,
      "learning_rate": 0.0001773121819814427,
      "loss": 1.3434,
      "step": 764
    },
    {
      "epoch": 1.0296096904441454,
      "grad_norm": 0.6157622337341309,
      "learning_rate": 0.00017728225082310685,
      "loss": 1.3918,
      "step": 765
    },
    {
      "epoch": 1.0309555854643337,
      "grad_norm": 0.471264511346817,
      "learning_rate": 0.00017725231966477103,
      "loss": 1.1415,
      "step": 766
    },
    {
      "epoch": 1.0323014804845223,
      "grad_norm": 0.5433130264282227,
      "learning_rate": 0.0001772223885064352,
      "loss": 0.9313,
      "step": 767
    },
    {
      "epoch": 1.0336473755047106,
      "grad_norm": 0.6086214184761047,
      "learning_rate": 0.00017719245734809937,
      "loss": 1.2656,
      "step": 768
    },
    {
      "epoch": 1.034993270524899,
      "grad_norm": 0.5574278235435486,
      "learning_rate": 0.00017716252618976355,
      "loss": 0.9979,
      "step": 769
    },
    {
      "epoch": 1.0363391655450875,
      "grad_norm": 0.6585392355918884,
      "learning_rate": 0.0001771325950314277,
      "loss": 1.1145,
      "step": 770
    },
    {
      "epoch": 1.0376850605652759,
      "grad_norm": 0.5416412949562073,
      "learning_rate": 0.0001771026638730919,
      "loss": 1.1513,
      "step": 771
    },
    {
      "epoch": 1.0390309555854644,
      "grad_norm": 0.5314515233039856,
      "learning_rate": 0.00017707273271475607,
      "loss": 1.2109,
      "step": 772
    },
    {
      "epoch": 1.0403768506056528,
      "grad_norm": 0.5869408845901489,
      "learning_rate": 0.00017704280155642023,
      "loss": 1.0594,
      "step": 773
    },
    {
      "epoch": 1.041722745625841,
      "grad_norm": 0.6019539833068848,
      "learning_rate": 0.0001770128703980844,
      "loss": 1.2036,
      "step": 774
    },
    {
      "epoch": 1.0430686406460297,
      "grad_norm": 0.5992686748504639,
      "learning_rate": 0.0001769829392397486,
      "loss": 1.4347,
      "step": 775
    },
    {
      "epoch": 1.044414535666218,
      "grad_norm": 0.5243468284606934,
      "learning_rate": 0.00017695300808141275,
      "loss": 1.1075,
      "step": 776
    },
    {
      "epoch": 1.0457604306864066,
      "grad_norm": 0.5623542666435242,
      "learning_rate": 0.00017692307692307693,
      "loss": 1.3309,
      "step": 777
    },
    {
      "epoch": 1.047106325706595,
      "grad_norm": 0.6427353024482727,
      "learning_rate": 0.00017689314576474109,
      "loss": 1.2485,
      "step": 778
    },
    {
      "epoch": 1.0484522207267832,
      "grad_norm": 0.5801008343696594,
      "learning_rate": 0.00017686321460640527,
      "loss": 0.9686,
      "step": 779
    },
    {
      "epoch": 1.0497981157469718,
      "grad_norm": 0.6479398012161255,
      "learning_rate": 0.00017683328344806945,
      "loss": 1.1417,
      "step": 780
    },
    {
      "epoch": 1.0511440107671601,
      "grad_norm": 0.5736455321311951,
      "learning_rate": 0.0001768033522897336,
      "loss": 1.2362,
      "step": 781
    },
    {
      "epoch": 1.0524899057873487,
      "grad_norm": 0.5996057391166687,
      "learning_rate": 0.0001767734211313978,
      "loss": 1.1905,
      "step": 782
    },
    {
      "epoch": 1.053835800807537,
      "grad_norm": 0.5676808953285217,
      "learning_rate": 0.00017674348997306197,
      "loss": 1.3207,
      "step": 783
    },
    {
      "epoch": 1.0551816958277254,
      "grad_norm": 0.5977170467376709,
      "learning_rate": 0.00017671355881472613,
      "loss": 1.1527,
      "step": 784
    },
    {
      "epoch": 1.056527590847914,
      "grad_norm": 0.5799267888069153,
      "learning_rate": 0.0001766836276563903,
      "loss": 1.2886,
      "step": 785
    },
    {
      "epoch": 1.0578734858681023,
      "grad_norm": 0.5824089646339417,
      "learning_rate": 0.00017665369649805447,
      "loss": 1.1221,
      "step": 786
    },
    {
      "epoch": 1.0592193808882908,
      "grad_norm": 0.7310004234313965,
      "learning_rate": 0.00017662376533971865,
      "loss": 1.0068,
      "step": 787
    },
    {
      "epoch": 1.0605652759084792,
      "grad_norm": 0.5657049417495728,
      "learning_rate": 0.00017659383418138283,
      "loss": 1.1393,
      "step": 788
    },
    {
      "epoch": 1.0619111709286675,
      "grad_norm": 0.7638142108917236,
      "learning_rate": 0.000176563903023047,
      "loss": 1.3128,
      "step": 789
    },
    {
      "epoch": 1.063257065948856,
      "grad_norm": 0.522049605846405,
      "learning_rate": 0.00017653397186471117,
      "loss": 1.6604,
      "step": 790
    },
    {
      "epoch": 1.0646029609690444,
      "grad_norm": 0.5847139358520508,
      "learning_rate": 0.00017650404070637535,
      "loss": 1.4703,
      "step": 791
    },
    {
      "epoch": 1.065948855989233,
      "grad_norm": 0.6464868187904358,
      "learning_rate": 0.0001764741095480395,
      "loss": 1.3381,
      "step": 792
    },
    {
      "epoch": 1.0672947510094213,
      "grad_norm": 0.5898944139480591,
      "learning_rate": 0.0001764441783897037,
      "loss": 1.0198,
      "step": 793
    },
    {
      "epoch": 1.0686406460296096,
      "grad_norm": 0.4753415286540985,
      "learning_rate": 0.00017641424723136785,
      "loss": 1.617,
      "step": 794
    },
    {
      "epoch": 1.0699865410497982,
      "grad_norm": 0.706257700920105,
      "learning_rate": 0.00017638431607303203,
      "loss": 1.1198,
      "step": 795
    },
    {
      "epoch": 1.0713324360699865,
      "grad_norm": 0.6294623613357544,
      "learning_rate": 0.0001763543849146962,
      "loss": 1.3281,
      "step": 796
    },
    {
      "epoch": 1.072678331090175,
      "grad_norm": 0.5517480373382568,
      "learning_rate": 0.00017632445375636037,
      "loss": 1.1562,
      "step": 797
    },
    {
      "epoch": 1.0740242261103634,
      "grad_norm": 0.4996050000190735,
      "learning_rate": 0.00017629452259802455,
      "loss": 1.2553,
      "step": 798
    },
    {
      "epoch": 1.0753701211305517,
      "grad_norm": 0.6281035542488098,
      "learning_rate": 0.00017626459143968873,
      "loss": 1.4513,
      "step": 799
    },
    {
      "epoch": 1.0767160161507403,
      "grad_norm": 0.7280247807502747,
      "learning_rate": 0.0001762346602813529,
      "loss": 1.5175,
      "step": 800
    },
    {
      "epoch": 1.0780619111709286,
      "grad_norm": 0.4877373278141022,
      "learning_rate": 0.00017620472912301707,
      "loss": 1.1996,
      "step": 801
    },
    {
      "epoch": 1.0794078061911172,
      "grad_norm": 0.6660406589508057,
      "learning_rate": 0.00017617479796468123,
      "loss": 1.1918,
      "step": 802
    },
    {
      "epoch": 1.0807537012113055,
      "grad_norm": 0.5695550441741943,
      "learning_rate": 0.0001761448668063454,
      "loss": 1.1742,
      "step": 803
    },
    {
      "epoch": 1.0820995962314939,
      "grad_norm": 0.5430701971054077,
      "learning_rate": 0.0001761149356480096,
      "loss": 1.1016,
      "step": 804
    },
    {
      "epoch": 1.0834454912516824,
      "grad_norm": 0.5822998285293579,
      "learning_rate": 0.00017608500448967375,
      "loss": 1.2752,
      "step": 805
    },
    {
      "epoch": 1.0847913862718708,
      "grad_norm": 0.6003591418266296,
      "learning_rate": 0.00017605507333133793,
      "loss": 1.2489,
      "step": 806
    },
    {
      "epoch": 1.0861372812920593,
      "grad_norm": 0.6111974716186523,
      "learning_rate": 0.0001760251421730021,
      "loss": 1.3854,
      "step": 807
    },
    {
      "epoch": 1.0874831763122477,
      "grad_norm": 0.5433512926101685,
      "learning_rate": 0.00017599521101466627,
      "loss": 1.4978,
      "step": 808
    },
    {
      "epoch": 1.088829071332436,
      "grad_norm": 0.5409501194953918,
      "learning_rate": 0.00017596527985633045,
      "loss": 1.1845,
      "step": 809
    },
    {
      "epoch": 1.0901749663526246,
      "grad_norm": 0.530272364616394,
      "learning_rate": 0.0001759353486979946,
      "loss": 1.2162,
      "step": 810
    },
    {
      "epoch": 1.091520861372813,
      "grad_norm": 0.6430871486663818,
      "learning_rate": 0.0001759054175396588,
      "loss": 1.42,
      "step": 811
    },
    {
      "epoch": 1.0928667563930015,
      "grad_norm": 0.617460310459137,
      "learning_rate": 0.00017587548638132297,
      "loss": 1.4413,
      "step": 812
    },
    {
      "epoch": 1.0942126514131898,
      "grad_norm": 0.8142316341400146,
      "learning_rate": 0.00017584555522298713,
      "loss": 1.1665,
      "step": 813
    },
    {
      "epoch": 1.0955585464333781,
      "grad_norm": 0.631626307964325,
      "learning_rate": 0.0001758156240646513,
      "loss": 1.3887,
      "step": 814
    },
    {
      "epoch": 1.0969044414535667,
      "grad_norm": 0.5367962718009949,
      "learning_rate": 0.00017578569290631547,
      "loss": 1.3178,
      "step": 815
    },
    {
      "epoch": 1.098250336473755,
      "grad_norm": 0.47874289751052856,
      "learning_rate": 0.00017575576174797965,
      "loss": 1.3575,
      "step": 816
    },
    {
      "epoch": 1.0995962314939436,
      "grad_norm": 0.5974141955375671,
      "learning_rate": 0.00017572583058964383,
      "loss": 1.1676,
      "step": 817
    },
    {
      "epoch": 1.100942126514132,
      "grad_norm": 0.5842007398605347,
      "learning_rate": 0.000175695899431308,
      "loss": 1.1178,
      "step": 818
    },
    {
      "epoch": 1.1022880215343203,
      "grad_norm": 0.7755333185195923,
      "learning_rate": 0.00017566596827297217,
      "loss": 1.2781,
      "step": 819
    },
    {
      "epoch": 1.1036339165545088,
      "grad_norm": 0.58973628282547,
      "learning_rate": 0.00017563603711463635,
      "loss": 1.4756,
      "step": 820
    },
    {
      "epoch": 1.1049798115746972,
      "grad_norm": 0.5868828296661377,
      "learning_rate": 0.0001756061059563005,
      "loss": 1.2477,
      "step": 821
    },
    {
      "epoch": 1.1063257065948857,
      "grad_norm": 0.6162770390510559,
      "learning_rate": 0.0001755761747979647,
      "loss": 1.4108,
      "step": 822
    },
    {
      "epoch": 1.107671601615074,
      "grad_norm": 0.4786010682582855,
      "learning_rate": 0.00017554624363962885,
      "loss": 1.6445,
      "step": 823
    },
    {
      "epoch": 1.1090174966352624,
      "grad_norm": 0.48372575640678406,
      "learning_rate": 0.00017551631248129303,
      "loss": 1.3803,
      "step": 824
    },
    {
      "epoch": 1.110363391655451,
      "grad_norm": 0.5306980609893799,
      "learning_rate": 0.0001754863813229572,
      "loss": 1.348,
      "step": 825
    },
    {
      "epoch": 1.1117092866756393,
      "grad_norm": 0.5626469850540161,
      "learning_rate": 0.00017545645016462137,
      "loss": 1.2922,
      "step": 826
    },
    {
      "epoch": 1.1130551816958276,
      "grad_norm": 0.6416095495223999,
      "learning_rate": 0.00017542651900628555,
      "loss": 1.3615,
      "step": 827
    },
    {
      "epoch": 1.1144010767160162,
      "grad_norm": 0.633536696434021,
      "learning_rate": 0.00017539658784794973,
      "loss": 1.0918,
      "step": 828
    },
    {
      "epoch": 1.1157469717362045,
      "grad_norm": 0.4775666296482086,
      "learning_rate": 0.0001753666566896139,
      "loss": 1.6655,
      "step": 829
    },
    {
      "epoch": 1.117092866756393,
      "grad_norm": 0.658600926399231,
      "learning_rate": 0.00017533672553127807,
      "loss": 1.3203,
      "step": 830
    },
    {
      "epoch": 1.1184387617765814,
      "grad_norm": 0.6397086977958679,
      "learning_rate": 0.00017530679437294223,
      "loss": 1.5262,
      "step": 831
    },
    {
      "epoch": 1.1197846567967698,
      "grad_norm": 0.5367344617843628,
      "learning_rate": 0.0001752768632146064,
      "loss": 1.2518,
      "step": 832
    },
    {
      "epoch": 1.1211305518169583,
      "grad_norm": 0.6165607571601868,
      "learning_rate": 0.0001752469320562706,
      "loss": 1.4914,
      "step": 833
    },
    {
      "epoch": 1.1224764468371466,
      "grad_norm": 0.5483354926109314,
      "learning_rate": 0.00017521700089793475,
      "loss": 1.2158,
      "step": 834
    },
    {
      "epoch": 1.1238223418573352,
      "grad_norm": 0.4923606812953949,
      "learning_rate": 0.00017518706973959893,
      "loss": 1.1786,
      "step": 835
    },
    {
      "epoch": 1.1251682368775235,
      "grad_norm": 0.7126825451850891,
      "learning_rate": 0.00017515713858126311,
      "loss": 1.358,
      "step": 836
    },
    {
      "epoch": 1.126514131897712,
      "grad_norm": 0.659583568572998,
      "learning_rate": 0.00017512720742292727,
      "loss": 1.1327,
      "step": 837
    },
    {
      "epoch": 1.1278600269179004,
      "grad_norm": 0.728690505027771,
      "learning_rate": 0.00017509727626459145,
      "loss": 1.0685,
      "step": 838
    },
    {
      "epoch": 1.1292059219380888,
      "grad_norm": 0.6510341763496399,
      "learning_rate": 0.0001750673451062556,
      "loss": 1.3413,
      "step": 839
    },
    {
      "epoch": 1.1305518169582773,
      "grad_norm": 0.723155677318573,
      "learning_rate": 0.0001750374139479198,
      "loss": 1.3363,
      "step": 840
    },
    {
      "epoch": 1.1318977119784657,
      "grad_norm": 0.6150811314582825,
      "learning_rate": 0.00017500748278958397,
      "loss": 1.1034,
      "step": 841
    },
    {
      "epoch": 1.1332436069986542,
      "grad_norm": 0.6775942444801331,
      "learning_rate": 0.00017497755163124813,
      "loss": 1.2958,
      "step": 842
    },
    {
      "epoch": 1.1345895020188426,
      "grad_norm": 0.5130316615104675,
      "learning_rate": 0.0001749476204729123,
      "loss": 1.3252,
      "step": 843
    },
    {
      "epoch": 1.135935397039031,
      "grad_norm": 0.6058533787727356,
      "learning_rate": 0.0001749176893145765,
      "loss": 1.0227,
      "step": 844
    },
    {
      "epoch": 1.1372812920592195,
      "grad_norm": 0.8055586218833923,
      "learning_rate": 0.00017488775815624065,
      "loss": 1.6649,
      "step": 845
    },
    {
      "epoch": 1.1386271870794078,
      "grad_norm": 0.5601949095726013,
      "learning_rate": 0.00017485782699790483,
      "loss": 1.0286,
      "step": 846
    },
    {
      "epoch": 1.1399730820995961,
      "grad_norm": 0.6504278779029846,
      "learning_rate": 0.000174827895839569,
      "loss": 1.0466,
      "step": 847
    },
    {
      "epoch": 1.1413189771197847,
      "grad_norm": 0.5148431062698364,
      "learning_rate": 0.00017479796468123317,
      "loss": 1.3704,
      "step": 848
    },
    {
      "epoch": 1.142664872139973,
      "grad_norm": 0.7616201639175415,
      "learning_rate": 0.00017476803352289735,
      "loss": 1.4987,
      "step": 849
    },
    {
      "epoch": 1.1440107671601616,
      "grad_norm": 0.6025723218917847,
      "learning_rate": 0.0001747381023645615,
      "loss": 1.2538,
      "step": 850
    },
    {
      "epoch": 1.14535666218035,
      "grad_norm": 0.6774541735649109,
      "learning_rate": 0.0001747081712062257,
      "loss": 1.1193,
      "step": 851
    },
    {
      "epoch": 1.1467025572005383,
      "grad_norm": 0.6010856032371521,
      "learning_rate": 0.00017467824004788987,
      "loss": 1.1463,
      "step": 852
    },
    {
      "epoch": 1.1480484522207268,
      "grad_norm": 0.6609386801719666,
      "learning_rate": 0.00017464830888955403,
      "loss": 1.2023,
      "step": 853
    },
    {
      "epoch": 1.1493943472409152,
      "grad_norm": 0.558402955532074,
      "learning_rate": 0.0001746183777312182,
      "loss": 1.1461,
      "step": 854
    },
    {
      "epoch": 1.1507402422611037,
      "grad_norm": 0.4893462061882019,
      "learning_rate": 0.00017458844657288237,
      "loss": 1.0283,
      "step": 855
    },
    {
      "epoch": 1.152086137281292,
      "grad_norm": 0.6318915486335754,
      "learning_rate": 0.00017455851541454655,
      "loss": 1.3485,
      "step": 856
    },
    {
      "epoch": 1.1534320323014804,
      "grad_norm": 0.7598322629928589,
      "learning_rate": 0.00017452858425621073,
      "loss": 1.3816,
      "step": 857
    },
    {
      "epoch": 1.154777927321669,
      "grad_norm": 0.6796474456787109,
      "learning_rate": 0.0001744986530978749,
      "loss": 1.2248,
      "step": 858
    },
    {
      "epoch": 1.1561238223418573,
      "grad_norm": 0.5673767924308777,
      "learning_rate": 0.00017446872193953907,
      "loss": 1.593,
      "step": 859
    },
    {
      "epoch": 1.1574697173620458,
      "grad_norm": 0.6150751113891602,
      "learning_rate": 0.00017443879078120325,
      "loss": 1.0466,
      "step": 860
    },
    {
      "epoch": 1.1588156123822342,
      "grad_norm": 0.6139540076255798,
      "learning_rate": 0.0001744088596228674,
      "loss": 1.2509,
      "step": 861
    },
    {
      "epoch": 1.1601615074024225,
      "grad_norm": 0.6236910820007324,
      "learning_rate": 0.0001743789284645316,
      "loss": 1.446,
      "step": 862
    },
    {
      "epoch": 1.161507402422611,
      "grad_norm": 0.7931322455406189,
      "learning_rate": 0.00017434899730619575,
      "loss": 1.3112,
      "step": 863
    },
    {
      "epoch": 1.1628532974427994,
      "grad_norm": 0.47115740180015564,
      "learning_rate": 0.00017431906614785993,
      "loss": 1.3736,
      "step": 864
    },
    {
      "epoch": 1.164199192462988,
      "grad_norm": 0.6298012137413025,
      "learning_rate": 0.00017428913498952411,
      "loss": 1.2775,
      "step": 865
    },
    {
      "epoch": 1.1655450874831763,
      "grad_norm": 0.5340365767478943,
      "learning_rate": 0.00017425920383118827,
      "loss": 1.354,
      "step": 866
    },
    {
      "epoch": 1.1668909825033646,
      "grad_norm": 0.7377905249595642,
      "learning_rate": 0.00017422927267285245,
      "loss": 1.1657,
      "step": 867
    },
    {
      "epoch": 1.1682368775235532,
      "grad_norm": 0.5303571820259094,
      "learning_rate": 0.00017419934151451663,
      "loss": 1.4935,
      "step": 868
    },
    {
      "epoch": 1.1695827725437415,
      "grad_norm": 0.7320892214775085,
      "learning_rate": 0.0001741694103561808,
      "loss": 1.3044,
      "step": 869
    },
    {
      "epoch": 1.17092866756393,
      "grad_norm": 0.5963239073753357,
      "learning_rate": 0.00017413947919784497,
      "loss": 1.2124,
      "step": 870
    },
    {
      "epoch": 1.1722745625841184,
      "grad_norm": 0.5837951898574829,
      "learning_rate": 0.00017410954803950913,
      "loss": 1.5379,
      "step": 871
    },
    {
      "epoch": 1.1736204576043068,
      "grad_norm": 0.6260203123092651,
      "learning_rate": 0.0001740796168811733,
      "loss": 1.366,
      "step": 872
    },
    {
      "epoch": 1.1749663526244953,
      "grad_norm": 0.6370972394943237,
      "learning_rate": 0.0001740496857228375,
      "loss": 1.2157,
      "step": 873
    },
    {
      "epoch": 1.1763122476446837,
      "grad_norm": 0.47696420550346375,
      "learning_rate": 0.00017401975456450165,
      "loss": 1.4738,
      "step": 874
    },
    {
      "epoch": 1.1776581426648722,
      "grad_norm": 0.6062101125717163,
      "learning_rate": 0.00017398982340616583,
      "loss": 1.0989,
      "step": 875
    },
    {
      "epoch": 1.1790040376850606,
      "grad_norm": 0.5589105486869812,
      "learning_rate": 0.00017395989224783002,
      "loss": 1.1148,
      "step": 876
    },
    {
      "epoch": 1.180349932705249,
      "grad_norm": 0.599806547164917,
      "learning_rate": 0.00017392996108949417,
      "loss": 1.17,
      "step": 877
    },
    {
      "epoch": 1.1816958277254375,
      "grad_norm": 0.6365493535995483,
      "learning_rate": 0.00017390002993115835,
      "loss": 1.1901,
      "step": 878
    },
    {
      "epoch": 1.1830417227456258,
      "grad_norm": 0.6008426547050476,
      "learning_rate": 0.0001738700987728225,
      "loss": 1.2276,
      "step": 879
    },
    {
      "epoch": 1.1843876177658144,
      "grad_norm": 0.6209347248077393,
      "learning_rate": 0.0001738401676144867,
      "loss": 1.1571,
      "step": 880
    },
    {
      "epoch": 1.1857335127860027,
      "grad_norm": 0.7523049116134644,
      "learning_rate": 0.00017381023645615087,
      "loss": 1.3016,
      "step": 881
    },
    {
      "epoch": 1.187079407806191,
      "grad_norm": 0.6905624866485596,
      "learning_rate": 0.00017378030529781503,
      "loss": 1.2447,
      "step": 882
    },
    {
      "epoch": 1.1884253028263796,
      "grad_norm": 0.5919517874717712,
      "learning_rate": 0.0001737503741394792,
      "loss": 1.3024,
      "step": 883
    },
    {
      "epoch": 1.189771197846568,
      "grad_norm": 0.5593265891075134,
      "learning_rate": 0.0001737204429811434,
      "loss": 1.3052,
      "step": 884
    },
    {
      "epoch": 1.1911170928667565,
      "grad_norm": 0.584956169128418,
      "learning_rate": 0.00017369051182280755,
      "loss": 1.4853,
      "step": 885
    },
    {
      "epoch": 1.1924629878869448,
      "grad_norm": 0.6485495567321777,
      "learning_rate": 0.00017366058066447173,
      "loss": 1.1902,
      "step": 886
    },
    {
      "epoch": 1.1938088829071332,
      "grad_norm": 0.5937191247940063,
      "learning_rate": 0.0001736306495061359,
      "loss": 1.1094,
      "step": 887
    },
    {
      "epoch": 1.1951547779273217,
      "grad_norm": 0.6755240559577942,
      "learning_rate": 0.00017360071834780007,
      "loss": 1.3073,
      "step": 888
    },
    {
      "epoch": 1.19650067294751,
      "grad_norm": 0.5491424202919006,
      "learning_rate": 0.00017357078718946425,
      "loss": 1.1268,
      "step": 889
    },
    {
      "epoch": 1.1978465679676986,
      "grad_norm": 0.5892201066017151,
      "learning_rate": 0.0001735408560311284,
      "loss": 1.3966,
      "step": 890
    },
    {
      "epoch": 1.199192462987887,
      "grad_norm": 0.602888286113739,
      "learning_rate": 0.0001735109248727926,
      "loss": 1.0753,
      "step": 891
    },
    {
      "epoch": 1.2005383580080753,
      "grad_norm": 0.576443076133728,
      "learning_rate": 0.00017348099371445678,
      "loss": 1.3554,
      "step": 892
    },
    {
      "epoch": 1.2018842530282638,
      "grad_norm": 0.5946112275123596,
      "learning_rate": 0.00017345106255612093,
      "loss": 1.3034,
      "step": 893
    },
    {
      "epoch": 1.2032301480484522,
      "grad_norm": 0.580014705657959,
      "learning_rate": 0.00017342113139778511,
      "loss": 1.592,
      "step": 894
    },
    {
      "epoch": 1.2045760430686405,
      "grad_norm": 0.6919142603874207,
      "learning_rate": 0.00017339120023944927,
      "loss": 1.0969,
      "step": 895
    },
    {
      "epoch": 1.205921938088829,
      "grad_norm": 0.5086020231246948,
      "learning_rate": 0.00017336126908111345,
      "loss": 1.4553,
      "step": 896
    },
    {
      "epoch": 1.2072678331090174,
      "grad_norm": 0.568366527557373,
      "learning_rate": 0.00017333133792277763,
      "loss": 1.1719,
      "step": 897
    },
    {
      "epoch": 1.208613728129206,
      "grad_norm": 0.5852729082107544,
      "learning_rate": 0.0001733014067644418,
      "loss": 1.0652,
      "step": 898
    },
    {
      "epoch": 1.2099596231493943,
      "grad_norm": 0.6602036356925964,
      "learning_rate": 0.00017327147560610597,
      "loss": 1.034,
      "step": 899
    },
    {
      "epoch": 1.2113055181695827,
      "grad_norm": 0.649246335029602,
      "learning_rate": 0.00017324154444777016,
      "loss": 1.156,
      "step": 900
    },
    {
      "epoch": 1.2126514131897712,
      "grad_norm": 0.6347299814224243,
      "learning_rate": 0.0001732116132894343,
      "loss": 1.1867,
      "step": 901
    },
    {
      "epoch": 1.2139973082099595,
      "grad_norm": 0.5647450685501099,
      "learning_rate": 0.0001731816821310985,
      "loss": 1.2108,
      "step": 902
    },
    {
      "epoch": 1.215343203230148,
      "grad_norm": 0.6157217025756836,
      "learning_rate": 0.00017315175097276265,
      "loss": 1.4278,
      "step": 903
    },
    {
      "epoch": 1.2166890982503364,
      "grad_norm": 0.7990874648094177,
      "learning_rate": 0.00017312181981442683,
      "loss": 1.055,
      "step": 904
    },
    {
      "epoch": 1.2180349932705248,
      "grad_norm": 0.6512701511383057,
      "learning_rate": 0.00017309188865609102,
      "loss": 1.142,
      "step": 905
    },
    {
      "epoch": 1.2193808882907133,
      "grad_norm": 0.688770592212677,
      "learning_rate": 0.00017306195749775517,
      "loss": 1.2277,
      "step": 906
    },
    {
      "epoch": 1.2207267833109017,
      "grad_norm": 0.6349213123321533,
      "learning_rate": 0.00017303202633941935,
      "loss": 1.1277,
      "step": 907
    },
    {
      "epoch": 1.2220726783310902,
      "grad_norm": 0.5488699078559875,
      "learning_rate": 0.00017300209518108354,
      "loss": 1.1839,
      "step": 908
    },
    {
      "epoch": 1.2234185733512786,
      "grad_norm": 0.6375088095664978,
      "learning_rate": 0.0001729721640227477,
      "loss": 1.388,
      "step": 909
    },
    {
      "epoch": 1.224764468371467,
      "grad_norm": 0.5603605508804321,
      "learning_rate": 0.00017294223286441187,
      "loss": 0.9905,
      "step": 910
    },
    {
      "epoch": 1.2261103633916555,
      "grad_norm": 0.5027154684066772,
      "learning_rate": 0.00017291230170607603,
      "loss": 1.4249,
      "step": 911
    },
    {
      "epoch": 1.2274562584118438,
      "grad_norm": 0.5701888203620911,
      "learning_rate": 0.0001728823705477402,
      "loss": 1.1086,
      "step": 912
    },
    {
      "epoch": 1.2288021534320324,
      "grad_norm": 0.6105141639709473,
      "learning_rate": 0.0001728524393894044,
      "loss": 1.1407,
      "step": 913
    },
    {
      "epoch": 1.2301480484522207,
      "grad_norm": 0.6490916609764099,
      "learning_rate": 0.00017282250823106855,
      "loss": 1.1806,
      "step": 914
    },
    {
      "epoch": 1.231493943472409,
      "grad_norm": 0.8491512537002563,
      "learning_rate": 0.00017279257707273273,
      "loss": 1.2653,
      "step": 915
    },
    {
      "epoch": 1.2328398384925976,
      "grad_norm": 0.6591253876686096,
      "learning_rate": 0.00017276264591439692,
      "loss": 1.0794,
      "step": 916
    },
    {
      "epoch": 1.234185733512786,
      "grad_norm": 0.6745705008506775,
      "learning_rate": 0.00017273271475606107,
      "loss": 1.091,
      "step": 917
    },
    {
      "epoch": 1.2355316285329745,
      "grad_norm": 0.6465033888816833,
      "learning_rate": 0.00017270278359772525,
      "loss": 1.2029,
      "step": 918
    },
    {
      "epoch": 1.2368775235531628,
      "grad_norm": 0.6820705533027649,
      "learning_rate": 0.0001726728524393894,
      "loss": 1.5832,
      "step": 919
    },
    {
      "epoch": 1.2382234185733512,
      "grad_norm": 0.6817814111709595,
      "learning_rate": 0.0001726429212810536,
      "loss": 1.1336,
      "step": 920
    },
    {
      "epoch": 1.2395693135935397,
      "grad_norm": 0.5790548324584961,
      "learning_rate": 0.00017261299012271778,
      "loss": 1.1457,
      "step": 921
    },
    {
      "epoch": 1.240915208613728,
      "grad_norm": 0.5702813863754272,
      "learning_rate": 0.00017258305896438193,
      "loss": 1.0607,
      "step": 922
    },
    {
      "epoch": 1.2422611036339166,
      "grad_norm": 0.714303731918335,
      "learning_rate": 0.00017255312780604611,
      "loss": 1.1385,
      "step": 923
    },
    {
      "epoch": 1.243606998654105,
      "grad_norm": 0.7148311734199524,
      "learning_rate": 0.0001725231966477103,
      "loss": 1.3048,
      "step": 924
    },
    {
      "epoch": 1.2449528936742933,
      "grad_norm": 0.6343257427215576,
      "learning_rate": 0.00017249326548937445,
      "loss": 1.254,
      "step": 925
    },
    {
      "epoch": 1.2462987886944819,
      "grad_norm": 0.5517590045928955,
      "learning_rate": 0.00017246333433103864,
      "loss": 1.2253,
      "step": 926
    },
    {
      "epoch": 1.2476446837146702,
      "grad_norm": 0.6456019878387451,
      "learning_rate": 0.0001724334031727028,
      "loss": 1.242,
      "step": 927
    },
    {
      "epoch": 1.2489905787348587,
      "grad_norm": 0.6092912554740906,
      "learning_rate": 0.00017240347201436697,
      "loss": 1.5143,
      "step": 928
    },
    {
      "epoch": 1.250336473755047,
      "grad_norm": 0.5766844153404236,
      "learning_rate": 0.00017237354085603116,
      "loss": 1.2947,
      "step": 929
    },
    {
      "epoch": 1.2516823687752354,
      "grad_norm": 0.6640570759773254,
      "learning_rate": 0.0001723436096976953,
      "loss": 1.4408,
      "step": 930
    },
    {
      "epoch": 1.253028263795424,
      "grad_norm": 0.7059255242347717,
      "learning_rate": 0.0001723136785393595,
      "loss": 1.1004,
      "step": 931
    },
    {
      "epoch": 1.2543741588156123,
      "grad_norm": 0.534144937992096,
      "learning_rate": 0.00017228374738102365,
      "loss": 1.1323,
      "step": 932
    },
    {
      "epoch": 1.2557200538358009,
      "grad_norm": 0.6020293235778809,
      "learning_rate": 0.00017225381622268783,
      "loss": 0.882,
      "step": 933
    },
    {
      "epoch": 1.2570659488559892,
      "grad_norm": 0.7078106999397278,
      "learning_rate": 0.00017222388506435202,
      "loss": 1.3341,
      "step": 934
    },
    {
      "epoch": 1.2584118438761775,
      "grad_norm": 0.9414507150650024,
      "learning_rate": 0.00017219395390601617,
      "loss": 1.3961,
      "step": 935
    },
    {
      "epoch": 1.259757738896366,
      "grad_norm": 0.5983312726020813,
      "learning_rate": 0.00017216402274768035,
      "loss": 0.9751,
      "step": 936
    },
    {
      "epoch": 1.2611036339165544,
      "grad_norm": 0.7853455543518066,
      "learning_rate": 0.00017213409158934454,
      "loss": 1.4518,
      "step": 937
    },
    {
      "epoch": 1.262449528936743,
      "grad_norm": 0.5121594071388245,
      "learning_rate": 0.0001721041604310087,
      "loss": 1.2469,
      "step": 938
    },
    {
      "epoch": 1.2637954239569313,
      "grad_norm": 0.6914095282554626,
      "learning_rate": 0.00017207422927267287,
      "loss": 1.4319,
      "step": 939
    },
    {
      "epoch": 1.2651413189771197,
      "grad_norm": 0.6644412875175476,
      "learning_rate": 0.00017204429811433703,
      "loss": 1.3007,
      "step": 940
    },
    {
      "epoch": 1.2664872139973082,
      "grad_norm": 0.5621863603591919,
      "learning_rate": 0.0001720143669560012,
      "loss": 1.1796,
      "step": 941
    },
    {
      "epoch": 1.2678331090174966,
      "grad_norm": 0.6493807435035706,
      "learning_rate": 0.0001719844357976654,
      "loss": 1.3442,
      "step": 942
    },
    {
      "epoch": 1.2691790040376851,
      "grad_norm": 0.6016849279403687,
      "learning_rate": 0.00017195450463932955,
      "loss": 1.2902,
      "step": 943
    },
    {
      "epoch": 1.2705248990578735,
      "grad_norm": 0.47429901361465454,
      "learning_rate": 0.00017192457348099373,
      "loss": 1.3957,
      "step": 944
    },
    {
      "epoch": 1.2718707940780618,
      "grad_norm": 0.6676551103591919,
      "learning_rate": 0.00017189464232265792,
      "loss": 1.1856,
      "step": 945
    },
    {
      "epoch": 1.2732166890982504,
      "grad_norm": 0.6367472410202026,
      "learning_rate": 0.00017186471116432205,
      "loss": 1.2405,
      "step": 946
    },
    {
      "epoch": 1.2745625841184387,
      "grad_norm": 0.693037211894989,
      "learning_rate": 0.00017183478000598623,
      "loss": 1.6283,
      "step": 947
    },
    {
      "epoch": 1.2759084791386273,
      "grad_norm": 0.6520193219184875,
      "learning_rate": 0.0001718048488476504,
      "loss": 1.1937,
      "step": 948
    },
    {
      "epoch": 1.2772543741588156,
      "grad_norm": 0.6135796308517456,
      "learning_rate": 0.00017177491768931457,
      "loss": 1.0754,
      "step": 949
    },
    {
      "epoch": 1.278600269179004,
      "grad_norm": 0.6109960079193115,
      "learning_rate": 0.00017174498653097875,
      "loss": 1.2471,
      "step": 950
    },
    {
      "epoch": 1.2799461641991925,
      "grad_norm": 0.6485664248466492,
      "learning_rate": 0.00017171505537264293,
      "loss": 1.1348,
      "step": 951
    },
    {
      "epoch": 1.2812920592193808,
      "grad_norm": 0.5186022520065308,
      "learning_rate": 0.0001716851242143071,
      "loss": 1.0198,
      "step": 952
    },
    {
      "epoch": 1.2826379542395694,
      "grad_norm": 0.6568232774734497,
      "learning_rate": 0.00017165519305597127,
      "loss": 0.9951,
      "step": 953
    },
    {
      "epoch": 1.2839838492597577,
      "grad_norm": 0.5739855766296387,
      "learning_rate": 0.00017162526189763543,
      "loss": 1.0441,
      "step": 954
    },
    {
      "epoch": 1.285329744279946,
      "grad_norm": 0.5846482515335083,
      "learning_rate": 0.0001715953307392996,
      "loss": 1.2063,
      "step": 955
    },
    {
      "epoch": 1.2866756393001346,
      "grad_norm": 0.630056619644165,
      "learning_rate": 0.0001715653995809638,
      "loss": 1.1308,
      "step": 956
    },
    {
      "epoch": 1.288021534320323,
      "grad_norm": 0.708992063999176,
      "learning_rate": 0.00017153546842262795,
      "loss": 1.3134,
      "step": 957
    },
    {
      "epoch": 1.2893674293405115,
      "grad_norm": 0.565085232257843,
      "learning_rate": 0.00017150553726429213,
      "loss": 1.0524,
      "step": 958
    },
    {
      "epoch": 1.2907133243606999,
      "grad_norm": 0.45584636926651,
      "learning_rate": 0.0001714756061059563,
      "loss": 1.4627,
      "step": 959
    },
    {
      "epoch": 1.2920592193808882,
      "grad_norm": 0.8876814246177673,
      "learning_rate": 0.00017144567494762047,
      "loss": 1.0542,
      "step": 960
    },
    {
      "epoch": 1.2934051144010767,
      "grad_norm": 0.5723375082015991,
      "learning_rate": 0.00017141574378928465,
      "loss": 1.0378,
      "step": 961
    },
    {
      "epoch": 1.294751009421265,
      "grad_norm": 0.76710444688797,
      "learning_rate": 0.0001713858126309488,
      "loss": 1.3901,
      "step": 962
    },
    {
      "epoch": 1.2960969044414536,
      "grad_norm": 0.6019208431243896,
      "learning_rate": 0.000171355881472613,
      "loss": 1.0189,
      "step": 963
    },
    {
      "epoch": 1.297442799461642,
      "grad_norm": 0.4705497622489929,
      "learning_rate": 0.00017132595031427717,
      "loss": 1.3679,
      "step": 964
    },
    {
      "epoch": 1.2987886944818303,
      "grad_norm": 0.6212670803070068,
      "learning_rate": 0.00017129601915594133,
      "loss": 1.2114,
      "step": 965
    },
    {
      "epoch": 1.3001345895020189,
      "grad_norm": 0.505431592464447,
      "learning_rate": 0.0001712660879976055,
      "loss": 1.0949,
      "step": 966
    },
    {
      "epoch": 1.3014804845222072,
      "grad_norm": 0.6103727221488953,
      "learning_rate": 0.0001712361568392697,
      "loss": 1.2535,
      "step": 967
    },
    {
      "epoch": 1.3028263795423958,
      "grad_norm": 0.46135151386260986,
      "learning_rate": 0.00017120622568093385,
      "loss": 1.2428,
      "step": 968
    },
    {
      "epoch": 1.304172274562584,
      "grad_norm": 0.6395987868309021,
      "learning_rate": 0.00017117629452259803,
      "loss": 1.3286,
      "step": 969
    },
    {
      "epoch": 1.3055181695827724,
      "grad_norm": 0.6836921572685242,
      "learning_rate": 0.00017114636336426219,
      "loss": 1.1469,
      "step": 970
    },
    {
      "epoch": 1.306864064602961,
      "grad_norm": 0.6422379612922668,
      "learning_rate": 0.00017111643220592637,
      "loss": 1.5171,
      "step": 971
    },
    {
      "epoch": 1.3082099596231493,
      "grad_norm": 0.6922329068183899,
      "learning_rate": 0.00017108650104759055,
      "loss": 1.3307,
      "step": 972
    },
    {
      "epoch": 1.309555854643338,
      "grad_norm": 0.6631689071655273,
      "learning_rate": 0.0001710565698892547,
      "loss": 1.2747,
      "step": 973
    },
    {
      "epoch": 1.3109017496635262,
      "grad_norm": 0.6335679292678833,
      "learning_rate": 0.0001710266387309189,
      "loss": 1.2305,
      "step": 974
    },
    {
      "epoch": 1.3122476446837146,
      "grad_norm": 0.7108306884765625,
      "learning_rate": 0.00017099670757258307,
      "loss": 1.1125,
      "step": 975
    },
    {
      "epoch": 1.3135935397039031,
      "grad_norm": 0.6683301329612732,
      "learning_rate": 0.00017096677641424723,
      "loss": 1.6046,
      "step": 976
    },
    {
      "epoch": 1.3149394347240915,
      "grad_norm": 0.5974613428115845,
      "learning_rate": 0.0001709368452559114,
      "loss": 0.9626,
      "step": 977
    },
    {
      "epoch": 1.31628532974428,
      "grad_norm": 0.5227862596511841,
      "learning_rate": 0.00017090691409757557,
      "loss": 1.2967,
      "step": 978
    },
    {
      "epoch": 1.3176312247644684,
      "grad_norm": 0.6259615421295166,
      "learning_rate": 0.00017087698293923975,
      "loss": 1.1577,
      "step": 979
    },
    {
      "epoch": 1.3189771197846567,
      "grad_norm": 0.8140676021575928,
      "learning_rate": 0.00017084705178090393,
      "loss": 1.2567,
      "step": 980
    },
    {
      "epoch": 1.3203230148048453,
      "grad_norm": 0.726435661315918,
      "learning_rate": 0.0001708171206225681,
      "loss": 0.9479,
      "step": 981
    },
    {
      "epoch": 1.3216689098250336,
      "grad_norm": 0.6042033433914185,
      "learning_rate": 0.00017078718946423227,
      "loss": 1.6415,
      "step": 982
    },
    {
      "epoch": 1.3230148048452222,
      "grad_norm": 0.7272891998291016,
      "learning_rate": 0.00017075725830589645,
      "loss": 1.3329,
      "step": 983
    },
    {
      "epoch": 1.3243606998654105,
      "grad_norm": 0.538023829460144,
      "learning_rate": 0.0001707273271475606,
      "loss": 1.3413,
      "step": 984
    },
    {
      "epoch": 1.3257065948855988,
      "grad_norm": 0.549507200717926,
      "learning_rate": 0.0001706973959892248,
      "loss": 1.0611,
      "step": 985
    },
    {
      "epoch": 1.3270524899057874,
      "grad_norm": 0.6868657469749451,
      "learning_rate": 0.00017066746483088895,
      "loss": 1.4113,
      "step": 986
    },
    {
      "epoch": 1.3283983849259757,
      "grad_norm": 0.7090669870376587,
      "learning_rate": 0.00017063753367255313,
      "loss": 1.2173,
      "step": 987
    },
    {
      "epoch": 1.3297442799461643,
      "grad_norm": 0.591012716293335,
      "learning_rate": 0.0001706076025142173,
      "loss": 1.1189,
      "step": 988
    },
    {
      "epoch": 1.3310901749663526,
      "grad_norm": 0.5539861917495728,
      "learning_rate": 0.00017057767135588147,
      "loss": 1.4282,
      "step": 989
    },
    {
      "epoch": 1.332436069986541,
      "grad_norm": 0.6685119271278381,
      "learning_rate": 0.00017054774019754565,
      "loss": 0.9969,
      "step": 990
    },
    {
      "epoch": 1.3337819650067295,
      "grad_norm": 0.6387453079223633,
      "learning_rate": 0.0001705178090392098,
      "loss": 1.1068,
      "step": 991
    },
    {
      "epoch": 1.3351278600269179,
      "grad_norm": 0.6254691481590271,
      "learning_rate": 0.000170487877880874,
      "loss": 1.2169,
      "step": 992
    },
    {
      "epoch": 1.3364737550471064,
      "grad_norm": 0.5774839520454407,
      "learning_rate": 0.00017045794672253817,
      "loss": 1.2877,
      "step": 993
    },
    {
      "epoch": 1.3378196500672948,
      "grad_norm": 0.7499284744262695,
      "learning_rate": 0.00017042801556420233,
      "loss": 1.2736,
      "step": 994
    },
    {
      "epoch": 1.339165545087483,
      "grad_norm": 0.6007223725318909,
      "learning_rate": 0.0001703980844058665,
      "loss": 1.3425,
      "step": 995
    },
    {
      "epoch": 1.3405114401076716,
      "grad_norm": 0.5454638004302979,
      "learning_rate": 0.0001703681532475307,
      "loss": 1.3275,
      "step": 996
    },
    {
      "epoch": 1.34185733512786,
      "grad_norm": 0.5573009848594666,
      "learning_rate": 0.00017033822208919485,
      "loss": 1.1226,
      "step": 997
    },
    {
      "epoch": 1.3432032301480485,
      "grad_norm": 0.6540215015411377,
      "learning_rate": 0.00017030829093085903,
      "loss": 1.4301,
      "step": 998
    },
    {
      "epoch": 1.3445491251682369,
      "grad_norm": 0.6643530130386353,
      "learning_rate": 0.00017027835977252319,
      "loss": 1.3258,
      "step": 999
    },
    {
      "epoch": 1.3458950201884252,
      "grad_norm": 0.6353902816772461,
      "learning_rate": 0.00017024842861418737,
      "loss": 1.2141,
      "step": 1000
    },
    {
      "epoch": 1.3472409152086138,
      "grad_norm": 0.6009384989738464,
      "learning_rate": 0.00017021849745585155,
      "loss": 1.2435,
      "step": 1001
    },
    {
      "epoch": 1.3485868102288021,
      "grad_norm": 0.6422474384307861,
      "learning_rate": 0.0001701885662975157,
      "loss": 0.9881,
      "step": 1002
    },
    {
      "epoch": 1.3499327052489907,
      "grad_norm": 0.5928472876548767,
      "learning_rate": 0.0001701586351391799,
      "loss": 1.1828,
      "step": 1003
    },
    {
      "epoch": 1.351278600269179,
      "grad_norm": 0.646816611289978,
      "learning_rate": 0.00017012870398084407,
      "loss": 1.3475,
      "step": 1004
    },
    {
      "epoch": 1.3526244952893673,
      "grad_norm": 0.6695206761360168,
      "learning_rate": 0.00017009877282250823,
      "loss": 1.2125,
      "step": 1005
    },
    {
      "epoch": 1.353970390309556,
      "grad_norm": 0.5992833375930786,
      "learning_rate": 0.0001700688416641724,
      "loss": 1.1018,
      "step": 1006
    },
    {
      "epoch": 1.3553162853297442,
      "grad_norm": 0.5776898860931396,
      "learning_rate": 0.00017003891050583657,
      "loss": 1.1128,
      "step": 1007
    },
    {
      "epoch": 1.3566621803499328,
      "grad_norm": 0.5528098344802856,
      "learning_rate": 0.00017000897934750075,
      "loss": 1.3389,
      "step": 1008
    },
    {
      "epoch": 1.3580080753701211,
      "grad_norm": 0.5049309730529785,
      "learning_rate": 0.00016997904818916493,
      "loss": 1.0287,
      "step": 1009
    },
    {
      "epoch": 1.3593539703903095,
      "grad_norm": 0.5894854068756104,
      "learning_rate": 0.0001699491170308291,
      "loss": 1.186,
      "step": 1010
    },
    {
      "epoch": 1.360699865410498,
      "grad_norm": 0.7217922806739807,
      "learning_rate": 0.00016991918587249327,
      "loss": 1.2815,
      "step": 1011
    },
    {
      "epoch": 1.3620457604306864,
      "grad_norm": 0.6531991362571716,
      "learning_rate": 0.00016988925471415745,
      "loss": 1.2543,
      "step": 1012
    },
    {
      "epoch": 1.363391655450875,
      "grad_norm": 0.7062066197395325,
      "learning_rate": 0.0001698593235558216,
      "loss": 1.1872,
      "step": 1013
    },
    {
      "epoch": 1.3647375504710633,
      "grad_norm": 0.6926756501197815,
      "learning_rate": 0.0001698293923974858,
      "loss": 1.1279,
      "step": 1014
    },
    {
      "epoch": 1.3660834454912516,
      "grad_norm": 0.6254701614379883,
      "learning_rate": 0.00016979946123914995,
      "loss": 1.0894,
      "step": 1015
    },
    {
      "epoch": 1.3674293405114402,
      "grad_norm": 0.6585856080055237,
      "learning_rate": 0.00016976953008081413,
      "loss": 1.3291,
      "step": 1016
    },
    {
      "epoch": 1.3687752355316285,
      "grad_norm": 0.6149603724479675,
      "learning_rate": 0.0001697395989224783,
      "loss": 1.3859,
      "step": 1017
    },
    {
      "epoch": 1.370121130551817,
      "grad_norm": 0.6754806041717529,
      "learning_rate": 0.00016970966776414247,
      "loss": 1.0019,
      "step": 1018
    },
    {
      "epoch": 1.3714670255720054,
      "grad_norm": 0.6963784098625183,
      "learning_rate": 0.00016967973660580665,
      "loss": 1.0902,
      "step": 1019
    },
    {
      "epoch": 1.3728129205921937,
      "grad_norm": 0.6292170286178589,
      "learning_rate": 0.00016964980544747083,
      "loss": 1.1746,
      "step": 1020
    },
    {
      "epoch": 1.3741588156123823,
      "grad_norm": 0.7155236601829529,
      "learning_rate": 0.000169619874289135,
      "loss": 1.0896,
      "step": 1021
    },
    {
      "epoch": 1.3755047106325706,
      "grad_norm": 0.7385435104370117,
      "learning_rate": 0.00016958994313079917,
      "loss": 1.2877,
      "step": 1022
    },
    {
      "epoch": 1.3768506056527592,
      "grad_norm": 0.6371598243713379,
      "learning_rate": 0.00016956001197246333,
      "loss": 1.3125,
      "step": 1023
    },
    {
      "epoch": 1.3781965006729475,
      "grad_norm": 0.6872754096984863,
      "learning_rate": 0.0001695300808141275,
      "loss": 1.2592,
      "step": 1024
    },
    {
      "epoch": 1.3795423956931359,
      "grad_norm": 0.7064940333366394,
      "learning_rate": 0.0001695001496557917,
      "loss": 1.2531,
      "step": 1025
    },
    {
      "epoch": 1.3808882907133244,
      "grad_norm": 0.8154745101928711,
      "learning_rate": 0.00016947021849745585,
      "loss": 1.1721,
      "step": 1026
    },
    {
      "epoch": 1.3822341857335128,
      "grad_norm": 0.5255954265594482,
      "learning_rate": 0.00016944028733912003,
      "loss": 1.1123,
      "step": 1027
    },
    {
      "epoch": 1.3835800807537013,
      "grad_norm": 0.615570068359375,
      "learning_rate": 0.00016941035618078421,
      "loss": 1.5123,
      "step": 1028
    },
    {
      "epoch": 1.3849259757738897,
      "grad_norm": 0.610416054725647,
      "learning_rate": 0.00016938042502244837,
      "loss": 1.113,
      "step": 1029
    },
    {
      "epoch": 1.386271870794078,
      "grad_norm": 0.5937425494194031,
      "learning_rate": 0.00016935049386411255,
      "loss": 1.5338,
      "step": 1030
    },
    {
      "epoch": 1.3876177658142665,
      "grad_norm": 0.6498006582260132,
      "learning_rate": 0.0001693205627057767,
      "loss": 1.4264,
      "step": 1031
    },
    {
      "epoch": 1.3889636608344549,
      "grad_norm": 0.6851295828819275,
      "learning_rate": 0.0001692906315474409,
      "loss": 0.9397,
      "step": 1032
    },
    {
      "epoch": 1.3903095558546434,
      "grad_norm": 0.6326887011528015,
      "learning_rate": 0.00016926070038910507,
      "loss": 1.2591,
      "step": 1033
    },
    {
      "epoch": 1.3916554508748318,
      "grad_norm": 0.6503538489341736,
      "learning_rate": 0.00016923076923076923,
      "loss": 1.2667,
      "step": 1034
    },
    {
      "epoch": 1.3930013458950201,
      "grad_norm": 0.6032746434211731,
      "learning_rate": 0.0001692008380724334,
      "loss": 1.195,
      "step": 1035
    },
    {
      "epoch": 1.3943472409152087,
      "grad_norm": 0.6628158092498779,
      "learning_rate": 0.0001691709069140976,
      "loss": 1.1571,
      "step": 1036
    },
    {
      "epoch": 1.395693135935397,
      "grad_norm": 0.9143061637878418,
      "learning_rate": 0.00016914097575576175,
      "loss": 1.5817,
      "step": 1037
    },
    {
      "epoch": 1.3970390309555856,
      "grad_norm": 0.6796111464500427,
      "learning_rate": 0.00016911104459742593,
      "loss": 1.0435,
      "step": 1038
    },
    {
      "epoch": 1.398384925975774,
      "grad_norm": 0.5792295336723328,
      "learning_rate": 0.0001690811134390901,
      "loss": 1.2267,
      "step": 1039
    },
    {
      "epoch": 1.3997308209959622,
      "grad_norm": 0.7038381695747375,
      "learning_rate": 0.00016905118228075427,
      "loss": 1.3197,
      "step": 1040
    },
    {
      "epoch": 1.4010767160161508,
      "grad_norm": 0.6141360402107239,
      "learning_rate": 0.00016902125112241845,
      "loss": 1.2842,
      "step": 1041
    },
    {
      "epoch": 1.4024226110363391,
      "grad_norm": 0.6513080596923828,
      "learning_rate": 0.0001689913199640826,
      "loss": 1.3481,
      "step": 1042
    },
    {
      "epoch": 1.4037685060565277,
      "grad_norm": 0.6211339235305786,
      "learning_rate": 0.0001689613888057468,
      "loss": 1.0589,
      "step": 1043
    },
    {
      "epoch": 1.405114401076716,
      "grad_norm": 0.8664960265159607,
      "learning_rate": 0.00016893145764741097,
      "loss": 1.3937,
      "step": 1044
    },
    {
      "epoch": 1.4064602960969044,
      "grad_norm": 0.5183087587356567,
      "learning_rate": 0.00016890152648907513,
      "loss": 0.9143,
      "step": 1045
    },
    {
      "epoch": 1.407806191117093,
      "grad_norm": 0.6013584136962891,
      "learning_rate": 0.0001688715953307393,
      "loss": 1.0075,
      "step": 1046
    },
    {
      "epoch": 1.4091520861372813,
      "grad_norm": 0.6855741143226624,
      "learning_rate": 0.00016884166417240347,
      "loss": 1.1605,
      "step": 1047
    },
    {
      "epoch": 1.4104979811574698,
      "grad_norm": 0.6390990018844604,
      "learning_rate": 0.00016881173301406765,
      "loss": 1.1105,
      "step": 1048
    },
    {
      "epoch": 1.4118438761776582,
      "grad_norm": 0.7180373072624207,
      "learning_rate": 0.00016878180185573183,
      "loss": 1.2785,
      "step": 1049
    },
    {
      "epoch": 1.4131897711978465,
      "grad_norm": 0.6208230257034302,
      "learning_rate": 0.000168751870697396,
      "loss": 0.9991,
      "step": 1050
    },
    {
      "epoch": 1.414535666218035,
      "grad_norm": 0.5390047430992126,
      "learning_rate": 0.00016872193953906017,
      "loss": 1.2276,
      "step": 1051
    },
    {
      "epoch": 1.4158815612382234,
      "grad_norm": 0.5647265315055847,
      "learning_rate": 0.00016869200838072435,
      "loss": 1.3324,
      "step": 1052
    },
    {
      "epoch": 1.417227456258412,
      "grad_norm": 0.6550635099411011,
      "learning_rate": 0.0001686620772223885,
      "loss": 1.2974,
      "step": 1053
    },
    {
      "epoch": 1.4185733512786003,
      "grad_norm": 0.5775601267814636,
      "learning_rate": 0.0001686321460640527,
      "loss": 1.2587,
      "step": 1054
    },
    {
      "epoch": 1.4199192462987886,
      "grad_norm": 0.6086835861206055,
      "learning_rate": 0.00016860221490571685,
      "loss": 1.0301,
      "step": 1055
    },
    {
      "epoch": 1.4212651413189772,
      "grad_norm": 0.5852712392807007,
      "learning_rate": 0.00016857228374738103,
      "loss": 1.3257,
      "step": 1056
    },
    {
      "epoch": 1.4226110363391655,
      "grad_norm": 0.5488479137420654,
      "learning_rate": 0.00016854235258904521,
      "loss": 1.2641,
      "step": 1057
    },
    {
      "epoch": 1.423956931359354,
      "grad_norm": 0.5567564368247986,
      "learning_rate": 0.00016851242143070937,
      "loss": 1.1951,
      "step": 1058
    },
    {
      "epoch": 1.4253028263795424,
      "grad_norm": 0.7485677599906921,
      "learning_rate": 0.00016848249027237355,
      "loss": 1.2108,
      "step": 1059
    },
    {
      "epoch": 1.4266487213997308,
      "grad_norm": 0.6850695013999939,
      "learning_rate": 0.00016845255911403773,
      "loss": 1.3771,
      "step": 1060
    },
    {
      "epoch": 1.4279946164199193,
      "grad_norm": 0.8617699146270752,
      "learning_rate": 0.0001684226279557019,
      "loss": 1.2752,
      "step": 1061
    },
    {
      "epoch": 1.4293405114401077,
      "grad_norm": 0.6790148019790649,
      "learning_rate": 0.00016839269679736607,
      "loss": 0.9645,
      "step": 1062
    },
    {
      "epoch": 1.4306864064602962,
      "grad_norm": 0.44791179895401,
      "learning_rate": 0.00016836276563903023,
      "loss": 0.9669,
      "step": 1063
    },
    {
      "epoch": 1.4320323014804845,
      "grad_norm": 0.7448158264160156,
      "learning_rate": 0.0001683328344806944,
      "loss": 1.2122,
      "step": 1064
    },
    {
      "epoch": 1.4333781965006729,
      "grad_norm": 0.699360728263855,
      "learning_rate": 0.0001683029033223586,
      "loss": 1.2175,
      "step": 1065
    },
    {
      "epoch": 1.4347240915208614,
      "grad_norm": 0.6588878035545349,
      "learning_rate": 0.00016827297216402275,
      "loss": 1.1249,
      "step": 1066
    },
    {
      "epoch": 1.4360699865410498,
      "grad_norm": 0.6064303517341614,
      "learning_rate": 0.00016824304100568693,
      "loss": 1.0898,
      "step": 1067
    },
    {
      "epoch": 1.4374158815612383,
      "grad_norm": 0.7042167782783508,
      "learning_rate": 0.00016821310984735112,
      "loss": 0.9504,
      "step": 1068
    },
    {
      "epoch": 1.4387617765814267,
      "grad_norm": 0.6004106402397156,
      "learning_rate": 0.00016818317868901527,
      "loss": 1.3321,
      "step": 1069
    },
    {
      "epoch": 1.440107671601615,
      "grad_norm": 0.5994800925254822,
      "learning_rate": 0.00016815324753067945,
      "loss": 1.157,
      "step": 1070
    },
    {
      "epoch": 1.4414535666218036,
      "grad_norm": 0.7132018208503723,
      "learning_rate": 0.0001681233163723436,
      "loss": 1.2309,
      "step": 1071
    },
    {
      "epoch": 1.442799461641992,
      "grad_norm": 0.6938067078590393,
      "learning_rate": 0.0001680933852140078,
      "loss": 0.9072,
      "step": 1072
    },
    {
      "epoch": 1.4441453566621805,
      "grad_norm": 0.6192070841789246,
      "learning_rate": 0.00016806345405567197,
      "loss": 1.0353,
      "step": 1073
    },
    {
      "epoch": 1.4454912516823688,
      "grad_norm": 0.7563109993934631,
      "learning_rate": 0.00016803352289733613,
      "loss": 1.1651,
      "step": 1074
    },
    {
      "epoch": 1.4468371467025571,
      "grad_norm": 0.7235682606697083,
      "learning_rate": 0.0001680035917390003,
      "loss": 1.1771,
      "step": 1075
    },
    {
      "epoch": 1.4481830417227457,
      "grad_norm": 0.668668270111084,
      "learning_rate": 0.0001679736605806645,
      "loss": 1.0905,
      "step": 1076
    },
    {
      "epoch": 1.449528936742934,
      "grad_norm": 0.6227564215660095,
      "learning_rate": 0.00016794372942232865,
      "loss": 1.2011,
      "step": 1077
    },
    {
      "epoch": 1.4508748317631226,
      "grad_norm": 0.581212043762207,
      "learning_rate": 0.00016791379826399283,
      "loss": 1.2323,
      "step": 1078
    },
    {
      "epoch": 1.452220726783311,
      "grad_norm": 0.6355856657028198,
      "learning_rate": 0.000167883867105657,
      "loss": 1.2252,
      "step": 1079
    },
    {
      "epoch": 1.4535666218034993,
      "grad_norm": 0.6112579107284546,
      "learning_rate": 0.00016785393594732117,
      "loss": 1.2792,
      "step": 1080
    },
    {
      "epoch": 1.4549125168236878,
      "grad_norm": 0.8111351132392883,
      "learning_rate": 0.00016782400478898535,
      "loss": 1.4992,
      "step": 1081
    },
    {
      "epoch": 1.4562584118438762,
      "grad_norm": 0.6618685126304626,
      "learning_rate": 0.0001677940736306495,
      "loss": 1.3799,
      "step": 1082
    },
    {
      "epoch": 1.4576043068640647,
      "grad_norm": 0.6171788573265076,
      "learning_rate": 0.0001677641424723137,
      "loss": 1.4763,
      "step": 1083
    },
    {
      "epoch": 1.458950201884253,
      "grad_norm": 0.6292049288749695,
      "learning_rate": 0.00016773421131397788,
      "loss": 1.4515,
      "step": 1084
    },
    {
      "epoch": 1.4602960969044414,
      "grad_norm": 0.8129245638847351,
      "learning_rate": 0.00016770428015564203,
      "loss": 1.3258,
      "step": 1085
    },
    {
      "epoch": 1.46164199192463,
      "grad_norm": 0.5765256285667419,
      "learning_rate": 0.00016767434899730621,
      "loss": 1.2062,
      "step": 1086
    },
    {
      "epoch": 1.4629878869448183,
      "grad_norm": 0.6623412370681763,
      "learning_rate": 0.00016764441783897037,
      "loss": 1.1705,
      "step": 1087
    },
    {
      "epoch": 1.4643337819650069,
      "grad_norm": 0.7919392585754395,
      "learning_rate": 0.00016761448668063455,
      "loss": 1.4043,
      "step": 1088
    },
    {
      "epoch": 1.4656796769851952,
      "grad_norm": 0.6369699835777283,
      "learning_rate": 0.00016758455552229873,
      "loss": 1.2316,
      "step": 1089
    },
    {
      "epoch": 1.4670255720053835,
      "grad_norm": 0.6118282675743103,
      "learning_rate": 0.0001675546243639629,
      "loss": 1.0945,
      "step": 1090
    },
    {
      "epoch": 1.468371467025572,
      "grad_norm": 0.6212046146392822,
      "learning_rate": 0.00016752469320562707,
      "loss": 1.1575,
      "step": 1091
    },
    {
      "epoch": 1.4697173620457604,
      "grad_norm": 0.7526653409004211,
      "learning_rate": 0.00016749476204729126,
      "loss": 1.1146,
      "step": 1092
    },
    {
      "epoch": 1.471063257065949,
      "grad_norm": 0.5631806254386902,
      "learning_rate": 0.0001674648308889554,
      "loss": 0.9741,
      "step": 1093
    },
    {
      "epoch": 1.4724091520861373,
      "grad_norm": 0.6919214725494385,
      "learning_rate": 0.0001674348997306196,
      "loss": 1.2247,
      "step": 1094
    },
    {
      "epoch": 1.4737550471063257,
      "grad_norm": 0.6030492186546326,
      "learning_rate": 0.00016740496857228375,
      "loss": 1.3418,
      "step": 1095
    },
    {
      "epoch": 1.4751009421265142,
      "grad_norm": 0.5893734693527222,
      "learning_rate": 0.00016737503741394793,
      "loss": 1.055,
      "step": 1096
    },
    {
      "epoch": 1.4764468371467026,
      "grad_norm": 0.6073622703552246,
      "learning_rate": 0.00016734510625561212,
      "loss": 1.3581,
      "step": 1097
    },
    {
      "epoch": 1.477792732166891,
      "grad_norm": 0.6133224964141846,
      "learning_rate": 0.00016731517509727627,
      "loss": 1.1241,
      "step": 1098
    },
    {
      "epoch": 1.4791386271870794,
      "grad_norm": 0.7152321338653564,
      "learning_rate": 0.00016728524393894045,
      "loss": 1.3565,
      "step": 1099
    },
    {
      "epoch": 1.4804845222072678,
      "grad_norm": 0.7152643799781799,
      "learning_rate": 0.00016725531278060464,
      "loss": 1.4958,
      "step": 1100
    },
    {
      "epoch": 1.4818304172274561,
      "grad_norm": 0.7161058783531189,
      "learning_rate": 0.0001672253816222688,
      "loss": 1.0613,
      "step": 1101
    },
    {
      "epoch": 1.4831763122476447,
      "grad_norm": 0.7096794843673706,
      "learning_rate": 0.00016719545046393297,
      "loss": 1.3417,
      "step": 1102
    },
    {
      "epoch": 1.4845222072678332,
      "grad_norm": 0.6070328950881958,
      "learning_rate": 0.00016716551930559713,
      "loss": 0.9609,
      "step": 1103
    },
    {
      "epoch": 1.4858681022880216,
      "grad_norm": 0.5748027563095093,
      "learning_rate": 0.0001671355881472613,
      "loss": 1.2452,
      "step": 1104
    },
    {
      "epoch": 1.48721399730821,
      "grad_norm": 0.5749783515930176,
      "learning_rate": 0.0001671056569889255,
      "loss": 1.0145,
      "step": 1105
    },
    {
      "epoch": 1.4885598923283982,
      "grad_norm": 0.5959537625312805,
      "learning_rate": 0.00016707572583058965,
      "loss": 1.1252,
      "step": 1106
    },
    {
      "epoch": 1.4899057873485868,
      "grad_norm": 0.6809303760528564,
      "learning_rate": 0.00016704579467225383,
      "loss": 1.0346,
      "step": 1107
    },
    {
      "epoch": 1.4912516823687754,
      "grad_norm": 0.6762775182723999,
      "learning_rate": 0.000167015863513918,
      "loss": 1.253,
      "step": 1108
    },
    {
      "epoch": 1.4925975773889637,
      "grad_norm": 0.6782536506652832,
      "learning_rate": 0.00016698593235558217,
      "loss": 1.1591,
      "step": 1109
    },
    {
      "epoch": 1.493943472409152,
      "grad_norm": 0.7375417351722717,
      "learning_rate": 0.00016695600119724635,
      "loss": 1.2226,
      "step": 1110
    },
    {
      "epoch": 1.4952893674293404,
      "grad_norm": 0.7231298685073853,
      "learning_rate": 0.0001669260700389105,
      "loss": 1.1151,
      "step": 1111
    },
    {
      "epoch": 1.496635262449529,
      "grad_norm": 0.753159761428833,
      "learning_rate": 0.0001668961388805747,
      "loss": 1.2268,
      "step": 1112
    },
    {
      "epoch": 1.4979811574697175,
      "grad_norm": 0.6134502291679382,
      "learning_rate": 0.00016686620772223888,
      "loss": 1.5582,
      "step": 1113
    },
    {
      "epoch": 1.4993270524899058,
      "grad_norm": 0.6608828902244568,
      "learning_rate": 0.00016683627656390303,
      "loss": 1.0754,
      "step": 1114
    },
    {
      "epoch": 1.5006729475100942,
      "grad_norm": 0.7096613049507141,
      "learning_rate": 0.00016680634540556721,
      "loss": 1.4357,
      "step": 1115
    },
    {
      "epoch": 1.5020188425302825,
      "grad_norm": 0.6637451648712158,
      "learning_rate": 0.00016677641424723137,
      "loss": 1.2802,
      "step": 1116
    },
    {
      "epoch": 1.503364737550471,
      "grad_norm": 0.5830910801887512,
      "learning_rate": 0.00016674648308889555,
      "loss": 1.2486,
      "step": 1117
    },
    {
      "epoch": 1.5047106325706596,
      "grad_norm": 0.7085295915603638,
      "learning_rate": 0.00016671655193055974,
      "loss": 1.1435,
      "step": 1118
    },
    {
      "epoch": 1.506056527590848,
      "grad_norm": 0.6128540635108948,
      "learning_rate": 0.0001666866207722239,
      "loss": 1.3782,
      "step": 1119
    },
    {
      "epoch": 1.5074024226110363,
      "grad_norm": 0.6291713118553162,
      "learning_rate": 0.00016665668961388807,
      "loss": 1.0095,
      "step": 1120
    },
    {
      "epoch": 1.5087483176312246,
      "grad_norm": 0.7596892714500427,
      "learning_rate": 0.00016662675845555226,
      "loss": 1.379,
      "step": 1121
    },
    {
      "epoch": 1.5100942126514132,
      "grad_norm": 0.6650584936141968,
      "learning_rate": 0.0001665968272972164,
      "loss": 1.0288,
      "step": 1122
    },
    {
      "epoch": 1.5114401076716018,
      "grad_norm": 0.7108428478240967,
      "learning_rate": 0.0001665668961388806,
      "loss": 1.118,
      "step": 1123
    },
    {
      "epoch": 1.51278600269179,
      "grad_norm": 0.6675194501876831,
      "learning_rate": 0.00016653696498054475,
      "loss": 1.1224,
      "step": 1124
    },
    {
      "epoch": 1.5141318977119784,
      "grad_norm": 0.737949013710022,
      "learning_rate": 0.00016650703382220893,
      "loss": 1.062,
      "step": 1125
    },
    {
      "epoch": 1.5154777927321668,
      "grad_norm": 0.6842624545097351,
      "learning_rate": 0.00016647710266387312,
      "loss": 1.473,
      "step": 1126
    },
    {
      "epoch": 1.5168236877523553,
      "grad_norm": 0.5853663086891174,
      "learning_rate": 0.00016644717150553727,
      "loss": 1.1855,
      "step": 1127
    },
    {
      "epoch": 1.5181695827725439,
      "grad_norm": 0.8106382489204407,
      "learning_rate": 0.00016641724034720145,
      "loss": 1.0914,
      "step": 1128
    },
    {
      "epoch": 1.5195154777927322,
      "grad_norm": 0.9230209589004517,
      "learning_rate": 0.00016638730918886564,
      "loss": 1.5735,
      "step": 1129
    },
    {
      "epoch": 1.5208613728129206,
      "grad_norm": 0.6498026251792908,
      "learning_rate": 0.0001663573780305298,
      "loss": 1.3317,
      "step": 1130
    },
    {
      "epoch": 1.522207267833109,
      "grad_norm": 0.6210387945175171,
      "learning_rate": 0.00016632744687219397,
      "loss": 1.2384,
      "step": 1131
    },
    {
      "epoch": 1.5235531628532974,
      "grad_norm": 0.5844303965568542,
      "learning_rate": 0.00016629751571385813,
      "loss": 1.4242,
      "step": 1132
    },
    {
      "epoch": 1.524899057873486,
      "grad_norm": 0.5487210154533386,
      "learning_rate": 0.0001662675845555223,
      "loss": 1.2639,
      "step": 1133
    },
    {
      "epoch": 1.5262449528936743,
      "grad_norm": 0.7385808825492859,
      "learning_rate": 0.0001662376533971865,
      "loss": 1.3003,
      "step": 1134
    },
    {
      "epoch": 1.5275908479138627,
      "grad_norm": 0.8046606183052063,
      "learning_rate": 0.00016620772223885065,
      "loss": 1.366,
      "step": 1135
    },
    {
      "epoch": 1.528936742934051,
      "grad_norm": 0.6906509399414062,
      "learning_rate": 0.00016617779108051483,
      "loss": 1.053,
      "step": 1136
    },
    {
      "epoch": 1.5302826379542396,
      "grad_norm": 0.6880000829696655,
      "learning_rate": 0.00016614785992217902,
      "loss": 1.1496,
      "step": 1137
    },
    {
      "epoch": 1.5316285329744281,
      "grad_norm": 0.6124751567840576,
      "learning_rate": 0.00016611792876384317,
      "loss": 1.3653,
      "step": 1138
    },
    {
      "epoch": 1.5329744279946165,
      "grad_norm": 0.7449413537979126,
      "learning_rate": 0.00016608799760550735,
      "loss": 1.4269,
      "step": 1139
    },
    {
      "epoch": 1.5343203230148048,
      "grad_norm": 0.6239196062088013,
      "learning_rate": 0.0001660580664471715,
      "loss": 1.2329,
      "step": 1140
    },
    {
      "epoch": 1.5356662180349931,
      "grad_norm": 0.580146074295044,
      "learning_rate": 0.0001660281352888357,
      "loss": 1.2796,
      "step": 1141
    },
    {
      "epoch": 1.5370121130551817,
      "grad_norm": 0.6422921419143677,
      "learning_rate": 0.00016599820413049988,
      "loss": 1.2144,
      "step": 1142
    },
    {
      "epoch": 1.5383580080753703,
      "grad_norm": 0.7109206914901733,
      "learning_rate": 0.00016596827297216403,
      "loss": 1.3205,
      "step": 1143
    },
    {
      "epoch": 1.5397039030955586,
      "grad_norm": 0.570705771446228,
      "learning_rate": 0.00016593834181382821,
      "loss": 1.2252,
      "step": 1144
    },
    {
      "epoch": 1.541049798115747,
      "grad_norm": 0.7183952927589417,
      "learning_rate": 0.0001659084106554924,
      "loss": 1.7855,
      "step": 1145
    },
    {
      "epoch": 1.5423956931359353,
      "grad_norm": 0.8537347912788391,
      "learning_rate": 0.00016587847949715655,
      "loss": 1.4711,
      "step": 1146
    },
    {
      "epoch": 1.5437415881561238,
      "grad_norm": 0.4894208014011383,
      "learning_rate": 0.00016584854833882074,
      "loss": 1.0255,
      "step": 1147
    },
    {
      "epoch": 1.5450874831763124,
      "grad_norm": 0.6122481226921082,
      "learning_rate": 0.0001658186171804849,
      "loss": 1.2638,
      "step": 1148
    },
    {
      "epoch": 1.5464333781965007,
      "grad_norm": 0.5001810789108276,
      "learning_rate": 0.00016578868602214907,
      "loss": 1.66,
      "step": 1149
    },
    {
      "epoch": 1.547779273216689,
      "grad_norm": 0.699486255645752,
      "learning_rate": 0.00016575875486381326,
      "loss": 1.3899,
      "step": 1150
    },
    {
      "epoch": 1.5491251682368774,
      "grad_norm": 0.7048830986022949,
      "learning_rate": 0.0001657288237054774,
      "loss": 1.0125,
      "step": 1151
    },
    {
      "epoch": 1.550471063257066,
      "grad_norm": 0.6269116401672363,
      "learning_rate": 0.0001656988925471416,
      "loss": 1.3832,
      "step": 1152
    },
    {
      "epoch": 1.5518169582772545,
      "grad_norm": 0.5737480521202087,
      "learning_rate": 0.00016566896138880578,
      "loss": 1.2669,
      "step": 1153
    },
    {
      "epoch": 1.5531628532974429,
      "grad_norm": 0.543493926525116,
      "learning_rate": 0.00016563903023046993,
      "loss": 1.3054,
      "step": 1154
    },
    {
      "epoch": 1.5545087483176312,
      "grad_norm": 0.7282365560531616,
      "learning_rate": 0.0001656090990721341,
      "loss": 1.3247,
      "step": 1155
    },
    {
      "epoch": 1.5558546433378195,
      "grad_norm": 0.6706054210662842,
      "learning_rate": 0.00016557916791379827,
      "loss": 1.3103,
      "step": 1156
    },
    {
      "epoch": 1.557200538358008,
      "grad_norm": 0.6991321444511414,
      "learning_rate": 0.00016554923675546243,
      "loss": 1.5967,
      "step": 1157
    },
    {
      "epoch": 1.5585464333781966,
      "grad_norm": 0.6892146468162537,
      "learning_rate": 0.0001655193055971266,
      "loss": 1.0905,
      "step": 1158
    },
    {
      "epoch": 1.559892328398385,
      "grad_norm": 0.6891471743583679,
      "learning_rate": 0.00016548937443879077,
      "loss": 1.3267,
      "step": 1159
    },
    {
      "epoch": 1.5612382234185733,
      "grad_norm": 0.6522842049598694,
      "learning_rate": 0.00016545944328045495,
      "loss": 1.1231,
      "step": 1160
    },
    {
      "epoch": 1.5625841184387617,
      "grad_norm": 0.5742062926292419,
      "learning_rate": 0.00016542951212211913,
      "loss": 1.1484,
      "step": 1161
    },
    {
      "epoch": 1.5639300134589502,
      "grad_norm": 0.547214150428772,
      "learning_rate": 0.00016539958096378329,
      "loss": 1.2405,
      "step": 1162
    },
    {
      "epoch": 1.5652759084791388,
      "grad_norm": 0.770878255367279,
      "learning_rate": 0.00016536964980544747,
      "loss": 1.6513,
      "step": 1163
    },
    {
      "epoch": 1.5666218034993271,
      "grad_norm": 0.5629683136940002,
      "learning_rate": 0.00016533971864711165,
      "loss": 1.2141,
      "step": 1164
    },
    {
      "epoch": 1.5679676985195155,
      "grad_norm": 0.6407068967819214,
      "learning_rate": 0.0001653097874887758,
      "loss": 1.2887,
      "step": 1165
    },
    {
      "epoch": 1.5693135935397038,
      "grad_norm": 0.6267627477645874,
      "learning_rate": 0.00016527985633044,
      "loss": 1.2781,
      "step": 1166
    },
    {
      "epoch": 1.5706594885598923,
      "grad_norm": 0.6810601353645325,
      "learning_rate": 0.00016524992517210415,
      "loss": 1.3042,
      "step": 1167
    },
    {
      "epoch": 1.572005383580081,
      "grad_norm": 0.6899741888046265,
      "learning_rate": 0.00016521999401376833,
      "loss": 1.345,
      "step": 1168
    },
    {
      "epoch": 1.5733512786002692,
      "grad_norm": 0.5970705151557922,
      "learning_rate": 0.0001651900628554325,
      "loss": 1.2471,
      "step": 1169
    },
    {
      "epoch": 1.5746971736204576,
      "grad_norm": 0.6121214032173157,
      "learning_rate": 0.00016516013169709667,
      "loss": 0.9632,
      "step": 1170
    },
    {
      "epoch": 1.576043068640646,
      "grad_norm": 0.7878214716911316,
      "learning_rate": 0.00016513020053876085,
      "loss": 1.1584,
      "step": 1171
    },
    {
      "epoch": 1.5773889636608345,
      "grad_norm": 0.6293394565582275,
      "learning_rate": 0.00016510026938042503,
      "loss": 1.1857,
      "step": 1172
    },
    {
      "epoch": 1.578734858681023,
      "grad_norm": 0.6350700855255127,
      "learning_rate": 0.0001650703382220892,
      "loss": 1.268,
      "step": 1173
    },
    {
      "epoch": 1.5800807537012114,
      "grad_norm": 0.6043171882629395,
      "learning_rate": 0.00016504040706375337,
      "loss": 1.4089,
      "step": 1174
    },
    {
      "epoch": 1.5814266487213997,
      "grad_norm": 0.7630804777145386,
      "learning_rate": 0.00016501047590541753,
      "loss": 1.3513,
      "step": 1175
    },
    {
      "epoch": 1.582772543741588,
      "grad_norm": 0.6730436086654663,
      "learning_rate": 0.0001649805447470817,
      "loss": 1.3316,
      "step": 1176
    },
    {
      "epoch": 1.5841184387617766,
      "grad_norm": 0.6195889115333557,
      "learning_rate": 0.0001649506135887459,
      "loss": 1.2185,
      "step": 1177
    },
    {
      "epoch": 1.5854643337819652,
      "grad_norm": 0.6347321271896362,
      "learning_rate": 0.00016492068243041005,
      "loss": 1.3288,
      "step": 1178
    },
    {
      "epoch": 1.5868102288021535,
      "grad_norm": 0.5454583764076233,
      "learning_rate": 0.00016489075127207423,
      "loss": 1.3215,
      "step": 1179
    },
    {
      "epoch": 1.5881561238223418,
      "grad_norm": 0.7599051594734192,
      "learning_rate": 0.0001648608201137384,
      "loss": 1.2405,
      "step": 1180
    },
    {
      "epoch": 1.5895020188425302,
      "grad_norm": 0.6243854761123657,
      "learning_rate": 0.00016483088895540257,
      "loss": 1.188,
      "step": 1181
    },
    {
      "epoch": 1.5908479138627187,
      "grad_norm": 0.5780974626541138,
      "learning_rate": 0.00016480095779706675,
      "loss": 1.3529,
      "step": 1182
    },
    {
      "epoch": 1.5921938088829073,
      "grad_norm": 0.6925984621047974,
      "learning_rate": 0.0001647710266387309,
      "loss": 1.1286,
      "step": 1183
    },
    {
      "epoch": 1.5935397039030956,
      "grad_norm": 0.6267715692520142,
      "learning_rate": 0.0001647410954803951,
      "loss": 1.1166,
      "step": 1184
    },
    {
      "epoch": 1.594885598923284,
      "grad_norm": 0.5659332871437073,
      "learning_rate": 0.00016471116432205927,
      "loss": 1.0403,
      "step": 1185
    },
    {
      "epoch": 1.5962314939434723,
      "grad_norm": 0.6174984574317932,
      "learning_rate": 0.00016468123316372343,
      "loss": 1.2447,
      "step": 1186
    },
    {
      "epoch": 1.5975773889636609,
      "grad_norm": 0.6350029706954956,
      "learning_rate": 0.0001646513020053876,
      "loss": 1.3788,
      "step": 1187
    },
    {
      "epoch": 1.5989232839838494,
      "grad_norm": 0.6263322830200195,
      "learning_rate": 0.0001646213708470518,
      "loss": 1.2729,
      "step": 1188
    },
    {
      "epoch": 1.6002691790040378,
      "grad_norm": 0.574284553527832,
      "learning_rate": 0.00016459143968871595,
      "loss": 1.0603,
      "step": 1189
    },
    {
      "epoch": 1.601615074024226,
      "grad_norm": 0.6645520329475403,
      "learning_rate": 0.00016456150853038013,
      "loss": 1.0144,
      "step": 1190
    },
    {
      "epoch": 1.6029609690444144,
      "grad_norm": 0.5702682137489319,
      "learning_rate": 0.00016453157737204429,
      "loss": 1.2958,
      "step": 1191
    },
    {
      "epoch": 1.604306864064603,
      "grad_norm": 0.568457305431366,
      "learning_rate": 0.00016450164621370847,
      "loss": 1.3076,
      "step": 1192
    },
    {
      "epoch": 1.6056527590847915,
      "grad_norm": 0.6040560603141785,
      "learning_rate": 0.00016447171505537265,
      "loss": 1.1411,
      "step": 1193
    },
    {
      "epoch": 1.6069986541049799,
      "grad_norm": 0.6110609173774719,
      "learning_rate": 0.0001644417838970368,
      "loss": 1.2614,
      "step": 1194
    },
    {
      "epoch": 1.6083445491251682,
      "grad_norm": 0.7436688542366028,
      "learning_rate": 0.000164411852738701,
      "loss": 1.4987,
      "step": 1195
    },
    {
      "epoch": 1.6096904441453566,
      "grad_norm": 0.6647034883499146,
      "learning_rate": 0.00016438192158036517,
      "loss": 1.3338,
      "step": 1196
    },
    {
      "epoch": 1.6110363391655451,
      "grad_norm": 0.669638991355896,
      "learning_rate": 0.00016435199042202933,
      "loss": 0.8819,
      "step": 1197
    },
    {
      "epoch": 1.6123822341857337,
      "grad_norm": 0.7014567852020264,
      "learning_rate": 0.0001643220592636935,
      "loss": 0.9391,
      "step": 1198
    },
    {
      "epoch": 1.613728129205922,
      "grad_norm": 0.7158928513526917,
      "learning_rate": 0.00016429212810535767,
      "loss": 1.1526,
      "step": 1199
    },
    {
      "epoch": 1.6150740242261103,
      "grad_norm": 0.7177414894104004,
      "learning_rate": 0.00016426219694702185,
      "loss": 1.3803,
      "step": 1200
    },
    {
      "epoch": 1.6164199192462987,
      "grad_norm": 0.6676463484764099,
      "learning_rate": 0.00016423226578868603,
      "loss": 1.1299,
      "step": 1201
    },
    {
      "epoch": 1.6177658142664872,
      "grad_norm": 0.6807700991630554,
      "learning_rate": 0.0001642023346303502,
      "loss": 1.3708,
      "step": 1202
    },
    {
      "epoch": 1.6191117092866758,
      "grad_norm": 0.6355730295181274,
      "learning_rate": 0.00016417240347201437,
      "loss": 1.3958,
      "step": 1203
    },
    {
      "epoch": 1.6204576043068641,
      "grad_norm": 0.6249085664749146,
      "learning_rate": 0.00016414247231367855,
      "loss": 1.1446,
      "step": 1204
    },
    {
      "epoch": 1.6218034993270525,
      "grad_norm": 0.6904925107955933,
      "learning_rate": 0.0001641125411553427,
      "loss": 1.0578,
      "step": 1205
    },
    {
      "epoch": 1.6231493943472408,
      "grad_norm": 0.6462888717651367,
      "learning_rate": 0.0001640826099970069,
      "loss": 1.357,
      "step": 1206
    },
    {
      "epoch": 1.6244952893674294,
      "grad_norm": 0.6372109651565552,
      "learning_rate": 0.00016405267883867105,
      "loss": 1.6662,
      "step": 1207
    },
    {
      "epoch": 1.6258411843876177,
      "grad_norm": 0.7962064743041992,
      "learning_rate": 0.00016402274768033523,
      "loss": 1.2721,
      "step": 1208
    },
    {
      "epoch": 1.6271870794078063,
      "grad_norm": 0.6761196255683899,
      "learning_rate": 0.0001639928165219994,
      "loss": 1.2751,
      "step": 1209
    },
    {
      "epoch": 1.6285329744279946,
      "grad_norm": 0.7719804048538208,
      "learning_rate": 0.00016396288536366357,
      "loss": 1.1092,
      "step": 1210
    },
    {
      "epoch": 1.629878869448183,
      "grad_norm": 0.4914848804473877,
      "learning_rate": 0.00016393295420532775,
      "loss": 1.3186,
      "step": 1211
    },
    {
      "epoch": 1.6312247644683715,
      "grad_norm": 0.6018468737602234,
      "learning_rate": 0.00016390302304699193,
      "loss": 1.054,
      "step": 1212
    },
    {
      "epoch": 1.6325706594885598,
      "grad_norm": 0.6667299866676331,
      "learning_rate": 0.0001638730918886561,
      "loss": 1.3037,
      "step": 1213
    },
    {
      "epoch": 1.6339165545087484,
      "grad_norm": 0.7453160881996155,
      "learning_rate": 0.00016384316073032027,
      "loss": 1.1711,
      "step": 1214
    },
    {
      "epoch": 1.6352624495289367,
      "grad_norm": 0.6923564076423645,
      "learning_rate": 0.00016381322957198443,
      "loss": 1.2379,
      "step": 1215
    },
    {
      "epoch": 1.636608344549125,
      "grad_norm": 0.7009096741676331,
      "learning_rate": 0.0001637832984136486,
      "loss": 1.1763,
      "step": 1216
    },
    {
      "epoch": 1.6379542395693136,
      "grad_norm": 0.7094359993934631,
      "learning_rate": 0.0001637533672553128,
      "loss": 1.0862,
      "step": 1217
    },
    {
      "epoch": 1.639300134589502,
      "grad_norm": 0.6428154706954956,
      "learning_rate": 0.00016372343609697695,
      "loss": 1.5055,
      "step": 1218
    },
    {
      "epoch": 1.6406460296096905,
      "grad_norm": 0.6566012501716614,
      "learning_rate": 0.00016369350493864113,
      "loss": 1.036,
      "step": 1219
    },
    {
      "epoch": 1.6419919246298789,
      "grad_norm": 0.704792857170105,
      "learning_rate": 0.0001636635737803053,
      "loss": 1.2605,
      "step": 1220
    },
    {
      "epoch": 1.6433378196500672,
      "grad_norm": 0.6238446831703186,
      "learning_rate": 0.00016363364262196947,
      "loss": 1.4097,
      "step": 1221
    },
    {
      "epoch": 1.6446837146702558,
      "grad_norm": 0.6421612501144409,
      "learning_rate": 0.00016360371146363365,
      "loss": 1.6597,
      "step": 1222
    },
    {
      "epoch": 1.646029609690444,
      "grad_norm": 0.7048020362854004,
      "learning_rate": 0.0001635737803052978,
      "loss": 1.3122,
      "step": 1223
    },
    {
      "epoch": 1.6473755047106327,
      "grad_norm": 0.6755229234695435,
      "learning_rate": 0.000163543849146962,
      "loss": 1.4188,
      "step": 1224
    },
    {
      "epoch": 1.648721399730821,
      "grad_norm": 0.6372172236442566,
      "learning_rate": 0.00016351391798862617,
      "loss": 1.4521,
      "step": 1225
    },
    {
      "epoch": 1.6500672947510093,
      "grad_norm": 0.6711638569831848,
      "learning_rate": 0.00016348398683029033,
      "loss": 1.1005,
      "step": 1226
    },
    {
      "epoch": 1.6514131897711979,
      "grad_norm": 0.7600840926170349,
      "learning_rate": 0.0001634540556719545,
      "loss": 1.1706,
      "step": 1227
    },
    {
      "epoch": 1.6527590847913862,
      "grad_norm": 0.6306014060974121,
      "learning_rate": 0.0001634241245136187,
      "loss": 1.2766,
      "step": 1228
    },
    {
      "epoch": 1.6541049798115748,
      "grad_norm": 0.6950700879096985,
      "learning_rate": 0.00016339419335528285,
      "loss": 1.157,
      "step": 1229
    },
    {
      "epoch": 1.6554508748317631,
      "grad_norm": 0.5806699991226196,
      "learning_rate": 0.00016336426219694703,
      "loss": 1.0612,
      "step": 1230
    },
    {
      "epoch": 1.6567967698519515,
      "grad_norm": 0.7068142890930176,
      "learning_rate": 0.0001633343310386112,
      "loss": 1.1845,
      "step": 1231
    },
    {
      "epoch": 1.65814266487214,
      "grad_norm": 0.8165701627731323,
      "learning_rate": 0.00016330439988027537,
      "loss": 1.2476,
      "step": 1232
    },
    {
      "epoch": 1.6594885598923284,
      "grad_norm": 0.7595880031585693,
      "learning_rate": 0.00016327446872193955,
      "loss": 1.2724,
      "step": 1233
    },
    {
      "epoch": 1.660834454912517,
      "grad_norm": 0.6754533052444458,
      "learning_rate": 0.0001632445375636037,
      "loss": 1.1739,
      "step": 1234
    },
    {
      "epoch": 1.6621803499327052,
      "grad_norm": 0.5644988417625427,
      "learning_rate": 0.0001632146064052679,
      "loss": 1.1927,
      "step": 1235
    },
    {
      "epoch": 1.6635262449528936,
      "grad_norm": 0.5638201832771301,
      "learning_rate": 0.00016318467524693207,
      "loss": 1.2707,
      "step": 1236
    },
    {
      "epoch": 1.6648721399730821,
      "grad_norm": 0.7035913467407227,
      "learning_rate": 0.00016315474408859623,
      "loss": 1.3432,
      "step": 1237
    },
    {
      "epoch": 1.6662180349932705,
      "grad_norm": 0.6258923411369324,
      "learning_rate": 0.0001631248129302604,
      "loss": 1.1722,
      "step": 1238
    },
    {
      "epoch": 1.667563930013459,
      "grad_norm": 0.48163896799087524,
      "learning_rate": 0.00016309488177192457,
      "loss": 1.0277,
      "step": 1239
    },
    {
      "epoch": 1.6689098250336474,
      "grad_norm": 0.5690738558769226,
      "learning_rate": 0.00016306495061358875,
      "loss": 1.0353,
      "step": 1240
    },
    {
      "epoch": 1.6702557200538357,
      "grad_norm": 0.6051254272460938,
      "learning_rate": 0.00016303501945525293,
      "loss": 1.338,
      "step": 1241
    },
    {
      "epoch": 1.6716016150740243,
      "grad_norm": 0.6647540926933289,
      "learning_rate": 0.0001630050882969171,
      "loss": 1.3001,
      "step": 1242
    },
    {
      "epoch": 1.6729475100942126,
      "grad_norm": 0.6104379296302795,
      "learning_rate": 0.00016297515713858127,
      "loss": 1.3738,
      "step": 1243
    },
    {
      "epoch": 1.6742934051144012,
      "grad_norm": 0.6634279489517212,
      "learning_rate": 0.00016294522598024545,
      "loss": 1.187,
      "step": 1244
    },
    {
      "epoch": 1.6756393001345895,
      "grad_norm": 0.6233025193214417,
      "learning_rate": 0.0001629152948219096,
      "loss": 0.9998,
      "step": 1245
    },
    {
      "epoch": 1.6769851951547778,
      "grad_norm": 0.6025418043136597,
      "learning_rate": 0.0001628853636635738,
      "loss": 1.2445,
      "step": 1246
    },
    {
      "epoch": 1.6783310901749664,
      "grad_norm": 0.6258389353752136,
      "learning_rate": 0.00016285543250523795,
      "loss": 1.1465,
      "step": 1247
    },
    {
      "epoch": 1.6796769851951547,
      "grad_norm": 0.5924398303031921,
      "learning_rate": 0.00016282550134690213,
      "loss": 1.0747,
      "step": 1248
    },
    {
      "epoch": 1.6810228802153433,
      "grad_norm": 0.7451741695404053,
      "learning_rate": 0.00016279557018856631,
      "loss": 1.3664,
      "step": 1249
    },
    {
      "epoch": 1.6823687752355316,
      "grad_norm": 0.74378901720047,
      "learning_rate": 0.00016276563903023047,
      "loss": 1.1567,
      "step": 1250
    },
    {
      "epoch": 1.68371467025572,
      "grad_norm": 0.8631839752197266,
      "learning_rate": 0.00016273570787189465,
      "loss": 1.3337,
      "step": 1251
    },
    {
      "epoch": 1.6850605652759085,
      "grad_norm": 0.5774991512298584,
      "learning_rate": 0.00016270577671355883,
      "loss": 1.2101,
      "step": 1252
    },
    {
      "epoch": 1.6864064602960969,
      "grad_norm": 0.5654281973838806,
      "learning_rate": 0.000162675845555223,
      "loss": 1.3801,
      "step": 1253
    },
    {
      "epoch": 1.6877523553162854,
      "grad_norm": 0.550328254699707,
      "learning_rate": 0.00016264591439688717,
      "loss": 1.2058,
      "step": 1254
    },
    {
      "epoch": 1.6890982503364738,
      "grad_norm": 0.7069441676139832,
      "learning_rate": 0.00016261598323855133,
      "loss": 1.1566,
      "step": 1255
    },
    {
      "epoch": 1.690444145356662,
      "grad_norm": 0.607555091381073,
      "learning_rate": 0.0001625860520802155,
      "loss": 1.1945,
      "step": 1256
    },
    {
      "epoch": 1.6917900403768507,
      "grad_norm": 0.6613381505012512,
      "learning_rate": 0.0001625561209218797,
      "loss": 1.1881,
      "step": 1257
    },
    {
      "epoch": 1.693135935397039,
      "grad_norm": 0.732536792755127,
      "learning_rate": 0.00016252618976354385,
      "loss": 1.3388,
      "step": 1258
    },
    {
      "epoch": 1.6944818304172276,
      "grad_norm": 0.6132896542549133,
      "learning_rate": 0.00016249625860520803,
      "loss": 1.2757,
      "step": 1259
    },
    {
      "epoch": 1.695827725437416,
      "grad_norm": 0.6383343935012817,
      "learning_rate": 0.00016246632744687222,
      "loss": 1.6527,
      "step": 1260
    },
    {
      "epoch": 1.6971736204576042,
      "grad_norm": 0.6164518594741821,
      "learning_rate": 0.00016243639628853637,
      "loss": 1.2531,
      "step": 1261
    },
    {
      "epoch": 1.6985195154777928,
      "grad_norm": 0.7228917479515076,
      "learning_rate": 0.00016240646513020055,
      "loss": 1.2924,
      "step": 1262
    },
    {
      "epoch": 1.6998654104979811,
      "grad_norm": 0.7156686782836914,
      "learning_rate": 0.0001623765339718647,
      "loss": 1.1489,
      "step": 1263
    },
    {
      "epoch": 1.7012113055181697,
      "grad_norm": 0.6176480650901794,
      "learning_rate": 0.0001623466028135289,
      "loss": 1.3211,
      "step": 1264
    },
    {
      "epoch": 1.702557200538358,
      "grad_norm": 0.7003266215324402,
      "learning_rate": 0.00016231667165519307,
      "loss": 1.1819,
      "step": 1265
    },
    {
      "epoch": 1.7039030955585464,
      "grad_norm": 0.6999966502189636,
      "learning_rate": 0.00016228674049685723,
      "loss": 1.3808,
      "step": 1266
    },
    {
      "epoch": 1.705248990578735,
      "grad_norm": 0.5362846255302429,
      "learning_rate": 0.0001622568093385214,
      "loss": 1.2545,
      "step": 1267
    },
    {
      "epoch": 1.7065948855989233,
      "grad_norm": 0.636773943901062,
      "learning_rate": 0.0001622268781801856,
      "loss": 1.3007,
      "step": 1268
    },
    {
      "epoch": 1.7079407806191118,
      "grad_norm": 0.6472941040992737,
      "learning_rate": 0.00016219694702184975,
      "loss": 1.4313,
      "step": 1269
    },
    {
      "epoch": 1.7092866756393001,
      "grad_norm": 0.5830205082893372,
      "learning_rate": 0.00016216701586351393,
      "loss": 1.242,
      "step": 1270
    },
    {
      "epoch": 1.7106325706594885,
      "grad_norm": 0.5851984024047852,
      "learning_rate": 0.0001621370847051781,
      "loss": 1.1589,
      "step": 1271
    },
    {
      "epoch": 1.7119784656796768,
      "grad_norm": 0.6179282665252686,
      "learning_rate": 0.00016210715354684227,
      "loss": 0.95,
      "step": 1272
    },
    {
      "epoch": 1.7133243606998654,
      "grad_norm": 0.5807061195373535,
      "learning_rate": 0.00016207722238850645,
      "loss": 1.3243,
      "step": 1273
    },
    {
      "epoch": 1.714670255720054,
      "grad_norm": 0.6733301281929016,
      "learning_rate": 0.0001620472912301706,
      "loss": 1.2557,
      "step": 1274
    },
    {
      "epoch": 1.7160161507402423,
      "grad_norm": 0.5508975982666016,
      "learning_rate": 0.0001620173600718348,
      "loss": 1.2408,
      "step": 1275
    },
    {
      "epoch": 1.7173620457604306,
      "grad_norm": 0.6294298768043518,
      "learning_rate": 0.00016198742891349895,
      "loss": 1.1873,
      "step": 1276
    },
    {
      "epoch": 1.718707940780619,
      "grad_norm": 0.5694933533668518,
      "learning_rate": 0.00016195749775516313,
      "loss": 1.4421,
      "step": 1277
    },
    {
      "epoch": 1.7200538358008075,
      "grad_norm": 0.5907388925552368,
      "learning_rate": 0.00016192756659682731,
      "loss": 1.0782,
      "step": 1278
    },
    {
      "epoch": 1.721399730820996,
      "grad_norm": 0.581466019153595,
      "learning_rate": 0.00016189763543849147,
      "loss": 1.2792,
      "step": 1279
    },
    {
      "epoch": 1.7227456258411844,
      "grad_norm": 0.6009664535522461,
      "learning_rate": 0.00016186770428015565,
      "loss": 1.3424,
      "step": 1280
    },
    {
      "epoch": 1.7240915208613727,
      "grad_norm": 0.6633785367012024,
      "learning_rate": 0.00016183777312181983,
      "loss": 1.1893,
      "step": 1281
    },
    {
      "epoch": 1.725437415881561,
      "grad_norm": 0.7142037749290466,
      "learning_rate": 0.000161807841963484,
      "loss": 1.1605,
      "step": 1282
    },
    {
      "epoch": 1.7267833109017496,
      "grad_norm": 0.6348984837532043,
      "learning_rate": 0.00016177791080514817,
      "loss": 1.1466,
      "step": 1283
    },
    {
      "epoch": 1.7281292059219382,
      "grad_norm": 0.7009410858154297,
      "learning_rate": 0.00016174797964681233,
      "loss": 1.276,
      "step": 1284
    },
    {
      "epoch": 1.7294751009421265,
      "grad_norm": 0.7942853569984436,
      "learning_rate": 0.0001617180484884765,
      "loss": 1.5364,
      "step": 1285
    },
    {
      "epoch": 1.7308209959623149,
      "grad_norm": 0.7491145730018616,
      "learning_rate": 0.0001616881173301407,
      "loss": 1.0304,
      "step": 1286
    },
    {
      "epoch": 1.7321668909825032,
      "grad_norm": 0.6314314603805542,
      "learning_rate": 0.00016165818617180485,
      "loss": 1.1341,
      "step": 1287
    },
    {
      "epoch": 1.7335127860026918,
      "grad_norm": 0.7080572843551636,
      "learning_rate": 0.00016162825501346903,
      "loss": 1.2295,
      "step": 1288
    },
    {
      "epoch": 1.7348586810228803,
      "grad_norm": 0.6336363554000854,
      "learning_rate": 0.00016159832385513322,
      "loss": 1.3123,
      "step": 1289
    },
    {
      "epoch": 1.7362045760430687,
      "grad_norm": 0.6838037371635437,
      "learning_rate": 0.00016156839269679737,
      "loss": 1.1781,
      "step": 1290
    },
    {
      "epoch": 1.737550471063257,
      "grad_norm": 0.49130526185035706,
      "learning_rate": 0.00016153846153846155,
      "loss": 1.15,
      "step": 1291
    },
    {
      "epoch": 1.7388963660834453,
      "grad_norm": 0.6273691058158875,
      "learning_rate": 0.0001615085303801257,
      "loss": 1.255,
      "step": 1292
    },
    {
      "epoch": 1.740242261103634,
      "grad_norm": 0.669867217540741,
      "learning_rate": 0.0001614785992217899,
      "loss": 1.4662,
      "step": 1293
    },
    {
      "epoch": 1.7415881561238225,
      "grad_norm": 0.7188664674758911,
      "learning_rate": 0.00016144866806345407,
      "loss": 1.2107,
      "step": 1294
    },
    {
      "epoch": 1.7429340511440108,
      "grad_norm": 0.6146803498268127,
      "learning_rate": 0.00016141873690511823,
      "loss": 1.2327,
      "step": 1295
    },
    {
      "epoch": 1.7442799461641991,
      "grad_norm": 0.592496395111084,
      "learning_rate": 0.0001613888057467824,
      "loss": 1.2411,
      "step": 1296
    },
    {
      "epoch": 1.7456258411843875,
      "grad_norm": 0.6708683371543884,
      "learning_rate": 0.0001613588745884466,
      "loss": 1.2387,
      "step": 1297
    },
    {
      "epoch": 1.746971736204576,
      "grad_norm": 0.5177879333496094,
      "learning_rate": 0.00016132894343011075,
      "loss": 1.2923,
      "step": 1298
    },
    {
      "epoch": 1.7483176312247646,
      "grad_norm": 0.7120869159698486,
      "learning_rate": 0.00016129901227177493,
      "loss": 1.1499,
      "step": 1299
    },
    {
      "epoch": 1.749663526244953,
      "grad_norm": 0.683409571647644,
      "learning_rate": 0.0001612690811134391,
      "loss": 1.2909,
      "step": 1300
    },
    {
      "epoch": 1.7510094212651413,
      "grad_norm": 0.7892698049545288,
      "learning_rate": 0.00016123914995510327,
      "loss": 1.1483,
      "step": 1301
    },
    {
      "epoch": 1.7523553162853296,
      "grad_norm": 0.523542582988739,
      "learning_rate": 0.00016120921879676745,
      "loss": 1.4146,
      "step": 1302
    },
    {
      "epoch": 1.7537012113055181,
      "grad_norm": 0.7859563231468201,
      "learning_rate": 0.0001611792876384316,
      "loss": 1.1631,
      "step": 1303
    },
    {
      "epoch": 1.7550471063257067,
      "grad_norm": 0.5956973433494568,
      "learning_rate": 0.0001611493564800958,
      "loss": 1.1785,
      "step": 1304
    },
    {
      "epoch": 1.756393001345895,
      "grad_norm": 0.6623668074607849,
      "learning_rate": 0.00016111942532175998,
      "loss": 1.1945,
      "step": 1305
    },
    {
      "epoch": 1.7577388963660834,
      "grad_norm": 0.5374583601951599,
      "learning_rate": 0.00016108949416342413,
      "loss": 0.9431,
      "step": 1306
    },
    {
      "epoch": 1.7590847913862717,
      "grad_norm": 0.5099894404411316,
      "learning_rate": 0.00016105956300508831,
      "loss": 1.0857,
      "step": 1307
    },
    {
      "epoch": 1.7604306864064603,
      "grad_norm": 0.6048617362976074,
      "learning_rate": 0.00016102963184675247,
      "loss": 1.3834,
      "step": 1308
    },
    {
      "epoch": 1.7617765814266488,
      "grad_norm": 0.6109626293182373,
      "learning_rate": 0.00016099970068841665,
      "loss": 1.0853,
      "step": 1309
    },
    {
      "epoch": 1.7631224764468372,
      "grad_norm": 0.6828793883323669,
      "learning_rate": 0.00016096976953008084,
      "loss": 1.1199,
      "step": 1310
    },
    {
      "epoch": 1.7644683714670255,
      "grad_norm": 0.6687034964561462,
      "learning_rate": 0.000160939838371745,
      "loss": 1.3921,
      "step": 1311
    },
    {
      "epoch": 1.7658142664872138,
      "grad_norm": 0.6898810863494873,
      "learning_rate": 0.00016090990721340917,
      "loss": 1.306,
      "step": 1312
    },
    {
      "epoch": 1.7671601615074024,
      "grad_norm": 0.6469880938529968,
      "learning_rate": 0.00016087997605507336,
      "loss": 0.9111,
      "step": 1313
    },
    {
      "epoch": 1.768506056527591,
      "grad_norm": 0.5993456840515137,
      "learning_rate": 0.0001608500448967375,
      "loss": 1.2059,
      "step": 1314
    },
    {
      "epoch": 1.7698519515477793,
      "grad_norm": 0.5665874481201172,
      "learning_rate": 0.0001608201137384017,
      "loss": 0.9537,
      "step": 1315
    },
    {
      "epoch": 1.7711978465679676,
      "grad_norm": 0.6879111528396606,
      "learning_rate": 0.00016079018258006585,
      "loss": 1.1642,
      "step": 1316
    },
    {
      "epoch": 1.772543741588156,
      "grad_norm": 0.5981922745704651,
      "learning_rate": 0.00016076025142173003,
      "loss": 1.2735,
      "step": 1317
    },
    {
      "epoch": 1.7738896366083445,
      "grad_norm": 0.5980116128921509,
      "learning_rate": 0.00016073032026339422,
      "loss": 1.3955,
      "step": 1318
    },
    {
      "epoch": 1.775235531628533,
      "grad_norm": 0.6233866810798645,
      "learning_rate": 0.00016070038910505837,
      "loss": 1.1838,
      "step": 1319
    },
    {
      "epoch": 1.7765814266487214,
      "grad_norm": 0.5284020900726318,
      "learning_rate": 0.00016067045794672255,
      "loss": 1.2445,
      "step": 1320
    },
    {
      "epoch": 1.7779273216689098,
      "grad_norm": 0.8747891187667847,
      "learning_rate": 0.00016064052678838674,
      "loss": 1.5366,
      "step": 1321
    },
    {
      "epoch": 1.779273216689098,
      "grad_norm": 0.5628015995025635,
      "learning_rate": 0.0001606105956300509,
      "loss": 1.0997,
      "step": 1322
    },
    {
      "epoch": 1.7806191117092867,
      "grad_norm": 0.5835517644882202,
      "learning_rate": 0.00016058066447171507,
      "loss": 1.3322,
      "step": 1323
    },
    {
      "epoch": 1.7819650067294752,
      "grad_norm": 0.828016459941864,
      "learning_rate": 0.00016055073331337923,
      "loss": 1.2715,
      "step": 1324
    },
    {
      "epoch": 1.7833109017496636,
      "grad_norm": 0.6472226977348328,
      "learning_rate": 0.0001605208021550434,
      "loss": 1.2783,
      "step": 1325
    },
    {
      "epoch": 1.784656796769852,
      "grad_norm": 0.5986982583999634,
      "learning_rate": 0.0001604908709967076,
      "loss": 1.113,
      "step": 1326
    },
    {
      "epoch": 1.7860026917900402,
      "grad_norm": 0.590365469455719,
      "learning_rate": 0.00016046093983837175,
      "loss": 1.0083,
      "step": 1327
    },
    {
      "epoch": 1.7873485868102288,
      "grad_norm": 0.6026398539543152,
      "learning_rate": 0.00016043100868003593,
      "loss": 1.192,
      "step": 1328
    },
    {
      "epoch": 1.7886944818304173,
      "grad_norm": 0.7552654147148132,
      "learning_rate": 0.00016040107752170012,
      "loss": 1.135,
      "step": 1329
    },
    {
      "epoch": 1.7900403768506057,
      "grad_norm": 0.5822807550430298,
      "learning_rate": 0.00016037114636336427,
      "loss": 1.2066,
      "step": 1330
    },
    {
      "epoch": 1.791386271870794,
      "grad_norm": 0.6118819117546082,
      "learning_rate": 0.00016034121520502845,
      "loss": 1.1135,
      "step": 1331
    },
    {
      "epoch": 1.7927321668909824,
      "grad_norm": 0.6395220756530762,
      "learning_rate": 0.0001603112840466926,
      "loss": 1.2011,
      "step": 1332
    },
    {
      "epoch": 1.794078061911171,
      "grad_norm": 0.7013895511627197,
      "learning_rate": 0.0001602813528883568,
      "loss": 1.1446,
      "step": 1333
    },
    {
      "epoch": 1.7954239569313595,
      "grad_norm": 0.609437882900238,
      "learning_rate": 0.00016025142173002098,
      "loss": 1.02,
      "step": 1334
    },
    {
      "epoch": 1.7967698519515478,
      "grad_norm": 0.5985745191574097,
      "learning_rate": 0.00016022149057168513,
      "loss": 1.0817,
      "step": 1335
    },
    {
      "epoch": 1.7981157469717362,
      "grad_norm": 0.6223121285438538,
      "learning_rate": 0.00016019155941334931,
      "loss": 1.2334,
      "step": 1336
    },
    {
      "epoch": 1.7994616419919245,
      "grad_norm": 0.5830342173576355,
      "learning_rate": 0.0001601616282550135,
      "loss": 1.2697,
      "step": 1337
    },
    {
      "epoch": 1.800807537012113,
      "grad_norm": 0.6544480323791504,
      "learning_rate": 0.00016013169709667765,
      "loss": 1.497,
      "step": 1338
    },
    {
      "epoch": 1.8021534320323016,
      "grad_norm": 0.6091829538345337,
      "learning_rate": 0.00016010176593834184,
      "loss": 1.199,
      "step": 1339
    },
    {
      "epoch": 1.80349932705249,
      "grad_norm": 0.6289336681365967,
      "learning_rate": 0.000160071834780006,
      "loss": 1.3384,
      "step": 1340
    },
    {
      "epoch": 1.8048452220726783,
      "grad_norm": 0.6091790795326233,
      "learning_rate": 0.00016004190362167017,
      "loss": 1.1617,
      "step": 1341
    },
    {
      "epoch": 1.8061911170928666,
      "grad_norm": 0.6675990223884583,
      "learning_rate": 0.00016001197246333436,
      "loss": 1.0827,
      "step": 1342
    },
    {
      "epoch": 1.8075370121130552,
      "grad_norm": 0.7053064107894897,
      "learning_rate": 0.0001599820413049985,
      "loss": 1.2464,
      "step": 1343
    },
    {
      "epoch": 1.8088829071332437,
      "grad_norm": 0.6757038235664368,
      "learning_rate": 0.0001599521101466627,
      "loss": 1.2748,
      "step": 1344
    },
    {
      "epoch": 1.810228802153432,
      "grad_norm": 0.5867164731025696,
      "learning_rate": 0.00015992217898832688,
      "loss": 1.0399,
      "step": 1345
    },
    {
      "epoch": 1.8115746971736204,
      "grad_norm": 0.6043073534965515,
      "learning_rate": 0.00015989224782999103,
      "loss": 1.189,
      "step": 1346
    },
    {
      "epoch": 1.8129205921938087,
      "grad_norm": 0.7458982467651367,
      "learning_rate": 0.00015986231667165522,
      "loss": 1.3085,
      "step": 1347
    },
    {
      "epoch": 1.8142664872139973,
      "grad_norm": 0.6583566665649414,
      "learning_rate": 0.00015983238551331937,
      "loss": 1.2949,
      "step": 1348
    },
    {
      "epoch": 1.8156123822341859,
      "grad_norm": 0.586326539516449,
      "learning_rate": 0.00015980245435498355,
      "loss": 1.2317,
      "step": 1349
    },
    {
      "epoch": 1.8169582772543742,
      "grad_norm": 0.6297808885574341,
      "learning_rate": 0.00015977252319664774,
      "loss": 1.3426,
      "step": 1350
    },
    {
      "epoch": 1.8183041722745625,
      "grad_norm": 0.7040205001831055,
      "learning_rate": 0.0001597425920383119,
      "loss": 1.2216,
      "step": 1351
    },
    {
      "epoch": 1.8196500672947509,
      "grad_norm": 0.6638835072517395,
      "learning_rate": 0.00015971266087997607,
      "loss": 1.2953,
      "step": 1352
    },
    {
      "epoch": 1.8209959623149394,
      "grad_norm": 0.6510089039802551,
      "learning_rate": 0.00015968272972164026,
      "loss": 1.0963,
      "step": 1353
    },
    {
      "epoch": 1.822341857335128,
      "grad_norm": 0.5582414269447327,
      "learning_rate": 0.0001596527985633044,
      "loss": 1.3385,
      "step": 1354
    },
    {
      "epoch": 1.8236877523553163,
      "grad_norm": 0.8856554627418518,
      "learning_rate": 0.0001596228674049686,
      "loss": 1.1444,
      "step": 1355
    },
    {
      "epoch": 1.8250336473755047,
      "grad_norm": 0.6350832581520081,
      "learning_rate": 0.00015959293624663275,
      "loss": 1.1433,
      "step": 1356
    },
    {
      "epoch": 1.826379542395693,
      "grad_norm": 0.7230709195137024,
      "learning_rate": 0.00015956300508829693,
      "loss": 1.3999,
      "step": 1357
    },
    {
      "epoch": 1.8277254374158816,
      "grad_norm": 0.6646110415458679,
      "learning_rate": 0.00015953307392996112,
      "loss": 1.2034,
      "step": 1358
    },
    {
      "epoch": 1.8290713324360701,
      "grad_norm": 0.7380844950675964,
      "learning_rate": 0.00015950314277162527,
      "loss": 1.2221,
      "step": 1359
    },
    {
      "epoch": 1.8304172274562585,
      "grad_norm": 0.587401270866394,
      "learning_rate": 0.00015947321161328946,
      "loss": 1.4005,
      "step": 1360
    },
    {
      "epoch": 1.8317631224764468,
      "grad_norm": 0.6596425175666809,
      "learning_rate": 0.00015944328045495364,
      "loss": 1.1358,
      "step": 1361
    },
    {
      "epoch": 1.8331090174966351,
      "grad_norm": 0.6007677316665649,
      "learning_rate": 0.0001594133492966178,
      "loss": 1.1346,
      "step": 1362
    },
    {
      "epoch": 1.8344549125168237,
      "grad_norm": 0.663318395614624,
      "learning_rate": 0.00015938341813828198,
      "loss": 1.0762,
      "step": 1363
    },
    {
      "epoch": 1.8358008075370122,
      "grad_norm": 0.692256510257721,
      "learning_rate": 0.00015935348697994613,
      "loss": 1.2067,
      "step": 1364
    },
    {
      "epoch": 1.8371467025572006,
      "grad_norm": 0.5287525057792664,
      "learning_rate": 0.0001593235558216103,
      "loss": 1.0944,
      "step": 1365
    },
    {
      "epoch": 1.838492597577389,
      "grad_norm": 0.5924580693244934,
      "learning_rate": 0.00015929362466327447,
      "loss": 1.4226,
      "step": 1366
    },
    {
      "epoch": 1.8398384925975773,
      "grad_norm": 0.7601609230041504,
      "learning_rate": 0.00015926369350493863,
      "loss": 1.1135,
      "step": 1367
    },
    {
      "epoch": 1.8411843876177658,
      "grad_norm": 0.6859882473945618,
      "learning_rate": 0.0001592337623466028,
      "loss": 1.0767,
      "step": 1368
    },
    {
      "epoch": 1.8425302826379544,
      "grad_norm": 0.6234051585197449,
      "learning_rate": 0.000159203831188267,
      "loss": 1.2057,
      "step": 1369
    },
    {
      "epoch": 1.8438761776581427,
      "grad_norm": 0.6700024008750916,
      "learning_rate": 0.00015917390002993115,
      "loss": 0.9657,
      "step": 1370
    },
    {
      "epoch": 1.845222072678331,
      "grad_norm": 0.6889744400978088,
      "learning_rate": 0.00015914396887159533,
      "loss": 1.3336,
      "step": 1371
    },
    {
      "epoch": 1.8465679676985194,
      "grad_norm": 0.6086926460266113,
      "learning_rate": 0.0001591140377132595,
      "loss": 1.5256,
      "step": 1372
    },
    {
      "epoch": 1.847913862718708,
      "grad_norm": 0.5879335999488831,
      "learning_rate": 0.00015908410655492367,
      "loss": 1.1168,
      "step": 1373
    },
    {
      "epoch": 1.8492597577388965,
      "grad_norm": 0.5858213901519775,
      "learning_rate": 0.00015905417539658785,
      "loss": 1.1222,
      "step": 1374
    },
    {
      "epoch": 1.8506056527590848,
      "grad_norm": 0.6530058979988098,
      "learning_rate": 0.000159024244238252,
      "loss": 1.2808,
      "step": 1375
    },
    {
      "epoch": 1.8519515477792732,
      "grad_norm": 0.6883113384246826,
      "learning_rate": 0.0001589943130799162,
      "loss": 1.4225,
      "step": 1376
    },
    {
      "epoch": 1.8532974427994615,
      "grad_norm": 0.8318554759025574,
      "learning_rate": 0.00015896438192158037,
      "loss": 1.2987,
      "step": 1377
    },
    {
      "epoch": 1.85464333781965,
      "grad_norm": 0.5646892786026001,
      "learning_rate": 0.00015893445076324453,
      "loss": 1.0282,
      "step": 1378
    },
    {
      "epoch": 1.8559892328398386,
      "grad_norm": 0.6214845180511475,
      "learning_rate": 0.0001589045196049087,
      "loss": 1.2234,
      "step": 1379
    },
    {
      "epoch": 1.857335127860027,
      "grad_norm": 0.49283599853515625,
      "learning_rate": 0.0001588745884465729,
      "loss": 1.0242,
      "step": 1380
    },
    {
      "epoch": 1.8586810228802153,
      "grad_norm": 0.6166630387306213,
      "learning_rate": 0.00015884465728823705,
      "loss": 1.4873,
      "step": 1381
    },
    {
      "epoch": 1.8600269179004036,
      "grad_norm": 0.6408758759498596,
      "learning_rate": 0.00015881472612990123,
      "loss": 1.2389,
      "step": 1382
    },
    {
      "epoch": 1.8613728129205922,
      "grad_norm": 0.5574899911880493,
      "learning_rate": 0.00015878479497156539,
      "loss": 1.2455,
      "step": 1383
    },
    {
      "epoch": 1.8627187079407808,
      "grad_norm": 0.5874636769294739,
      "learning_rate": 0.00015875486381322957,
      "loss": 1.0647,
      "step": 1384
    },
    {
      "epoch": 1.864064602960969,
      "grad_norm": 0.6540279984474182,
      "learning_rate": 0.00015872493265489375,
      "loss": 1.198,
      "step": 1385
    },
    {
      "epoch": 1.8654104979811574,
      "grad_norm": 0.6883217096328735,
      "learning_rate": 0.0001586950014965579,
      "loss": 1.4543,
      "step": 1386
    },
    {
      "epoch": 1.8667563930013458,
      "grad_norm": 0.632621169090271,
      "learning_rate": 0.0001586650703382221,
      "loss": 1.3446,
      "step": 1387
    },
    {
      "epoch": 1.8681022880215343,
      "grad_norm": 0.686027467250824,
      "learning_rate": 0.00015863513917988627,
      "loss": 1.2847,
      "step": 1388
    },
    {
      "epoch": 1.8694481830417229,
      "grad_norm": 0.6876628398895264,
      "learning_rate": 0.00015860520802155043,
      "loss": 1.3337,
      "step": 1389
    },
    {
      "epoch": 1.8707940780619112,
      "grad_norm": 0.6633261442184448,
      "learning_rate": 0.0001585752768632146,
      "loss": 1.5972,
      "step": 1390
    },
    {
      "epoch": 1.8721399730820996,
      "grad_norm": 0.6202484369277954,
      "learning_rate": 0.00015854534570487877,
      "loss": 0.9788,
      "step": 1391
    },
    {
      "epoch": 1.873485868102288,
      "grad_norm": 0.6775609254837036,
      "learning_rate": 0.00015851541454654295,
      "loss": 1.1828,
      "step": 1392
    },
    {
      "epoch": 1.8748317631224765,
      "grad_norm": 0.7035114765167236,
      "learning_rate": 0.00015848548338820713,
      "loss": 1.1521,
      "step": 1393
    },
    {
      "epoch": 1.876177658142665,
      "grad_norm": 0.7189344167709351,
      "learning_rate": 0.0001584555522298713,
      "loss": 1.3446,
      "step": 1394
    },
    {
      "epoch": 1.8775235531628534,
      "grad_norm": 0.6154974102973938,
      "learning_rate": 0.00015842562107153547,
      "loss": 1.1612,
      "step": 1395
    },
    {
      "epoch": 1.8788694481830417,
      "grad_norm": 0.7294836640357971,
      "learning_rate": 0.00015839568991319965,
      "loss": 1.379,
      "step": 1396
    },
    {
      "epoch": 1.88021534320323,
      "grad_norm": 0.6074212789535522,
      "learning_rate": 0.0001583657587548638,
      "loss": 1.1734,
      "step": 1397
    },
    {
      "epoch": 1.8815612382234186,
      "grad_norm": 0.5385746955871582,
      "learning_rate": 0.000158335827596528,
      "loss": 1.2696,
      "step": 1398
    },
    {
      "epoch": 1.8829071332436071,
      "grad_norm": 0.6903482675552368,
      "learning_rate": 0.00015830589643819215,
      "loss": 1.2723,
      "step": 1399
    },
    {
      "epoch": 1.8842530282637955,
      "grad_norm": 0.6634740829467773,
      "learning_rate": 0.00015827596527985633,
      "loss": 1.1339,
      "step": 1400
    },
    {
      "epoch": 1.8855989232839838,
      "grad_norm": 0.5969931483268738,
      "learning_rate": 0.0001582460341215205,
      "loss": 1.193,
      "step": 1401
    },
    {
      "epoch": 1.8869448183041722,
      "grad_norm": 0.5989328026771545,
      "learning_rate": 0.00015821610296318467,
      "loss": 1.3385,
      "step": 1402
    },
    {
      "epoch": 1.8882907133243607,
      "grad_norm": 0.7151175141334534,
      "learning_rate": 0.00015818617180484885,
      "loss": 1.3065,
      "step": 1403
    },
    {
      "epoch": 1.8896366083445493,
      "grad_norm": 0.6557611227035522,
      "learning_rate": 0.00015815624064651303,
      "loss": 1.1888,
      "step": 1404
    },
    {
      "epoch": 1.8909825033647376,
      "grad_norm": 0.8018819689750671,
      "learning_rate": 0.0001581263094881772,
      "loss": 1.4023,
      "step": 1405
    },
    {
      "epoch": 1.892328398384926,
      "grad_norm": 0.6809384822845459,
      "learning_rate": 0.00015809637832984137,
      "loss": 1.2613,
      "step": 1406
    },
    {
      "epoch": 1.8936742934051143,
      "grad_norm": 0.7026821970939636,
      "learning_rate": 0.00015806644717150553,
      "loss": 0.968,
      "step": 1407
    },
    {
      "epoch": 1.8950201884253028,
      "grad_norm": 0.6734697818756104,
      "learning_rate": 0.0001580365160131697,
      "loss": 1.4019,
      "step": 1408
    },
    {
      "epoch": 1.8963660834454914,
      "grad_norm": 0.657265841960907,
      "learning_rate": 0.0001580065848548339,
      "loss": 1.143,
      "step": 1409
    },
    {
      "epoch": 1.8977119784656797,
      "grad_norm": 0.7691875696182251,
      "learning_rate": 0.00015797665369649805,
      "loss": 1.2959,
      "step": 1410
    },
    {
      "epoch": 1.899057873485868,
      "grad_norm": 0.6088029146194458,
      "learning_rate": 0.00015794672253816223,
      "loss": 1.1397,
      "step": 1411
    },
    {
      "epoch": 1.9004037685060564,
      "grad_norm": 0.728057324886322,
      "learning_rate": 0.0001579167913798264,
      "loss": 1.1601,
      "step": 1412
    },
    {
      "epoch": 1.901749663526245,
      "grad_norm": 0.8106998801231384,
      "learning_rate": 0.00015788686022149057,
      "loss": 1.3011,
      "step": 1413
    },
    {
      "epoch": 1.9030955585464335,
      "grad_norm": 0.7229302525520325,
      "learning_rate": 0.00015785692906315475,
      "loss": 1.0331,
      "step": 1414
    },
    {
      "epoch": 1.9044414535666219,
      "grad_norm": 0.7415332198143005,
      "learning_rate": 0.0001578269979048189,
      "loss": 1.4711,
      "step": 1415
    },
    {
      "epoch": 1.9057873485868102,
      "grad_norm": 0.7122513055801392,
      "learning_rate": 0.0001577970667464831,
      "loss": 1.4573,
      "step": 1416
    },
    {
      "epoch": 1.9071332436069985,
      "grad_norm": 0.7432560324668884,
      "learning_rate": 0.00015776713558814727,
      "loss": 1.4809,
      "step": 1417
    },
    {
      "epoch": 1.908479138627187,
      "grad_norm": 0.552436351776123,
      "learning_rate": 0.00015773720442981143,
      "loss": 1.4621,
      "step": 1418
    },
    {
      "epoch": 1.9098250336473757,
      "grad_norm": 0.7544198632240295,
      "learning_rate": 0.0001577072732714756,
      "loss": 1.3008,
      "step": 1419
    },
    {
      "epoch": 1.911170928667564,
      "grad_norm": 0.6592555046081543,
      "learning_rate": 0.0001576773421131398,
      "loss": 1.1622,
      "step": 1420
    },
    {
      "epoch": 1.9125168236877523,
      "grad_norm": 0.7273185849189758,
      "learning_rate": 0.00015764741095480395,
      "loss": 1.1531,
      "step": 1421
    },
    {
      "epoch": 1.9138627187079407,
      "grad_norm": 0.8871517777442932,
      "learning_rate": 0.00015761747979646813,
      "loss": 1.1094,
      "step": 1422
    },
    {
      "epoch": 1.9152086137281292,
      "grad_norm": 0.8352733850479126,
      "learning_rate": 0.0001575875486381323,
      "loss": 1.5236,
      "step": 1423
    },
    {
      "epoch": 1.9165545087483178,
      "grad_norm": 0.6641095280647278,
      "learning_rate": 0.00015755761747979647,
      "loss": 1.2215,
      "step": 1424
    },
    {
      "epoch": 1.9179004037685061,
      "grad_norm": 0.6057230234146118,
      "learning_rate": 0.00015752768632146065,
      "loss": 1.2256,
      "step": 1425
    },
    {
      "epoch": 1.9192462987886945,
      "grad_norm": 0.7152158617973328,
      "learning_rate": 0.0001574977551631248,
      "loss": 1.0761,
      "step": 1426
    },
    {
      "epoch": 1.9205921938088828,
      "grad_norm": 0.7027897238731384,
      "learning_rate": 0.000157467824004789,
      "loss": 1.314,
      "step": 1427
    },
    {
      "epoch": 1.9219380888290714,
      "grad_norm": 0.6588833928108215,
      "learning_rate": 0.00015743789284645317,
      "loss": 1.1708,
      "step": 1428
    },
    {
      "epoch": 1.92328398384926,
      "grad_norm": 0.6169861555099487,
      "learning_rate": 0.00015740796168811733,
      "loss": 1.2501,
      "step": 1429
    },
    {
      "epoch": 1.9246298788694483,
      "grad_norm": 0.5831501483917236,
      "learning_rate": 0.0001573780305297815,
      "loss": 1.1572,
      "step": 1430
    },
    {
      "epoch": 1.9259757738896366,
      "grad_norm": 0.6481071710586548,
      "learning_rate": 0.00015734809937144567,
      "loss": 1.2813,
      "step": 1431
    },
    {
      "epoch": 1.927321668909825,
      "grad_norm": 0.6506379246711731,
      "learning_rate": 0.00015731816821310985,
      "loss": 1.1362,
      "step": 1432
    },
    {
      "epoch": 1.9286675639300135,
      "grad_norm": 0.5844634771347046,
      "learning_rate": 0.00015728823705477403,
      "loss": 1.2465,
      "step": 1433
    },
    {
      "epoch": 1.930013458950202,
      "grad_norm": 0.6619353890419006,
      "learning_rate": 0.0001572583058964382,
      "loss": 1.131,
      "step": 1434
    },
    {
      "epoch": 1.9313593539703904,
      "grad_norm": 0.7317864298820496,
      "learning_rate": 0.00015722837473810237,
      "loss": 1.3914,
      "step": 1435
    },
    {
      "epoch": 1.9327052489905787,
      "grad_norm": 0.7080686092376709,
      "learning_rate": 0.00015719844357976655,
      "loss": 1.3776,
      "step": 1436
    },
    {
      "epoch": 1.934051144010767,
      "grad_norm": 0.7585915923118591,
      "learning_rate": 0.0001571685124214307,
      "loss": 1.4152,
      "step": 1437
    },
    {
      "epoch": 1.9353970390309556,
      "grad_norm": 0.6297810077667236,
      "learning_rate": 0.0001571385812630949,
      "loss": 1.2071,
      "step": 1438
    },
    {
      "epoch": 1.9367429340511442,
      "grad_norm": 0.6972113251686096,
      "learning_rate": 0.00015710865010475905,
      "loss": 1.2421,
      "step": 1439
    },
    {
      "epoch": 1.9380888290713325,
      "grad_norm": 0.7804901003837585,
      "learning_rate": 0.00015707871894642323,
      "loss": 1.3861,
      "step": 1440
    },
    {
      "epoch": 1.9394347240915208,
      "grad_norm": 0.6462268233299255,
      "learning_rate": 0.00015704878778808741,
      "loss": 1.1817,
      "step": 1441
    },
    {
      "epoch": 1.9407806191117092,
      "grad_norm": 0.7447550892829895,
      "learning_rate": 0.00015701885662975157,
      "loss": 1.4006,
      "step": 1442
    },
    {
      "epoch": 1.9421265141318977,
      "grad_norm": 0.6014191508293152,
      "learning_rate": 0.00015698892547141575,
      "loss": 1.1916,
      "step": 1443
    },
    {
      "epoch": 1.9434724091520863,
      "grad_norm": 0.768700122833252,
      "learning_rate": 0.00015695899431307993,
      "loss": 1.2937,
      "step": 1444
    },
    {
      "epoch": 1.9448183041722746,
      "grad_norm": 0.7445749640464783,
      "learning_rate": 0.0001569290631547441,
      "loss": 1.0513,
      "step": 1445
    },
    {
      "epoch": 1.946164199192463,
      "grad_norm": 0.5513935089111328,
      "learning_rate": 0.00015689913199640827,
      "loss": 1.1834,
      "step": 1446
    },
    {
      "epoch": 1.9475100942126513,
      "grad_norm": 0.6680911779403687,
      "learning_rate": 0.00015686920083807243,
      "loss": 1.1932,
      "step": 1447
    },
    {
      "epoch": 1.9488559892328399,
      "grad_norm": 0.5873083472251892,
      "learning_rate": 0.0001568392696797366,
      "loss": 1.4094,
      "step": 1448
    },
    {
      "epoch": 1.9502018842530284,
      "grad_norm": 0.6696431040763855,
      "learning_rate": 0.0001568093385214008,
      "loss": 1.2608,
      "step": 1449
    },
    {
      "epoch": 1.9515477792732168,
      "grad_norm": 0.522546648979187,
      "learning_rate": 0.00015677940736306495,
      "loss": 1.159,
      "step": 1450
    },
    {
      "epoch": 1.952893674293405,
      "grad_norm": 0.7385554909706116,
      "learning_rate": 0.00015674947620472913,
      "loss": 1.1788,
      "step": 1451
    },
    {
      "epoch": 1.9542395693135934,
      "grad_norm": 0.5939211845397949,
      "learning_rate": 0.0001567195450463933,
      "loss": 1.3885,
      "step": 1452
    },
    {
      "epoch": 1.955585464333782,
      "grad_norm": 0.7259278893470764,
      "learning_rate": 0.00015668961388805747,
      "loss": 1.0569,
      "step": 1453
    },
    {
      "epoch": 1.9569313593539706,
      "grad_norm": 0.6868517994880676,
      "learning_rate": 0.00015665968272972165,
      "loss": 1.0911,
      "step": 1454
    },
    {
      "epoch": 1.958277254374159,
      "grad_norm": 0.6859127879142761,
      "learning_rate": 0.0001566297515713858,
      "loss": 1.1422,
      "step": 1455
    },
    {
      "epoch": 1.9596231493943472,
      "grad_norm": 0.8485366106033325,
      "learning_rate": 0.00015659982041305,
      "loss": 1.4679,
      "step": 1456
    },
    {
      "epoch": 1.9609690444145356,
      "grad_norm": 0.7122189402580261,
      "learning_rate": 0.00015656988925471417,
      "loss": 1.3357,
      "step": 1457
    },
    {
      "epoch": 1.9623149394347241,
      "grad_norm": 0.5938192009925842,
      "learning_rate": 0.00015653995809637833,
      "loss": 1.1058,
      "step": 1458
    },
    {
      "epoch": 1.9636608344549125,
      "grad_norm": 0.6056537628173828,
      "learning_rate": 0.0001565100269380425,
      "loss": 1.2565,
      "step": 1459
    },
    {
      "epoch": 1.965006729475101,
      "grad_norm": 0.7828239798545837,
      "learning_rate": 0.00015648009577970667,
      "loss": 1.2994,
      "step": 1460
    },
    {
      "epoch": 1.9663526244952894,
      "grad_norm": 0.580909013748169,
      "learning_rate": 0.00015645016462137085,
      "loss": 0.9728,
      "step": 1461
    },
    {
      "epoch": 1.9676985195154777,
      "grad_norm": 0.7818027138710022,
      "learning_rate": 0.00015642023346303503,
      "loss": 1.1868,
      "step": 1462
    },
    {
      "epoch": 1.9690444145356663,
      "grad_norm": 0.6706185340881348,
      "learning_rate": 0.0001563903023046992,
      "loss": 1.1658,
      "step": 1463
    },
    {
      "epoch": 1.9703903095558546,
      "grad_norm": 0.6427832841873169,
      "learning_rate": 0.00015636037114636337,
      "loss": 1.1062,
      "step": 1464
    },
    {
      "epoch": 1.9717362045760431,
      "grad_norm": 0.6002992391586304,
      "learning_rate": 0.00015633043998802755,
      "loss": 1.3413,
      "step": 1465
    },
    {
      "epoch": 1.9730820995962315,
      "grad_norm": 0.6531703472137451,
      "learning_rate": 0.0001563005088296917,
      "loss": 1.1411,
      "step": 1466
    },
    {
      "epoch": 1.9744279946164198,
      "grad_norm": 0.5994337201118469,
      "learning_rate": 0.0001562705776713559,
      "loss": 1.231,
      "step": 1467
    },
    {
      "epoch": 1.9757738896366084,
      "grad_norm": 0.7201400399208069,
      "learning_rate": 0.00015624064651302005,
      "loss": 1.1118,
      "step": 1468
    },
    {
      "epoch": 1.9771197846567967,
      "grad_norm": 0.5834812521934509,
      "learning_rate": 0.00015621071535468423,
      "loss": 1.1873,
      "step": 1469
    },
    {
      "epoch": 1.9784656796769853,
      "grad_norm": 0.7342109084129333,
      "learning_rate": 0.00015618078419634841,
      "loss": 1.0571,
      "step": 1470
    },
    {
      "epoch": 1.9798115746971736,
      "grad_norm": 0.7402820587158203,
      "learning_rate": 0.00015615085303801257,
      "loss": 1.279,
      "step": 1471
    },
    {
      "epoch": 1.981157469717362,
      "grad_norm": 0.6687713265419006,
      "learning_rate": 0.00015612092187967675,
      "loss": 1.3259,
      "step": 1472
    },
    {
      "epoch": 1.9825033647375505,
      "grad_norm": 0.7821213006973267,
      "learning_rate": 0.00015609099072134093,
      "loss": 1.175,
      "step": 1473
    },
    {
      "epoch": 1.9838492597577388,
      "grad_norm": 0.5959514379501343,
      "learning_rate": 0.0001560610595630051,
      "loss": 1.2826,
      "step": 1474
    },
    {
      "epoch": 1.9851951547779274,
      "grad_norm": 0.6887415647506714,
      "learning_rate": 0.00015603112840466927,
      "loss": 1.0087,
      "step": 1475
    },
    {
      "epoch": 1.9865410497981157,
      "grad_norm": 0.7357552647590637,
      "learning_rate": 0.00015600119724633343,
      "loss": 1.2708,
      "step": 1476
    },
    {
      "epoch": 1.987886944818304,
      "grad_norm": 0.6135392785072327,
      "learning_rate": 0.0001559712660879976,
      "loss": 1.1463,
      "step": 1477
    },
    {
      "epoch": 1.9892328398384926,
      "grad_norm": 0.5859721302986145,
      "learning_rate": 0.0001559413349296618,
      "loss": 1.1985,
      "step": 1478
    },
    {
      "epoch": 1.990578734858681,
      "grad_norm": 0.672045111656189,
      "learning_rate": 0.00015591140377132595,
      "loss": 1.5899,
      "step": 1479
    },
    {
      "epoch": 1.9919246298788695,
      "grad_norm": 0.6681638956069946,
      "learning_rate": 0.00015588147261299013,
      "loss": 1.0689,
      "step": 1480
    },
    {
      "epoch": 1.9932705248990579,
      "grad_norm": 0.5544987916946411,
      "learning_rate": 0.00015585154145465432,
      "loss": 1.0086,
      "step": 1481
    },
    {
      "epoch": 1.9946164199192462,
      "grad_norm": 0.9499432444572449,
      "learning_rate": 0.00015582161029631847,
      "loss": 1.2217,
      "step": 1482
    },
    {
      "epoch": 1.9959623149394348,
      "grad_norm": 0.7579907178878784,
      "learning_rate": 0.00015579167913798265,
      "loss": 1.1285,
      "step": 1483
    },
    {
      "epoch": 1.997308209959623,
      "grad_norm": 0.6443124413490295,
      "learning_rate": 0.0001557617479796468,
      "loss": 1.2909,
      "step": 1484
    },
    {
      "epoch": 1.9986541049798117,
      "grad_norm": 0.6873032450675964,
      "learning_rate": 0.000155731816821311,
      "loss": 1.1889,
      "step": 1485
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.6900546550750732,
      "learning_rate": 0.00015570188566297517,
      "loss": 1.2651,
      "step": 1486
    },
    {
      "epoch": 2.0013458950201883,
      "grad_norm": 0.5157869458198547,
      "learning_rate": 0.00015567195450463933,
      "loss": 1.397,
      "step": 1487
    },
    {
      "epoch": 2.0026917900403767,
      "grad_norm": 0.6540727615356445,
      "learning_rate": 0.0001556420233463035,
      "loss": 0.6955,
      "step": 1488
    },
    {
      "epoch": 2.0040376850605655,
      "grad_norm": 0.7415268421173096,
      "learning_rate": 0.0001556120921879677,
      "loss": 1.0283,
      "step": 1489
    },
    {
      "epoch": 2.005383580080754,
      "grad_norm": 0.5758304595947266,
      "learning_rate": 0.00015558216102963185,
      "loss": 1.0711,
      "step": 1490
    },
    {
      "epoch": 2.006729475100942,
      "grad_norm": 0.5698645114898682,
      "learning_rate": 0.00015555222987129603,
      "loss": 1.1148,
      "step": 1491
    },
    {
      "epoch": 2.0080753701211305,
      "grad_norm": 0.6396982669830322,
      "learning_rate": 0.0001555222987129602,
      "loss": 0.9961,
      "step": 1492
    },
    {
      "epoch": 2.009421265141319,
      "grad_norm": 0.5624166131019592,
      "learning_rate": 0.00015549236755462437,
      "loss": 0.8452,
      "step": 1493
    },
    {
      "epoch": 2.0107671601615076,
      "grad_norm": 0.6352960467338562,
      "learning_rate": 0.00015546243639628855,
      "loss": 0.8683,
      "step": 1494
    },
    {
      "epoch": 2.012113055181696,
      "grad_norm": 0.6098929047584534,
      "learning_rate": 0.0001554325052379527,
      "loss": 0.9742,
      "step": 1495
    },
    {
      "epoch": 2.0134589502018843,
      "grad_norm": 0.6589899659156799,
      "learning_rate": 0.0001554025740796169,
      "loss": 1.0127,
      "step": 1496
    },
    {
      "epoch": 2.0148048452220726,
      "grad_norm": 0.8394904136657715,
      "learning_rate": 0.00015537264292128108,
      "loss": 0.9792,
      "step": 1497
    },
    {
      "epoch": 2.016150740242261,
      "grad_norm": 0.6129952073097229,
      "learning_rate": 0.00015534271176294523,
      "loss": 0.8268,
      "step": 1498
    },
    {
      "epoch": 2.0174966352624497,
      "grad_norm": 0.80559241771698,
      "learning_rate": 0.00015531278060460941,
      "loss": 0.9197,
      "step": 1499
    },
    {
      "epoch": 2.018842530282638,
      "grad_norm": 0.6915819644927979,
      "learning_rate": 0.00015528284944627357,
      "loss": 0.9282,
      "step": 1500
    },
    {
      "epoch": 2.0201884253028264,
      "grad_norm": 0.4876091778278351,
      "learning_rate": 0.00015525291828793775,
      "loss": 1.1423,
      "step": 1501
    },
    {
      "epoch": 2.0215343203230147,
      "grad_norm": 0.7158130407333374,
      "learning_rate": 0.00015522298712960194,
      "loss": 0.7542,
      "step": 1502
    },
    {
      "epoch": 2.022880215343203,
      "grad_norm": 0.825628936290741,
      "learning_rate": 0.0001551930559712661,
      "loss": 0.9941,
      "step": 1503
    },
    {
      "epoch": 2.024226110363392,
      "grad_norm": 0.9718024134635925,
      "learning_rate": 0.00015516312481293027,
      "loss": 1.1585,
      "step": 1504
    },
    {
      "epoch": 2.02557200538358,
      "grad_norm": 0.7078129649162292,
      "learning_rate": 0.00015513319365459446,
      "loss": 1.059,
      "step": 1505
    },
    {
      "epoch": 2.0269179004037685,
      "grad_norm": 0.8099305033683777,
      "learning_rate": 0.0001551032624962586,
      "loss": 1.0282,
      "step": 1506
    },
    {
      "epoch": 2.028263795423957,
      "grad_norm": 0.7247793674468994,
      "learning_rate": 0.0001550733313379228,
      "loss": 0.955,
      "step": 1507
    },
    {
      "epoch": 2.029609690444145,
      "grad_norm": 0.6933260560035706,
      "learning_rate": 0.00015504340017958695,
      "loss": 0.8601,
      "step": 1508
    },
    {
      "epoch": 2.030955585464334,
      "grad_norm": 0.7165217995643616,
      "learning_rate": 0.00015501346902125113,
      "loss": 0.8431,
      "step": 1509
    },
    {
      "epoch": 2.0323014804845223,
      "grad_norm": 0.7401472926139832,
      "learning_rate": 0.00015498353786291532,
      "loss": 0.9125,
      "step": 1510
    },
    {
      "epoch": 2.0336473755047106,
      "grad_norm": 0.6562045216560364,
      "learning_rate": 0.00015495360670457947,
      "loss": 0.6727,
      "step": 1511
    },
    {
      "epoch": 2.034993270524899,
      "grad_norm": 0.7255260348320007,
      "learning_rate": 0.00015492367554624365,
      "loss": 0.8158,
      "step": 1512
    },
    {
      "epoch": 2.0363391655450873,
      "grad_norm": 0.6429377198219299,
      "learning_rate": 0.00015489374438790784,
      "loss": 0.9426,
      "step": 1513
    },
    {
      "epoch": 2.037685060565276,
      "grad_norm": 0.8681187033653259,
      "learning_rate": 0.000154863813229572,
      "loss": 0.8993,
      "step": 1514
    },
    {
      "epoch": 2.0390309555854644,
      "grad_norm": 0.8510393500328064,
      "learning_rate": 0.00015483388207123617,
      "loss": 1.0639,
      "step": 1515
    },
    {
      "epoch": 2.0403768506056528,
      "grad_norm": 0.9032027721405029,
      "learning_rate": 0.00015480395091290033,
      "loss": 0.8486,
      "step": 1516
    },
    {
      "epoch": 2.041722745625841,
      "grad_norm": 0.7763326168060303,
      "learning_rate": 0.0001547740197545645,
      "loss": 0.7493,
      "step": 1517
    },
    {
      "epoch": 2.0430686406460294,
      "grad_norm": 0.7235771417617798,
      "learning_rate": 0.0001547440885962287,
      "loss": 0.716,
      "step": 1518
    },
    {
      "epoch": 2.0444145356662182,
      "grad_norm": 1.0844467878341675,
      "learning_rate": 0.00015471415743789285,
      "loss": 1.0234,
      "step": 1519
    },
    {
      "epoch": 2.0457604306864066,
      "grad_norm": 0.8122111558914185,
      "learning_rate": 0.00015468422627955703,
      "loss": 1.0387,
      "step": 1520
    },
    {
      "epoch": 2.047106325706595,
      "grad_norm": 0.7604650855064392,
      "learning_rate": 0.00015465429512122122,
      "loss": 1.1987,
      "step": 1521
    },
    {
      "epoch": 2.0484522207267832,
      "grad_norm": 0.8418589234352112,
      "learning_rate": 0.00015462436396288537,
      "loss": 0.9695,
      "step": 1522
    },
    {
      "epoch": 2.0497981157469716,
      "grad_norm": 0.70891273021698,
      "learning_rate": 0.00015459443280454955,
      "loss": 0.9083,
      "step": 1523
    },
    {
      "epoch": 2.0511440107671604,
      "grad_norm": 0.7577754855155945,
      "learning_rate": 0.0001545645016462137,
      "loss": 0.8065,
      "step": 1524
    },
    {
      "epoch": 2.0524899057873487,
      "grad_norm": 0.8593953847885132,
      "learning_rate": 0.0001545345704878779,
      "loss": 0.772,
      "step": 1525
    },
    {
      "epoch": 2.053835800807537,
      "grad_norm": 0.5913217067718506,
      "learning_rate": 0.00015450463932954208,
      "loss": 0.7728,
      "step": 1526
    },
    {
      "epoch": 2.0551816958277254,
      "grad_norm": 0.594022810459137,
      "learning_rate": 0.00015447470817120623,
      "loss": 0.824,
      "step": 1527
    },
    {
      "epoch": 2.0565275908479137,
      "grad_norm": 0.7488406896591187,
      "learning_rate": 0.00015444477701287041,
      "loss": 0.867,
      "step": 1528
    },
    {
      "epoch": 2.0578734858681025,
      "grad_norm": 0.8658323884010315,
      "learning_rate": 0.0001544148458545346,
      "loss": 0.9175,
      "step": 1529
    },
    {
      "epoch": 2.059219380888291,
      "grad_norm": 0.9419140815734863,
      "learning_rate": 0.00015438491469619875,
      "loss": 0.9364,
      "step": 1530
    },
    {
      "epoch": 2.060565275908479,
      "grad_norm": 0.6955963373184204,
      "learning_rate": 0.00015435498353786294,
      "loss": 0.876,
      "step": 1531
    },
    {
      "epoch": 2.0619111709286675,
      "grad_norm": 0.8589543104171753,
      "learning_rate": 0.0001543250523795271,
      "loss": 0.9696,
      "step": 1532
    },
    {
      "epoch": 2.063257065948856,
      "grad_norm": 0.8559412360191345,
      "learning_rate": 0.00015429512122119127,
      "loss": 0.9429,
      "step": 1533
    },
    {
      "epoch": 2.0646029609690446,
      "grad_norm": 0.7208817601203918,
      "learning_rate": 0.00015426519006285546,
      "loss": 0.6741,
      "step": 1534
    },
    {
      "epoch": 2.065948855989233,
      "grad_norm": 0.7399240732192993,
      "learning_rate": 0.0001542352589045196,
      "loss": 0.9938,
      "step": 1535
    },
    {
      "epoch": 2.0672947510094213,
      "grad_norm": 0.8079234957695007,
      "learning_rate": 0.0001542053277461838,
      "loss": 1.0395,
      "step": 1536
    },
    {
      "epoch": 2.0686406460296096,
      "grad_norm": 0.8102609515190125,
      "learning_rate": 0.00015417539658784798,
      "loss": 1.0514,
      "step": 1537
    },
    {
      "epoch": 2.069986541049798,
      "grad_norm": 0.7165748476982117,
      "learning_rate": 0.00015414546542951213,
      "loss": 1.0133,
      "step": 1538
    },
    {
      "epoch": 2.0713324360699867,
      "grad_norm": 0.8094316124916077,
      "learning_rate": 0.00015411553427117632,
      "loss": 0.8363,
      "step": 1539
    },
    {
      "epoch": 2.072678331090175,
      "grad_norm": 0.8371199369430542,
      "learning_rate": 0.00015408560311284047,
      "loss": 0.9723,
      "step": 1540
    },
    {
      "epoch": 2.0740242261103634,
      "grad_norm": 0.7207016348838806,
      "learning_rate": 0.00015405567195450465,
      "loss": 0.9612,
      "step": 1541
    },
    {
      "epoch": 2.0753701211305517,
      "grad_norm": 0.8120953440666199,
      "learning_rate": 0.00015402574079616884,
      "loss": 0.8887,
      "step": 1542
    },
    {
      "epoch": 2.07671601615074,
      "grad_norm": 0.7301130890846252,
      "learning_rate": 0.000153995809637833,
      "loss": 0.9742,
      "step": 1543
    },
    {
      "epoch": 2.078061911170929,
      "grad_norm": 0.9544197916984558,
      "learning_rate": 0.00015396587847949717,
      "loss": 1.0782,
      "step": 1544
    },
    {
      "epoch": 2.079407806191117,
      "grad_norm": 0.8852181434631348,
      "learning_rate": 0.00015393594732116136,
      "loss": 1.0239,
      "step": 1545
    },
    {
      "epoch": 2.0807537012113055,
      "grad_norm": 0.8111228346824646,
      "learning_rate": 0.0001539060161628255,
      "loss": 1.1233,
      "step": 1546
    },
    {
      "epoch": 2.082099596231494,
      "grad_norm": 0.7649134397506714,
      "learning_rate": 0.0001538760850044897,
      "loss": 0.8941,
      "step": 1547
    },
    {
      "epoch": 2.083445491251682,
      "grad_norm": 0.7660064697265625,
      "learning_rate": 0.00015384615384615385,
      "loss": 0.7902,
      "step": 1548
    },
    {
      "epoch": 2.084791386271871,
      "grad_norm": 0.698092520236969,
      "learning_rate": 0.00015381622268781803,
      "loss": 0.9031,
      "step": 1549
    },
    {
      "epoch": 2.0861372812920593,
      "grad_norm": 0.736009418964386,
      "learning_rate": 0.00015378629152948222,
      "loss": 0.8794,
      "step": 1550
    },
    {
      "epoch": 2.0874831763122477,
      "grad_norm": 0.752608060836792,
      "learning_rate": 0.00015375636037114637,
      "loss": 0.8593,
      "step": 1551
    },
    {
      "epoch": 2.088829071332436,
      "grad_norm": 0.9410753846168518,
      "learning_rate": 0.00015372642921281056,
      "loss": 0.8496,
      "step": 1552
    },
    {
      "epoch": 2.0901749663526243,
      "grad_norm": 0.7467669248580933,
      "learning_rate": 0.00015369649805447474,
      "loss": 1.2288,
      "step": 1553
    },
    {
      "epoch": 2.091520861372813,
      "grad_norm": 0.6524387001991272,
      "learning_rate": 0.0001536665668961389,
      "loss": 1.1355,
      "step": 1554
    },
    {
      "epoch": 2.0928667563930015,
      "grad_norm": 0.8059555292129517,
      "learning_rate": 0.00015363663573780308,
      "loss": 1.1032,
      "step": 1555
    },
    {
      "epoch": 2.09421265141319,
      "grad_norm": 0.8747115135192871,
      "learning_rate": 0.00015360670457946723,
      "loss": 0.981,
      "step": 1556
    },
    {
      "epoch": 2.095558546433378,
      "grad_norm": 0.9598897099494934,
      "learning_rate": 0.00015357677342113141,
      "loss": 0.7775,
      "step": 1557
    },
    {
      "epoch": 2.0969044414535665,
      "grad_norm": 0.6818312406539917,
      "learning_rate": 0.0001535468422627956,
      "loss": 0.7793,
      "step": 1558
    },
    {
      "epoch": 2.0982503364737553,
      "grad_norm": 0.6880746483802795,
      "learning_rate": 0.00015351691110445975,
      "loss": 0.8469,
      "step": 1559
    },
    {
      "epoch": 2.0995962314939436,
      "grad_norm": 0.6775444149971008,
      "learning_rate": 0.00015348697994612394,
      "loss": 0.8996,
      "step": 1560
    },
    {
      "epoch": 2.100942126514132,
      "grad_norm": 0.8921405076980591,
      "learning_rate": 0.00015345704878778812,
      "loss": 0.8662,
      "step": 1561
    },
    {
      "epoch": 2.1022880215343203,
      "grad_norm": 0.7751302123069763,
      "learning_rate": 0.00015342711762945227,
      "loss": 0.7677,
      "step": 1562
    },
    {
      "epoch": 2.1036339165545086,
      "grad_norm": 0.8480002880096436,
      "learning_rate": 0.00015339718647111646,
      "loss": 0.8115,
      "step": 1563
    },
    {
      "epoch": 2.1049798115746974,
      "grad_norm": 0.7732334733009338,
      "learning_rate": 0.0001533672553127806,
      "loss": 0.7481,
      "step": 1564
    },
    {
      "epoch": 2.1063257065948857,
      "grad_norm": 0.8440864682197571,
      "learning_rate": 0.0001533373241544448,
      "loss": 1.1765,
      "step": 1565
    },
    {
      "epoch": 2.107671601615074,
      "grad_norm": 0.6964865922927856,
      "learning_rate": 0.00015330739299610898,
      "loss": 0.8722,
      "step": 1566
    },
    {
      "epoch": 2.1090174966352624,
      "grad_norm": 1.0113950967788696,
      "learning_rate": 0.00015327746183777313,
      "loss": 1.1258,
      "step": 1567
    },
    {
      "epoch": 2.1103633916554507,
      "grad_norm": 0.7246955037117004,
      "learning_rate": 0.00015324753067943732,
      "loss": 0.8105,
      "step": 1568
    },
    {
      "epoch": 2.1117092866756395,
      "grad_norm": 0.6717846393585205,
      "learning_rate": 0.00015321759952110147,
      "loss": 0.7865,
      "step": 1569
    },
    {
      "epoch": 2.113055181695828,
      "grad_norm": 0.9196405410766602,
      "learning_rate": 0.00015318766836276565,
      "loss": 0.8249,
      "step": 1570
    },
    {
      "epoch": 2.114401076716016,
      "grad_norm": 0.7282848358154297,
      "learning_rate": 0.00015315773720442984,
      "loss": 1.2838,
      "step": 1571
    },
    {
      "epoch": 2.1157469717362045,
      "grad_norm": 0.9089689254760742,
      "learning_rate": 0.000153127806046094,
      "loss": 0.9275,
      "step": 1572
    },
    {
      "epoch": 2.117092866756393,
      "grad_norm": 0.8708540201187134,
      "learning_rate": 0.00015309787488775815,
      "loss": 0.8791,
      "step": 1573
    },
    {
      "epoch": 2.1184387617765816,
      "grad_norm": 1.0042922496795654,
      "learning_rate": 0.00015306794372942233,
      "loss": 1.1256,
      "step": 1574
    },
    {
      "epoch": 2.11978465679677,
      "grad_norm": 1.1267677545547485,
      "learning_rate": 0.00015303801257108649,
      "loss": 1.0238,
      "step": 1575
    },
    {
      "epoch": 2.1211305518169583,
      "grad_norm": 1.0332591533660889,
      "learning_rate": 0.00015300808141275067,
      "loss": 0.9939,
      "step": 1576
    },
    {
      "epoch": 2.1224764468371466,
      "grad_norm": 0.9711175560951233,
      "learning_rate": 0.00015297815025441485,
      "loss": 0.8899,
      "step": 1577
    },
    {
      "epoch": 2.123822341857335,
      "grad_norm": 0.8387614488601685,
      "learning_rate": 0.000152948219096079,
      "loss": 1.0265,
      "step": 1578
    },
    {
      "epoch": 2.1251682368775233,
      "grad_norm": 0.888444185256958,
      "learning_rate": 0.0001529182879377432,
      "loss": 0.9619,
      "step": 1579
    },
    {
      "epoch": 2.126514131897712,
      "grad_norm": 0.9275752305984497,
      "learning_rate": 0.00015288835677940737,
      "loss": 0.9537,
      "step": 1580
    },
    {
      "epoch": 2.1278600269179004,
      "grad_norm": 0.9287041425704956,
      "learning_rate": 0.00015285842562107153,
      "loss": 1.0326,
      "step": 1581
    },
    {
      "epoch": 2.1292059219380888,
      "grad_norm": 0.9375171661376953,
      "learning_rate": 0.0001528284944627357,
      "loss": 0.9448,
      "step": 1582
    },
    {
      "epoch": 2.130551816958277,
      "grad_norm": 0.7392767071723938,
      "learning_rate": 0.00015279856330439987,
      "loss": 0.7497,
      "step": 1583
    },
    {
      "epoch": 2.131897711978466,
      "grad_norm": 0.7457869052886963,
      "learning_rate": 0.00015276863214606405,
      "loss": 0.8522,
      "step": 1584
    },
    {
      "epoch": 2.1332436069986542,
      "grad_norm": 0.8374726176261902,
      "learning_rate": 0.00015273870098772823,
      "loss": 0.9257,
      "step": 1585
    },
    {
      "epoch": 2.1345895020188426,
      "grad_norm": 0.9278567433357239,
      "learning_rate": 0.0001527087698293924,
      "loss": 0.9937,
      "step": 1586
    },
    {
      "epoch": 2.135935397039031,
      "grad_norm": 0.9490303993225098,
      "learning_rate": 0.00015267883867105657,
      "loss": 0.8024,
      "step": 1587
    },
    {
      "epoch": 2.1372812920592192,
      "grad_norm": 0.7115077376365662,
      "learning_rate": 0.00015264890751272075,
      "loss": 1.06,
      "step": 1588
    },
    {
      "epoch": 2.1386271870794076,
      "grad_norm": 0.8525353670120239,
      "learning_rate": 0.0001526189763543849,
      "loss": 0.8234,
      "step": 1589
    },
    {
      "epoch": 2.1399730820995964,
      "grad_norm": 0.9403812289237976,
      "learning_rate": 0.0001525890451960491,
      "loss": 1.0129,
      "step": 1590
    },
    {
      "epoch": 2.1413189771197847,
      "grad_norm": 0.6105162501335144,
      "learning_rate": 0.00015255911403771325,
      "loss": 0.993,
      "step": 1591
    },
    {
      "epoch": 2.142664872139973,
      "grad_norm": 0.946902871131897,
      "learning_rate": 0.00015252918287937743,
      "loss": 1.1276,
      "step": 1592
    },
    {
      "epoch": 2.1440107671601614,
      "grad_norm": 0.887523353099823,
      "learning_rate": 0.0001524992517210416,
      "loss": 0.7423,
      "step": 1593
    },
    {
      "epoch": 2.14535666218035,
      "grad_norm": 0.7892943620681763,
      "learning_rate": 0.00015246932056270577,
      "loss": 0.8713,
      "step": 1594
    },
    {
      "epoch": 2.1467025572005385,
      "grad_norm": 0.6824181079864502,
      "learning_rate": 0.00015243938940436995,
      "loss": 0.782,
      "step": 1595
    },
    {
      "epoch": 2.148048452220727,
      "grad_norm": 0.9335501790046692,
      "learning_rate": 0.00015240945824603413,
      "loss": 1.0306,
      "step": 1596
    },
    {
      "epoch": 2.149394347240915,
      "grad_norm": 0.9896729588508606,
      "learning_rate": 0.0001523795270876983,
      "loss": 0.8349,
      "step": 1597
    },
    {
      "epoch": 2.1507402422611035,
      "grad_norm": 0.8529231548309326,
      "learning_rate": 0.00015234959592936247,
      "loss": 0.7457,
      "step": 1598
    },
    {
      "epoch": 2.152086137281292,
      "grad_norm": 0.7911294102668762,
      "learning_rate": 0.00015231966477102663,
      "loss": 0.866,
      "step": 1599
    },
    {
      "epoch": 2.1534320323014806,
      "grad_norm": 0.7400152683258057,
      "learning_rate": 0.0001522897336126908,
      "loss": 1.02,
      "step": 1600
    },
    {
      "epoch": 2.154777927321669,
      "grad_norm": 1.0467699766159058,
      "learning_rate": 0.000152259802454355,
      "loss": 1.1553,
      "step": 1601
    },
    {
      "epoch": 2.1561238223418573,
      "grad_norm": 0.8916530609130859,
      "learning_rate": 0.00015222987129601915,
      "loss": 0.9585,
      "step": 1602
    },
    {
      "epoch": 2.1574697173620456,
      "grad_norm": 0.7958456873893738,
      "learning_rate": 0.00015219994013768333,
      "loss": 0.9446,
      "step": 1603
    },
    {
      "epoch": 2.1588156123822344,
      "grad_norm": 0.7790566682815552,
      "learning_rate": 0.0001521700089793475,
      "loss": 0.7642,
      "step": 1604
    },
    {
      "epoch": 2.1601615074024227,
      "grad_norm": 0.7392928600311279,
      "learning_rate": 0.00015214007782101167,
      "loss": 1.022,
      "step": 1605
    },
    {
      "epoch": 2.161507402422611,
      "grad_norm": 0.774634838104248,
      "learning_rate": 0.00015211014666267585,
      "loss": 1.0142,
      "step": 1606
    },
    {
      "epoch": 2.1628532974427994,
      "grad_norm": 1.0112718343734741,
      "learning_rate": 0.00015208021550434,
      "loss": 1.1683,
      "step": 1607
    },
    {
      "epoch": 2.1641991924629878,
      "grad_norm": 0.8516260981559753,
      "learning_rate": 0.0001520502843460042,
      "loss": 1.1409,
      "step": 1608
    },
    {
      "epoch": 2.165545087483176,
      "grad_norm": 0.7916167974472046,
      "learning_rate": 0.00015202035318766837,
      "loss": 0.7824,
      "step": 1609
    },
    {
      "epoch": 2.166890982503365,
      "grad_norm": 0.6080799698829651,
      "learning_rate": 0.00015199042202933253,
      "loss": 0.7931,
      "step": 1610
    },
    {
      "epoch": 2.168236877523553,
      "grad_norm": 0.9663360714912415,
      "learning_rate": 0.0001519604908709967,
      "loss": 0.8501,
      "step": 1611
    },
    {
      "epoch": 2.1695827725437415,
      "grad_norm": 0.7229516506195068,
      "learning_rate": 0.0001519305597126609,
      "loss": 0.9082,
      "step": 1612
    },
    {
      "epoch": 2.17092866756393,
      "grad_norm": 0.8819720149040222,
      "learning_rate": 0.00015190062855432505,
      "loss": 1.1268,
      "step": 1613
    },
    {
      "epoch": 2.1722745625841187,
      "grad_norm": 0.9735128283500671,
      "learning_rate": 0.00015187069739598923,
      "loss": 0.8639,
      "step": 1614
    },
    {
      "epoch": 2.173620457604307,
      "grad_norm": 0.882996678352356,
      "learning_rate": 0.0001518407662376534,
      "loss": 0.9241,
      "step": 1615
    },
    {
      "epoch": 2.1749663526244953,
      "grad_norm": 0.9062973260879517,
      "learning_rate": 0.00015181083507931757,
      "loss": 1.0804,
      "step": 1616
    },
    {
      "epoch": 2.1763122476446837,
      "grad_norm": 1.0438798666000366,
      "learning_rate": 0.00015178090392098175,
      "loss": 1.3431,
      "step": 1617
    },
    {
      "epoch": 2.177658142664872,
      "grad_norm": 0.8387920260429382,
      "learning_rate": 0.0001517509727626459,
      "loss": 1.0149,
      "step": 1618
    },
    {
      "epoch": 2.1790040376850603,
      "grad_norm": 0.8964909911155701,
      "learning_rate": 0.0001517210416043101,
      "loss": 1.2125,
      "step": 1619
    },
    {
      "epoch": 2.180349932705249,
      "grad_norm": 0.9209612607955933,
      "learning_rate": 0.00015169111044597425,
      "loss": 1.0504,
      "step": 1620
    },
    {
      "epoch": 2.1816958277254375,
      "grad_norm": 0.7625036835670471,
      "learning_rate": 0.00015166117928763843,
      "loss": 0.9841,
      "step": 1621
    },
    {
      "epoch": 2.183041722745626,
      "grad_norm": 0.9292798638343811,
      "learning_rate": 0.0001516312481293026,
      "loss": 0.9895,
      "step": 1622
    },
    {
      "epoch": 2.184387617765814,
      "grad_norm": 0.7985967397689819,
      "learning_rate": 0.00015160131697096677,
      "loss": 0.7616,
      "step": 1623
    },
    {
      "epoch": 2.185733512786003,
      "grad_norm": 0.731503963470459,
      "learning_rate": 0.00015157138581263095,
      "loss": 0.9713,
      "step": 1624
    },
    {
      "epoch": 2.1870794078061913,
      "grad_norm": 0.7806804180145264,
      "learning_rate": 0.00015154145465429513,
      "loss": 1.0364,
      "step": 1625
    },
    {
      "epoch": 2.1884253028263796,
      "grad_norm": 0.7817140221595764,
      "learning_rate": 0.0001515115234959593,
      "loss": 0.9231,
      "step": 1626
    },
    {
      "epoch": 2.189771197846568,
      "grad_norm": 0.7959664463996887,
      "learning_rate": 0.00015148159233762347,
      "loss": 1.0771,
      "step": 1627
    },
    {
      "epoch": 2.1911170928667563,
      "grad_norm": 0.8981191515922546,
      "learning_rate": 0.00015145166117928763,
      "loss": 1.1111,
      "step": 1628
    },
    {
      "epoch": 2.1924629878869446,
      "grad_norm": 0.8144710063934326,
      "learning_rate": 0.0001514217300209518,
      "loss": 1.0333,
      "step": 1629
    },
    {
      "epoch": 2.1938088829071334,
      "grad_norm": 0.9597890973091125,
      "learning_rate": 0.000151391798862616,
      "loss": 0.7783,
      "step": 1630
    },
    {
      "epoch": 2.1951547779273217,
      "grad_norm": 0.7737728357315063,
      "learning_rate": 0.00015136186770428015,
      "loss": 1.0003,
      "step": 1631
    },
    {
      "epoch": 2.19650067294751,
      "grad_norm": 0.8585055470466614,
      "learning_rate": 0.00015133193654594433,
      "loss": 0.8981,
      "step": 1632
    },
    {
      "epoch": 2.1978465679676984,
      "grad_norm": 0.9066269993782043,
      "learning_rate": 0.00015130200538760851,
      "loss": 1.0229,
      "step": 1633
    },
    {
      "epoch": 2.199192462987887,
      "grad_norm": 0.9151017069816589,
      "learning_rate": 0.00015127207422927267,
      "loss": 0.7716,
      "step": 1634
    },
    {
      "epoch": 2.2005383580080755,
      "grad_norm": 0.7374339699745178,
      "learning_rate": 0.00015124214307093685,
      "loss": 0.7877,
      "step": 1635
    },
    {
      "epoch": 2.201884253028264,
      "grad_norm": 1.0249128341674805,
      "learning_rate": 0.000151212211912601,
      "loss": 0.9283,
      "step": 1636
    },
    {
      "epoch": 2.203230148048452,
      "grad_norm": 0.8932633996009827,
      "learning_rate": 0.0001511822807542652,
      "loss": 0.9867,
      "step": 1637
    },
    {
      "epoch": 2.2045760430686405,
      "grad_norm": 0.8677987456321716,
      "learning_rate": 0.00015115234959592937,
      "loss": 0.874,
      "step": 1638
    },
    {
      "epoch": 2.205921938088829,
      "grad_norm": 0.9906472563743591,
      "learning_rate": 0.00015112241843759353,
      "loss": 0.9494,
      "step": 1639
    },
    {
      "epoch": 2.2072678331090176,
      "grad_norm": 0.8697112798690796,
      "learning_rate": 0.0001510924872792577,
      "loss": 1.0005,
      "step": 1640
    },
    {
      "epoch": 2.208613728129206,
      "grad_norm": 1.1270586252212524,
      "learning_rate": 0.0001510625561209219,
      "loss": 0.9834,
      "step": 1641
    },
    {
      "epoch": 2.2099596231493943,
      "grad_norm": 0.9114258289337158,
      "learning_rate": 0.00015103262496258605,
      "loss": 0.6904,
      "step": 1642
    },
    {
      "epoch": 2.2113055181695827,
      "grad_norm": 0.8463177680969238,
      "learning_rate": 0.00015100269380425023,
      "loss": 0.8811,
      "step": 1643
    },
    {
      "epoch": 2.2126514131897714,
      "grad_norm": 0.7956216931343079,
      "learning_rate": 0.0001509727626459144,
      "loss": 0.8931,
      "step": 1644
    },
    {
      "epoch": 2.2139973082099598,
      "grad_norm": 0.9763880968093872,
      "learning_rate": 0.00015094283148757857,
      "loss": 0.9338,
      "step": 1645
    },
    {
      "epoch": 2.215343203230148,
      "grad_norm": 0.9580280184745789,
      "learning_rate": 0.00015091290032924275,
      "loss": 1.0851,
      "step": 1646
    },
    {
      "epoch": 2.2166890982503364,
      "grad_norm": 0.8245996832847595,
      "learning_rate": 0.0001508829691709069,
      "loss": 1.07,
      "step": 1647
    },
    {
      "epoch": 2.218034993270525,
      "grad_norm": 0.7633854746818542,
      "learning_rate": 0.0001508530380125711,
      "loss": 1.2317,
      "step": 1648
    },
    {
      "epoch": 2.219380888290713,
      "grad_norm": 0.8235129117965698,
      "learning_rate": 0.00015082310685423527,
      "loss": 1.043,
      "step": 1649
    },
    {
      "epoch": 2.220726783310902,
      "grad_norm": 0.7790830135345459,
      "learning_rate": 0.00015079317569589943,
      "loss": 0.8399,
      "step": 1650
    },
    {
      "epoch": 2.2220726783310902,
      "grad_norm": 0.8269524574279785,
      "learning_rate": 0.0001507632445375636,
      "loss": 1.0506,
      "step": 1651
    },
    {
      "epoch": 2.2234185733512786,
      "grad_norm": 0.8585672974586487,
      "learning_rate": 0.00015073331337922777,
      "loss": 1.0272,
      "step": 1652
    },
    {
      "epoch": 2.224764468371467,
      "grad_norm": 0.8429161310195923,
      "learning_rate": 0.00015070338222089195,
      "loss": 0.878,
      "step": 1653
    },
    {
      "epoch": 2.2261103633916552,
      "grad_norm": 0.9927927851676941,
      "learning_rate": 0.00015067345106255613,
      "loss": 0.9139,
      "step": 1654
    },
    {
      "epoch": 2.227456258411844,
      "grad_norm": 0.8591813445091248,
      "learning_rate": 0.0001506435199042203,
      "loss": 0.9407,
      "step": 1655
    },
    {
      "epoch": 2.2288021534320324,
      "grad_norm": 0.9077178239822388,
      "learning_rate": 0.00015061358874588447,
      "loss": 0.9364,
      "step": 1656
    },
    {
      "epoch": 2.2301480484522207,
      "grad_norm": 0.8086464405059814,
      "learning_rate": 0.00015058365758754865,
      "loss": 0.9299,
      "step": 1657
    },
    {
      "epoch": 2.231493943472409,
      "grad_norm": 0.830389142036438,
      "learning_rate": 0.0001505537264292128,
      "loss": 1.0767,
      "step": 1658
    },
    {
      "epoch": 2.2328398384925974,
      "grad_norm": 0.913110077381134,
      "learning_rate": 0.000150523795270877,
      "loss": 0.9015,
      "step": 1659
    },
    {
      "epoch": 2.234185733512786,
      "grad_norm": 0.7151327729225159,
      "learning_rate": 0.00015049386411254115,
      "loss": 1.1658,
      "step": 1660
    },
    {
      "epoch": 2.2355316285329745,
      "grad_norm": 0.8635636568069458,
      "learning_rate": 0.00015046393295420533,
      "loss": 0.8114,
      "step": 1661
    },
    {
      "epoch": 2.236877523553163,
      "grad_norm": 0.8784220814704895,
      "learning_rate": 0.00015043400179586951,
      "loss": 0.6856,
      "step": 1662
    },
    {
      "epoch": 2.238223418573351,
      "grad_norm": 0.8508644700050354,
      "learning_rate": 0.00015040407063753367,
      "loss": 0.9073,
      "step": 1663
    },
    {
      "epoch": 2.2395693135935395,
      "grad_norm": 0.9182564616203308,
      "learning_rate": 0.00015037413947919785,
      "loss": 0.9592,
      "step": 1664
    },
    {
      "epoch": 2.2409152086137283,
      "grad_norm": 0.8381797671318054,
      "learning_rate": 0.00015034420832086203,
      "loss": 0.6431,
      "step": 1665
    },
    {
      "epoch": 2.2422611036339166,
      "grad_norm": 0.9005530476570129,
      "learning_rate": 0.0001503142771625262,
      "loss": 1.069,
      "step": 1666
    },
    {
      "epoch": 2.243606998654105,
      "grad_norm": 0.8619869947433472,
      "learning_rate": 0.00015028434600419037,
      "loss": 0.8558,
      "step": 1667
    },
    {
      "epoch": 2.2449528936742933,
      "grad_norm": 0.9144594669342041,
      "learning_rate": 0.00015025441484585453,
      "loss": 0.9424,
      "step": 1668
    },
    {
      "epoch": 2.2462987886944816,
      "grad_norm": 0.8760172724723816,
      "learning_rate": 0.0001502244836875187,
      "loss": 0.8827,
      "step": 1669
    },
    {
      "epoch": 2.2476446837146704,
      "grad_norm": 1.0217639207839966,
      "learning_rate": 0.0001501945525291829,
      "loss": 0.9365,
      "step": 1670
    },
    {
      "epoch": 2.2489905787348587,
      "grad_norm": 0.8797653317451477,
      "learning_rate": 0.00015016462137084705,
      "loss": 0.9462,
      "step": 1671
    },
    {
      "epoch": 2.250336473755047,
      "grad_norm": 0.9622105360031128,
      "learning_rate": 0.00015013469021251123,
      "loss": 0.9778,
      "step": 1672
    },
    {
      "epoch": 2.2516823687752354,
      "grad_norm": 0.8244521617889404,
      "learning_rate": 0.00015010475905417542,
      "loss": 0.9548,
      "step": 1673
    },
    {
      "epoch": 2.253028263795424,
      "grad_norm": 0.8896222710609436,
      "learning_rate": 0.00015007482789583957,
      "loss": 0.9725,
      "step": 1674
    },
    {
      "epoch": 2.2543741588156125,
      "grad_norm": 0.8475268483161926,
      "learning_rate": 0.00015004489673750375,
      "loss": 0.8934,
      "step": 1675
    },
    {
      "epoch": 2.255720053835801,
      "grad_norm": 0.9726254940032959,
      "learning_rate": 0.0001500149655791679,
      "loss": 0.9816,
      "step": 1676
    },
    {
      "epoch": 2.257065948855989,
      "grad_norm": 0.8084104061126709,
      "learning_rate": 0.0001499850344208321,
      "loss": 0.827,
      "step": 1677
    },
    {
      "epoch": 2.2584118438761775,
      "grad_norm": 0.7195165753364563,
      "learning_rate": 0.00014995510326249627,
      "loss": 0.9404,
      "step": 1678
    },
    {
      "epoch": 2.259757738896366,
      "grad_norm": 0.910392165184021,
      "learning_rate": 0.00014992517210416043,
      "loss": 0.7623,
      "step": 1679
    },
    {
      "epoch": 2.2611036339165547,
      "grad_norm": 0.7852543592453003,
      "learning_rate": 0.0001498952409458246,
      "loss": 1.056,
      "step": 1680
    },
    {
      "epoch": 2.262449528936743,
      "grad_norm": 0.7341415882110596,
      "learning_rate": 0.0001498653097874888,
      "loss": 1.0198,
      "step": 1681
    },
    {
      "epoch": 2.2637954239569313,
      "grad_norm": 0.78850257396698,
      "learning_rate": 0.00014983537862915295,
      "loss": 1.1741,
      "step": 1682
    },
    {
      "epoch": 2.2651413189771197,
      "grad_norm": 0.9218676686286926,
      "learning_rate": 0.00014980544747081713,
      "loss": 0.9433,
      "step": 1683
    },
    {
      "epoch": 2.2664872139973085,
      "grad_norm": 0.8771491646766663,
      "learning_rate": 0.0001497755163124813,
      "loss": 0.7722,
      "step": 1684
    },
    {
      "epoch": 2.267833109017497,
      "grad_norm": 0.8248311281204224,
      "learning_rate": 0.00014974558515414547,
      "loss": 0.9817,
      "step": 1685
    },
    {
      "epoch": 2.269179004037685,
      "grad_norm": 0.8203827142715454,
      "learning_rate": 0.00014971565399580965,
      "loss": 1.0423,
      "step": 1686
    },
    {
      "epoch": 2.2705248990578735,
      "grad_norm": 0.8838653564453125,
      "learning_rate": 0.0001496857228374738,
      "loss": 0.9836,
      "step": 1687
    },
    {
      "epoch": 2.271870794078062,
      "grad_norm": 0.7335965633392334,
      "learning_rate": 0.000149655791679138,
      "loss": 0.8471,
      "step": 1688
    },
    {
      "epoch": 2.27321668909825,
      "grad_norm": 0.9308990836143494,
      "learning_rate": 0.00014962586052080218,
      "loss": 1.0169,
      "step": 1689
    },
    {
      "epoch": 2.274562584118439,
      "grad_norm": 0.6352199912071228,
      "learning_rate": 0.00014959592936246633,
      "loss": 0.9285,
      "step": 1690
    },
    {
      "epoch": 2.2759084791386273,
      "grad_norm": 0.7742179036140442,
      "learning_rate": 0.00014956599820413051,
      "loss": 0.8877,
      "step": 1691
    },
    {
      "epoch": 2.2772543741588156,
      "grad_norm": 0.945253849029541,
      "learning_rate": 0.00014953606704579467,
      "loss": 1.001,
      "step": 1692
    },
    {
      "epoch": 2.278600269179004,
      "grad_norm": 0.8722368478775024,
      "learning_rate": 0.00014950613588745885,
      "loss": 0.698,
      "step": 1693
    },
    {
      "epoch": 2.2799461641991923,
      "grad_norm": 0.7851073741912842,
      "learning_rate": 0.00014947620472912304,
      "loss": 0.7159,
      "step": 1694
    },
    {
      "epoch": 2.281292059219381,
      "grad_norm": 0.9431909918785095,
      "learning_rate": 0.0001494462735707872,
      "loss": 0.7623,
      "step": 1695
    },
    {
      "epoch": 2.2826379542395694,
      "grad_norm": 0.8937345743179321,
      "learning_rate": 0.00014941634241245137,
      "loss": 0.8192,
      "step": 1696
    },
    {
      "epoch": 2.2839838492597577,
      "grad_norm": 0.8173230290412903,
      "learning_rate": 0.00014938641125411556,
      "loss": 0.8575,
      "step": 1697
    },
    {
      "epoch": 2.285329744279946,
      "grad_norm": 0.7609385251998901,
      "learning_rate": 0.0001493564800957797,
      "loss": 1.0088,
      "step": 1698
    },
    {
      "epoch": 2.2866756393001344,
      "grad_norm": 1.0274677276611328,
      "learning_rate": 0.0001493265489374439,
      "loss": 0.9821,
      "step": 1699
    },
    {
      "epoch": 2.288021534320323,
      "grad_norm": 1.106413722038269,
      "learning_rate": 0.00014929661777910805,
      "loss": 1.0362,
      "step": 1700
    },
    {
      "epoch": 2.2893674293405115,
      "grad_norm": 0.9961568117141724,
      "learning_rate": 0.00014926668662077223,
      "loss": 0.9413,
      "step": 1701
    },
    {
      "epoch": 2.2907133243607,
      "grad_norm": 0.8858038783073425,
      "learning_rate": 0.00014923675546243642,
      "loss": 0.9486,
      "step": 1702
    },
    {
      "epoch": 2.292059219380888,
      "grad_norm": 1.0012667179107666,
      "learning_rate": 0.00014920682430410057,
      "loss": 1.0094,
      "step": 1703
    },
    {
      "epoch": 2.2934051144010765,
      "grad_norm": 0.8679286241531372,
      "learning_rate": 0.00014917689314576475,
      "loss": 0.8959,
      "step": 1704
    },
    {
      "epoch": 2.2947510094212653,
      "grad_norm": 0.9109845161437988,
      "learning_rate": 0.00014914696198742894,
      "loss": 0.7326,
      "step": 1705
    },
    {
      "epoch": 2.2960969044414536,
      "grad_norm": 0.7701609134674072,
      "learning_rate": 0.0001491170308290931,
      "loss": 0.8014,
      "step": 1706
    },
    {
      "epoch": 2.297442799461642,
      "grad_norm": 0.8144152760505676,
      "learning_rate": 0.00014908709967075727,
      "loss": 0.9105,
      "step": 1707
    },
    {
      "epoch": 2.2987886944818303,
      "grad_norm": 0.8301169872283936,
      "learning_rate": 0.00014905716851242143,
      "loss": 0.9572,
      "step": 1708
    },
    {
      "epoch": 2.3001345895020187,
      "grad_norm": 1.043493390083313,
      "learning_rate": 0.0001490272373540856,
      "loss": 1.0424,
      "step": 1709
    },
    {
      "epoch": 2.3014804845222074,
      "grad_norm": 0.9155335426330566,
      "learning_rate": 0.0001489973061957498,
      "loss": 0.9417,
      "step": 1710
    },
    {
      "epoch": 2.3028263795423958,
      "grad_norm": 0.9552680850028992,
      "learning_rate": 0.00014896737503741395,
      "loss": 0.8957,
      "step": 1711
    },
    {
      "epoch": 2.304172274562584,
      "grad_norm": 1.0297831296920776,
      "learning_rate": 0.00014893744387907813,
      "loss": 1.0028,
      "step": 1712
    },
    {
      "epoch": 2.3055181695827724,
      "grad_norm": 0.9151962995529175,
      "learning_rate": 0.00014890751272074232,
      "loss": 0.9586,
      "step": 1713
    },
    {
      "epoch": 2.306864064602961,
      "grad_norm": 0.7996988296508789,
      "learning_rate": 0.00014887758156240647,
      "loss": 1.0152,
      "step": 1714
    },
    {
      "epoch": 2.3082099596231496,
      "grad_norm": 0.8638168573379517,
      "learning_rate": 0.00014884765040407065,
      "loss": 1.0502,
      "step": 1715
    },
    {
      "epoch": 2.309555854643338,
      "grad_norm": 0.9268887639045715,
      "learning_rate": 0.0001488177192457348,
      "loss": 0.855,
      "step": 1716
    },
    {
      "epoch": 2.3109017496635262,
      "grad_norm": 0.7764784097671509,
      "learning_rate": 0.000148787788087399,
      "loss": 0.9158,
      "step": 1717
    },
    {
      "epoch": 2.3122476446837146,
      "grad_norm": 0.8083735108375549,
      "learning_rate": 0.00014875785692906318,
      "loss": 1.0498,
      "step": 1718
    },
    {
      "epoch": 2.313593539703903,
      "grad_norm": 0.81717449426651,
      "learning_rate": 0.00014872792577072733,
      "loss": 0.7997,
      "step": 1719
    },
    {
      "epoch": 2.3149394347240917,
      "grad_norm": 0.914786696434021,
      "learning_rate": 0.00014869799461239151,
      "loss": 0.8933,
      "step": 1720
    },
    {
      "epoch": 2.31628532974428,
      "grad_norm": 1.0218335390090942,
      "learning_rate": 0.0001486680634540557,
      "loss": 1.0321,
      "step": 1721
    },
    {
      "epoch": 2.3176312247644684,
      "grad_norm": 0.817206084728241,
      "learning_rate": 0.00014863813229571985,
      "loss": 0.9127,
      "step": 1722
    },
    {
      "epoch": 2.3189771197846567,
      "grad_norm": 0.9540868401527405,
      "learning_rate": 0.00014860820113738404,
      "loss": 1.0401,
      "step": 1723
    },
    {
      "epoch": 2.320323014804845,
      "grad_norm": 0.9553465247154236,
      "learning_rate": 0.0001485782699790482,
      "loss": 1.1017,
      "step": 1724
    },
    {
      "epoch": 2.321668909825034,
      "grad_norm": 0.8401023149490356,
      "learning_rate": 0.00014854833882071237,
      "loss": 0.8859,
      "step": 1725
    },
    {
      "epoch": 2.323014804845222,
      "grad_norm": 0.8127554655075073,
      "learning_rate": 0.00014851840766237656,
      "loss": 0.9469,
      "step": 1726
    },
    {
      "epoch": 2.3243606998654105,
      "grad_norm": 1.1292108297348022,
      "learning_rate": 0.0001484884765040407,
      "loss": 1.2045,
      "step": 1727
    },
    {
      "epoch": 2.325706594885599,
      "grad_norm": 0.9571633338928223,
      "learning_rate": 0.0001484585453457049,
      "loss": 1.1103,
      "step": 1728
    },
    {
      "epoch": 2.327052489905787,
      "grad_norm": 0.9974838495254517,
      "learning_rate": 0.00014842861418736908,
      "loss": 0.7891,
      "step": 1729
    },
    {
      "epoch": 2.328398384925976,
      "grad_norm": 0.7908419966697693,
      "learning_rate": 0.00014839868302903323,
      "loss": 0.7165,
      "step": 1730
    },
    {
      "epoch": 2.3297442799461643,
      "grad_norm": 0.9706036448478699,
      "learning_rate": 0.00014836875187069742,
      "loss": 0.9216,
      "step": 1731
    },
    {
      "epoch": 2.3310901749663526,
      "grad_norm": 0.904499888420105,
      "learning_rate": 0.00014833882071236157,
      "loss": 0.9712,
      "step": 1732
    },
    {
      "epoch": 2.332436069986541,
      "grad_norm": 0.8989810943603516,
      "learning_rate": 0.00014830888955402575,
      "loss": 0.917,
      "step": 1733
    },
    {
      "epoch": 2.3337819650067293,
      "grad_norm": 0.8076501488685608,
      "learning_rate": 0.00014827895839568994,
      "loss": 0.864,
      "step": 1734
    },
    {
      "epoch": 2.335127860026918,
      "grad_norm": 1.0099729299545288,
      "learning_rate": 0.0001482490272373541,
      "loss": 0.727,
      "step": 1735
    },
    {
      "epoch": 2.3364737550471064,
      "grad_norm": 0.8603557348251343,
      "learning_rate": 0.00014821909607901827,
      "loss": 0.9247,
      "step": 1736
    },
    {
      "epoch": 2.3378196500672948,
      "grad_norm": 0.8823745846748352,
      "learning_rate": 0.00014818916492068243,
      "loss": 0.8686,
      "step": 1737
    },
    {
      "epoch": 2.339165545087483,
      "grad_norm": 0.7862164974212646,
      "learning_rate": 0.0001481592337623466,
      "loss": 1.0795,
      "step": 1738
    },
    {
      "epoch": 2.3405114401076714,
      "grad_norm": 0.7823902368545532,
      "learning_rate": 0.0001481293026040108,
      "loss": 0.9447,
      "step": 1739
    },
    {
      "epoch": 2.34185733512786,
      "grad_norm": 0.7867583632469177,
      "learning_rate": 0.00014809937144567495,
      "loss": 1.136,
      "step": 1740
    },
    {
      "epoch": 2.3432032301480485,
      "grad_norm": 0.8200593590736389,
      "learning_rate": 0.00014806944028733913,
      "loss": 1.0964,
      "step": 1741
    },
    {
      "epoch": 2.344549125168237,
      "grad_norm": 0.823331356048584,
      "learning_rate": 0.00014803950912900332,
      "loss": 0.7294,
      "step": 1742
    },
    {
      "epoch": 2.345895020188425,
      "grad_norm": 0.8072540760040283,
      "learning_rate": 0.00014800957797066747,
      "loss": 0.8599,
      "step": 1743
    },
    {
      "epoch": 2.3472409152086136,
      "grad_norm": 1.011918306350708,
      "learning_rate": 0.00014797964681233166,
      "loss": 1.1831,
      "step": 1744
    },
    {
      "epoch": 2.3485868102288023,
      "grad_norm": 0.9769917726516724,
      "learning_rate": 0.0001479497156539958,
      "loss": 1.0525,
      "step": 1745
    },
    {
      "epoch": 2.3499327052489907,
      "grad_norm": 0.931451141834259,
      "learning_rate": 0.00014791978449566,
      "loss": 0.8547,
      "step": 1746
    },
    {
      "epoch": 2.351278600269179,
      "grad_norm": 0.8866501450538635,
      "learning_rate": 0.00014788985333732418,
      "loss": 0.8688,
      "step": 1747
    },
    {
      "epoch": 2.3526244952893673,
      "grad_norm": 0.9343724250793457,
      "learning_rate": 0.00014785992217898833,
      "loss": 1.1146,
      "step": 1748
    },
    {
      "epoch": 2.3539703903095557,
      "grad_norm": 0.7805073857307434,
      "learning_rate": 0.00014782999102065251,
      "loss": 0.8228,
      "step": 1749
    },
    {
      "epoch": 2.3553162853297445,
      "grad_norm": 0.878299355506897,
      "learning_rate": 0.0001478000598623167,
      "loss": 0.7632,
      "step": 1750
    },
    {
      "epoch": 2.356662180349933,
      "grad_norm": 0.829578697681427,
      "learning_rate": 0.00014777012870398085,
      "loss": 0.7976,
      "step": 1751
    },
    {
      "epoch": 2.358008075370121,
      "grad_norm": 1.1169495582580566,
      "learning_rate": 0.00014774019754564504,
      "loss": 1.1721,
      "step": 1752
    },
    {
      "epoch": 2.3593539703903095,
      "grad_norm": 0.9934091567993164,
      "learning_rate": 0.0001477102663873092,
      "loss": 0.976,
      "step": 1753
    },
    {
      "epoch": 2.360699865410498,
      "grad_norm": 0.7817792892456055,
      "learning_rate": 0.00014768033522897337,
      "loss": 0.7856,
      "step": 1754
    },
    {
      "epoch": 2.3620457604306866,
      "grad_norm": 0.8872517943382263,
      "learning_rate": 0.00014765040407063756,
      "loss": 0.8213,
      "step": 1755
    },
    {
      "epoch": 2.363391655450875,
      "grad_norm": 0.9943072199821472,
      "learning_rate": 0.0001476204729123017,
      "loss": 0.91,
      "step": 1756
    },
    {
      "epoch": 2.3647375504710633,
      "grad_norm": 0.8584299683570862,
      "learning_rate": 0.0001475905417539659,
      "loss": 1.1049,
      "step": 1757
    },
    {
      "epoch": 2.3660834454912516,
      "grad_norm": 0.881054699420929,
      "learning_rate": 0.00014756061059563008,
      "loss": 0.9249,
      "step": 1758
    },
    {
      "epoch": 2.36742934051144,
      "grad_norm": 0.9705824255943298,
      "learning_rate": 0.00014753067943729423,
      "loss": 0.7759,
      "step": 1759
    },
    {
      "epoch": 2.3687752355316287,
      "grad_norm": 1.0012308359146118,
      "learning_rate": 0.00014750074827895842,
      "loss": 0.9948,
      "step": 1760
    },
    {
      "epoch": 2.370121130551817,
      "grad_norm": 0.9340789318084717,
      "learning_rate": 0.00014747081712062257,
      "loss": 0.8828,
      "step": 1761
    },
    {
      "epoch": 2.3714670255720054,
      "grad_norm": 0.8608691096305847,
      "learning_rate": 0.00014744088596228675,
      "loss": 0.7009,
      "step": 1762
    },
    {
      "epoch": 2.3728129205921937,
      "grad_norm": 0.7937964797019958,
      "learning_rate": 0.00014741095480395094,
      "loss": 0.8803,
      "step": 1763
    },
    {
      "epoch": 2.374158815612382,
      "grad_norm": 0.8445737957954407,
      "learning_rate": 0.0001473810236456151,
      "loss": 0.7914,
      "step": 1764
    },
    {
      "epoch": 2.375504710632571,
      "grad_norm": 0.820506751537323,
      "learning_rate": 0.00014735109248727928,
      "loss": 0.9106,
      "step": 1765
    },
    {
      "epoch": 2.376850605652759,
      "grad_norm": 0.9620611071586609,
      "learning_rate": 0.00014732116132894346,
      "loss": 0.8295,
      "step": 1766
    },
    {
      "epoch": 2.3781965006729475,
      "grad_norm": 0.7893847823143005,
      "learning_rate": 0.0001472912301706076,
      "loss": 0.8785,
      "step": 1767
    },
    {
      "epoch": 2.379542395693136,
      "grad_norm": 0.8519418835639954,
      "learning_rate": 0.0001472612990122718,
      "loss": 1.0343,
      "step": 1768
    },
    {
      "epoch": 2.380888290713324,
      "grad_norm": 0.9707055687904358,
      "learning_rate": 0.00014723136785393595,
      "loss": 0.88,
      "step": 1769
    },
    {
      "epoch": 2.382234185733513,
      "grad_norm": 0.7935454249382019,
      "learning_rate": 0.00014720143669560013,
      "loss": 0.8371,
      "step": 1770
    },
    {
      "epoch": 2.3835800807537013,
      "grad_norm": 1.0213631391525269,
      "learning_rate": 0.00014717150553726432,
      "loss": 0.9741,
      "step": 1771
    },
    {
      "epoch": 2.3849259757738897,
      "grad_norm": 1.1047199964523315,
      "learning_rate": 0.00014714157437892847,
      "loss": 1.0445,
      "step": 1772
    },
    {
      "epoch": 2.386271870794078,
      "grad_norm": 0.9396607875823975,
      "learning_rate": 0.00014711164322059266,
      "loss": 0.7833,
      "step": 1773
    },
    {
      "epoch": 2.3876177658142663,
      "grad_norm": 0.7945905923843384,
      "learning_rate": 0.00014708171206225684,
      "loss": 0.959,
      "step": 1774
    },
    {
      "epoch": 2.388963660834455,
      "grad_norm": 0.9359691739082336,
      "learning_rate": 0.000147051780903921,
      "loss": 0.9535,
      "step": 1775
    },
    {
      "epoch": 2.3903095558546434,
      "grad_norm": 0.8000748753547668,
      "learning_rate": 0.00014702184974558518,
      "loss": 1.0469,
      "step": 1776
    },
    {
      "epoch": 2.391655450874832,
      "grad_norm": 0.9754595756530762,
      "learning_rate": 0.00014699191858724933,
      "loss": 0.9304,
      "step": 1777
    },
    {
      "epoch": 2.39300134589502,
      "grad_norm": 0.9749485850334167,
      "learning_rate": 0.00014696198742891351,
      "loss": 1.0221,
      "step": 1778
    },
    {
      "epoch": 2.3943472409152085,
      "grad_norm": 0.8113424181938171,
      "learning_rate": 0.0001469320562705777,
      "loss": 0.9497,
      "step": 1779
    },
    {
      "epoch": 2.3956931359353972,
      "grad_norm": 0.7931556701660156,
      "learning_rate": 0.00014690212511224185,
      "loss": 0.7393,
      "step": 1780
    },
    {
      "epoch": 2.3970390309555856,
      "grad_norm": 0.8161292672157288,
      "learning_rate": 0.000146872193953906,
      "loss": 0.8984,
      "step": 1781
    },
    {
      "epoch": 2.398384925975774,
      "grad_norm": 0.8706502914428711,
      "learning_rate": 0.0001468422627955702,
      "loss": 0.8175,
      "step": 1782
    },
    {
      "epoch": 2.3997308209959622,
      "grad_norm": 0.8569291830062866,
      "learning_rate": 0.00014681233163723435,
      "loss": 0.8791,
      "step": 1783
    },
    {
      "epoch": 2.4010767160161506,
      "grad_norm": 0.8923172354698181,
      "learning_rate": 0.00014678240047889853,
      "loss": 0.9487,
      "step": 1784
    },
    {
      "epoch": 2.4024226110363394,
      "grad_norm": 0.870574414730072,
      "learning_rate": 0.0001467524693205627,
      "loss": 0.8812,
      "step": 1785
    },
    {
      "epoch": 2.4037685060565277,
      "grad_norm": 0.8422874808311462,
      "learning_rate": 0.00014672253816222687,
      "loss": 0.8631,
      "step": 1786
    },
    {
      "epoch": 2.405114401076716,
      "grad_norm": 0.8641422390937805,
      "learning_rate": 0.00014669260700389105,
      "loss": 0.837,
      "step": 1787
    },
    {
      "epoch": 2.4064602960969044,
      "grad_norm": 0.891111433506012,
      "learning_rate": 0.00014666267584555523,
      "loss": 1.2494,
      "step": 1788
    },
    {
      "epoch": 2.4078061911170927,
      "grad_norm": 1.0077804327011108,
      "learning_rate": 0.0001466327446872194,
      "loss": 1.2061,
      "step": 1789
    },
    {
      "epoch": 2.409152086137281,
      "grad_norm": 0.9567338228225708,
      "learning_rate": 0.00014660281352888357,
      "loss": 0.8594,
      "step": 1790
    },
    {
      "epoch": 2.41049798115747,
      "grad_norm": 0.8689830899238586,
      "learning_rate": 0.00014657288237054773,
      "loss": 0.9183,
      "step": 1791
    },
    {
      "epoch": 2.411843876177658,
      "grad_norm": 0.8814077377319336,
      "learning_rate": 0.0001465429512122119,
      "loss": 1.0056,
      "step": 1792
    },
    {
      "epoch": 2.4131897711978465,
      "grad_norm": 0.8392244577407837,
      "learning_rate": 0.0001465130200538761,
      "loss": 0.9856,
      "step": 1793
    },
    {
      "epoch": 2.414535666218035,
      "grad_norm": 0.7520036697387695,
      "learning_rate": 0.00014648308889554025,
      "loss": 0.8783,
      "step": 1794
    },
    {
      "epoch": 2.4158815612382236,
      "grad_norm": 0.9200599193572998,
      "learning_rate": 0.00014645315773720443,
      "loss": 0.9147,
      "step": 1795
    },
    {
      "epoch": 2.417227456258412,
      "grad_norm": 0.9559215307235718,
      "learning_rate": 0.00014642322657886859,
      "loss": 0.9417,
      "step": 1796
    },
    {
      "epoch": 2.4185733512786003,
      "grad_norm": 0.8091204762458801,
      "learning_rate": 0.00014639329542053277,
      "loss": 1.1008,
      "step": 1797
    },
    {
      "epoch": 2.4199192462987886,
      "grad_norm": 0.9345000982284546,
      "learning_rate": 0.00014636336426219695,
      "loss": 0.8693,
      "step": 1798
    },
    {
      "epoch": 2.421265141318977,
      "grad_norm": 0.8289446234703064,
      "learning_rate": 0.0001463334331038611,
      "loss": 0.8343,
      "step": 1799
    },
    {
      "epoch": 2.4226110363391653,
      "grad_norm": 0.8966443538665771,
      "learning_rate": 0.0001463035019455253,
      "loss": 0.8907,
      "step": 1800
    },
    {
      "epoch": 2.423956931359354,
      "grad_norm": 0.927489161491394,
      "learning_rate": 0.00014627357078718947,
      "loss": 1.0639,
      "step": 1801
    },
    {
      "epoch": 2.4253028263795424,
      "grad_norm": 0.7817131280899048,
      "learning_rate": 0.00014624363962885363,
      "loss": 0.6895,
      "step": 1802
    },
    {
      "epoch": 2.4266487213997308,
      "grad_norm": 0.8764805793762207,
      "learning_rate": 0.0001462137084705178,
      "loss": 0.9221,
      "step": 1803
    },
    {
      "epoch": 2.427994616419919,
      "grad_norm": 0.8306906223297119,
      "learning_rate": 0.00014618377731218197,
      "loss": 0.7825,
      "step": 1804
    },
    {
      "epoch": 2.429340511440108,
      "grad_norm": 1.0068397521972656,
      "learning_rate": 0.00014615384615384615,
      "loss": 0.855,
      "step": 1805
    },
    {
      "epoch": 2.430686406460296,
      "grad_norm": 0.8714388012886047,
      "learning_rate": 0.00014612391499551033,
      "loss": 0.7524,
      "step": 1806
    },
    {
      "epoch": 2.4320323014804845,
      "grad_norm": 0.799263596534729,
      "learning_rate": 0.0001460939838371745,
      "loss": 0.8139,
      "step": 1807
    },
    {
      "epoch": 2.433378196500673,
      "grad_norm": 0.8924602270126343,
      "learning_rate": 0.00014606405267883867,
      "loss": 0.8281,
      "step": 1808
    },
    {
      "epoch": 2.4347240915208612,
      "grad_norm": 1.1197400093078613,
      "learning_rate": 0.00014603412152050285,
      "loss": 0.9855,
      "step": 1809
    },
    {
      "epoch": 2.4360699865410496,
      "grad_norm": 0.8454907536506653,
      "learning_rate": 0.000146004190362167,
      "loss": 0.8509,
      "step": 1810
    },
    {
      "epoch": 2.4374158815612383,
      "grad_norm": 0.8808004856109619,
      "learning_rate": 0.0001459742592038312,
      "loss": 0.8374,
      "step": 1811
    },
    {
      "epoch": 2.4387617765814267,
      "grad_norm": 0.8717572093009949,
      "learning_rate": 0.00014594432804549535,
      "loss": 0.6534,
      "step": 1812
    },
    {
      "epoch": 2.440107671601615,
      "grad_norm": 0.9996355175971985,
      "learning_rate": 0.00014591439688715953,
      "loss": 0.9076,
      "step": 1813
    },
    {
      "epoch": 2.4414535666218034,
      "grad_norm": 1.0319297313690186,
      "learning_rate": 0.0001458844657288237,
      "loss": 1.1092,
      "step": 1814
    },
    {
      "epoch": 2.442799461641992,
      "grad_norm": 0.8410360217094421,
      "learning_rate": 0.00014585453457048787,
      "loss": 0.943,
      "step": 1815
    },
    {
      "epoch": 2.4441453566621805,
      "grad_norm": 1.1197011470794678,
      "learning_rate": 0.00014582460341215205,
      "loss": 0.8684,
      "step": 1816
    },
    {
      "epoch": 2.445491251682369,
      "grad_norm": 0.678585946559906,
      "learning_rate": 0.00014579467225381623,
      "loss": 0.8459,
      "step": 1817
    },
    {
      "epoch": 2.446837146702557,
      "grad_norm": 0.9773644208908081,
      "learning_rate": 0.0001457647410954804,
      "loss": 0.8736,
      "step": 1818
    },
    {
      "epoch": 2.4481830417227455,
      "grad_norm": 0.9349305629730225,
      "learning_rate": 0.00014573480993714457,
      "loss": 1.0877,
      "step": 1819
    },
    {
      "epoch": 2.449528936742934,
      "grad_norm": 1.0909428596496582,
      "learning_rate": 0.00014570487877880873,
      "loss": 0.9962,
      "step": 1820
    },
    {
      "epoch": 2.4508748317631226,
      "grad_norm": 1.1510838270187378,
      "learning_rate": 0.0001456749476204729,
      "loss": 0.9638,
      "step": 1821
    },
    {
      "epoch": 2.452220726783311,
      "grad_norm": 1.0758081674575806,
      "learning_rate": 0.0001456450164621371,
      "loss": 1.2061,
      "step": 1822
    },
    {
      "epoch": 2.4535666218034993,
      "grad_norm": 0.7545437812805176,
      "learning_rate": 0.00014561508530380125,
      "loss": 0.9393,
      "step": 1823
    },
    {
      "epoch": 2.4549125168236876,
      "grad_norm": 1.119282603263855,
      "learning_rate": 0.00014558515414546543,
      "loss": 1.0283,
      "step": 1824
    },
    {
      "epoch": 2.4562584118438764,
      "grad_norm": 0.8141812086105347,
      "learning_rate": 0.00014555522298712961,
      "loss": 1.149,
      "step": 1825
    },
    {
      "epoch": 2.4576043068640647,
      "grad_norm": 0.8474212884902954,
      "learning_rate": 0.00014552529182879377,
      "loss": 0.8094,
      "step": 1826
    },
    {
      "epoch": 2.458950201884253,
      "grad_norm": 0.7579929232597351,
      "learning_rate": 0.00014549536067045795,
      "loss": 0.7893,
      "step": 1827
    },
    {
      "epoch": 2.4602960969044414,
      "grad_norm": 0.9466695785522461,
      "learning_rate": 0.0001454654295121221,
      "loss": 1.145,
      "step": 1828
    },
    {
      "epoch": 2.4616419919246297,
      "grad_norm": 1.0196876525878906,
      "learning_rate": 0.0001454354983537863,
      "loss": 0.999,
      "step": 1829
    },
    {
      "epoch": 2.462987886944818,
      "grad_norm": 0.8303744792938232,
      "learning_rate": 0.00014540556719545047,
      "loss": 0.652,
      "step": 1830
    },
    {
      "epoch": 2.464333781965007,
      "grad_norm": 0.940326452255249,
      "learning_rate": 0.00014537563603711463,
      "loss": 0.9187,
      "step": 1831
    },
    {
      "epoch": 2.465679676985195,
      "grad_norm": 0.8377199172973633,
      "learning_rate": 0.0001453457048787788,
      "loss": 0.8963,
      "step": 1832
    },
    {
      "epoch": 2.4670255720053835,
      "grad_norm": 0.9068521857261658,
      "learning_rate": 0.000145315773720443,
      "loss": 0.8778,
      "step": 1833
    },
    {
      "epoch": 2.468371467025572,
      "grad_norm": 1.0350866317749023,
      "learning_rate": 0.00014528584256210715,
      "loss": 0.8665,
      "step": 1834
    },
    {
      "epoch": 2.4697173620457606,
      "grad_norm": 0.8534631729125977,
      "learning_rate": 0.00014525591140377133,
      "loss": 1.0545,
      "step": 1835
    },
    {
      "epoch": 2.471063257065949,
      "grad_norm": 1.037377953529358,
      "learning_rate": 0.0001452259802454355,
      "loss": 0.9087,
      "step": 1836
    },
    {
      "epoch": 2.4724091520861373,
      "grad_norm": 0.8626461625099182,
      "learning_rate": 0.00014519604908709967,
      "loss": 0.7126,
      "step": 1837
    },
    {
      "epoch": 2.4737550471063257,
      "grad_norm": 0.94883131980896,
      "learning_rate": 0.00014516611792876385,
      "loss": 0.9354,
      "step": 1838
    },
    {
      "epoch": 2.475100942126514,
      "grad_norm": 0.9043793082237244,
      "learning_rate": 0.000145136186770428,
      "loss": 1.0563,
      "step": 1839
    },
    {
      "epoch": 2.4764468371467023,
      "grad_norm": 0.9782963395118713,
      "learning_rate": 0.0001451062556120922,
      "loss": 1.2043,
      "step": 1840
    },
    {
      "epoch": 2.477792732166891,
      "grad_norm": 0.9660763740539551,
      "learning_rate": 0.00014507632445375637,
      "loss": 1.2246,
      "step": 1841
    },
    {
      "epoch": 2.4791386271870794,
      "grad_norm": 0.9475153684616089,
      "learning_rate": 0.00014504639329542053,
      "loss": 0.9469,
      "step": 1842
    },
    {
      "epoch": 2.480484522207268,
      "grad_norm": 0.9273200631141663,
      "learning_rate": 0.0001450164621370847,
      "loss": 0.903,
      "step": 1843
    },
    {
      "epoch": 2.481830417227456,
      "grad_norm": 0.900312602519989,
      "learning_rate": 0.00014498653097874887,
      "loss": 0.8372,
      "step": 1844
    },
    {
      "epoch": 2.483176312247645,
      "grad_norm": 0.8556723594665527,
      "learning_rate": 0.00014495659982041305,
      "loss": 0.9572,
      "step": 1845
    },
    {
      "epoch": 2.4845222072678332,
      "grad_norm": 1.0285907983779907,
      "learning_rate": 0.00014492666866207723,
      "loss": 0.9912,
      "step": 1846
    },
    {
      "epoch": 2.4858681022880216,
      "grad_norm": 0.7784583568572998,
      "learning_rate": 0.0001448967375037414,
      "loss": 0.7513,
      "step": 1847
    },
    {
      "epoch": 2.48721399730821,
      "grad_norm": 0.9491059184074402,
      "learning_rate": 0.00014486680634540557,
      "loss": 0.9393,
      "step": 1848
    },
    {
      "epoch": 2.4885598923283982,
      "grad_norm": 0.8223363757133484,
      "learning_rate": 0.00014483687518706975,
      "loss": 0.8606,
      "step": 1849
    },
    {
      "epoch": 2.4899057873485866,
      "grad_norm": 0.8869485855102539,
      "learning_rate": 0.0001448069440287339,
      "loss": 0.8124,
      "step": 1850
    },
    {
      "epoch": 2.4912516823687754,
      "grad_norm": 0.7467525005340576,
      "learning_rate": 0.0001447770128703981,
      "loss": 0.9159,
      "step": 1851
    },
    {
      "epoch": 2.4925975773889637,
      "grad_norm": 0.8580583930015564,
      "learning_rate": 0.00014474708171206225,
      "loss": 1.0112,
      "step": 1852
    },
    {
      "epoch": 2.493943472409152,
      "grad_norm": 0.8213965892791748,
      "learning_rate": 0.00014471715055372643,
      "loss": 1.0068,
      "step": 1853
    },
    {
      "epoch": 2.4952893674293404,
      "grad_norm": 0.837655782699585,
      "learning_rate": 0.00014468721939539061,
      "loss": 0.9226,
      "step": 1854
    },
    {
      "epoch": 2.496635262449529,
      "grad_norm": 0.8805615305900574,
      "learning_rate": 0.00014465728823705477,
      "loss": 0.7975,
      "step": 1855
    },
    {
      "epoch": 2.4979811574697175,
      "grad_norm": 0.8588677644729614,
      "learning_rate": 0.00014462735707871895,
      "loss": 1.3411,
      "step": 1856
    },
    {
      "epoch": 2.499327052489906,
      "grad_norm": 0.8259397745132446,
      "learning_rate": 0.00014459742592038313,
      "loss": 0.8119,
      "step": 1857
    },
    {
      "epoch": 2.500672947510094,
      "grad_norm": 1.0022592544555664,
      "learning_rate": 0.0001445674947620473,
      "loss": 0.9346,
      "step": 1858
    },
    {
      "epoch": 2.5020188425302825,
      "grad_norm": 1.0672357082366943,
      "learning_rate": 0.00014453756360371147,
      "loss": 0.9952,
      "step": 1859
    },
    {
      "epoch": 2.503364737550471,
      "grad_norm": 0.8383547067642212,
      "learning_rate": 0.00014450763244537563,
      "loss": 0.8257,
      "step": 1860
    },
    {
      "epoch": 2.5047106325706596,
      "grad_norm": 1.1108660697937012,
      "learning_rate": 0.0001444777012870398,
      "loss": 1.0486,
      "step": 1861
    },
    {
      "epoch": 2.506056527590848,
      "grad_norm": 0.9581031203269958,
      "learning_rate": 0.000144447770128704,
      "loss": 0.8979,
      "step": 1862
    },
    {
      "epoch": 2.5074024226110363,
      "grad_norm": 0.8745521903038025,
      "learning_rate": 0.00014441783897036815,
      "loss": 1.1821,
      "step": 1863
    },
    {
      "epoch": 2.5087483176312246,
      "grad_norm": 0.9500108361244202,
      "learning_rate": 0.00014438790781203233,
      "loss": 1.0113,
      "step": 1864
    },
    {
      "epoch": 2.5100942126514134,
      "grad_norm": 1.1361974477767944,
      "learning_rate": 0.00014435797665369652,
      "loss": 0.9307,
      "step": 1865
    },
    {
      "epoch": 2.5114401076716018,
      "grad_norm": 0.8584793210029602,
      "learning_rate": 0.00014432804549536067,
      "loss": 0.8802,
      "step": 1866
    },
    {
      "epoch": 2.51278600269179,
      "grad_norm": 0.7349395751953125,
      "learning_rate": 0.00014429811433702485,
      "loss": 1.2018,
      "step": 1867
    },
    {
      "epoch": 2.5141318977119784,
      "grad_norm": 0.9904695749282837,
      "learning_rate": 0.000144268183178689,
      "loss": 0.8278,
      "step": 1868
    },
    {
      "epoch": 2.5154777927321668,
      "grad_norm": 0.9841049909591675,
      "learning_rate": 0.0001442382520203532,
      "loss": 0.879,
      "step": 1869
    },
    {
      "epoch": 2.516823687752355,
      "grad_norm": 0.799787163734436,
      "learning_rate": 0.00014420832086201737,
      "loss": 0.8052,
      "step": 1870
    },
    {
      "epoch": 2.518169582772544,
      "grad_norm": 0.7033540606498718,
      "learning_rate": 0.00014417838970368153,
      "loss": 0.8261,
      "step": 1871
    },
    {
      "epoch": 2.519515477792732,
      "grad_norm": 0.5645187497138977,
      "learning_rate": 0.0001441484585453457,
      "loss": 0.8281,
      "step": 1872
    },
    {
      "epoch": 2.5208613728129206,
      "grad_norm": 0.8536680936813354,
      "learning_rate": 0.0001441185273870099,
      "loss": 0.6854,
      "step": 1873
    },
    {
      "epoch": 2.522207267833109,
      "grad_norm": 0.9015926718711853,
      "learning_rate": 0.00014408859622867405,
      "loss": 0.8941,
      "step": 1874
    },
    {
      "epoch": 2.5235531628532977,
      "grad_norm": 0.9284735321998596,
      "learning_rate": 0.00014405866507033823,
      "loss": 0.9522,
      "step": 1875
    },
    {
      "epoch": 2.524899057873486,
      "grad_norm": 0.9955298900604248,
      "learning_rate": 0.0001440287339120024,
      "loss": 1.0736,
      "step": 1876
    },
    {
      "epoch": 2.5262449528936743,
      "grad_norm": 0.8757269978523254,
      "learning_rate": 0.00014399880275366657,
      "loss": 0.9889,
      "step": 1877
    },
    {
      "epoch": 2.5275908479138627,
      "grad_norm": 1.1337074041366577,
      "learning_rate": 0.00014396887159533075,
      "loss": 0.7228,
      "step": 1878
    },
    {
      "epoch": 2.528936742934051,
      "grad_norm": 0.9111213088035583,
      "learning_rate": 0.0001439389404369949,
      "loss": 0.8412,
      "step": 1879
    },
    {
      "epoch": 2.5302826379542394,
      "grad_norm": 0.9417024850845337,
      "learning_rate": 0.0001439090092786591,
      "loss": 0.968,
      "step": 1880
    },
    {
      "epoch": 2.531628532974428,
      "grad_norm": 1.0064697265625,
      "learning_rate": 0.00014387907812032328,
      "loss": 1.1043,
      "step": 1881
    },
    {
      "epoch": 2.5329744279946165,
      "grad_norm": 0.9313193559646606,
      "learning_rate": 0.00014384914696198743,
      "loss": 1.0438,
      "step": 1882
    },
    {
      "epoch": 2.534320323014805,
      "grad_norm": 0.9389359951019287,
      "learning_rate": 0.00014381921580365161,
      "loss": 0.8552,
      "step": 1883
    },
    {
      "epoch": 2.535666218034993,
      "grad_norm": 0.9272369742393494,
      "learning_rate": 0.00014378928464531577,
      "loss": 0.999,
      "step": 1884
    },
    {
      "epoch": 2.537012113055182,
      "grad_norm": 0.8411651253700256,
      "learning_rate": 0.00014375935348697995,
      "loss": 0.7614,
      "step": 1885
    },
    {
      "epoch": 2.5383580080753703,
      "grad_norm": 0.9017593860626221,
      "learning_rate": 0.00014372942232864414,
      "loss": 0.8806,
      "step": 1886
    },
    {
      "epoch": 2.5397039030955586,
      "grad_norm": 1.0185648202896118,
      "learning_rate": 0.0001436994911703083,
      "loss": 0.9463,
      "step": 1887
    },
    {
      "epoch": 2.541049798115747,
      "grad_norm": 0.8396753072738647,
      "learning_rate": 0.00014366956001197247,
      "loss": 1.0811,
      "step": 1888
    },
    {
      "epoch": 2.5423956931359353,
      "grad_norm": 0.846866250038147,
      "learning_rate": 0.00014363962885363666,
      "loss": 1.1183,
      "step": 1889
    },
    {
      "epoch": 2.5437415881561236,
      "grad_norm": 0.8148003220558167,
      "learning_rate": 0.0001436096976953008,
      "loss": 1.0476,
      "step": 1890
    },
    {
      "epoch": 2.5450874831763124,
      "grad_norm": 0.7108558416366577,
      "learning_rate": 0.000143579766536965,
      "loss": 1.1813,
      "step": 1891
    },
    {
      "epoch": 2.5464333781965007,
      "grad_norm": 0.7966070771217346,
      "learning_rate": 0.00014354983537862915,
      "loss": 0.8453,
      "step": 1892
    },
    {
      "epoch": 2.547779273216689,
      "grad_norm": 0.8471769690513611,
      "learning_rate": 0.00014351990422029333,
      "loss": 0.9808,
      "step": 1893
    },
    {
      "epoch": 2.5491251682368774,
      "grad_norm": 0.8856841921806335,
      "learning_rate": 0.00014348997306195752,
      "loss": 0.8574,
      "step": 1894
    },
    {
      "epoch": 2.550471063257066,
      "grad_norm": 0.8272574543952942,
      "learning_rate": 0.00014346004190362167,
      "loss": 0.7819,
      "step": 1895
    },
    {
      "epoch": 2.5518169582772545,
      "grad_norm": 1.0971142053604126,
      "learning_rate": 0.00014343011074528585,
      "loss": 0.9488,
      "step": 1896
    },
    {
      "epoch": 2.553162853297443,
      "grad_norm": 0.9634423851966858,
      "learning_rate": 0.00014340017958695004,
      "loss": 0.9946,
      "step": 1897
    },
    {
      "epoch": 2.554508748317631,
      "grad_norm": 0.834885835647583,
      "learning_rate": 0.0001433702484286142,
      "loss": 0.9678,
      "step": 1898
    },
    {
      "epoch": 2.5558546433378195,
      "grad_norm": 0.9246526956558228,
      "learning_rate": 0.00014334031727027837,
      "loss": 1.0182,
      "step": 1899
    },
    {
      "epoch": 2.557200538358008,
      "grad_norm": 0.8158206343650818,
      "learning_rate": 0.00014331038611194253,
      "loss": 0.8363,
      "step": 1900
    },
    {
      "epoch": 2.5585464333781966,
      "grad_norm": 0.776054322719574,
      "learning_rate": 0.0001432804549536067,
      "loss": 0.9985,
      "step": 1901
    },
    {
      "epoch": 2.559892328398385,
      "grad_norm": 1.1231539249420166,
      "learning_rate": 0.0001432505237952709,
      "loss": 1.0734,
      "step": 1902
    },
    {
      "epoch": 2.5612382234185733,
      "grad_norm": 0.8252171277999878,
      "learning_rate": 0.00014322059263693505,
      "loss": 0.8337,
      "step": 1903
    },
    {
      "epoch": 2.5625841184387617,
      "grad_norm": 1.0033658742904663,
      "learning_rate": 0.00014319066147859923,
      "loss": 0.931,
      "step": 1904
    },
    {
      "epoch": 2.5639300134589504,
      "grad_norm": 1.0163589715957642,
      "learning_rate": 0.00014316073032026342,
      "loss": 0.9857,
      "step": 1905
    },
    {
      "epoch": 2.5652759084791388,
      "grad_norm": 0.8926025629043579,
      "learning_rate": 0.00014313079916192757,
      "loss": 0.8049,
      "step": 1906
    },
    {
      "epoch": 2.566621803499327,
      "grad_norm": 0.9800416827201843,
      "learning_rate": 0.00014310086800359175,
      "loss": 1.2708,
      "step": 1907
    },
    {
      "epoch": 2.5679676985195155,
      "grad_norm": 1.110506534576416,
      "learning_rate": 0.0001430709368452559,
      "loss": 0.8988,
      "step": 1908
    },
    {
      "epoch": 2.569313593539704,
      "grad_norm": 0.980789303779602,
      "learning_rate": 0.0001430410056869201,
      "loss": 1.1028,
      "step": 1909
    },
    {
      "epoch": 2.570659488559892,
      "grad_norm": 1.0279985666275024,
      "learning_rate": 0.00014301107452858428,
      "loss": 1.0315,
      "step": 1910
    },
    {
      "epoch": 2.572005383580081,
      "grad_norm": 0.954360842704773,
      "learning_rate": 0.00014298114337024843,
      "loss": 0.93,
      "step": 1911
    },
    {
      "epoch": 2.5733512786002692,
      "grad_norm": 1.0102646350860596,
      "learning_rate": 0.00014295121221191261,
      "loss": 0.9708,
      "step": 1912
    },
    {
      "epoch": 2.5746971736204576,
      "grad_norm": 0.9710964560508728,
      "learning_rate": 0.00014292128105357677,
      "loss": 0.881,
      "step": 1913
    },
    {
      "epoch": 2.576043068640646,
      "grad_norm": 0.7384480834007263,
      "learning_rate": 0.00014289134989524095,
      "loss": 0.8735,
      "step": 1914
    },
    {
      "epoch": 2.5773889636608347,
      "grad_norm": 1.1198915243148804,
      "learning_rate": 0.00014286141873690514,
      "loss": 1.0169,
      "step": 1915
    },
    {
      "epoch": 2.578734858681023,
      "grad_norm": 1.0939403772354126,
      "learning_rate": 0.0001428314875785693,
      "loss": 1.1628,
      "step": 1916
    },
    {
      "epoch": 2.5800807537012114,
      "grad_norm": 0.9383240342140198,
      "learning_rate": 0.00014280155642023347,
      "loss": 0.9878,
      "step": 1917
    },
    {
      "epoch": 2.5814266487213997,
      "grad_norm": 0.9367483854293823,
      "learning_rate": 0.00014277162526189766,
      "loss": 1.033,
      "step": 1918
    },
    {
      "epoch": 2.582772543741588,
      "grad_norm": 0.7882514595985413,
      "learning_rate": 0.0001427416941035618,
      "loss": 0.8619,
      "step": 1919
    },
    {
      "epoch": 2.5841184387617764,
      "grad_norm": 0.8494101166725159,
      "learning_rate": 0.000142711762945226,
      "loss": 1.0048,
      "step": 1920
    },
    {
      "epoch": 2.585464333781965,
      "grad_norm": 0.9006249308586121,
      "learning_rate": 0.00014268183178689015,
      "loss": 0.8521,
      "step": 1921
    },
    {
      "epoch": 2.5868102288021535,
      "grad_norm": 1.0085912942886353,
      "learning_rate": 0.00014265190062855433,
      "loss": 1.1416,
      "step": 1922
    },
    {
      "epoch": 2.588156123822342,
      "grad_norm": 1.067001223564148,
      "learning_rate": 0.00014262196947021852,
      "loss": 1.0082,
      "step": 1923
    },
    {
      "epoch": 2.58950201884253,
      "grad_norm": 0.8570555448532104,
      "learning_rate": 0.00014259203831188267,
      "loss": 0.96,
      "step": 1924
    },
    {
      "epoch": 2.590847913862719,
      "grad_norm": 0.9429517984390259,
      "learning_rate": 0.00014256210715354685,
      "loss": 1.0421,
      "step": 1925
    },
    {
      "epoch": 2.5921938088829073,
      "grad_norm": 0.9851733446121216,
      "learning_rate": 0.00014253217599521104,
      "loss": 1.0892,
      "step": 1926
    },
    {
      "epoch": 2.5935397039030956,
      "grad_norm": 1.028748869895935,
      "learning_rate": 0.0001425022448368752,
      "loss": 1.2833,
      "step": 1927
    },
    {
      "epoch": 2.594885598923284,
      "grad_norm": 0.8327109813690186,
      "learning_rate": 0.00014247231367853937,
      "loss": 0.6413,
      "step": 1928
    },
    {
      "epoch": 2.5962314939434723,
      "grad_norm": 0.7668997049331665,
      "learning_rate": 0.00014244238252020353,
      "loss": 0.8363,
      "step": 1929
    },
    {
      "epoch": 2.5975773889636606,
      "grad_norm": 0.9496322870254517,
      "learning_rate": 0.0001424124513618677,
      "loss": 0.9317,
      "step": 1930
    },
    {
      "epoch": 2.5989232839838494,
      "grad_norm": 0.7846347093582153,
      "learning_rate": 0.0001423825202035319,
      "loss": 0.8593,
      "step": 1931
    },
    {
      "epoch": 2.6002691790040378,
      "grad_norm": 0.8445879817008972,
      "learning_rate": 0.00014235258904519605,
      "loss": 0.9552,
      "step": 1932
    },
    {
      "epoch": 2.601615074024226,
      "grad_norm": 0.7780489325523376,
      "learning_rate": 0.00014232265788686023,
      "loss": 0.7887,
      "step": 1933
    },
    {
      "epoch": 2.6029609690444144,
      "grad_norm": 0.9941480159759521,
      "learning_rate": 0.00014229272672852442,
      "loss": 1.1167,
      "step": 1934
    },
    {
      "epoch": 2.604306864064603,
      "grad_norm": 0.9981669783592224,
      "learning_rate": 0.00014226279557018857,
      "loss": 0.9712,
      "step": 1935
    },
    {
      "epoch": 2.6056527590847915,
      "grad_norm": 0.766381025314331,
      "learning_rate": 0.00014223286441185276,
      "loss": 0.8301,
      "step": 1936
    },
    {
      "epoch": 2.60699865410498,
      "grad_norm": 0.862978458404541,
      "learning_rate": 0.0001422029332535169,
      "loss": 0.8197,
      "step": 1937
    },
    {
      "epoch": 2.608344549125168,
      "grad_norm": 0.9402764439582825,
      "learning_rate": 0.0001421730020951811,
      "loss": 0.9623,
      "step": 1938
    },
    {
      "epoch": 2.6096904441453566,
      "grad_norm": 1.0420870780944824,
      "learning_rate": 0.00014214307093684528,
      "loss": 0.9105,
      "step": 1939
    },
    {
      "epoch": 2.611036339165545,
      "grad_norm": 0.8970006108283997,
      "learning_rate": 0.00014211313977850943,
      "loss": 1.1124,
      "step": 1940
    },
    {
      "epoch": 2.6123822341857337,
      "grad_norm": 0.8980141878128052,
      "learning_rate": 0.00014208320862017361,
      "loss": 0.9896,
      "step": 1941
    },
    {
      "epoch": 2.613728129205922,
      "grad_norm": 0.699402928352356,
      "learning_rate": 0.0001420532774618378,
      "loss": 1.0156,
      "step": 1942
    },
    {
      "epoch": 2.6150740242261103,
      "grad_norm": 1.0003770589828491,
      "learning_rate": 0.00014202334630350195,
      "loss": 0.8603,
      "step": 1943
    },
    {
      "epoch": 2.6164199192462987,
      "grad_norm": 0.8146401047706604,
      "learning_rate": 0.00014199341514516614,
      "loss": 0.8854,
      "step": 1944
    },
    {
      "epoch": 2.6177658142664875,
      "grad_norm": 0.6857123970985413,
      "learning_rate": 0.0001419634839868303,
      "loss": 1.207,
      "step": 1945
    },
    {
      "epoch": 2.619111709286676,
      "grad_norm": 0.8866344094276428,
      "learning_rate": 0.00014193355282849447,
      "loss": 1.2102,
      "step": 1946
    },
    {
      "epoch": 2.620457604306864,
      "grad_norm": 0.7261441349983215,
      "learning_rate": 0.00014190362167015866,
      "loss": 0.8992,
      "step": 1947
    },
    {
      "epoch": 2.6218034993270525,
      "grad_norm": 0.8775966763496399,
      "learning_rate": 0.0001418736905118228,
      "loss": 0.7876,
      "step": 1948
    },
    {
      "epoch": 2.623149394347241,
      "grad_norm": 1.1563960313796997,
      "learning_rate": 0.000141843759353487,
      "loss": 0.9799,
      "step": 1949
    },
    {
      "epoch": 2.624495289367429,
      "grad_norm": 0.9628524780273438,
      "learning_rate": 0.00014181382819515118,
      "loss": 1.0321,
      "step": 1950
    },
    {
      "epoch": 2.6258411843876175,
      "grad_norm": 0.8653351068496704,
      "learning_rate": 0.00014178389703681533,
      "loss": 1.0983,
      "step": 1951
    },
    {
      "epoch": 2.6271870794078063,
      "grad_norm": 0.8826859593391418,
      "learning_rate": 0.00014175396587847952,
      "loss": 0.862,
      "step": 1952
    },
    {
      "epoch": 2.6285329744279946,
      "grad_norm": 0.9191800355911255,
      "learning_rate": 0.00014172403472014367,
      "loss": 0.8405,
      "step": 1953
    },
    {
      "epoch": 2.629878869448183,
      "grad_norm": 0.851672351360321,
      "learning_rate": 0.00014169410356180785,
      "loss": 1.1927,
      "step": 1954
    },
    {
      "epoch": 2.6312247644683717,
      "grad_norm": 1.268596887588501,
      "learning_rate": 0.00014166417240347204,
      "loss": 0.9571,
      "step": 1955
    },
    {
      "epoch": 2.63257065948856,
      "grad_norm": 0.8924561738967896,
      "learning_rate": 0.0001416342412451362,
      "loss": 0.8838,
      "step": 1956
    },
    {
      "epoch": 2.6339165545087484,
      "grad_norm": 0.9357538223266602,
      "learning_rate": 0.00014160431008680037,
      "loss": 0.7915,
      "step": 1957
    },
    {
      "epoch": 2.6352624495289367,
      "grad_norm": 1.0339120626449585,
      "learning_rate": 0.00014157437892846456,
      "loss": 1.001,
      "step": 1958
    },
    {
      "epoch": 2.636608344549125,
      "grad_norm": 0.9507840871810913,
      "learning_rate": 0.0001415444477701287,
      "loss": 1.0421,
      "step": 1959
    },
    {
      "epoch": 2.6379542395693134,
      "grad_norm": 0.8382057547569275,
      "learning_rate": 0.0001415145166117929,
      "loss": 1.0323,
      "step": 1960
    },
    {
      "epoch": 2.6393001345895017,
      "grad_norm": 0.8318641781806946,
      "learning_rate": 0.00014148458545345705,
      "loss": 0.8542,
      "step": 1961
    },
    {
      "epoch": 2.6406460296096905,
      "grad_norm": 0.7610837817192078,
      "learning_rate": 0.00014145465429512123,
      "loss": 0.9955,
      "step": 1962
    },
    {
      "epoch": 2.641991924629879,
      "grad_norm": 0.8597626686096191,
      "learning_rate": 0.00014142472313678542,
      "loss": 0.9591,
      "step": 1963
    },
    {
      "epoch": 2.643337819650067,
      "grad_norm": 0.8268775939941406,
      "learning_rate": 0.00014139479197844957,
      "loss": 0.9361,
      "step": 1964
    },
    {
      "epoch": 2.644683714670256,
      "grad_norm": 0.780538022518158,
      "learning_rate": 0.00014136486082011376,
      "loss": 1.0575,
      "step": 1965
    },
    {
      "epoch": 2.6460296096904443,
      "grad_norm": 0.8902973532676697,
      "learning_rate": 0.00014133492966177794,
      "loss": 1.1259,
      "step": 1966
    },
    {
      "epoch": 2.6473755047106327,
      "grad_norm": 1.0380864143371582,
      "learning_rate": 0.0001413049985034421,
      "loss": 0.8793,
      "step": 1967
    },
    {
      "epoch": 2.648721399730821,
      "grad_norm": 0.9649828672409058,
      "learning_rate": 0.00014127506734510628,
      "loss": 1.0306,
      "step": 1968
    },
    {
      "epoch": 2.6500672947510093,
      "grad_norm": 0.8368391990661621,
      "learning_rate": 0.00014124513618677043,
      "loss": 0.816,
      "step": 1969
    },
    {
      "epoch": 2.6514131897711977,
      "grad_norm": 1.0000442266464233,
      "learning_rate": 0.00014121520502843461,
      "loss": 1.0993,
      "step": 1970
    },
    {
      "epoch": 2.652759084791386,
      "grad_norm": 0.92550128698349,
      "learning_rate": 0.0001411852738700988,
      "loss": 1.0257,
      "step": 1971
    },
    {
      "epoch": 2.654104979811575,
      "grad_norm": 0.9611586928367615,
      "learning_rate": 0.00014115534271176295,
      "loss": 0.9521,
      "step": 1972
    },
    {
      "epoch": 2.655450874831763,
      "grad_norm": 0.7941980361938477,
      "learning_rate": 0.00014112541155342714,
      "loss": 1.0372,
      "step": 1973
    },
    {
      "epoch": 2.6567967698519515,
      "grad_norm": 0.912856936454773,
      "learning_rate": 0.00014109548039509132,
      "loss": 1.0178,
      "step": 1974
    },
    {
      "epoch": 2.6581426648721402,
      "grad_norm": 0.8683199882507324,
      "learning_rate": 0.00014106554923675547,
      "loss": 1.0977,
      "step": 1975
    },
    {
      "epoch": 2.6594885598923286,
      "grad_norm": 1.0831499099731445,
      "learning_rate": 0.00014103561807841966,
      "loss": 1.0242,
      "step": 1976
    },
    {
      "epoch": 2.660834454912517,
      "grad_norm": 1.0268374681472778,
      "learning_rate": 0.0001410056869200838,
      "loss": 0.9386,
      "step": 1977
    },
    {
      "epoch": 2.6621803499327052,
      "grad_norm": 0.9631621241569519,
      "learning_rate": 0.000140975755761748,
      "loss": 1.1327,
      "step": 1978
    },
    {
      "epoch": 2.6635262449528936,
      "grad_norm": 0.929951548576355,
      "learning_rate": 0.00014094582460341218,
      "loss": 0.9882,
      "step": 1979
    },
    {
      "epoch": 2.664872139973082,
      "grad_norm": 0.9342637658119202,
      "learning_rate": 0.00014091589344507633,
      "loss": 0.8851,
      "step": 1980
    },
    {
      "epoch": 2.6662180349932703,
      "grad_norm": 0.8385491371154785,
      "learning_rate": 0.00014088596228674052,
      "loss": 0.9304,
      "step": 1981
    },
    {
      "epoch": 2.667563930013459,
      "grad_norm": 0.9145476222038269,
      "learning_rate": 0.0001408560311284047,
      "loss": 0.9829,
      "step": 1982
    },
    {
      "epoch": 2.6689098250336474,
      "grad_norm": 1.05769681930542,
      "learning_rate": 0.00014082609997006885,
      "loss": 1.0153,
      "step": 1983
    },
    {
      "epoch": 2.6702557200538357,
      "grad_norm": 0.9018247723579407,
      "learning_rate": 0.00014079616881173304,
      "loss": 1.0041,
      "step": 1984
    },
    {
      "epoch": 2.6716016150740245,
      "grad_norm": 0.8017328381538391,
      "learning_rate": 0.0001407662376533972,
      "loss": 0.8731,
      "step": 1985
    },
    {
      "epoch": 2.672947510094213,
      "grad_norm": 0.8392586708068848,
      "learning_rate": 0.00014073630649506138,
      "loss": 0.8914,
      "step": 1986
    },
    {
      "epoch": 2.674293405114401,
      "grad_norm": 0.9117894172668457,
      "learning_rate": 0.00014070637533672556,
      "loss": 1.0528,
      "step": 1987
    },
    {
      "epoch": 2.6756393001345895,
      "grad_norm": 0.8512818813323975,
      "learning_rate": 0.0001406764441783897,
      "loss": 1.2174,
      "step": 1988
    },
    {
      "epoch": 2.676985195154778,
      "grad_norm": 0.8302447199821472,
      "learning_rate": 0.0001406465130200539,
      "loss": 0.8818,
      "step": 1989
    },
    {
      "epoch": 2.678331090174966,
      "grad_norm": 0.7475055456161499,
      "learning_rate": 0.00014061658186171805,
      "loss": 0.8312,
      "step": 1990
    },
    {
      "epoch": 2.6796769851951545,
      "grad_norm": 0.7891108393669128,
      "learning_rate": 0.0001405866507033822,
      "loss": 1.0242,
      "step": 1991
    },
    {
      "epoch": 2.6810228802153433,
      "grad_norm": 0.8335509896278381,
      "learning_rate": 0.0001405567195450464,
      "loss": 0.9266,
      "step": 1992
    },
    {
      "epoch": 2.6823687752355316,
      "grad_norm": 1.0094618797302246,
      "learning_rate": 0.00014052678838671057,
      "loss": 0.8858,
      "step": 1993
    },
    {
      "epoch": 2.68371467025572,
      "grad_norm": 0.8908945918083191,
      "learning_rate": 0.00014049685722837473,
      "loss": 1.1186,
      "step": 1994
    },
    {
      "epoch": 2.6850605652759088,
      "grad_norm": 0.9406561851501465,
      "learning_rate": 0.0001404669260700389,
      "loss": 1.0817,
      "step": 1995
    },
    {
      "epoch": 2.686406460296097,
      "grad_norm": 0.9085218906402588,
      "learning_rate": 0.00014043699491170307,
      "loss": 0.9395,
      "step": 1996
    },
    {
      "epoch": 2.6877523553162854,
      "grad_norm": 0.9179137945175171,
      "learning_rate": 0.00014040706375336725,
      "loss": 1.0182,
      "step": 1997
    },
    {
      "epoch": 2.6890982503364738,
      "grad_norm": 0.7925297021865845,
      "learning_rate": 0.00014037713259503143,
      "loss": 0.9826,
      "step": 1998
    },
    {
      "epoch": 2.690444145356662,
      "grad_norm": 0.905472993850708,
      "learning_rate": 0.0001403472014366956,
      "loss": 0.8416,
      "step": 1999
    },
    {
      "epoch": 2.6917900403768504,
      "grad_norm": 0.7929385304450989,
      "learning_rate": 0.00014031727027835977,
      "loss": 0.8984,
      "step": 2000
    },
    {
      "epoch": 2.6931359353970388,
      "grad_norm": 1.0174709558486938,
      "learning_rate": 0.00014028733912002395,
      "loss": 0.7853,
      "step": 2001
    },
    {
      "epoch": 2.6944818304172276,
      "grad_norm": 0.8454589247703552,
      "learning_rate": 0.0001402574079616881,
      "loss": 0.8925,
      "step": 2002
    },
    {
      "epoch": 2.695827725437416,
      "grad_norm": 0.800772488117218,
      "learning_rate": 0.0001402274768033523,
      "loss": 0.9064,
      "step": 2003
    },
    {
      "epoch": 2.6971736204576042,
      "grad_norm": 0.7686465978622437,
      "learning_rate": 0.00014019754564501645,
      "loss": 1.0171,
      "step": 2004
    },
    {
      "epoch": 2.698519515477793,
      "grad_norm": 0.8178613781929016,
      "learning_rate": 0.00014016761448668063,
      "loss": 0.9362,
      "step": 2005
    },
    {
      "epoch": 2.6998654104979813,
      "grad_norm": 0.8637716770172119,
      "learning_rate": 0.0001401376833283448,
      "loss": 0.7205,
      "step": 2006
    },
    {
      "epoch": 2.7012113055181697,
      "grad_norm": 0.9108185768127441,
      "learning_rate": 0.00014010775217000897,
      "loss": 1.0691,
      "step": 2007
    },
    {
      "epoch": 2.702557200538358,
      "grad_norm": 0.7422510981559753,
      "learning_rate": 0.00014007782101167315,
      "loss": 0.8377,
      "step": 2008
    },
    {
      "epoch": 2.7039030955585464,
      "grad_norm": 0.9616763591766357,
      "learning_rate": 0.00014004788985333733,
      "loss": 0.908,
      "step": 2009
    },
    {
      "epoch": 2.7052489905787347,
      "grad_norm": 0.8964998722076416,
      "learning_rate": 0.0001400179586950015,
      "loss": 0.8382,
      "step": 2010
    },
    {
      "epoch": 2.706594885598923,
      "grad_norm": 1.0374449491500854,
      "learning_rate": 0.00013998802753666567,
      "loss": 1.05,
      "step": 2011
    },
    {
      "epoch": 2.707940780619112,
      "grad_norm": 0.8926346898078918,
      "learning_rate": 0.00013995809637832983,
      "loss": 0.9441,
      "step": 2012
    },
    {
      "epoch": 2.7092866756393,
      "grad_norm": 1.0982416868209839,
      "learning_rate": 0.000139928165219994,
      "loss": 1.0455,
      "step": 2013
    },
    {
      "epoch": 2.7106325706594885,
      "grad_norm": 1.0409877300262451,
      "learning_rate": 0.0001398982340616582,
      "loss": 0.98,
      "step": 2014
    },
    {
      "epoch": 2.711978465679677,
      "grad_norm": 0.8427836894989014,
      "learning_rate": 0.00013986830290332235,
      "loss": 0.9652,
      "step": 2015
    },
    {
      "epoch": 2.7133243606998656,
      "grad_norm": 0.809437096118927,
      "learning_rate": 0.00013983837174498653,
      "loss": 0.7464,
      "step": 2016
    },
    {
      "epoch": 2.714670255720054,
      "grad_norm": 0.901054859161377,
      "learning_rate": 0.00013980844058665071,
      "loss": 1.0303,
      "step": 2017
    },
    {
      "epoch": 2.7160161507402423,
      "grad_norm": 0.9204721450805664,
      "learning_rate": 0.00013977850942831487,
      "loss": 0.8603,
      "step": 2018
    },
    {
      "epoch": 2.7173620457604306,
      "grad_norm": 0.7994709610939026,
      "learning_rate": 0.00013974857826997905,
      "loss": 1.0115,
      "step": 2019
    },
    {
      "epoch": 2.718707940780619,
      "grad_norm": 1.1436859369277954,
      "learning_rate": 0.0001397186471116432,
      "loss": 1.1331,
      "step": 2020
    },
    {
      "epoch": 2.7200538358008073,
      "grad_norm": 0.9259606003761292,
      "learning_rate": 0.0001396887159533074,
      "loss": 1.0117,
      "step": 2021
    },
    {
      "epoch": 2.721399730820996,
      "grad_norm": 0.938611626625061,
      "learning_rate": 0.00013965878479497157,
      "loss": 0.9425,
      "step": 2022
    },
    {
      "epoch": 2.7227456258411844,
      "grad_norm": 0.6849371194839478,
      "learning_rate": 0.00013962885363663573,
      "loss": 1.114,
      "step": 2023
    },
    {
      "epoch": 2.7240915208613727,
      "grad_norm": 0.9515274167060852,
      "learning_rate": 0.0001395989224782999,
      "loss": 0.9203,
      "step": 2024
    },
    {
      "epoch": 2.725437415881561,
      "grad_norm": 0.7405835390090942,
      "learning_rate": 0.0001395689913199641,
      "loss": 0.907,
      "step": 2025
    },
    {
      "epoch": 2.72678331090175,
      "grad_norm": 0.6338518261909485,
      "learning_rate": 0.00013953906016162825,
      "loss": 0.7641,
      "step": 2026
    },
    {
      "epoch": 2.728129205921938,
      "grad_norm": 0.9289835095405579,
      "learning_rate": 0.00013950912900329243,
      "loss": 1.0419,
      "step": 2027
    },
    {
      "epoch": 2.7294751009421265,
      "grad_norm": 0.9240381121635437,
      "learning_rate": 0.0001394791978449566,
      "loss": 1.0242,
      "step": 2028
    },
    {
      "epoch": 2.730820995962315,
      "grad_norm": 0.7656287550926208,
      "learning_rate": 0.00013944926668662077,
      "loss": 1.0741,
      "step": 2029
    },
    {
      "epoch": 2.732166890982503,
      "grad_norm": 1.0248591899871826,
      "learning_rate": 0.00013941933552828495,
      "loss": 0.947,
      "step": 2030
    },
    {
      "epoch": 2.7335127860026915,
      "grad_norm": 0.8985410332679749,
      "learning_rate": 0.0001393894043699491,
      "loss": 0.9137,
      "step": 2031
    },
    {
      "epoch": 2.7348586810228803,
      "grad_norm": 1.1064705848693848,
      "learning_rate": 0.0001393594732116133,
      "loss": 1.1693,
      "step": 2032
    },
    {
      "epoch": 2.7362045760430687,
      "grad_norm": 0.89136803150177,
      "learning_rate": 0.00013932954205327747,
      "loss": 0.6665,
      "step": 2033
    },
    {
      "epoch": 2.737550471063257,
      "grad_norm": 0.8516898155212402,
      "learning_rate": 0.00013929961089494163,
      "loss": 0.9356,
      "step": 2034
    },
    {
      "epoch": 2.7388963660834453,
      "grad_norm": 0.8172802925109863,
      "learning_rate": 0.0001392696797366058,
      "loss": 0.8969,
      "step": 2035
    },
    {
      "epoch": 2.740242261103634,
      "grad_norm": 0.868489146232605,
      "learning_rate": 0.00013923974857826997,
      "loss": 1.0632,
      "step": 2036
    },
    {
      "epoch": 2.7415881561238225,
      "grad_norm": 1.0626444816589355,
      "learning_rate": 0.00013920981741993415,
      "loss": 0.9034,
      "step": 2037
    },
    {
      "epoch": 2.742934051144011,
      "grad_norm": 0.8948919773101807,
      "learning_rate": 0.00013917988626159833,
      "loss": 0.7763,
      "step": 2038
    },
    {
      "epoch": 2.744279946164199,
      "grad_norm": 0.9913966655731201,
      "learning_rate": 0.0001391499551032625,
      "loss": 0.9949,
      "step": 2039
    },
    {
      "epoch": 2.7456258411843875,
      "grad_norm": 0.7641518115997314,
      "learning_rate": 0.00013912002394492667,
      "loss": 0.7972,
      "step": 2040
    },
    {
      "epoch": 2.746971736204576,
      "grad_norm": 1.0422091484069824,
      "learning_rate": 0.00013909009278659085,
      "loss": 0.8932,
      "step": 2041
    },
    {
      "epoch": 2.7483176312247646,
      "grad_norm": 1.1655360460281372,
      "learning_rate": 0.000139060161628255,
      "loss": 1.0571,
      "step": 2042
    },
    {
      "epoch": 2.749663526244953,
      "grad_norm": 0.9212619066238403,
      "learning_rate": 0.0001390302304699192,
      "loss": 0.9687,
      "step": 2043
    },
    {
      "epoch": 2.7510094212651413,
      "grad_norm": 0.8594796657562256,
      "learning_rate": 0.00013900029931158335,
      "loss": 0.7434,
      "step": 2044
    },
    {
      "epoch": 2.7523553162853296,
      "grad_norm": 0.8991284966468811,
      "learning_rate": 0.00013897036815324753,
      "loss": 0.9386,
      "step": 2045
    },
    {
      "epoch": 2.7537012113055184,
      "grad_norm": 0.95985347032547,
      "learning_rate": 0.00013894043699491171,
      "loss": 1.2183,
      "step": 2046
    },
    {
      "epoch": 2.7550471063257067,
      "grad_norm": 1.0161174535751343,
      "learning_rate": 0.00013891050583657587,
      "loss": 1.1134,
      "step": 2047
    },
    {
      "epoch": 2.756393001345895,
      "grad_norm": 0.8524967432022095,
      "learning_rate": 0.00013888057467824005,
      "loss": 1.0459,
      "step": 2048
    },
    {
      "epoch": 2.7577388963660834,
      "grad_norm": 1.143823504447937,
      "learning_rate": 0.00013885064351990423,
      "loss": 1.1734,
      "step": 2049
    },
    {
      "epoch": 2.7590847913862717,
      "grad_norm": 0.8256825804710388,
      "learning_rate": 0.0001388207123615684,
      "loss": 0.9665,
      "step": 2050
    },
    {
      "epoch": 2.76043068640646,
      "grad_norm": 0.7818701863288879,
      "learning_rate": 0.00013879078120323257,
      "loss": 0.7723,
      "step": 2051
    },
    {
      "epoch": 2.761776581426649,
      "grad_norm": 0.9096440076828003,
      "learning_rate": 0.00013876085004489673,
      "loss": 0.8794,
      "step": 2052
    },
    {
      "epoch": 2.763122476446837,
      "grad_norm": 0.8917648196220398,
      "learning_rate": 0.0001387309188865609,
      "loss": 0.9021,
      "step": 2053
    },
    {
      "epoch": 2.7644683714670255,
      "grad_norm": 0.858578622341156,
      "learning_rate": 0.0001387009877282251,
      "loss": 0.7862,
      "step": 2054
    },
    {
      "epoch": 2.765814266487214,
      "grad_norm": 1.005216121673584,
      "learning_rate": 0.00013867105656988925,
      "loss": 0.9386,
      "step": 2055
    },
    {
      "epoch": 2.7671601615074026,
      "grad_norm": 0.8822292685508728,
      "learning_rate": 0.00013864112541155343,
      "loss": 0.7179,
      "step": 2056
    },
    {
      "epoch": 2.768506056527591,
      "grad_norm": 0.815855085849762,
      "learning_rate": 0.00013861119425321762,
      "loss": 0.9968,
      "step": 2057
    },
    {
      "epoch": 2.7698519515477793,
      "grad_norm": 0.8939087986946106,
      "learning_rate": 0.00013858126309488177,
      "loss": 1.0465,
      "step": 2058
    },
    {
      "epoch": 2.7711978465679676,
      "grad_norm": 0.809878945350647,
      "learning_rate": 0.00013855133193654595,
      "loss": 0.9018,
      "step": 2059
    },
    {
      "epoch": 2.772543741588156,
      "grad_norm": 0.8575327396392822,
      "learning_rate": 0.0001385214007782101,
      "loss": 0.7824,
      "step": 2060
    },
    {
      "epoch": 2.7738896366083443,
      "grad_norm": 0.8182662725448608,
      "learning_rate": 0.0001384914696198743,
      "loss": 0.786,
      "step": 2061
    },
    {
      "epoch": 2.775235531628533,
      "grad_norm": 0.8665427565574646,
      "learning_rate": 0.00013846153846153847,
      "loss": 0.8982,
      "step": 2062
    },
    {
      "epoch": 2.7765814266487214,
      "grad_norm": 0.9757301211357117,
      "learning_rate": 0.00013843160730320263,
      "loss": 0.7757,
      "step": 2063
    },
    {
      "epoch": 2.7779273216689098,
      "grad_norm": 0.8856196403503418,
      "learning_rate": 0.0001384016761448668,
      "loss": 0.9417,
      "step": 2064
    },
    {
      "epoch": 2.779273216689098,
      "grad_norm": 0.9452406764030457,
      "learning_rate": 0.000138371744986531,
      "loss": 0.8876,
      "step": 2065
    },
    {
      "epoch": 2.780619111709287,
      "grad_norm": 0.9851189255714417,
      "learning_rate": 0.00013834181382819515,
      "loss": 0.983,
      "step": 2066
    },
    {
      "epoch": 2.781965006729475,
      "grad_norm": 0.9168566465377808,
      "learning_rate": 0.00013831188266985933,
      "loss": 0.9521,
      "step": 2067
    },
    {
      "epoch": 2.7833109017496636,
      "grad_norm": 1.0315008163452148,
      "learning_rate": 0.0001382819515115235,
      "loss": 0.9895,
      "step": 2068
    },
    {
      "epoch": 2.784656796769852,
      "grad_norm": 1.0565614700317383,
      "learning_rate": 0.00013825202035318767,
      "loss": 0.8156,
      "step": 2069
    },
    {
      "epoch": 2.7860026917900402,
      "grad_norm": 0.833631694316864,
      "learning_rate": 0.00013822208919485185,
      "loss": 1.0166,
      "step": 2070
    },
    {
      "epoch": 2.7873485868102286,
      "grad_norm": 0.8986115455627441,
      "learning_rate": 0.000138192158036516,
      "loss": 1.0384,
      "step": 2071
    },
    {
      "epoch": 2.7886944818304173,
      "grad_norm": 0.9437031149864197,
      "learning_rate": 0.0001381622268781802,
      "loss": 0.7898,
      "step": 2072
    },
    {
      "epoch": 2.7900403768506057,
      "grad_norm": 0.775298535823822,
      "learning_rate": 0.00013813229571984438,
      "loss": 1.3121,
      "step": 2073
    },
    {
      "epoch": 2.791386271870794,
      "grad_norm": 0.7710614204406738,
      "learning_rate": 0.00013810236456150853,
      "loss": 0.8036,
      "step": 2074
    },
    {
      "epoch": 2.7927321668909824,
      "grad_norm": 0.7870768904685974,
      "learning_rate": 0.00013807243340317271,
      "loss": 0.8258,
      "step": 2075
    },
    {
      "epoch": 2.794078061911171,
      "grad_norm": 0.9372384548187256,
      "learning_rate": 0.00013804250224483687,
      "loss": 1.0869,
      "step": 2076
    },
    {
      "epoch": 2.7954239569313595,
      "grad_norm": 0.9130479693412781,
      "learning_rate": 0.00013801257108650105,
      "loss": 0.9805,
      "step": 2077
    },
    {
      "epoch": 2.796769851951548,
      "grad_norm": 0.8932082653045654,
      "learning_rate": 0.00013798263992816524,
      "loss": 0.7684,
      "step": 2078
    },
    {
      "epoch": 2.798115746971736,
      "grad_norm": 0.8564324378967285,
      "learning_rate": 0.0001379527087698294,
      "loss": 0.8932,
      "step": 2079
    },
    {
      "epoch": 2.7994616419919245,
      "grad_norm": 0.9335441589355469,
      "learning_rate": 0.00013792277761149357,
      "loss": 0.9198,
      "step": 2080
    },
    {
      "epoch": 2.800807537012113,
      "grad_norm": 0.8750025033950806,
      "learning_rate": 0.00013789284645315773,
      "loss": 0.8604,
      "step": 2081
    },
    {
      "epoch": 2.8021534320323016,
      "grad_norm": 0.7298517227172852,
      "learning_rate": 0.0001378629152948219,
      "loss": 0.9515,
      "step": 2082
    },
    {
      "epoch": 2.80349932705249,
      "grad_norm": 0.7639656066894531,
      "learning_rate": 0.0001378329841364861,
      "loss": 1.1857,
      "step": 2083
    },
    {
      "epoch": 2.8048452220726783,
      "grad_norm": 0.8591052889823914,
      "learning_rate": 0.00013780305297815025,
      "loss": 0.7967,
      "step": 2084
    },
    {
      "epoch": 2.8061911170928666,
      "grad_norm": 0.8002710938453674,
      "learning_rate": 0.00013777312181981443,
      "loss": 0.9616,
      "step": 2085
    },
    {
      "epoch": 2.8075370121130554,
      "grad_norm": 0.9453826546669006,
      "learning_rate": 0.00013774319066147862,
      "loss": 1.025,
      "step": 2086
    },
    {
      "epoch": 2.8088829071332437,
      "grad_norm": 0.8421500325202942,
      "learning_rate": 0.00013771325950314277,
      "loss": 0.9494,
      "step": 2087
    },
    {
      "epoch": 2.810228802153432,
      "grad_norm": 0.8631484508514404,
      "learning_rate": 0.00013768332834480695,
      "loss": 0.8287,
      "step": 2088
    },
    {
      "epoch": 2.8115746971736204,
      "grad_norm": 0.8777075409889221,
      "learning_rate": 0.0001376533971864711,
      "loss": 0.8851,
      "step": 2089
    },
    {
      "epoch": 2.8129205921938087,
      "grad_norm": 0.7780700922012329,
      "learning_rate": 0.0001376234660281353,
      "loss": 0.962,
      "step": 2090
    },
    {
      "epoch": 2.814266487213997,
      "grad_norm": 0.8838365077972412,
      "learning_rate": 0.00013759353486979947,
      "loss": 1.1317,
      "step": 2091
    },
    {
      "epoch": 2.815612382234186,
      "grad_norm": 0.8568472862243652,
      "learning_rate": 0.00013756360371146363,
      "loss": 0.797,
      "step": 2092
    },
    {
      "epoch": 2.816958277254374,
      "grad_norm": 0.8500003814697266,
      "learning_rate": 0.0001375336725531278,
      "loss": 0.9736,
      "step": 2093
    },
    {
      "epoch": 2.8183041722745625,
      "grad_norm": 0.8851629495620728,
      "learning_rate": 0.000137503741394792,
      "loss": 0.9355,
      "step": 2094
    },
    {
      "epoch": 2.819650067294751,
      "grad_norm": 0.9163116812705994,
      "learning_rate": 0.00013747381023645615,
      "loss": 0.9829,
      "step": 2095
    },
    {
      "epoch": 2.8209959623149397,
      "grad_norm": 0.855280339717865,
      "learning_rate": 0.00013744387907812033,
      "loss": 0.853,
      "step": 2096
    },
    {
      "epoch": 2.822341857335128,
      "grad_norm": 0.8400127291679382,
      "learning_rate": 0.0001374139479197845,
      "loss": 1.0012,
      "step": 2097
    },
    {
      "epoch": 2.8236877523553163,
      "grad_norm": 1.0111348628997803,
      "learning_rate": 0.00013738401676144867,
      "loss": 0.7666,
      "step": 2098
    },
    {
      "epoch": 2.8250336473755047,
      "grad_norm": 0.8049708008766174,
      "learning_rate": 0.00013735408560311285,
      "loss": 0.9888,
      "step": 2099
    },
    {
      "epoch": 2.826379542395693,
      "grad_norm": 1.0323389768600464,
      "learning_rate": 0.000137324154444777,
      "loss": 1.0931,
      "step": 2100
    },
    {
      "epoch": 2.8277254374158813,
      "grad_norm": 1.1301714181900024,
      "learning_rate": 0.0001372942232864412,
      "loss": 1.2776,
      "step": 2101
    },
    {
      "epoch": 2.82907133243607,
      "grad_norm": 0.9321244955062866,
      "learning_rate": 0.00013726429212810538,
      "loss": 0.8846,
      "step": 2102
    },
    {
      "epoch": 2.8304172274562585,
      "grad_norm": 1.001067876815796,
      "learning_rate": 0.00013723436096976953,
      "loss": 1.0761,
      "step": 2103
    },
    {
      "epoch": 2.831763122476447,
      "grad_norm": 0.8290473222732544,
      "learning_rate": 0.00013720442981143371,
      "loss": 0.7967,
      "step": 2104
    },
    {
      "epoch": 2.833109017496635,
      "grad_norm": 1.189558982849121,
      "learning_rate": 0.00013717449865309787,
      "loss": 1.1224,
      "step": 2105
    },
    {
      "epoch": 2.834454912516824,
      "grad_norm": 0.9372385740280151,
      "learning_rate": 0.00013714456749476205,
      "loss": 1.0762,
      "step": 2106
    },
    {
      "epoch": 2.8358008075370122,
      "grad_norm": 0.95079505443573,
      "learning_rate": 0.00013711463633642624,
      "loss": 1.0541,
      "step": 2107
    },
    {
      "epoch": 2.8371467025572006,
      "grad_norm": 1.0025784969329834,
      "learning_rate": 0.0001370847051780904,
      "loss": 0.803,
      "step": 2108
    },
    {
      "epoch": 2.838492597577389,
      "grad_norm": 0.9145662784576416,
      "learning_rate": 0.00013705477401975457,
      "loss": 1.033,
      "step": 2109
    },
    {
      "epoch": 2.8398384925975773,
      "grad_norm": 0.8570256233215332,
      "learning_rate": 0.00013702484286141876,
      "loss": 1.0167,
      "step": 2110
    },
    {
      "epoch": 2.8411843876177656,
      "grad_norm": 0.9891858100891113,
      "learning_rate": 0.0001369949117030829,
      "loss": 0.9859,
      "step": 2111
    },
    {
      "epoch": 2.8425302826379544,
      "grad_norm": 0.8851664066314697,
      "learning_rate": 0.0001369649805447471,
      "loss": 1.3428,
      "step": 2112
    },
    {
      "epoch": 2.8438761776581427,
      "grad_norm": 0.8346694707870483,
      "learning_rate": 0.00013693504938641125,
      "loss": 0.8432,
      "step": 2113
    },
    {
      "epoch": 2.845222072678331,
      "grad_norm": 0.9198921322822571,
      "learning_rate": 0.00013690511822807543,
      "loss": 0.8261,
      "step": 2114
    },
    {
      "epoch": 2.8465679676985194,
      "grad_norm": 0.9090308547019958,
      "learning_rate": 0.00013687518706973962,
      "loss": 1.0476,
      "step": 2115
    },
    {
      "epoch": 2.847913862718708,
      "grad_norm": 1.0242363214492798,
      "learning_rate": 0.00013684525591140377,
      "loss": 1.0058,
      "step": 2116
    },
    {
      "epoch": 2.8492597577388965,
      "grad_norm": 0.9356306195259094,
      "learning_rate": 0.00013681532475306795,
      "loss": 0.8386,
      "step": 2117
    },
    {
      "epoch": 2.850605652759085,
      "grad_norm": 1.0116450786590576,
      "learning_rate": 0.00013678539359473214,
      "loss": 0.8158,
      "step": 2118
    },
    {
      "epoch": 2.851951547779273,
      "grad_norm": 0.9195355772972107,
      "learning_rate": 0.0001367554624363963,
      "loss": 0.952,
      "step": 2119
    },
    {
      "epoch": 2.8532974427994615,
      "grad_norm": 0.8315586447715759,
      "learning_rate": 0.00013672553127806047,
      "loss": 0.8541,
      "step": 2120
    },
    {
      "epoch": 2.85464333781965,
      "grad_norm": 0.9357895851135254,
      "learning_rate": 0.00013669560011972463,
      "loss": 1.2652,
      "step": 2121
    },
    {
      "epoch": 2.8559892328398386,
      "grad_norm": 0.8519395589828491,
      "learning_rate": 0.0001366656689613888,
      "loss": 1.0652,
      "step": 2122
    },
    {
      "epoch": 2.857335127860027,
      "grad_norm": 1.111202359199524,
      "learning_rate": 0.000136635737803053,
      "loss": 0.8639,
      "step": 2123
    },
    {
      "epoch": 2.8586810228802153,
      "grad_norm": 0.7878982424736023,
      "learning_rate": 0.00013660580664471715,
      "loss": 1.0131,
      "step": 2124
    },
    {
      "epoch": 2.8600269179004036,
      "grad_norm": 1.009293556213379,
      "learning_rate": 0.00013657587548638133,
      "loss": 1.2615,
      "step": 2125
    },
    {
      "epoch": 2.8613728129205924,
      "grad_norm": 0.8339253664016724,
      "learning_rate": 0.00013654594432804552,
      "loss": 0.8555,
      "step": 2126
    },
    {
      "epoch": 2.8627187079407808,
      "grad_norm": 0.9796618223190308,
      "learning_rate": 0.00013651601316970967,
      "loss": 1.0464,
      "step": 2127
    },
    {
      "epoch": 2.864064602960969,
      "grad_norm": 1.0918856859207153,
      "learning_rate": 0.00013648608201137386,
      "loss": 0.9112,
      "step": 2128
    },
    {
      "epoch": 2.8654104979811574,
      "grad_norm": 0.7834939956665039,
      "learning_rate": 0.000136456150853038,
      "loss": 0.7565,
      "step": 2129
    },
    {
      "epoch": 2.8667563930013458,
      "grad_norm": 0.8633351922035217,
      "learning_rate": 0.0001364262196947022,
      "loss": 0.7946,
      "step": 2130
    },
    {
      "epoch": 2.868102288021534,
      "grad_norm": 0.8589935898780823,
      "learning_rate": 0.00013639628853636638,
      "loss": 0.8486,
      "step": 2131
    },
    {
      "epoch": 2.869448183041723,
      "grad_norm": 0.8334883451461792,
      "learning_rate": 0.00013636635737803053,
      "loss": 0.7609,
      "step": 2132
    },
    {
      "epoch": 2.8707940780619112,
      "grad_norm": 1.1410784721374512,
      "learning_rate": 0.00013633642621969471,
      "loss": 1.1006,
      "step": 2133
    },
    {
      "epoch": 2.8721399730820996,
      "grad_norm": 0.7746306657791138,
      "learning_rate": 0.0001363064950613589,
      "loss": 0.9787,
      "step": 2134
    },
    {
      "epoch": 2.873485868102288,
      "grad_norm": 1.0464425086975098,
      "learning_rate": 0.00013627656390302305,
      "loss": 0.8593,
      "step": 2135
    },
    {
      "epoch": 2.8748317631224767,
      "grad_norm": 0.924208402633667,
      "learning_rate": 0.00013624663274468724,
      "loss": 0.832,
      "step": 2136
    },
    {
      "epoch": 2.876177658142665,
      "grad_norm": 0.810685932636261,
      "learning_rate": 0.0001362167015863514,
      "loss": 1.0062,
      "step": 2137
    },
    {
      "epoch": 2.8775235531628534,
      "grad_norm": 0.8351840376853943,
      "learning_rate": 0.00013618677042801557,
      "loss": 0.869,
      "step": 2138
    },
    {
      "epoch": 2.8788694481830417,
      "grad_norm": 0.8880133628845215,
      "learning_rate": 0.00013615683926967976,
      "loss": 1.0088,
      "step": 2139
    },
    {
      "epoch": 2.88021534320323,
      "grad_norm": 0.9175209999084473,
      "learning_rate": 0.0001361269081113439,
      "loss": 1.1211,
      "step": 2140
    },
    {
      "epoch": 2.8815612382234184,
      "grad_norm": 0.9557884931564331,
      "learning_rate": 0.0001360969769530081,
      "loss": 0.9483,
      "step": 2141
    },
    {
      "epoch": 2.882907133243607,
      "grad_norm": 0.830328106880188,
      "learning_rate": 0.00013606704579467228,
      "loss": 0.8531,
      "step": 2142
    },
    {
      "epoch": 2.8842530282637955,
      "grad_norm": 0.6728867292404175,
      "learning_rate": 0.00013603711463633643,
      "loss": 1.1551,
      "step": 2143
    },
    {
      "epoch": 2.885598923283984,
      "grad_norm": 0.9102112650871277,
      "learning_rate": 0.00013600718347800062,
      "loss": 0.8639,
      "step": 2144
    },
    {
      "epoch": 2.886944818304172,
      "grad_norm": 0.8906497359275818,
      "learning_rate": 0.00013597725231966477,
      "loss": 0.8874,
      "step": 2145
    },
    {
      "epoch": 2.888290713324361,
      "grad_norm": 1.0101628303527832,
      "learning_rate": 0.00013594732116132895,
      "loss": 1.058,
      "step": 2146
    },
    {
      "epoch": 2.8896366083445493,
      "grad_norm": 0.9334383010864258,
      "learning_rate": 0.00013591739000299314,
      "loss": 1.006,
      "step": 2147
    },
    {
      "epoch": 2.8909825033647376,
      "grad_norm": 0.9049475193023682,
      "learning_rate": 0.0001358874588446573,
      "loss": 0.8449,
      "step": 2148
    },
    {
      "epoch": 2.892328398384926,
      "grad_norm": 0.815674901008606,
      "learning_rate": 0.00013585752768632147,
      "loss": 0.9547,
      "step": 2149
    },
    {
      "epoch": 2.8936742934051143,
      "grad_norm": 0.9028818607330322,
      "learning_rate": 0.00013582759652798566,
      "loss": 0.9422,
      "step": 2150
    },
    {
      "epoch": 2.8950201884253026,
      "grad_norm": 0.7014427185058594,
      "learning_rate": 0.0001357976653696498,
      "loss": 1.1024,
      "step": 2151
    },
    {
      "epoch": 2.8963660834454914,
      "grad_norm": 0.7733325958251953,
      "learning_rate": 0.000135767734211314,
      "loss": 0.9307,
      "step": 2152
    },
    {
      "epoch": 2.8977119784656797,
      "grad_norm": 0.9493801593780518,
      "learning_rate": 0.00013573780305297815,
      "loss": 0.763,
      "step": 2153
    },
    {
      "epoch": 2.899057873485868,
      "grad_norm": 0.9056911468505859,
      "learning_rate": 0.00013570787189464233,
      "loss": 0.7978,
      "step": 2154
    },
    {
      "epoch": 2.9004037685060564,
      "grad_norm": 0.8244704604148865,
      "learning_rate": 0.00013567794073630652,
      "loss": 0.8515,
      "step": 2155
    },
    {
      "epoch": 2.901749663526245,
      "grad_norm": 1.0223366022109985,
      "learning_rate": 0.00013564800957797067,
      "loss": 1.0946,
      "step": 2156
    },
    {
      "epoch": 2.9030955585464335,
      "grad_norm": 1.0311459302902222,
      "learning_rate": 0.00013561807841963486,
      "loss": 0.9768,
      "step": 2157
    },
    {
      "epoch": 2.904441453566622,
      "grad_norm": 0.9030935168266296,
      "learning_rate": 0.00013558814726129904,
      "loss": 0.9765,
      "step": 2158
    },
    {
      "epoch": 2.90578734858681,
      "grad_norm": 0.945600152015686,
      "learning_rate": 0.0001355582161029632,
      "loss": 0.847,
      "step": 2159
    },
    {
      "epoch": 2.9071332436069985,
      "grad_norm": 0.8222911953926086,
      "learning_rate": 0.00013552828494462738,
      "loss": 0.8372,
      "step": 2160
    },
    {
      "epoch": 2.908479138627187,
      "grad_norm": 0.911724865436554,
      "learning_rate": 0.00013549835378629153,
      "loss": 0.8701,
      "step": 2161
    },
    {
      "epoch": 2.9098250336473757,
      "grad_norm": 0.8976936936378479,
      "learning_rate": 0.00013546842262795571,
      "loss": 1.0413,
      "step": 2162
    },
    {
      "epoch": 2.911170928667564,
      "grad_norm": 1.0567433834075928,
      "learning_rate": 0.0001354384914696199,
      "loss": 0.924,
      "step": 2163
    },
    {
      "epoch": 2.9125168236877523,
      "grad_norm": 0.9397270083427429,
      "learning_rate": 0.00013540856031128405,
      "loss": 0.9015,
      "step": 2164
    },
    {
      "epoch": 2.9138627187079407,
      "grad_norm": 0.9895216822624207,
      "learning_rate": 0.00013537862915294824,
      "loss": 0.9379,
      "step": 2165
    },
    {
      "epoch": 2.9152086137281294,
      "grad_norm": 0.984456479549408,
      "learning_rate": 0.00013534869799461242,
      "loss": 0.9762,
      "step": 2166
    },
    {
      "epoch": 2.916554508748318,
      "grad_norm": 0.7677732706069946,
      "learning_rate": 0.00013531876683627657,
      "loss": 1.0123,
      "step": 2167
    },
    {
      "epoch": 2.917900403768506,
      "grad_norm": 0.922424852848053,
      "learning_rate": 0.00013528883567794076,
      "loss": 0.9447,
      "step": 2168
    },
    {
      "epoch": 2.9192462987886945,
      "grad_norm": 0.7355620265007019,
      "learning_rate": 0.0001352589045196049,
      "loss": 1.0905,
      "step": 2169
    },
    {
      "epoch": 2.920592193808883,
      "grad_norm": 1.0586960315704346,
      "learning_rate": 0.0001352289733612691,
      "loss": 0.9132,
      "step": 2170
    },
    {
      "epoch": 2.921938088829071,
      "grad_norm": 0.9277518391609192,
      "learning_rate": 0.00013519904220293328,
      "loss": 1.0708,
      "step": 2171
    },
    {
      "epoch": 2.92328398384926,
      "grad_norm": 0.7812301516532898,
      "learning_rate": 0.00013516911104459743,
      "loss": 1.0903,
      "step": 2172
    },
    {
      "epoch": 2.9246298788694483,
      "grad_norm": 1.2550204992294312,
      "learning_rate": 0.00013513917988626162,
      "loss": 0.9423,
      "step": 2173
    },
    {
      "epoch": 2.9259757738896366,
      "grad_norm": 0.8383128643035889,
      "learning_rate": 0.0001351092487279258,
      "loss": 1.0925,
      "step": 2174
    },
    {
      "epoch": 2.927321668909825,
      "grad_norm": 0.8959033489227295,
      "learning_rate": 0.00013507931756958995,
      "loss": 0.8533,
      "step": 2175
    },
    {
      "epoch": 2.9286675639300137,
      "grad_norm": 1.1309418678283691,
      "learning_rate": 0.00013504938641125414,
      "loss": 0.9154,
      "step": 2176
    },
    {
      "epoch": 2.930013458950202,
      "grad_norm": 1.0368608236312866,
      "learning_rate": 0.0001350194552529183,
      "loss": 0.9454,
      "step": 2177
    },
    {
      "epoch": 2.9313593539703904,
      "grad_norm": 0.9014801979064941,
      "learning_rate": 0.00013498952409458248,
      "loss": 0.9961,
      "step": 2178
    },
    {
      "epoch": 2.9327052489905787,
      "grad_norm": 1.0551464557647705,
      "learning_rate": 0.00013495959293624666,
      "loss": 0.9371,
      "step": 2179
    },
    {
      "epoch": 2.934051144010767,
      "grad_norm": 0.996700644493103,
      "learning_rate": 0.0001349296617779108,
      "loss": 1.1256,
      "step": 2180
    },
    {
      "epoch": 2.9353970390309554,
      "grad_norm": 0.7767385244369507,
      "learning_rate": 0.000134899730619575,
      "loss": 0.9303,
      "step": 2181
    },
    {
      "epoch": 2.936742934051144,
      "grad_norm": 1.0940529108047485,
      "learning_rate": 0.00013486979946123918,
      "loss": 0.9101,
      "step": 2182
    },
    {
      "epoch": 2.9380888290713325,
      "grad_norm": 1.052142858505249,
      "learning_rate": 0.00013483986830290333,
      "loss": 1.1074,
      "step": 2183
    },
    {
      "epoch": 2.939434724091521,
      "grad_norm": 1.3779276609420776,
      "learning_rate": 0.00013480993714456752,
      "loss": 0.9228,
      "step": 2184
    },
    {
      "epoch": 2.940780619111709,
      "grad_norm": 0.8147059679031372,
      "learning_rate": 0.00013478000598623167,
      "loss": 0.8694,
      "step": 2185
    },
    {
      "epoch": 2.942126514131898,
      "grad_norm": 1.022468090057373,
      "learning_rate": 0.00013475007482789586,
      "loss": 0.8867,
      "step": 2186
    },
    {
      "epoch": 2.9434724091520863,
      "grad_norm": 0.8178580403327942,
      "learning_rate": 0.00013472014366956004,
      "loss": 0.9706,
      "step": 2187
    },
    {
      "epoch": 2.9448183041722746,
      "grad_norm": 1.1398202180862427,
      "learning_rate": 0.0001346902125112242,
      "loss": 0.9814,
      "step": 2188
    },
    {
      "epoch": 2.946164199192463,
      "grad_norm": 0.7777194380760193,
      "learning_rate": 0.00013466028135288838,
      "loss": 0.9572,
      "step": 2189
    },
    {
      "epoch": 2.9475100942126513,
      "grad_norm": 0.9255863428115845,
      "learning_rate": 0.00013463035019455256,
      "loss": 0.893,
      "step": 2190
    },
    {
      "epoch": 2.9488559892328396,
      "grad_norm": 0.8277648091316223,
      "learning_rate": 0.00013460041903621671,
      "loss": 0.953,
      "step": 2191
    },
    {
      "epoch": 2.9502018842530284,
      "grad_norm": 0.9737271070480347,
      "learning_rate": 0.0001345704878778809,
      "loss": 0.9508,
      "step": 2192
    },
    {
      "epoch": 2.9515477792732168,
      "grad_norm": 0.8153208494186401,
      "learning_rate": 0.00013454055671954505,
      "loss": 0.9824,
      "step": 2193
    },
    {
      "epoch": 2.952893674293405,
      "grad_norm": 0.8938489556312561,
      "learning_rate": 0.00013451062556120924,
      "loss": 0.8236,
      "step": 2194
    },
    {
      "epoch": 2.9542395693135934,
      "grad_norm": 0.9617469906806946,
      "learning_rate": 0.00013448069440287342,
      "loss": 0.8616,
      "step": 2195
    },
    {
      "epoch": 2.955585464333782,
      "grad_norm": 0.8659482002258301,
      "learning_rate": 0.00013445076324453757,
      "loss": 0.7881,
      "step": 2196
    },
    {
      "epoch": 2.9569313593539706,
      "grad_norm": 1.074215054512024,
      "learning_rate": 0.00013442083208620176,
      "loss": 1.0016,
      "step": 2197
    },
    {
      "epoch": 2.958277254374159,
      "grad_norm": 0.8396754860877991,
      "learning_rate": 0.00013439090092786594,
      "loss": 0.8683,
      "step": 2198
    },
    {
      "epoch": 2.9596231493943472,
      "grad_norm": 0.8296336531639099,
      "learning_rate": 0.00013436096976953007,
      "loss": 1.1218,
      "step": 2199
    },
    {
      "epoch": 2.9609690444145356,
      "grad_norm": 0.8334075808525085,
      "learning_rate": 0.00013433103861119425,
      "loss": 0.8132,
      "step": 2200
    },
    {
      "epoch": 2.962314939434724,
      "grad_norm": 0.8400386571884155,
      "learning_rate": 0.00013430110745285843,
      "loss": 0.9883,
      "step": 2201
    },
    {
      "epoch": 2.9636608344549122,
      "grad_norm": 0.9910911321640015,
      "learning_rate": 0.0001342711762945226,
      "loss": 0.8395,
      "step": 2202
    },
    {
      "epoch": 2.965006729475101,
      "grad_norm": 0.813674807548523,
      "learning_rate": 0.00013424124513618677,
      "loss": 0.9609,
      "step": 2203
    },
    {
      "epoch": 2.9663526244952894,
      "grad_norm": 0.9786017537117004,
      "learning_rate": 0.00013421131397785093,
      "loss": 0.8921,
      "step": 2204
    },
    {
      "epoch": 2.9676985195154777,
      "grad_norm": 0.8900860548019409,
      "learning_rate": 0.0001341813828195151,
      "loss": 0.9693,
      "step": 2205
    },
    {
      "epoch": 2.9690444145356665,
      "grad_norm": 0.872957706451416,
      "learning_rate": 0.0001341514516611793,
      "loss": 0.8223,
      "step": 2206
    },
    {
      "epoch": 2.970390309555855,
      "grad_norm": 1.0293813943862915,
      "learning_rate": 0.00013412152050284345,
      "loss": 1.0042,
      "step": 2207
    },
    {
      "epoch": 2.971736204576043,
      "grad_norm": 0.8914231061935425,
      "learning_rate": 0.00013409158934450763,
      "loss": 0.8417,
      "step": 2208
    },
    {
      "epoch": 2.9730820995962315,
      "grad_norm": 0.8350495100021362,
      "learning_rate": 0.00013406165818617181,
      "loss": 0.6836,
      "step": 2209
    },
    {
      "epoch": 2.97442799461642,
      "grad_norm": 1.0979971885681152,
      "learning_rate": 0.00013403172702783597,
      "loss": 0.9356,
      "step": 2210
    },
    {
      "epoch": 2.975773889636608,
      "grad_norm": 0.8217231035232544,
      "learning_rate": 0.00013400179586950015,
      "loss": 0.8524,
      "step": 2211
    },
    {
      "epoch": 2.9771197846567965,
      "grad_norm": 0.8442936539649963,
      "learning_rate": 0.0001339718647111643,
      "loss": 0.8055,
      "step": 2212
    },
    {
      "epoch": 2.9784656796769853,
      "grad_norm": 0.8957442045211792,
      "learning_rate": 0.0001339419335528285,
      "loss": 0.9074,
      "step": 2213
    },
    {
      "epoch": 2.9798115746971736,
      "grad_norm": 1.0706372261047363,
      "learning_rate": 0.00013391200239449267,
      "loss": 0.8327,
      "step": 2214
    },
    {
      "epoch": 2.981157469717362,
      "grad_norm": 0.851715624332428,
      "learning_rate": 0.00013388207123615683,
      "loss": 0.8925,
      "step": 2215
    },
    {
      "epoch": 2.9825033647375507,
      "grad_norm": 1.0191247463226318,
      "learning_rate": 0.000133852140077821,
      "loss": 0.9245,
      "step": 2216
    },
    {
      "epoch": 2.983849259757739,
      "grad_norm": 0.9421170949935913,
      "learning_rate": 0.0001338222089194852,
      "loss": 1.0522,
      "step": 2217
    },
    {
      "epoch": 2.9851951547779274,
      "grad_norm": 1.004503846168518,
      "learning_rate": 0.00013379227776114935,
      "loss": 0.917,
      "step": 2218
    },
    {
      "epoch": 2.9865410497981157,
      "grad_norm": 0.8372405171394348,
      "learning_rate": 0.00013376234660281353,
      "loss": 0.8628,
      "step": 2219
    },
    {
      "epoch": 2.987886944818304,
      "grad_norm": 1.2159419059753418,
      "learning_rate": 0.0001337324154444777,
      "loss": 1.1227,
      "step": 2220
    },
    {
      "epoch": 2.9892328398384924,
      "grad_norm": 0.7884101867675781,
      "learning_rate": 0.00013370248428614187,
      "loss": 0.8735,
      "step": 2221
    },
    {
      "epoch": 2.9905787348586808,
      "grad_norm": 1.1447588205337524,
      "learning_rate": 0.00013367255312780605,
      "loss": 0.9814,
      "step": 2222
    },
    {
      "epoch": 2.9919246298788695,
      "grad_norm": 1.0338976383209229,
      "learning_rate": 0.0001336426219694702,
      "loss": 0.9021,
      "step": 2223
    },
    {
      "epoch": 2.993270524899058,
      "grad_norm": 0.9442302584648132,
      "learning_rate": 0.0001336126908111344,
      "loss": 0.9447,
      "step": 2224
    },
    {
      "epoch": 2.994616419919246,
      "grad_norm": 1.0526460409164429,
      "learning_rate": 0.00013358275965279857,
      "loss": 0.9902,
      "step": 2225
    },
    {
      "epoch": 2.995962314939435,
      "grad_norm": 0.9878349304199219,
      "learning_rate": 0.00013355282849446273,
      "loss": 0.8821,
      "step": 2226
    },
    {
      "epoch": 2.9973082099596233,
      "grad_norm": 0.7822288274765015,
      "learning_rate": 0.0001335228973361269,
      "loss": 1.0003,
      "step": 2227
    },
    {
      "epoch": 2.9986541049798117,
      "grad_norm": 0.8473265767097473,
      "learning_rate": 0.00013349296617779107,
      "loss": 0.9216,
      "step": 2228
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.9840117692947388,
      "learning_rate": 0.00013346303501945525,
      "loss": 1.0615,
      "step": 2229
    },
    {
      "epoch": 3.0013458950201883,
      "grad_norm": 0.804082989692688,
      "learning_rate": 0.00013343310386111943,
      "loss": 0.6879,
      "step": 2230
    },
    {
      "epoch": 3.0026917900403767,
      "grad_norm": 0.6268836259841919,
      "learning_rate": 0.0001334031727027836,
      "loss": 1.0313,
      "step": 2231
    },
    {
      "epoch": 3.0040376850605655,
      "grad_norm": 0.7465252876281738,
      "learning_rate": 0.00013337324154444777,
      "loss": 0.6417,
      "step": 2232
    },
    {
      "epoch": 3.005383580080754,
      "grad_norm": 0.7961080074310303,
      "learning_rate": 0.00013334331038611195,
      "loss": 0.6331,
      "step": 2233
    },
    {
      "epoch": 3.006729475100942,
      "grad_norm": 0.7974648475646973,
      "learning_rate": 0.0001333133792277761,
      "loss": 0.7023,
      "step": 2234
    },
    {
      "epoch": 3.0080753701211305,
      "grad_norm": 0.8395229578018188,
      "learning_rate": 0.0001332834480694403,
      "loss": 0.514,
      "step": 2235
    },
    {
      "epoch": 3.009421265141319,
      "grad_norm": 0.7118949890136719,
      "learning_rate": 0.00013325351691110445,
      "loss": 0.5498,
      "step": 2236
    },
    {
      "epoch": 3.0107671601615076,
      "grad_norm": 0.8404275178909302,
      "learning_rate": 0.00013322358575276863,
      "loss": 0.5479,
      "step": 2237
    },
    {
      "epoch": 3.012113055181696,
      "grad_norm": 0.8716529607772827,
      "learning_rate": 0.00013319365459443281,
      "loss": 0.605,
      "step": 2238
    },
    {
      "epoch": 3.0134589502018843,
      "grad_norm": 0.9298263192176819,
      "learning_rate": 0.00013316372343609697,
      "loss": 0.6587,
      "step": 2239
    },
    {
      "epoch": 3.0148048452220726,
      "grad_norm": 0.830462634563446,
      "learning_rate": 0.00013313379227776115,
      "loss": 0.6824,
      "step": 2240
    },
    {
      "epoch": 3.016150740242261,
      "grad_norm": 1.00750732421875,
      "learning_rate": 0.00013310386111942533,
      "loss": 0.6584,
      "step": 2241
    },
    {
      "epoch": 3.0174966352624497,
      "grad_norm": 1.3796286582946777,
      "learning_rate": 0.0001330739299610895,
      "loss": 0.9453,
      "step": 2242
    },
    {
      "epoch": 3.018842530282638,
      "grad_norm": 1.0639474391937256,
      "learning_rate": 0.00013304399880275367,
      "loss": 0.6532,
      "step": 2243
    },
    {
      "epoch": 3.0201884253028264,
      "grad_norm": 1.0022037029266357,
      "learning_rate": 0.00013301406764441783,
      "loss": 0.8772,
      "step": 2244
    },
    {
      "epoch": 3.0215343203230147,
      "grad_norm": 1.1811443567276,
      "learning_rate": 0.000132984136486082,
      "loss": 0.5741,
      "step": 2245
    },
    {
      "epoch": 3.022880215343203,
      "grad_norm": 1.2361156940460205,
      "learning_rate": 0.0001329542053277462,
      "loss": 0.6616,
      "step": 2246
    },
    {
      "epoch": 3.024226110363392,
      "grad_norm": 1.0879634618759155,
      "learning_rate": 0.00013292427416941035,
      "loss": 0.693,
      "step": 2247
    },
    {
      "epoch": 3.02557200538358,
      "grad_norm": 1.0928453207015991,
      "learning_rate": 0.00013289434301107453,
      "loss": 0.74,
      "step": 2248
    },
    {
      "epoch": 3.0269179004037685,
      "grad_norm": 1.1431305408477783,
      "learning_rate": 0.00013286441185273872,
      "loss": 0.5068,
      "step": 2249
    },
    {
      "epoch": 3.028263795423957,
      "grad_norm": 1.2155489921569824,
      "learning_rate": 0.00013283448069440287,
      "loss": 0.5672,
      "step": 2250
    },
    {
      "epoch": 3.029609690444145,
      "grad_norm": 1.1859761476516724,
      "learning_rate": 0.00013280454953606705,
      "loss": 0.6109,
      "step": 2251
    },
    {
      "epoch": 3.030955585464334,
      "grad_norm": 1.181159257888794,
      "learning_rate": 0.0001327746183777312,
      "loss": 0.5211,
      "step": 2252
    },
    {
      "epoch": 3.0323014804845223,
      "grad_norm": 1.0071463584899902,
      "learning_rate": 0.0001327446872193954,
      "loss": 0.6083,
      "step": 2253
    },
    {
      "epoch": 3.0336473755047106,
      "grad_norm": 1.3232953548431396,
      "learning_rate": 0.00013271475606105957,
      "loss": 0.7232,
      "step": 2254
    },
    {
      "epoch": 3.034993270524899,
      "grad_norm": 0.895098090171814,
      "learning_rate": 0.00013268482490272373,
      "loss": 0.4712,
      "step": 2255
    },
    {
      "epoch": 3.0363391655450873,
      "grad_norm": 1.0982112884521484,
      "learning_rate": 0.0001326548937443879,
      "loss": 0.5556,
      "step": 2256
    },
    {
      "epoch": 3.037685060565276,
      "grad_norm": 1.1450731754302979,
      "learning_rate": 0.00013262496258605207,
      "loss": 0.633,
      "step": 2257
    },
    {
      "epoch": 3.0390309555854644,
      "grad_norm": 0.8227702975273132,
      "learning_rate": 0.00013259503142771625,
      "loss": 0.5566,
      "step": 2258
    },
    {
      "epoch": 3.0403768506056528,
      "grad_norm": 1.048977255821228,
      "learning_rate": 0.00013256510026938043,
      "loss": 0.4661,
      "step": 2259
    },
    {
      "epoch": 3.041722745625841,
      "grad_norm": 0.9046300053596497,
      "learning_rate": 0.0001325351691110446,
      "loss": 0.5676,
      "step": 2260
    },
    {
      "epoch": 3.0430686406460294,
      "grad_norm": 0.8861531019210815,
      "learning_rate": 0.00013250523795270877,
      "loss": 0.4735,
      "step": 2261
    },
    {
      "epoch": 3.0444145356662182,
      "grad_norm": 0.8335589170455933,
      "learning_rate": 0.00013247530679437295,
      "loss": 0.494,
      "step": 2262
    },
    {
      "epoch": 3.0457604306864066,
      "grad_norm": 1.2254658937454224,
      "learning_rate": 0.0001324453756360371,
      "loss": 0.5802,
      "step": 2263
    },
    {
      "epoch": 3.047106325706595,
      "grad_norm": 0.7820638418197632,
      "learning_rate": 0.0001324154444777013,
      "loss": 0.5009,
      "step": 2264
    },
    {
      "epoch": 3.0484522207267832,
      "grad_norm": 1.1999584436416626,
      "learning_rate": 0.00013238551331936545,
      "loss": 0.6306,
      "step": 2265
    },
    {
      "epoch": 3.0497981157469716,
      "grad_norm": 1.0431609153747559,
      "learning_rate": 0.00013235558216102963,
      "loss": 0.4527,
      "step": 2266
    },
    {
      "epoch": 3.0511440107671604,
      "grad_norm": 1.1814948320388794,
      "learning_rate": 0.00013232565100269381,
      "loss": 0.5146,
      "step": 2267
    },
    {
      "epoch": 3.0524899057873487,
      "grad_norm": 1.2032511234283447,
      "learning_rate": 0.00013229571984435797,
      "loss": 0.7419,
      "step": 2268
    },
    {
      "epoch": 3.053835800807537,
      "grad_norm": 0.9251554608345032,
      "learning_rate": 0.00013226578868602215,
      "loss": 0.4129,
      "step": 2269
    },
    {
      "epoch": 3.0551816958277254,
      "grad_norm": 0.843001663684845,
      "learning_rate": 0.00013223585752768634,
      "loss": 0.5109,
      "step": 2270
    },
    {
      "epoch": 3.0565275908479137,
      "grad_norm": 1.0758358240127563,
      "learning_rate": 0.0001322059263693505,
      "loss": 0.7625,
      "step": 2271
    },
    {
      "epoch": 3.0578734858681025,
      "grad_norm": 1.135433554649353,
      "learning_rate": 0.00013217599521101467,
      "loss": 0.6058,
      "step": 2272
    },
    {
      "epoch": 3.059219380888291,
      "grad_norm": 1.0547430515289307,
      "learning_rate": 0.00013214606405267883,
      "loss": 0.5306,
      "step": 2273
    },
    {
      "epoch": 3.060565275908479,
      "grad_norm": 1.0308653116226196,
      "learning_rate": 0.000132116132894343,
      "loss": 0.3693,
      "step": 2274
    },
    {
      "epoch": 3.0619111709286675,
      "grad_norm": 1.3403769731521606,
      "learning_rate": 0.0001320862017360072,
      "loss": 0.5624,
      "step": 2275
    },
    {
      "epoch": 3.063257065948856,
      "grad_norm": 0.8936859965324402,
      "learning_rate": 0.00013205627057767135,
      "loss": 0.6895,
      "step": 2276
    },
    {
      "epoch": 3.0646029609690446,
      "grad_norm": 1.1000950336456299,
      "learning_rate": 0.00013202633941933553,
      "loss": 0.7342,
      "step": 2277
    },
    {
      "epoch": 3.065948855989233,
      "grad_norm": 1.1075447797775269,
      "learning_rate": 0.00013199640826099972,
      "loss": 0.7832,
      "step": 2278
    },
    {
      "epoch": 3.0672947510094213,
      "grad_norm": 1.0690083503723145,
      "learning_rate": 0.00013196647710266387,
      "loss": 0.6466,
      "step": 2279
    },
    {
      "epoch": 3.0686406460296096,
      "grad_norm": 0.9970313906669617,
      "learning_rate": 0.00013193654594432805,
      "loss": 0.6736,
      "step": 2280
    },
    {
      "epoch": 3.069986541049798,
      "grad_norm": 0.9322367906570435,
      "learning_rate": 0.0001319066147859922,
      "loss": 0.3879,
      "step": 2281
    },
    {
      "epoch": 3.0713324360699867,
      "grad_norm": 1.0989151000976562,
      "learning_rate": 0.0001318766836276564,
      "loss": 0.7422,
      "step": 2282
    },
    {
      "epoch": 3.072678331090175,
      "grad_norm": 0.9450658559799194,
      "learning_rate": 0.00013184675246932057,
      "loss": 0.6826,
      "step": 2283
    },
    {
      "epoch": 3.0740242261103634,
      "grad_norm": 0.9720445275306702,
      "learning_rate": 0.00013181682131098473,
      "loss": 0.6779,
      "step": 2284
    },
    {
      "epoch": 3.0753701211305517,
      "grad_norm": 1.0134310722351074,
      "learning_rate": 0.0001317868901526489,
      "loss": 0.5501,
      "step": 2285
    },
    {
      "epoch": 3.07671601615074,
      "grad_norm": 1.047480583190918,
      "learning_rate": 0.0001317569589943131,
      "loss": 0.647,
      "step": 2286
    },
    {
      "epoch": 3.078061911170929,
      "grad_norm": 0.9826000332832336,
      "learning_rate": 0.00013172702783597725,
      "loss": 0.5884,
      "step": 2287
    },
    {
      "epoch": 3.079407806191117,
      "grad_norm": 0.9078327417373657,
      "learning_rate": 0.00013169709667764143,
      "loss": 0.5859,
      "step": 2288
    },
    {
      "epoch": 3.0807537012113055,
      "grad_norm": 1.1413328647613525,
      "learning_rate": 0.0001316671655193056,
      "loss": 0.5336,
      "step": 2289
    },
    {
      "epoch": 3.082099596231494,
      "grad_norm": 1.6706955432891846,
      "learning_rate": 0.00013163723436096977,
      "loss": 0.9952,
      "step": 2290
    },
    {
      "epoch": 3.083445491251682,
      "grad_norm": 1.1876049041748047,
      "learning_rate": 0.00013160730320263395,
      "loss": 0.6751,
      "step": 2291
    },
    {
      "epoch": 3.084791386271871,
      "grad_norm": 1.0762617588043213,
      "learning_rate": 0.0001315773720442981,
      "loss": 0.6075,
      "step": 2292
    },
    {
      "epoch": 3.0861372812920593,
      "grad_norm": 1.3180606365203857,
      "learning_rate": 0.0001315474408859623,
      "loss": 0.7228,
      "step": 2293
    },
    {
      "epoch": 3.0874831763122477,
      "grad_norm": 0.920498251914978,
      "learning_rate": 0.00013151750972762648,
      "loss": 0.6454,
      "step": 2294
    },
    {
      "epoch": 3.088829071332436,
      "grad_norm": 1.4663203954696655,
      "learning_rate": 0.00013148757856929063,
      "loss": 0.6735,
      "step": 2295
    },
    {
      "epoch": 3.0901749663526243,
      "grad_norm": 1.161507248878479,
      "learning_rate": 0.00013145764741095481,
      "loss": 0.6311,
      "step": 2296
    },
    {
      "epoch": 3.091520861372813,
      "grad_norm": 1.1840713024139404,
      "learning_rate": 0.00013142771625261897,
      "loss": 0.6645,
      "step": 2297
    },
    {
      "epoch": 3.0928667563930015,
      "grad_norm": 0.8471901416778564,
      "learning_rate": 0.00013139778509428315,
      "loss": 0.5262,
      "step": 2298
    },
    {
      "epoch": 3.09421265141319,
      "grad_norm": 0.8664738535881042,
      "learning_rate": 0.00013136785393594734,
      "loss": 0.6581,
      "step": 2299
    },
    {
      "epoch": 3.095558546433378,
      "grad_norm": 0.9929924607276917,
      "learning_rate": 0.0001313379227776115,
      "loss": 0.5162,
      "step": 2300
    },
    {
      "epoch": 3.0969044414535665,
      "grad_norm": 1.3433805704116821,
      "learning_rate": 0.00013130799161927567,
      "loss": 0.9102,
      "step": 2301
    },
    {
      "epoch": 3.0982503364737553,
      "grad_norm": 1.1461083889007568,
      "learning_rate": 0.00013127806046093986,
      "loss": 0.7513,
      "step": 2302
    },
    {
      "epoch": 3.0995962314939436,
      "grad_norm": 1.1567633152008057,
      "learning_rate": 0.000131248129302604,
      "loss": 0.5535,
      "step": 2303
    },
    {
      "epoch": 3.100942126514132,
      "grad_norm": 1.0302573442459106,
      "learning_rate": 0.0001312181981442682,
      "loss": 0.7153,
      "step": 2304
    },
    {
      "epoch": 3.1022880215343203,
      "grad_norm": 1.1368402242660522,
      "learning_rate": 0.00013118826698593235,
      "loss": 0.58,
      "step": 2305
    },
    {
      "epoch": 3.1036339165545086,
      "grad_norm": 0.9996916651725769,
      "learning_rate": 0.00013115833582759653,
      "loss": 0.5761,
      "step": 2306
    },
    {
      "epoch": 3.1049798115746974,
      "grad_norm": 0.9986721873283386,
      "learning_rate": 0.00013112840466926072,
      "loss": 0.6284,
      "step": 2307
    },
    {
      "epoch": 3.1063257065948857,
      "grad_norm": 0.8982400298118591,
      "learning_rate": 0.00013109847351092487,
      "loss": 0.4439,
      "step": 2308
    },
    {
      "epoch": 3.107671601615074,
      "grad_norm": 1.0416162014007568,
      "learning_rate": 0.00013106854235258905,
      "loss": 0.6236,
      "step": 2309
    },
    {
      "epoch": 3.1090174966352624,
      "grad_norm": 1.1621617078781128,
      "learning_rate": 0.00013103861119425324,
      "loss": 0.6916,
      "step": 2310
    },
    {
      "epoch": 3.1103633916554507,
      "grad_norm": 1.091907024383545,
      "learning_rate": 0.0001310086800359174,
      "loss": 0.5164,
      "step": 2311
    },
    {
      "epoch": 3.1117092866756395,
      "grad_norm": 1.058181881904602,
      "learning_rate": 0.00013097874887758157,
      "loss": 0.5378,
      "step": 2312
    },
    {
      "epoch": 3.113055181695828,
      "grad_norm": 1.0572006702423096,
      "learning_rate": 0.00013094881771924573,
      "loss": 0.5527,
      "step": 2313
    },
    {
      "epoch": 3.114401076716016,
      "grad_norm": 0.983923077583313,
      "learning_rate": 0.0001309188865609099,
      "loss": 0.5816,
      "step": 2314
    },
    {
      "epoch": 3.1157469717362045,
      "grad_norm": 1.178168773651123,
      "learning_rate": 0.0001308889554025741,
      "loss": 0.5942,
      "step": 2315
    },
    {
      "epoch": 3.117092866756393,
      "grad_norm": 1.0153228044509888,
      "learning_rate": 0.00013085902424423825,
      "loss": 0.6974,
      "step": 2316
    },
    {
      "epoch": 3.1184387617765816,
      "grad_norm": 1.1196330785751343,
      "learning_rate": 0.00013082909308590243,
      "loss": 0.582,
      "step": 2317
    },
    {
      "epoch": 3.11978465679677,
      "grad_norm": 1.511885404586792,
      "learning_rate": 0.00013079916192756662,
      "loss": 0.6562,
      "step": 2318
    },
    {
      "epoch": 3.1211305518169583,
      "grad_norm": 1.4536546468734741,
      "learning_rate": 0.00013076923076923077,
      "loss": 0.7745,
      "step": 2319
    },
    {
      "epoch": 3.1224764468371466,
      "grad_norm": 1.1057199239730835,
      "learning_rate": 0.00013073929961089496,
      "loss": 0.6264,
      "step": 2320
    },
    {
      "epoch": 3.123822341857335,
      "grad_norm": 1.056708812713623,
      "learning_rate": 0.0001307093684525591,
      "loss": 0.6179,
      "step": 2321
    },
    {
      "epoch": 3.1251682368775233,
      "grad_norm": 1.2415850162506104,
      "learning_rate": 0.0001306794372942233,
      "loss": 0.6448,
      "step": 2322
    },
    {
      "epoch": 3.126514131897712,
      "grad_norm": 0.914425253868103,
      "learning_rate": 0.00013064950613588748,
      "loss": 0.6311,
      "step": 2323
    },
    {
      "epoch": 3.1278600269179004,
      "grad_norm": 1.0597003698349,
      "learning_rate": 0.00013061957497755163,
      "loss": 0.5429,
      "step": 2324
    },
    {
      "epoch": 3.1292059219380888,
      "grad_norm": 1.054162859916687,
      "learning_rate": 0.00013058964381921581,
      "loss": 0.5509,
      "step": 2325
    },
    {
      "epoch": 3.130551816958277,
      "grad_norm": 1.3026094436645508,
      "learning_rate": 0.00013055971266088,
      "loss": 0.5239,
      "step": 2326
    },
    {
      "epoch": 3.131897711978466,
      "grad_norm": 0.9116931557655334,
      "learning_rate": 0.00013052978150254415,
      "loss": 0.4399,
      "step": 2327
    },
    {
      "epoch": 3.1332436069986542,
      "grad_norm": 1.2771366834640503,
      "learning_rate": 0.00013049985034420834,
      "loss": 0.6018,
      "step": 2328
    },
    {
      "epoch": 3.1345895020188426,
      "grad_norm": 0.8016453385353088,
      "learning_rate": 0.0001304699191858725,
      "loss": 0.4327,
      "step": 2329
    },
    {
      "epoch": 3.135935397039031,
      "grad_norm": 1.058288335800171,
      "learning_rate": 0.00013043998802753667,
      "loss": 0.5272,
      "step": 2330
    },
    {
      "epoch": 3.1372812920592192,
      "grad_norm": 0.964253842830658,
      "learning_rate": 0.00013041005686920086,
      "loss": 0.5726,
      "step": 2331
    },
    {
      "epoch": 3.1386271870794076,
      "grad_norm": 1.1586891412734985,
      "learning_rate": 0.000130380125710865,
      "loss": 0.441,
      "step": 2332
    },
    {
      "epoch": 3.1399730820995964,
      "grad_norm": 1.2466809749603271,
      "learning_rate": 0.0001303501945525292,
      "loss": 0.7097,
      "step": 2333
    },
    {
      "epoch": 3.1413189771197847,
      "grad_norm": 1.1563591957092285,
      "learning_rate": 0.00013032026339419338,
      "loss": 0.6108,
      "step": 2334
    },
    {
      "epoch": 3.142664872139973,
      "grad_norm": 1.0888712406158447,
      "learning_rate": 0.00013029033223585753,
      "loss": 0.5344,
      "step": 2335
    },
    {
      "epoch": 3.1440107671601614,
      "grad_norm": 1.0838111639022827,
      "learning_rate": 0.00013026040107752172,
      "loss": 0.5696,
      "step": 2336
    },
    {
      "epoch": 3.14535666218035,
      "grad_norm": 1.0681068897247314,
      "learning_rate": 0.00013023046991918587,
      "loss": 0.5753,
      "step": 2337
    },
    {
      "epoch": 3.1467025572005385,
      "grad_norm": 1.0424340963363647,
      "learning_rate": 0.00013020053876085005,
      "loss": 0.5667,
      "step": 2338
    },
    {
      "epoch": 3.148048452220727,
      "grad_norm": 1.3640542030334473,
      "learning_rate": 0.00013017060760251424,
      "loss": 0.648,
      "step": 2339
    },
    {
      "epoch": 3.149394347240915,
      "grad_norm": 1.0757689476013184,
      "learning_rate": 0.0001301406764441784,
      "loss": 0.6068,
      "step": 2340
    },
    {
      "epoch": 3.1507402422611035,
      "grad_norm": 0.9939186573028564,
      "learning_rate": 0.00013011074528584257,
      "loss": 0.5646,
      "step": 2341
    },
    {
      "epoch": 3.152086137281292,
      "grad_norm": 1.3047184944152832,
      "learning_rate": 0.00013008081412750676,
      "loss": 0.5454,
      "step": 2342
    },
    {
      "epoch": 3.1534320323014806,
      "grad_norm": 1.5979607105255127,
      "learning_rate": 0.0001300508829691709,
      "loss": 0.6857,
      "step": 2343
    },
    {
      "epoch": 3.154777927321669,
      "grad_norm": 0.9517022371292114,
      "learning_rate": 0.0001300209518108351,
      "loss": 0.4053,
      "step": 2344
    },
    {
      "epoch": 3.1561238223418573,
      "grad_norm": 0.9768153429031372,
      "learning_rate": 0.00012999102065249925,
      "loss": 0.5134,
      "step": 2345
    },
    {
      "epoch": 3.1574697173620456,
      "grad_norm": 1.6151915788650513,
      "learning_rate": 0.00012996108949416343,
      "loss": 0.5967,
      "step": 2346
    },
    {
      "epoch": 3.1588156123822344,
      "grad_norm": 0.9222704768180847,
      "learning_rate": 0.00012993115833582762,
      "loss": 0.6219,
      "step": 2347
    },
    {
      "epoch": 3.1601615074024227,
      "grad_norm": 1.0339847803115845,
      "learning_rate": 0.00012990122717749177,
      "loss": 0.669,
      "step": 2348
    },
    {
      "epoch": 3.161507402422611,
      "grad_norm": 1.1057887077331543,
      "learning_rate": 0.00012987129601915596,
      "loss": 0.5732,
      "step": 2349
    },
    {
      "epoch": 3.1628532974427994,
      "grad_norm": 1.3727257251739502,
      "learning_rate": 0.00012984136486082014,
      "loss": 0.615,
      "step": 2350
    },
    {
      "epoch": 3.1641991924629878,
      "grad_norm": 1.1346944570541382,
      "learning_rate": 0.0001298114337024843,
      "loss": 0.7573,
      "step": 2351
    },
    {
      "epoch": 3.165545087483176,
      "grad_norm": 1.0850460529327393,
      "learning_rate": 0.00012978150254414848,
      "loss": 0.594,
      "step": 2352
    },
    {
      "epoch": 3.166890982503365,
      "grad_norm": 1.1266746520996094,
      "learning_rate": 0.00012975157138581263,
      "loss": 0.4851,
      "step": 2353
    },
    {
      "epoch": 3.168236877523553,
      "grad_norm": 1.3660082817077637,
      "learning_rate": 0.00012972164022747681,
      "loss": 0.6006,
      "step": 2354
    },
    {
      "epoch": 3.1695827725437415,
      "grad_norm": 0.9657272696495056,
      "learning_rate": 0.000129691709069141,
      "loss": 0.5133,
      "step": 2355
    },
    {
      "epoch": 3.17092866756393,
      "grad_norm": 1.115907073020935,
      "learning_rate": 0.00012966177791080515,
      "loss": 0.6022,
      "step": 2356
    },
    {
      "epoch": 3.1722745625841187,
      "grad_norm": 1.1199840307235718,
      "learning_rate": 0.00012963184675246934,
      "loss": 0.6135,
      "step": 2357
    },
    {
      "epoch": 3.173620457604307,
      "grad_norm": 1.239155888557434,
      "learning_rate": 0.00012960191559413352,
      "loss": 0.5689,
      "step": 2358
    },
    {
      "epoch": 3.1749663526244953,
      "grad_norm": 1.3261877298355103,
      "learning_rate": 0.00012957198443579767,
      "loss": 0.7813,
      "step": 2359
    },
    {
      "epoch": 3.1763122476446837,
      "grad_norm": 0.8635525107383728,
      "learning_rate": 0.00012954205327746186,
      "loss": 0.6373,
      "step": 2360
    },
    {
      "epoch": 3.177658142664872,
      "grad_norm": 1.2102984189987183,
      "learning_rate": 0.000129512122119126,
      "loss": 0.5733,
      "step": 2361
    },
    {
      "epoch": 3.1790040376850603,
      "grad_norm": 1.3445358276367188,
      "learning_rate": 0.0001294821909607902,
      "loss": 0.7641,
      "step": 2362
    },
    {
      "epoch": 3.180349932705249,
      "grad_norm": 1.2499151229858398,
      "learning_rate": 0.00012945225980245438,
      "loss": 0.6718,
      "step": 2363
    },
    {
      "epoch": 3.1816958277254375,
      "grad_norm": 0.8973095417022705,
      "learning_rate": 0.00012942232864411853,
      "loss": 0.5815,
      "step": 2364
    },
    {
      "epoch": 3.183041722745626,
      "grad_norm": 1.2125072479248047,
      "learning_rate": 0.00012939239748578272,
      "loss": 0.6717,
      "step": 2365
    },
    {
      "epoch": 3.184387617765814,
      "grad_norm": 1.0732946395874023,
      "learning_rate": 0.0001293624663274469,
      "loss": 0.6063,
      "step": 2366
    },
    {
      "epoch": 3.185733512786003,
      "grad_norm": 1.244907259941101,
      "learning_rate": 0.00012933253516911105,
      "loss": 0.7142,
      "step": 2367
    },
    {
      "epoch": 3.1870794078061913,
      "grad_norm": 0.9710415005683899,
      "learning_rate": 0.00012930260401077524,
      "loss": 0.5778,
      "step": 2368
    },
    {
      "epoch": 3.1884253028263796,
      "grad_norm": 1.181215524673462,
      "learning_rate": 0.0001292726728524394,
      "loss": 0.5649,
      "step": 2369
    },
    {
      "epoch": 3.189771197846568,
      "grad_norm": 1.0620232820510864,
      "learning_rate": 0.00012924274169410358,
      "loss": 0.7336,
      "step": 2370
    },
    {
      "epoch": 3.1911170928667563,
      "grad_norm": 1.1125247478485107,
      "learning_rate": 0.00012921281053576776,
      "loss": 0.5465,
      "step": 2371
    },
    {
      "epoch": 3.1924629878869446,
      "grad_norm": 1.1692438125610352,
      "learning_rate": 0.0001291828793774319,
      "loss": 0.4652,
      "step": 2372
    },
    {
      "epoch": 3.1938088829071334,
      "grad_norm": 0.9889833927154541,
      "learning_rate": 0.0001291529482190961,
      "loss": 0.6016,
      "step": 2373
    },
    {
      "epoch": 3.1951547779273217,
      "grad_norm": 1.058363676071167,
      "learning_rate": 0.00012912301706076025,
      "loss": 0.5852,
      "step": 2374
    },
    {
      "epoch": 3.19650067294751,
      "grad_norm": 1.0421361923217773,
      "learning_rate": 0.00012909308590242443,
      "loss": 0.6229,
      "step": 2375
    },
    {
      "epoch": 3.1978465679676984,
      "grad_norm": 1.0126807689666748,
      "learning_rate": 0.00012906315474408862,
      "loss": 0.6104,
      "step": 2376
    },
    {
      "epoch": 3.199192462987887,
      "grad_norm": 1.1462533473968506,
      "learning_rate": 0.00012903322358575277,
      "loss": 0.5888,
      "step": 2377
    },
    {
      "epoch": 3.2005383580080755,
      "grad_norm": 1.3145486116409302,
      "learning_rate": 0.00012900329242741696,
      "loss": 0.6584,
      "step": 2378
    },
    {
      "epoch": 3.201884253028264,
      "grad_norm": 1.0395984649658203,
      "learning_rate": 0.00012897336126908114,
      "loss": 0.6104,
      "step": 2379
    },
    {
      "epoch": 3.203230148048452,
      "grad_norm": 1.181278944015503,
      "learning_rate": 0.0001289434301107453,
      "loss": 0.6648,
      "step": 2380
    },
    {
      "epoch": 3.2045760430686405,
      "grad_norm": 1.0852714776992798,
      "learning_rate": 0.00012891349895240948,
      "loss": 0.5344,
      "step": 2381
    },
    {
      "epoch": 3.205921938088829,
      "grad_norm": 1.0622690916061401,
      "learning_rate": 0.00012888356779407363,
      "loss": 0.9248,
      "step": 2382
    },
    {
      "epoch": 3.2072678331090176,
      "grad_norm": 1.2679352760314941,
      "learning_rate": 0.00012885363663573781,
      "loss": 0.5329,
      "step": 2383
    },
    {
      "epoch": 3.208613728129206,
      "grad_norm": 1.1845260858535767,
      "learning_rate": 0.000128823705477402,
      "loss": 0.5953,
      "step": 2384
    },
    {
      "epoch": 3.2099596231493943,
      "grad_norm": 1.1979109048843384,
      "learning_rate": 0.00012879377431906615,
      "loss": 0.7665,
      "step": 2385
    },
    {
      "epoch": 3.2113055181695827,
      "grad_norm": 1.3831331729888916,
      "learning_rate": 0.00012876384316073034,
      "loss": 0.7669,
      "step": 2386
    },
    {
      "epoch": 3.2126514131897714,
      "grad_norm": 1.211868405342102,
      "learning_rate": 0.00012873391200239452,
      "loss": 0.5325,
      "step": 2387
    },
    {
      "epoch": 3.2139973082099598,
      "grad_norm": 1.1462477445602417,
      "learning_rate": 0.00012870398084405867,
      "loss": 0.8991,
      "step": 2388
    },
    {
      "epoch": 3.215343203230148,
      "grad_norm": 0.9614855647087097,
      "learning_rate": 0.00012867404968572286,
      "loss": 0.5444,
      "step": 2389
    },
    {
      "epoch": 3.2166890982503364,
      "grad_norm": 1.1047918796539307,
      "learning_rate": 0.000128644118527387,
      "loss": 0.5448,
      "step": 2390
    },
    {
      "epoch": 3.218034993270525,
      "grad_norm": 1.7046483755111694,
      "learning_rate": 0.0001286141873690512,
      "loss": 0.7275,
      "step": 2391
    },
    {
      "epoch": 3.219380888290713,
      "grad_norm": 1.0934258699417114,
      "learning_rate": 0.00012858425621071538,
      "loss": 0.582,
      "step": 2392
    },
    {
      "epoch": 3.220726783310902,
      "grad_norm": 1.0973646640777588,
      "learning_rate": 0.00012855432505237953,
      "loss": 0.6501,
      "step": 2393
    },
    {
      "epoch": 3.2220726783310902,
      "grad_norm": 1.2005001306533813,
      "learning_rate": 0.00012852439389404372,
      "loss": 0.6845,
      "step": 2394
    },
    {
      "epoch": 3.2234185733512786,
      "grad_norm": 1.150770902633667,
      "learning_rate": 0.0001284944627357079,
      "loss": 0.5296,
      "step": 2395
    },
    {
      "epoch": 3.224764468371467,
      "grad_norm": 1.117264986038208,
      "learning_rate": 0.00012846453157737205,
      "loss": 0.5916,
      "step": 2396
    },
    {
      "epoch": 3.2261103633916552,
      "grad_norm": 1.0605363845825195,
      "learning_rate": 0.00012843460041903624,
      "loss": 0.5165,
      "step": 2397
    },
    {
      "epoch": 3.227456258411844,
      "grad_norm": 1.1968618631362915,
      "learning_rate": 0.0001284046692607004,
      "loss": 0.6878,
      "step": 2398
    },
    {
      "epoch": 3.2288021534320324,
      "grad_norm": 1.354371428489685,
      "learning_rate": 0.00012837473810236458,
      "loss": 0.6042,
      "step": 2399
    },
    {
      "epoch": 3.2301480484522207,
      "grad_norm": 1.1681677103042603,
      "learning_rate": 0.00012834480694402876,
      "loss": 0.5475,
      "step": 2400
    },
    {
      "epoch": 3.231493943472409,
      "grad_norm": 1.1832467317581177,
      "learning_rate": 0.00012831487578569291,
      "loss": 0.6698,
      "step": 2401
    },
    {
      "epoch": 3.2328398384925974,
      "grad_norm": 1.1651989221572876,
      "learning_rate": 0.0001282849446273571,
      "loss": 0.5114,
      "step": 2402
    },
    {
      "epoch": 3.234185733512786,
      "grad_norm": 1.1156339645385742,
      "learning_rate": 0.00012825501346902128,
      "loss": 0.5846,
      "step": 2403
    },
    {
      "epoch": 3.2355316285329745,
      "grad_norm": 1.0427886247634888,
      "learning_rate": 0.00012822508231068543,
      "loss": 0.5185,
      "step": 2404
    },
    {
      "epoch": 3.236877523553163,
      "grad_norm": 1.2086594104766846,
      "learning_rate": 0.00012819515115234962,
      "loss": 0.6001,
      "step": 2405
    },
    {
      "epoch": 3.238223418573351,
      "grad_norm": 1.2340489625930786,
      "learning_rate": 0.00012816521999401377,
      "loss": 0.7116,
      "step": 2406
    },
    {
      "epoch": 3.2395693135935395,
      "grad_norm": 0.9870613813400269,
      "learning_rate": 0.00012813528883567796,
      "loss": 0.6175,
      "step": 2407
    },
    {
      "epoch": 3.2409152086137283,
      "grad_norm": 1.2194780111312866,
      "learning_rate": 0.0001281053576773421,
      "loss": 0.5724,
      "step": 2408
    },
    {
      "epoch": 3.2422611036339166,
      "grad_norm": 1.0313400030136108,
      "learning_rate": 0.0001280754265190063,
      "loss": 0.5434,
      "step": 2409
    },
    {
      "epoch": 3.243606998654105,
      "grad_norm": 0.9992988705635071,
      "learning_rate": 0.00012804549536067045,
      "loss": 0.6242,
      "step": 2410
    },
    {
      "epoch": 3.2449528936742933,
      "grad_norm": 0.9489100575447083,
      "learning_rate": 0.00012801556420233463,
      "loss": 0.6701,
      "step": 2411
    },
    {
      "epoch": 3.2462987886944816,
      "grad_norm": 1.2633769512176514,
      "learning_rate": 0.0001279856330439988,
      "loss": 0.6257,
      "step": 2412
    },
    {
      "epoch": 3.2476446837146704,
      "grad_norm": 1.0134954452514648,
      "learning_rate": 0.00012795570188566297,
      "loss": 0.5064,
      "step": 2413
    },
    {
      "epoch": 3.2489905787348587,
      "grad_norm": 1.120887279510498,
      "learning_rate": 0.00012792577072732715,
      "loss": 0.7105,
      "step": 2414
    },
    {
      "epoch": 3.250336473755047,
      "grad_norm": 1.3868168592453003,
      "learning_rate": 0.0001278958395689913,
      "loss": 0.5683,
      "step": 2415
    },
    {
      "epoch": 3.2516823687752354,
      "grad_norm": 1.0675537586212158,
      "learning_rate": 0.0001278659084106555,
      "loss": 0.549,
      "step": 2416
    },
    {
      "epoch": 3.253028263795424,
      "grad_norm": 1.0910965204238892,
      "learning_rate": 0.00012783597725231967,
      "loss": 0.6655,
      "step": 2417
    },
    {
      "epoch": 3.2543741588156125,
      "grad_norm": 1.0459023714065552,
      "learning_rate": 0.00012780604609398383,
      "loss": 0.4737,
      "step": 2418
    },
    {
      "epoch": 3.255720053835801,
      "grad_norm": 1.1346009969711304,
      "learning_rate": 0.000127776114935648,
      "loss": 0.5132,
      "step": 2419
    },
    {
      "epoch": 3.257065948855989,
      "grad_norm": 1.427538514137268,
      "learning_rate": 0.00012774618377731217,
      "loss": 0.7102,
      "step": 2420
    },
    {
      "epoch": 3.2584118438761775,
      "grad_norm": 1.0586926937103271,
      "learning_rate": 0.00012771625261897635,
      "loss": 0.5752,
      "step": 2421
    },
    {
      "epoch": 3.259757738896366,
      "grad_norm": 1.2452130317687988,
      "learning_rate": 0.00012768632146064053,
      "loss": 0.6905,
      "step": 2422
    },
    {
      "epoch": 3.2611036339165547,
      "grad_norm": 1.3118363618850708,
      "learning_rate": 0.0001276563903023047,
      "loss": 0.7192,
      "step": 2423
    },
    {
      "epoch": 3.262449528936743,
      "grad_norm": 0.9568797945976257,
      "learning_rate": 0.00012762645914396887,
      "loss": 0.5843,
      "step": 2424
    },
    {
      "epoch": 3.2637954239569313,
      "grad_norm": 1.1026471853256226,
      "learning_rate": 0.00012759652798563303,
      "loss": 0.5928,
      "step": 2425
    },
    {
      "epoch": 3.2651413189771197,
      "grad_norm": 1.1914492845535278,
      "learning_rate": 0.0001275665968272972,
      "loss": 0.8489,
      "step": 2426
    },
    {
      "epoch": 3.2664872139973085,
      "grad_norm": 0.9629024267196655,
      "learning_rate": 0.0001275366656689614,
      "loss": 0.4866,
      "step": 2427
    },
    {
      "epoch": 3.267833109017497,
      "grad_norm": 1.2273869514465332,
      "learning_rate": 0.00012750673451062555,
      "loss": 0.7379,
      "step": 2428
    },
    {
      "epoch": 3.269179004037685,
      "grad_norm": 0.9019356369972229,
      "learning_rate": 0.00012747680335228973,
      "loss": 0.4911,
      "step": 2429
    },
    {
      "epoch": 3.2705248990578735,
      "grad_norm": 0.992983341217041,
      "learning_rate": 0.00012744687219395391,
      "loss": 0.5855,
      "step": 2430
    },
    {
      "epoch": 3.271870794078062,
      "grad_norm": 1.249599814414978,
      "learning_rate": 0.00012741694103561807,
      "loss": 0.4647,
      "step": 2431
    },
    {
      "epoch": 3.27321668909825,
      "grad_norm": 1.1707243919372559,
      "learning_rate": 0.00012738700987728225,
      "loss": 0.5936,
      "step": 2432
    },
    {
      "epoch": 3.274562584118439,
      "grad_norm": 1.178441047668457,
      "learning_rate": 0.0001273570787189464,
      "loss": 0.6778,
      "step": 2433
    },
    {
      "epoch": 3.2759084791386273,
      "grad_norm": 1.1025489568710327,
      "learning_rate": 0.0001273271475606106,
      "loss": 0.634,
      "step": 2434
    },
    {
      "epoch": 3.2772543741588156,
      "grad_norm": 1.2194032669067383,
      "learning_rate": 0.00012729721640227477,
      "loss": 0.6296,
      "step": 2435
    },
    {
      "epoch": 3.278600269179004,
      "grad_norm": 1.1156498193740845,
      "learning_rate": 0.00012726728524393893,
      "loss": 0.6772,
      "step": 2436
    },
    {
      "epoch": 3.2799461641991923,
      "grad_norm": 0.9712564945220947,
      "learning_rate": 0.0001272373540856031,
      "loss": 0.5522,
      "step": 2437
    },
    {
      "epoch": 3.281292059219381,
      "grad_norm": 1.215110421180725,
      "learning_rate": 0.0001272074229272673,
      "loss": 0.596,
      "step": 2438
    },
    {
      "epoch": 3.2826379542395694,
      "grad_norm": 0.9002195000648499,
      "learning_rate": 0.00012717749176893145,
      "loss": 0.4599,
      "step": 2439
    },
    {
      "epoch": 3.2839838492597577,
      "grad_norm": 0.994563639163971,
      "learning_rate": 0.00012714756061059563,
      "loss": 0.6145,
      "step": 2440
    },
    {
      "epoch": 3.285329744279946,
      "grad_norm": 1.0839265584945679,
      "learning_rate": 0.0001271176294522598,
      "loss": 0.6091,
      "step": 2441
    },
    {
      "epoch": 3.2866756393001344,
      "grad_norm": 1.1635632514953613,
      "learning_rate": 0.00012708769829392397,
      "loss": 0.7916,
      "step": 2442
    },
    {
      "epoch": 3.288021534320323,
      "grad_norm": 1.2767819166183472,
      "learning_rate": 0.00012705776713558815,
      "loss": 0.6338,
      "step": 2443
    },
    {
      "epoch": 3.2893674293405115,
      "grad_norm": 1.0812240839004517,
      "learning_rate": 0.0001270278359772523,
      "loss": 0.4793,
      "step": 2444
    },
    {
      "epoch": 3.2907133243607,
      "grad_norm": 1.0531996488571167,
      "learning_rate": 0.0001269979048189165,
      "loss": 0.5803,
      "step": 2445
    },
    {
      "epoch": 3.292059219380888,
      "grad_norm": 1.314695119857788,
      "learning_rate": 0.00012696797366058067,
      "loss": 0.6538,
      "step": 2446
    },
    {
      "epoch": 3.2934051144010765,
      "grad_norm": 1.2300963401794434,
      "learning_rate": 0.00012693804250224483,
      "loss": 0.7172,
      "step": 2447
    },
    {
      "epoch": 3.2947510094212653,
      "grad_norm": 1.296794056892395,
      "learning_rate": 0.000126908111343909,
      "loss": 0.5864,
      "step": 2448
    },
    {
      "epoch": 3.2960969044414536,
      "grad_norm": 1.0899888277053833,
      "learning_rate": 0.00012687818018557317,
      "loss": 0.6055,
      "step": 2449
    },
    {
      "epoch": 3.297442799461642,
      "grad_norm": 1.065274715423584,
      "learning_rate": 0.00012684824902723735,
      "loss": 0.7415,
      "step": 2450
    },
    {
      "epoch": 3.2987886944818303,
      "grad_norm": 1.1416453123092651,
      "learning_rate": 0.00012681831786890153,
      "loss": 0.5571,
      "step": 2451
    },
    {
      "epoch": 3.3001345895020187,
      "grad_norm": 1.1885299682617188,
      "learning_rate": 0.0001267883867105657,
      "loss": 0.6457,
      "step": 2452
    },
    {
      "epoch": 3.3014804845222074,
      "grad_norm": 1.2934108972549438,
      "learning_rate": 0.00012675845555222987,
      "loss": 0.6049,
      "step": 2453
    },
    {
      "epoch": 3.3028263795423958,
      "grad_norm": 1.2016130685806274,
      "learning_rate": 0.00012672852439389405,
      "loss": 0.7341,
      "step": 2454
    },
    {
      "epoch": 3.304172274562584,
      "grad_norm": 1.2687424421310425,
      "learning_rate": 0.0001266985932355582,
      "loss": 0.8281,
      "step": 2455
    },
    {
      "epoch": 3.3055181695827724,
      "grad_norm": 1.1179828643798828,
      "learning_rate": 0.0001266686620772224,
      "loss": 0.6022,
      "step": 2456
    },
    {
      "epoch": 3.306864064602961,
      "grad_norm": 1.1389400959014893,
      "learning_rate": 0.00012663873091888655,
      "loss": 0.6364,
      "step": 2457
    },
    {
      "epoch": 3.3082099596231496,
      "grad_norm": 1.2690156698226929,
      "learning_rate": 0.00012660879976055073,
      "loss": 0.5762,
      "step": 2458
    },
    {
      "epoch": 3.309555854643338,
      "grad_norm": 0.9827086329460144,
      "learning_rate": 0.00012657886860221491,
      "loss": 0.9606,
      "step": 2459
    },
    {
      "epoch": 3.3109017496635262,
      "grad_norm": 1.0438355207443237,
      "learning_rate": 0.00012654893744387907,
      "loss": 0.5807,
      "step": 2460
    },
    {
      "epoch": 3.3122476446837146,
      "grad_norm": 1.4888826608657837,
      "learning_rate": 0.00012651900628554325,
      "loss": 0.6913,
      "step": 2461
    },
    {
      "epoch": 3.313593539703903,
      "grad_norm": 1.0634596347808838,
      "learning_rate": 0.00012648907512720743,
      "loss": 0.4836,
      "step": 2462
    },
    {
      "epoch": 3.3149394347240917,
      "grad_norm": 1.1011754274368286,
      "learning_rate": 0.0001264591439688716,
      "loss": 0.7394,
      "step": 2463
    },
    {
      "epoch": 3.31628532974428,
      "grad_norm": 1.060154676437378,
      "learning_rate": 0.00012642921281053577,
      "loss": 0.5295,
      "step": 2464
    },
    {
      "epoch": 3.3176312247644684,
      "grad_norm": 0.9475370645523071,
      "learning_rate": 0.00012639928165219993,
      "loss": 0.7847,
      "step": 2465
    },
    {
      "epoch": 3.3189771197846567,
      "grad_norm": 1.2412656545639038,
      "learning_rate": 0.0001263693504938641,
      "loss": 0.7024,
      "step": 2466
    },
    {
      "epoch": 3.320323014804845,
      "grad_norm": 1.3495612144470215,
      "learning_rate": 0.0001263394193355283,
      "loss": 0.6269,
      "step": 2467
    },
    {
      "epoch": 3.321668909825034,
      "grad_norm": 1.289500117301941,
      "learning_rate": 0.00012630948817719245,
      "loss": 0.6911,
      "step": 2468
    },
    {
      "epoch": 3.323014804845222,
      "grad_norm": 1.1273244619369507,
      "learning_rate": 0.00012627955701885663,
      "loss": 0.5863,
      "step": 2469
    },
    {
      "epoch": 3.3243606998654105,
      "grad_norm": 1.0435758829116821,
      "learning_rate": 0.00012624962586052082,
      "loss": 0.5753,
      "step": 2470
    },
    {
      "epoch": 3.325706594885599,
      "grad_norm": 1.0293055772781372,
      "learning_rate": 0.00012621969470218497,
      "loss": 0.4765,
      "step": 2471
    },
    {
      "epoch": 3.327052489905787,
      "grad_norm": 1.1508598327636719,
      "learning_rate": 0.00012618976354384915,
      "loss": 0.6218,
      "step": 2472
    },
    {
      "epoch": 3.328398384925976,
      "grad_norm": 1.0090266466140747,
      "learning_rate": 0.0001261598323855133,
      "loss": 0.5253,
      "step": 2473
    },
    {
      "epoch": 3.3297442799461643,
      "grad_norm": 1.3717350959777832,
      "learning_rate": 0.0001261299012271775,
      "loss": 0.6999,
      "step": 2474
    },
    {
      "epoch": 3.3310901749663526,
      "grad_norm": 1.011027216911316,
      "learning_rate": 0.00012609997006884167,
      "loss": 0.6509,
      "step": 2475
    },
    {
      "epoch": 3.332436069986541,
      "grad_norm": 1.133320927619934,
      "learning_rate": 0.00012607003891050583,
      "loss": 0.5856,
      "step": 2476
    },
    {
      "epoch": 3.3337819650067293,
      "grad_norm": 1.042845606803894,
      "learning_rate": 0.00012604010775217,
      "loss": 0.538,
      "step": 2477
    },
    {
      "epoch": 3.335127860026918,
      "grad_norm": 1.1564428806304932,
      "learning_rate": 0.0001260101765938342,
      "loss": 0.571,
      "step": 2478
    },
    {
      "epoch": 3.3364737550471064,
      "grad_norm": 0.9602070450782776,
      "learning_rate": 0.00012598024543549835,
      "loss": 0.5759,
      "step": 2479
    },
    {
      "epoch": 3.3378196500672948,
      "grad_norm": 1.2042592763900757,
      "learning_rate": 0.00012595031427716253,
      "loss": 0.7518,
      "step": 2480
    },
    {
      "epoch": 3.339165545087483,
      "grad_norm": 1.1010075807571411,
      "learning_rate": 0.0001259203831188267,
      "loss": 0.618,
      "step": 2481
    },
    {
      "epoch": 3.3405114401076714,
      "grad_norm": 1.2399098873138428,
      "learning_rate": 0.00012589045196049087,
      "loss": 0.6857,
      "step": 2482
    },
    {
      "epoch": 3.34185733512786,
      "grad_norm": 1.0933716297149658,
      "learning_rate": 0.00012586052080215505,
      "loss": 0.5626,
      "step": 2483
    },
    {
      "epoch": 3.3432032301480485,
      "grad_norm": 1.4308803081512451,
      "learning_rate": 0.0001258305896438192,
      "loss": 0.618,
      "step": 2484
    },
    {
      "epoch": 3.344549125168237,
      "grad_norm": 1.147757887840271,
      "learning_rate": 0.0001258006584854834,
      "loss": 0.6749,
      "step": 2485
    },
    {
      "epoch": 3.345895020188425,
      "grad_norm": 0.9670966863632202,
      "learning_rate": 0.00012577072732714758,
      "loss": 0.5726,
      "step": 2486
    },
    {
      "epoch": 3.3472409152086136,
      "grad_norm": 1.0241963863372803,
      "learning_rate": 0.00012574079616881173,
      "loss": 0.6754,
      "step": 2487
    },
    {
      "epoch": 3.3485868102288023,
      "grad_norm": 1.09404718875885,
      "learning_rate": 0.00012571086501047591,
      "loss": 0.8875,
      "step": 2488
    },
    {
      "epoch": 3.3499327052489907,
      "grad_norm": 1.1415835618972778,
      "learning_rate": 0.00012568093385214007,
      "loss": 0.5339,
      "step": 2489
    },
    {
      "epoch": 3.351278600269179,
      "grad_norm": 1.3030756711959839,
      "learning_rate": 0.00012565100269380425,
      "loss": 0.5737,
      "step": 2490
    },
    {
      "epoch": 3.3526244952893673,
      "grad_norm": 1.0340617895126343,
      "learning_rate": 0.00012562107153546844,
      "loss": 0.5724,
      "step": 2491
    },
    {
      "epoch": 3.3539703903095557,
      "grad_norm": 1.0400259494781494,
      "learning_rate": 0.0001255911403771326,
      "loss": 0.5676,
      "step": 2492
    },
    {
      "epoch": 3.3553162853297445,
      "grad_norm": 1.026913046836853,
      "learning_rate": 0.00012556120921879677,
      "loss": 0.5622,
      "step": 2493
    },
    {
      "epoch": 3.356662180349933,
      "grad_norm": 0.9040291905403137,
      "learning_rate": 0.00012553127806046096,
      "loss": 0.5785,
      "step": 2494
    },
    {
      "epoch": 3.358008075370121,
      "grad_norm": 0.9808554649353027,
      "learning_rate": 0.0001255013469021251,
      "loss": 0.5682,
      "step": 2495
    },
    {
      "epoch": 3.3593539703903095,
      "grad_norm": 1.1375592947006226,
      "learning_rate": 0.0001254714157437893,
      "loss": 0.4731,
      "step": 2496
    },
    {
      "epoch": 3.360699865410498,
      "grad_norm": 1.0062681436538696,
      "learning_rate": 0.00012544148458545345,
      "loss": 0.5864,
      "step": 2497
    },
    {
      "epoch": 3.3620457604306866,
      "grad_norm": 1.1293072700500488,
      "learning_rate": 0.00012541155342711763,
      "loss": 0.5737,
      "step": 2498
    },
    {
      "epoch": 3.363391655450875,
      "grad_norm": 1.2550363540649414,
      "learning_rate": 0.00012538162226878182,
      "loss": 0.5632,
      "step": 2499
    },
    {
      "epoch": 3.3647375504710633,
      "grad_norm": 1.3979532718658447,
      "learning_rate": 0.00012535169111044597,
      "loss": 0.7377,
      "step": 2500
    },
    {
      "epoch": 3.3660834454912516,
      "grad_norm": 1.075494408607483,
      "learning_rate": 0.00012532175995211015,
      "loss": 0.3781,
      "step": 2501
    },
    {
      "epoch": 3.36742934051144,
      "grad_norm": 1.1633192300796509,
      "learning_rate": 0.00012529182879377434,
      "loss": 0.5749,
      "step": 2502
    },
    {
      "epoch": 3.3687752355316287,
      "grad_norm": 1.1446141004562378,
      "learning_rate": 0.0001252618976354385,
      "loss": 0.5267,
      "step": 2503
    },
    {
      "epoch": 3.370121130551817,
      "grad_norm": 1.0945420265197754,
      "learning_rate": 0.00012523196647710267,
      "loss": 0.5833,
      "step": 2504
    },
    {
      "epoch": 3.3714670255720054,
      "grad_norm": 1.2828526496887207,
      "learning_rate": 0.00012520203531876683,
      "loss": 0.6107,
      "step": 2505
    },
    {
      "epoch": 3.3728129205921937,
      "grad_norm": 1.1298973560333252,
      "learning_rate": 0.000125172104160431,
      "loss": 0.6804,
      "step": 2506
    },
    {
      "epoch": 3.374158815612382,
      "grad_norm": 0.7997559309005737,
      "learning_rate": 0.0001251421730020952,
      "loss": 0.5584,
      "step": 2507
    },
    {
      "epoch": 3.375504710632571,
      "grad_norm": 1.1767066717147827,
      "learning_rate": 0.00012511224184375935,
      "loss": 1.0549,
      "step": 2508
    },
    {
      "epoch": 3.376850605652759,
      "grad_norm": 0.9328226447105408,
      "learning_rate": 0.00012508231068542353,
      "loss": 0.4787,
      "step": 2509
    },
    {
      "epoch": 3.3781965006729475,
      "grad_norm": 1.2560429573059082,
      "learning_rate": 0.00012505237952708772,
      "loss": 0.5843,
      "step": 2510
    },
    {
      "epoch": 3.379542395693136,
      "grad_norm": 1.1434487104415894,
      "learning_rate": 0.00012502244836875187,
      "loss": 0.6767,
      "step": 2511
    },
    {
      "epoch": 3.380888290713324,
      "grad_norm": 1.3596386909484863,
      "learning_rate": 0.00012499251721041606,
      "loss": 0.6669,
      "step": 2512
    },
    {
      "epoch": 3.382234185733513,
      "grad_norm": 1.6257050037384033,
      "learning_rate": 0.0001249625860520802,
      "loss": 0.6349,
      "step": 2513
    },
    {
      "epoch": 3.3835800807537013,
      "grad_norm": 0.9785590767860413,
      "learning_rate": 0.0001249326548937444,
      "loss": 0.5566,
      "step": 2514
    },
    {
      "epoch": 3.3849259757738897,
      "grad_norm": 1.0089272260665894,
      "learning_rate": 0.00012490272373540858,
      "loss": 0.6728,
      "step": 2515
    },
    {
      "epoch": 3.386271870794078,
      "grad_norm": 1.1402758359909058,
      "learning_rate": 0.00012487279257707273,
      "loss": 0.7301,
      "step": 2516
    },
    {
      "epoch": 3.3876177658142663,
      "grad_norm": 1.1654363870620728,
      "learning_rate": 0.00012484286141873691,
      "loss": 0.7514,
      "step": 2517
    },
    {
      "epoch": 3.388963660834455,
      "grad_norm": 1.2094794511795044,
      "learning_rate": 0.0001248129302604011,
      "loss": 0.7521,
      "step": 2518
    },
    {
      "epoch": 3.3903095558546434,
      "grad_norm": 1.1440635919570923,
      "learning_rate": 0.00012478299910206525,
      "loss": 0.6581,
      "step": 2519
    },
    {
      "epoch": 3.391655450874832,
      "grad_norm": 1.1094056367874146,
      "learning_rate": 0.00012475306794372944,
      "loss": 0.7737,
      "step": 2520
    },
    {
      "epoch": 3.39300134589502,
      "grad_norm": 1.31145441532135,
      "learning_rate": 0.0001247231367853936,
      "loss": 0.57,
      "step": 2521
    },
    {
      "epoch": 3.3943472409152085,
      "grad_norm": 0.9955007433891296,
      "learning_rate": 0.00012469320562705777,
      "loss": 0.672,
      "step": 2522
    },
    {
      "epoch": 3.3956931359353972,
      "grad_norm": 1.0661288499832153,
      "learning_rate": 0.00012466327446872196,
      "loss": 0.6211,
      "step": 2523
    },
    {
      "epoch": 3.3970390309555856,
      "grad_norm": 0.998500406742096,
      "learning_rate": 0.0001246333433103861,
      "loss": 0.5589,
      "step": 2524
    },
    {
      "epoch": 3.398384925975774,
      "grad_norm": 1.2885617017745972,
      "learning_rate": 0.0001246034121520503,
      "loss": 0.5105,
      "step": 2525
    },
    {
      "epoch": 3.3997308209959622,
      "grad_norm": 1.1095116138458252,
      "learning_rate": 0.00012457348099371448,
      "loss": 0.7586,
      "step": 2526
    },
    {
      "epoch": 3.4010767160161506,
      "grad_norm": 0.9498454332351685,
      "learning_rate": 0.00012454354983537863,
      "loss": 0.6123,
      "step": 2527
    },
    {
      "epoch": 3.4024226110363394,
      "grad_norm": 1.3655462265014648,
      "learning_rate": 0.00012451361867704282,
      "loss": 0.6465,
      "step": 2528
    },
    {
      "epoch": 3.4037685060565277,
      "grad_norm": 1.4235113859176636,
      "learning_rate": 0.00012448368751870697,
      "loss": 0.8149,
      "step": 2529
    },
    {
      "epoch": 3.405114401076716,
      "grad_norm": 1.1461234092712402,
      "learning_rate": 0.00012445375636037115,
      "loss": 0.6169,
      "step": 2530
    },
    {
      "epoch": 3.4064602960969044,
      "grad_norm": 1.1412582397460938,
      "learning_rate": 0.00012442382520203534,
      "loss": 0.5277,
      "step": 2531
    },
    {
      "epoch": 3.4078061911170927,
      "grad_norm": 1.1456220149993896,
      "learning_rate": 0.0001243938940436995,
      "loss": 0.7543,
      "step": 2532
    },
    {
      "epoch": 3.409152086137281,
      "grad_norm": 1.4137332439422607,
      "learning_rate": 0.00012436396288536367,
      "loss": 0.6399,
      "step": 2533
    },
    {
      "epoch": 3.41049798115747,
      "grad_norm": 1.3106199502944946,
      "learning_rate": 0.00012433403172702786,
      "loss": 0.5827,
      "step": 2534
    },
    {
      "epoch": 3.411843876177658,
      "grad_norm": 1.0860565900802612,
      "learning_rate": 0.000124304100568692,
      "loss": 0.5815,
      "step": 2535
    },
    {
      "epoch": 3.4131897711978465,
      "grad_norm": 1.061396837234497,
      "learning_rate": 0.0001242741694103562,
      "loss": 0.6622,
      "step": 2536
    },
    {
      "epoch": 3.414535666218035,
      "grad_norm": 0.9769836664199829,
      "learning_rate": 0.00012424423825202035,
      "loss": 0.5122,
      "step": 2537
    },
    {
      "epoch": 3.4158815612382236,
      "grad_norm": 1.0494080781936646,
      "learning_rate": 0.00012421430709368453,
      "loss": 0.6598,
      "step": 2538
    },
    {
      "epoch": 3.417227456258412,
      "grad_norm": 1.234465479850769,
      "learning_rate": 0.00012418437593534872,
      "loss": 0.703,
      "step": 2539
    },
    {
      "epoch": 3.4185733512786003,
      "grad_norm": 1.2939597368240356,
      "learning_rate": 0.00012415444477701287,
      "loss": 0.5642,
      "step": 2540
    },
    {
      "epoch": 3.4199192462987886,
      "grad_norm": 1.3359404802322388,
      "learning_rate": 0.00012412451361867706,
      "loss": 0.7058,
      "step": 2541
    },
    {
      "epoch": 3.421265141318977,
      "grad_norm": 1.2305963039398193,
      "learning_rate": 0.0001240945824603412,
      "loss": 0.615,
      "step": 2542
    },
    {
      "epoch": 3.4226110363391653,
      "grad_norm": 0.9898137450218201,
      "learning_rate": 0.0001240646513020054,
      "loss": 0.4855,
      "step": 2543
    },
    {
      "epoch": 3.423956931359354,
      "grad_norm": 1.3351391553878784,
      "learning_rate": 0.00012403472014366958,
      "loss": 0.6697,
      "step": 2544
    },
    {
      "epoch": 3.4253028263795424,
      "grad_norm": 1.0896563529968262,
      "learning_rate": 0.00012400478898533373,
      "loss": 0.5635,
      "step": 2545
    },
    {
      "epoch": 3.4266487213997308,
      "grad_norm": 1.057769775390625,
      "learning_rate": 0.00012397485782699791,
      "loss": 0.5194,
      "step": 2546
    },
    {
      "epoch": 3.427994616419919,
      "grad_norm": 1.1051133871078491,
      "learning_rate": 0.0001239449266686621,
      "loss": 0.5574,
      "step": 2547
    },
    {
      "epoch": 3.429340511440108,
      "grad_norm": 1.099329948425293,
      "learning_rate": 0.00012391499551032625,
      "loss": 0.4661,
      "step": 2548
    },
    {
      "epoch": 3.430686406460296,
      "grad_norm": 1.4650211334228516,
      "learning_rate": 0.00012388506435199044,
      "loss": 0.7003,
      "step": 2549
    },
    {
      "epoch": 3.4320323014804845,
      "grad_norm": 1.3334578275680542,
      "learning_rate": 0.0001238551331936546,
      "loss": 0.5191,
      "step": 2550
    },
    {
      "epoch": 3.433378196500673,
      "grad_norm": 1.1685301065444946,
      "learning_rate": 0.00012382520203531877,
      "loss": 0.5419,
      "step": 2551
    },
    {
      "epoch": 3.4347240915208612,
      "grad_norm": 1.1053285598754883,
      "learning_rate": 0.00012379527087698296,
      "loss": 0.5649,
      "step": 2552
    },
    {
      "epoch": 3.4360699865410496,
      "grad_norm": 1.1882609128952026,
      "learning_rate": 0.0001237653397186471,
      "loss": 0.6624,
      "step": 2553
    },
    {
      "epoch": 3.4374158815612383,
      "grad_norm": 1.6080164909362793,
      "learning_rate": 0.0001237354085603113,
      "loss": 0.7127,
      "step": 2554
    },
    {
      "epoch": 3.4387617765814267,
      "grad_norm": 1.485882043838501,
      "learning_rate": 0.00012370547740197548,
      "loss": 0.7285,
      "step": 2555
    },
    {
      "epoch": 3.440107671601615,
      "grad_norm": 1.3212343454360962,
      "learning_rate": 0.00012367554624363963,
      "loss": 0.7136,
      "step": 2556
    },
    {
      "epoch": 3.4414535666218034,
      "grad_norm": 1.340246558189392,
      "learning_rate": 0.00012364561508530382,
      "loss": 0.7024,
      "step": 2557
    },
    {
      "epoch": 3.442799461641992,
      "grad_norm": 1.3271249532699585,
      "learning_rate": 0.00012361568392696797,
      "loss": 0.6895,
      "step": 2558
    },
    {
      "epoch": 3.4441453566621805,
      "grad_norm": 1.260576605796814,
      "learning_rate": 0.00012358575276863215,
      "loss": 0.9433,
      "step": 2559
    },
    {
      "epoch": 3.445491251682369,
      "grad_norm": 1.016470193862915,
      "learning_rate": 0.00012355582161029634,
      "loss": 0.7203,
      "step": 2560
    },
    {
      "epoch": 3.446837146702557,
      "grad_norm": 1.3439346551895142,
      "learning_rate": 0.0001235258904519605,
      "loss": 0.6795,
      "step": 2561
    },
    {
      "epoch": 3.4481830417227455,
      "grad_norm": 1.0732735395431519,
      "learning_rate": 0.00012349595929362468,
      "loss": 0.5905,
      "step": 2562
    },
    {
      "epoch": 3.449528936742934,
      "grad_norm": 1.2455118894577026,
      "learning_rate": 0.00012346602813528886,
      "loss": 0.7889,
      "step": 2563
    },
    {
      "epoch": 3.4508748317631226,
      "grad_norm": 1.4618295431137085,
      "learning_rate": 0.000123436096976953,
      "loss": 0.6711,
      "step": 2564
    },
    {
      "epoch": 3.452220726783311,
      "grad_norm": 1.031662106513977,
      "learning_rate": 0.0001234061658186172,
      "loss": 0.8701,
      "step": 2565
    },
    {
      "epoch": 3.4535666218034993,
      "grad_norm": 1.3689554929733276,
      "learning_rate": 0.00012337623466028135,
      "loss": 0.706,
      "step": 2566
    },
    {
      "epoch": 3.4549125168236876,
      "grad_norm": 1.0194270610809326,
      "learning_rate": 0.00012334630350194553,
      "loss": 0.5661,
      "step": 2567
    },
    {
      "epoch": 3.4562584118438764,
      "grad_norm": 1.4271663427352905,
      "learning_rate": 0.00012331637234360972,
      "loss": 0.5597,
      "step": 2568
    },
    {
      "epoch": 3.4576043068640647,
      "grad_norm": 1.0741828680038452,
      "learning_rate": 0.00012328644118527387,
      "loss": 0.6352,
      "step": 2569
    },
    {
      "epoch": 3.458950201884253,
      "grad_norm": 1.0488684177398682,
      "learning_rate": 0.00012325651002693806,
      "loss": 0.5053,
      "step": 2570
    },
    {
      "epoch": 3.4602960969044414,
      "grad_norm": 1.0898531675338745,
      "learning_rate": 0.00012322657886860224,
      "loss": 0.8486,
      "step": 2571
    },
    {
      "epoch": 3.4616419919246297,
      "grad_norm": 0.9872033596038818,
      "learning_rate": 0.0001231966477102664,
      "loss": 0.7187,
      "step": 2572
    },
    {
      "epoch": 3.462987886944818,
      "grad_norm": 1.1315745115280151,
      "learning_rate": 0.00012316671655193058,
      "loss": 0.7657,
      "step": 2573
    },
    {
      "epoch": 3.464333781965007,
      "grad_norm": 1.1353110074996948,
      "learning_rate": 0.00012313678539359473,
      "loss": 0.6379,
      "step": 2574
    },
    {
      "epoch": 3.465679676985195,
      "grad_norm": 0.9915987849235535,
      "learning_rate": 0.00012310685423525891,
      "loss": 0.6032,
      "step": 2575
    },
    {
      "epoch": 3.4670255720053835,
      "grad_norm": 1.1438146829605103,
      "learning_rate": 0.0001230769230769231,
      "loss": 0.5984,
      "step": 2576
    },
    {
      "epoch": 3.468371467025572,
      "grad_norm": 1.0578688383102417,
      "learning_rate": 0.00012304699191858725,
      "loss": 0.5017,
      "step": 2577
    },
    {
      "epoch": 3.4697173620457606,
      "grad_norm": 0.8691965341567993,
      "learning_rate": 0.00012301706076025144,
      "loss": 0.6625,
      "step": 2578
    },
    {
      "epoch": 3.471063257065949,
      "grad_norm": 1.1724966764450073,
      "learning_rate": 0.00012298712960191562,
      "loss": 0.586,
      "step": 2579
    },
    {
      "epoch": 3.4724091520861373,
      "grad_norm": 1.4078948497772217,
      "learning_rate": 0.00012295719844357977,
      "loss": 0.7649,
      "step": 2580
    },
    {
      "epoch": 3.4737550471063257,
      "grad_norm": 1.328045129776001,
      "learning_rate": 0.00012292726728524396,
      "loss": 0.6509,
      "step": 2581
    },
    {
      "epoch": 3.475100942126514,
      "grad_norm": 1.0099965333938599,
      "learning_rate": 0.0001228973361269081,
      "loss": 0.6322,
      "step": 2582
    },
    {
      "epoch": 3.4764468371467023,
      "grad_norm": 1.1072643995285034,
      "learning_rate": 0.0001228674049685723,
      "loss": 0.5338,
      "step": 2583
    },
    {
      "epoch": 3.477792732166891,
      "grad_norm": 1.267810344696045,
      "learning_rate": 0.00012283747381023648,
      "loss": 0.5579,
      "step": 2584
    },
    {
      "epoch": 3.4791386271870794,
      "grad_norm": 1.1724960803985596,
      "learning_rate": 0.00012280754265190063,
      "loss": 0.5218,
      "step": 2585
    },
    {
      "epoch": 3.480484522207268,
      "grad_norm": 1.0175981521606445,
      "learning_rate": 0.00012277761149356482,
      "loss": 0.5783,
      "step": 2586
    },
    {
      "epoch": 3.481830417227456,
      "grad_norm": 1.0810141563415527,
      "learning_rate": 0.000122747680335229,
      "loss": 0.7047,
      "step": 2587
    },
    {
      "epoch": 3.483176312247645,
      "grad_norm": 1.2181422710418701,
      "learning_rate": 0.00012271774917689315,
      "loss": 0.8255,
      "step": 2588
    },
    {
      "epoch": 3.4845222072678332,
      "grad_norm": 1.0861783027648926,
      "learning_rate": 0.00012268781801855734,
      "loss": 0.5631,
      "step": 2589
    },
    {
      "epoch": 3.4858681022880216,
      "grad_norm": 1.3958230018615723,
      "learning_rate": 0.0001226578868602215,
      "loss": 0.7223,
      "step": 2590
    },
    {
      "epoch": 3.48721399730821,
      "grad_norm": 1.1113390922546387,
      "learning_rate": 0.00012262795570188568,
      "loss": 0.6221,
      "step": 2591
    },
    {
      "epoch": 3.4885598923283982,
      "grad_norm": 1.1099841594696045,
      "learning_rate": 0.00012259802454354986,
      "loss": 0.4983,
      "step": 2592
    },
    {
      "epoch": 3.4899057873485866,
      "grad_norm": 1.0686912536621094,
      "learning_rate": 0.00012256809338521401,
      "loss": 0.778,
      "step": 2593
    },
    {
      "epoch": 3.4912516823687754,
      "grad_norm": 1.0430736541748047,
      "learning_rate": 0.0001225381622268782,
      "loss": 0.6318,
      "step": 2594
    },
    {
      "epoch": 3.4925975773889637,
      "grad_norm": 1.476191520690918,
      "learning_rate": 0.00012250823106854238,
      "loss": 0.7886,
      "step": 2595
    },
    {
      "epoch": 3.493943472409152,
      "grad_norm": 1.4494191408157349,
      "learning_rate": 0.00012247829991020653,
      "loss": 0.6754,
      "step": 2596
    },
    {
      "epoch": 3.4952893674293404,
      "grad_norm": 1.0195167064666748,
      "learning_rate": 0.00012244836875187072,
      "loss": 0.5457,
      "step": 2597
    },
    {
      "epoch": 3.496635262449529,
      "grad_norm": 1.1865384578704834,
      "learning_rate": 0.00012241843759353487,
      "loss": 0.622,
      "step": 2598
    },
    {
      "epoch": 3.4979811574697175,
      "grad_norm": 1.3085308074951172,
      "learning_rate": 0.00012238850643519906,
      "loss": 0.8662,
      "step": 2599
    },
    {
      "epoch": 3.499327052489906,
      "grad_norm": 1.1318895816802979,
      "learning_rate": 0.00012235857527686324,
      "loss": 0.6649,
      "step": 2600
    },
    {
      "epoch": 3.500672947510094,
      "grad_norm": 1.0038450956344604,
      "learning_rate": 0.0001223286441185274,
      "loss": 0.4861,
      "step": 2601
    },
    {
      "epoch": 3.5020188425302825,
      "grad_norm": 0.935802161693573,
      "learning_rate": 0.00012229871296019158,
      "loss": 0.5317,
      "step": 2602
    },
    {
      "epoch": 3.503364737550471,
      "grad_norm": 1.323127269744873,
      "learning_rate": 0.00012226878180185576,
      "loss": 0.637,
      "step": 2603
    },
    {
      "epoch": 3.5047106325706596,
      "grad_norm": 1.385621428489685,
      "learning_rate": 0.00012223885064351991,
      "loss": 0.6057,
      "step": 2604
    },
    {
      "epoch": 3.506056527590848,
      "grad_norm": 1.0260323286056519,
      "learning_rate": 0.0001222089194851841,
      "loss": 0.5617,
      "step": 2605
    },
    {
      "epoch": 3.5074024226110363,
      "grad_norm": 1.2204445600509644,
      "learning_rate": 0.00012217898832684825,
      "loss": 0.5961,
      "step": 2606
    },
    {
      "epoch": 3.5087483176312246,
      "grad_norm": 1.1688300371170044,
      "learning_rate": 0.00012214905716851244,
      "loss": 0.6541,
      "step": 2607
    },
    {
      "epoch": 3.5100942126514134,
      "grad_norm": 1.4428706169128418,
      "learning_rate": 0.00012211912601017662,
      "loss": 0.5432,
      "step": 2608
    },
    {
      "epoch": 3.5114401076716018,
      "grad_norm": 1.2225171327590942,
      "learning_rate": 0.00012208919485184077,
      "loss": 0.6755,
      "step": 2609
    },
    {
      "epoch": 3.51278600269179,
      "grad_norm": 1.117794156074524,
      "learning_rate": 0.00012205926369350496,
      "loss": 0.6512,
      "step": 2610
    },
    {
      "epoch": 3.5141318977119784,
      "grad_norm": 1.1357321739196777,
      "learning_rate": 0.00012202933253516913,
      "loss": 0.5543,
      "step": 2611
    },
    {
      "epoch": 3.5154777927321668,
      "grad_norm": 1.0980225801467896,
      "learning_rate": 0.0001219994013768333,
      "loss": 0.6883,
      "step": 2612
    },
    {
      "epoch": 3.516823687752355,
      "grad_norm": 1.097515344619751,
      "learning_rate": 0.00012196947021849748,
      "loss": 0.6243,
      "step": 2613
    },
    {
      "epoch": 3.518169582772544,
      "grad_norm": 1.1618378162384033,
      "learning_rate": 0.00012193953906016165,
      "loss": 0.738,
      "step": 2614
    },
    {
      "epoch": 3.519515477792732,
      "grad_norm": 1.1860177516937256,
      "learning_rate": 0.00012190960790182582,
      "loss": 0.5365,
      "step": 2615
    },
    {
      "epoch": 3.5208613728129206,
      "grad_norm": 1.2107349634170532,
      "learning_rate": 0.00012187967674348999,
      "loss": 0.641,
      "step": 2616
    },
    {
      "epoch": 3.522207267833109,
      "grad_norm": 1.1220371723175049,
      "learning_rate": 0.00012184974558515414,
      "loss": 0.4893,
      "step": 2617
    },
    {
      "epoch": 3.5235531628532977,
      "grad_norm": 1.11746084690094,
      "learning_rate": 0.00012181981442681831,
      "loss": 0.5711,
      "step": 2618
    },
    {
      "epoch": 3.524899057873486,
      "grad_norm": 1.0914560556411743,
      "learning_rate": 0.00012178988326848248,
      "loss": 0.6549,
      "step": 2619
    },
    {
      "epoch": 3.5262449528936743,
      "grad_norm": 1.1878763437271118,
      "learning_rate": 0.00012175995211014666,
      "loss": 0.5644,
      "step": 2620
    },
    {
      "epoch": 3.5275908479138627,
      "grad_norm": 1.0687334537506104,
      "learning_rate": 0.00012173002095181083,
      "loss": 0.6776,
      "step": 2621
    },
    {
      "epoch": 3.528936742934051,
      "grad_norm": 1.1217379570007324,
      "learning_rate": 0.000121700089793475,
      "loss": 0.6651,
      "step": 2622
    },
    {
      "epoch": 3.5302826379542394,
      "grad_norm": 1.3228520154953003,
      "learning_rate": 0.00012167015863513917,
      "loss": 0.5687,
      "step": 2623
    },
    {
      "epoch": 3.531628532974428,
      "grad_norm": 1.0929595232009888,
      "learning_rate": 0.00012164022747680335,
      "loss": 0.5239,
      "step": 2624
    },
    {
      "epoch": 3.5329744279946165,
      "grad_norm": 1.0979617834091187,
      "learning_rate": 0.00012161029631846752,
      "loss": 0.5339,
      "step": 2625
    },
    {
      "epoch": 3.534320323014805,
      "grad_norm": 1.2096412181854248,
      "learning_rate": 0.00012158036516013169,
      "loss": 0.7729,
      "step": 2626
    },
    {
      "epoch": 3.535666218034993,
      "grad_norm": 1.1266889572143555,
      "learning_rate": 0.00012155043400179586,
      "loss": 0.5591,
      "step": 2627
    },
    {
      "epoch": 3.537012113055182,
      "grad_norm": 0.9549447298049927,
      "learning_rate": 0.00012152050284346004,
      "loss": 0.5174,
      "step": 2628
    },
    {
      "epoch": 3.5383580080753703,
      "grad_norm": 1.006367564201355,
      "learning_rate": 0.00012149057168512421,
      "loss": 0.5586,
      "step": 2629
    },
    {
      "epoch": 3.5397039030955586,
      "grad_norm": 1.1605485677719116,
      "learning_rate": 0.00012146064052678838,
      "loss": 0.7054,
      "step": 2630
    },
    {
      "epoch": 3.541049798115747,
      "grad_norm": 1.3133467435836792,
      "learning_rate": 0.00012143070936845255,
      "loss": 0.7487,
      "step": 2631
    },
    {
      "epoch": 3.5423956931359353,
      "grad_norm": 1.1041604280471802,
      "learning_rate": 0.00012140077821011673,
      "loss": 0.6471,
      "step": 2632
    },
    {
      "epoch": 3.5437415881561236,
      "grad_norm": 1.0092179775238037,
      "learning_rate": 0.0001213708470517809,
      "loss": 0.5413,
      "step": 2633
    },
    {
      "epoch": 3.5450874831763124,
      "grad_norm": 1.2378743886947632,
      "learning_rate": 0.00012134091589344507,
      "loss": 0.7128,
      "step": 2634
    },
    {
      "epoch": 3.5464333781965007,
      "grad_norm": 1.3820091485977173,
      "learning_rate": 0.00012131098473510924,
      "loss": 0.6354,
      "step": 2635
    },
    {
      "epoch": 3.547779273216689,
      "grad_norm": 1.1932027339935303,
      "learning_rate": 0.00012128105357677342,
      "loss": 0.6557,
      "step": 2636
    },
    {
      "epoch": 3.5491251682368774,
      "grad_norm": 0.9838342070579529,
      "learning_rate": 0.00012125112241843759,
      "loss": 0.6123,
      "step": 2637
    },
    {
      "epoch": 3.550471063257066,
      "grad_norm": 1.1250170469284058,
      "learning_rate": 0.00012122119126010176,
      "loss": 0.4343,
      "step": 2638
    },
    {
      "epoch": 3.5518169582772545,
      "grad_norm": 1.106611967086792,
      "learning_rate": 0.00012119126010176593,
      "loss": 0.6257,
      "step": 2639
    },
    {
      "epoch": 3.553162853297443,
      "grad_norm": 1.3200500011444092,
      "learning_rate": 0.00012116132894343011,
      "loss": 0.7064,
      "step": 2640
    },
    {
      "epoch": 3.554508748317631,
      "grad_norm": 1.0996875762939453,
      "learning_rate": 0.00012113139778509428,
      "loss": 0.6215,
      "step": 2641
    },
    {
      "epoch": 3.5558546433378195,
      "grad_norm": 1.0816794633865356,
      "learning_rate": 0.00012110146662675845,
      "loss": 0.6727,
      "step": 2642
    },
    {
      "epoch": 3.557200538358008,
      "grad_norm": 1.3438308238983154,
      "learning_rate": 0.00012107153546842262,
      "loss": 0.5523,
      "step": 2643
    },
    {
      "epoch": 3.5585464333781966,
      "grad_norm": 1.0942612886428833,
      "learning_rate": 0.0001210416043100868,
      "loss": 0.5502,
      "step": 2644
    },
    {
      "epoch": 3.559892328398385,
      "grad_norm": 1.0424370765686035,
      "learning_rate": 0.00012101167315175097,
      "loss": 0.8066,
      "step": 2645
    },
    {
      "epoch": 3.5612382234185733,
      "grad_norm": 1.1871446371078491,
      "learning_rate": 0.00012098174199341514,
      "loss": 0.6414,
      "step": 2646
    },
    {
      "epoch": 3.5625841184387617,
      "grad_norm": 1.1419947147369385,
      "learning_rate": 0.00012095181083507931,
      "loss": 0.6192,
      "step": 2647
    },
    {
      "epoch": 3.5639300134589504,
      "grad_norm": 1.4513444900512695,
      "learning_rate": 0.00012092187967674349,
      "loss": 0.8482,
      "step": 2648
    },
    {
      "epoch": 3.5652759084791388,
      "grad_norm": 1.2396140098571777,
      "learning_rate": 0.00012089194851840766,
      "loss": 0.6169,
      "step": 2649
    },
    {
      "epoch": 3.566621803499327,
      "grad_norm": 1.3268083333969116,
      "learning_rate": 0.00012086201736007183,
      "loss": 0.6338,
      "step": 2650
    },
    {
      "epoch": 3.5679676985195155,
      "grad_norm": 1.133178949356079,
      "learning_rate": 0.000120832086201736,
      "loss": 0.5306,
      "step": 2651
    },
    {
      "epoch": 3.569313593539704,
      "grad_norm": 1.1490797996520996,
      "learning_rate": 0.00012080215504340018,
      "loss": 0.6727,
      "step": 2652
    },
    {
      "epoch": 3.570659488559892,
      "grad_norm": 1.2048231363296509,
      "learning_rate": 0.00012077222388506435,
      "loss": 0.653,
      "step": 2653
    },
    {
      "epoch": 3.572005383580081,
      "grad_norm": 1.2339658737182617,
      "learning_rate": 0.00012074229272672852,
      "loss": 0.6483,
      "step": 2654
    },
    {
      "epoch": 3.5733512786002692,
      "grad_norm": 1.004159927368164,
      "learning_rate": 0.00012071236156839269,
      "loss": 0.5355,
      "step": 2655
    },
    {
      "epoch": 3.5746971736204576,
      "grad_norm": 1.1648510694503784,
      "learning_rate": 0.00012068243041005687,
      "loss": 0.5729,
      "step": 2656
    },
    {
      "epoch": 3.576043068640646,
      "grad_norm": 1.246987223625183,
      "learning_rate": 0.00012065249925172104,
      "loss": 0.671,
      "step": 2657
    },
    {
      "epoch": 3.5773889636608347,
      "grad_norm": 0.8279834389686584,
      "learning_rate": 0.00012062256809338521,
      "loss": 0.5114,
      "step": 2658
    },
    {
      "epoch": 3.578734858681023,
      "grad_norm": 1.0651901960372925,
      "learning_rate": 0.00012059263693504938,
      "loss": 0.5881,
      "step": 2659
    },
    {
      "epoch": 3.5800807537012114,
      "grad_norm": 1.2160578966140747,
      "learning_rate": 0.00012056270577671356,
      "loss": 0.6903,
      "step": 2660
    },
    {
      "epoch": 3.5814266487213997,
      "grad_norm": 1.079344630241394,
      "learning_rate": 0.00012053277461837773,
      "loss": 0.515,
      "step": 2661
    },
    {
      "epoch": 3.582772543741588,
      "grad_norm": 1.425328016281128,
      "learning_rate": 0.0001205028434600419,
      "loss": 0.626,
      "step": 2662
    },
    {
      "epoch": 3.5841184387617764,
      "grad_norm": 1.1052767038345337,
      "learning_rate": 0.00012047291230170607,
      "loss": 0.9001,
      "step": 2663
    },
    {
      "epoch": 3.585464333781965,
      "grad_norm": 1.3609110116958618,
      "learning_rate": 0.00012044298114337025,
      "loss": 0.5989,
      "step": 2664
    },
    {
      "epoch": 3.5868102288021535,
      "grad_norm": 1.1216849088668823,
      "learning_rate": 0.00012041304998503442,
      "loss": 0.6677,
      "step": 2665
    },
    {
      "epoch": 3.588156123822342,
      "grad_norm": 1.107351541519165,
      "learning_rate": 0.00012038311882669859,
      "loss": 0.4814,
      "step": 2666
    },
    {
      "epoch": 3.58950201884253,
      "grad_norm": 1.10335373878479,
      "learning_rate": 0.00012035318766836276,
      "loss": 0.6202,
      "step": 2667
    },
    {
      "epoch": 3.590847913862719,
      "grad_norm": 1.027559518814087,
      "learning_rate": 0.00012032325651002694,
      "loss": 0.5472,
      "step": 2668
    },
    {
      "epoch": 3.5921938088829073,
      "grad_norm": 0.9519580006599426,
      "learning_rate": 0.00012029332535169111,
      "loss": 0.5064,
      "step": 2669
    },
    {
      "epoch": 3.5935397039030956,
      "grad_norm": 1.268676996231079,
      "learning_rate": 0.00012026339419335528,
      "loss": 0.7177,
      "step": 2670
    },
    {
      "epoch": 3.594885598923284,
      "grad_norm": 1.2258576154708862,
      "learning_rate": 0.00012023346303501945,
      "loss": 0.6138,
      "step": 2671
    },
    {
      "epoch": 3.5962314939434723,
      "grad_norm": 1.1751028299331665,
      "learning_rate": 0.00012020353187668363,
      "loss": 0.7334,
      "step": 2672
    },
    {
      "epoch": 3.5975773889636606,
      "grad_norm": 1.0656318664550781,
      "learning_rate": 0.0001201736007183478,
      "loss": 0.4285,
      "step": 2673
    },
    {
      "epoch": 3.5989232839838494,
      "grad_norm": 1.071255087852478,
      "learning_rate": 0.00012014366956001197,
      "loss": 0.5167,
      "step": 2674
    },
    {
      "epoch": 3.6002691790040378,
      "grad_norm": 1.1035327911376953,
      "learning_rate": 0.00012011373840167614,
      "loss": 0.4676,
      "step": 2675
    },
    {
      "epoch": 3.601615074024226,
      "grad_norm": 1.1558023691177368,
      "learning_rate": 0.00012008380724334032,
      "loss": 0.8151,
      "step": 2676
    },
    {
      "epoch": 3.6029609690444144,
      "grad_norm": 1.193913221359253,
      "learning_rate": 0.00012005387608500449,
      "loss": 0.6321,
      "step": 2677
    },
    {
      "epoch": 3.604306864064603,
      "grad_norm": 1.1918766498565674,
      "learning_rate": 0.00012002394492666866,
      "loss": 0.5027,
      "step": 2678
    },
    {
      "epoch": 3.6056527590847915,
      "grad_norm": 1.349870204925537,
      "learning_rate": 0.00011999401376833283,
      "loss": 0.7556,
      "step": 2679
    },
    {
      "epoch": 3.60699865410498,
      "grad_norm": 1.2035022974014282,
      "learning_rate": 0.00011996408260999701,
      "loss": 0.5853,
      "step": 2680
    },
    {
      "epoch": 3.608344549125168,
      "grad_norm": 1.0910040140151978,
      "learning_rate": 0.00011993415145166118,
      "loss": 0.587,
      "step": 2681
    },
    {
      "epoch": 3.6096904441453566,
      "grad_norm": 1.0305744409561157,
      "learning_rate": 0.00011990422029332535,
      "loss": 0.5455,
      "step": 2682
    },
    {
      "epoch": 3.611036339165545,
      "grad_norm": 1.032157063484192,
      "learning_rate": 0.00011987428913498952,
      "loss": 0.5968,
      "step": 2683
    },
    {
      "epoch": 3.6123822341857337,
      "grad_norm": 1.0788954496383667,
      "learning_rate": 0.0001198443579766537,
      "loss": 0.8185,
      "step": 2684
    },
    {
      "epoch": 3.613728129205922,
      "grad_norm": 1.1052873134613037,
      "learning_rate": 0.00011981442681831787,
      "loss": 0.5877,
      "step": 2685
    },
    {
      "epoch": 3.6150740242261103,
      "grad_norm": 1.3290460109710693,
      "learning_rate": 0.00011978449565998204,
      "loss": 0.6203,
      "step": 2686
    },
    {
      "epoch": 3.6164199192462987,
      "grad_norm": 1.2979207038879395,
      "learning_rate": 0.00011975456450164621,
      "loss": 0.577,
      "step": 2687
    },
    {
      "epoch": 3.6177658142664875,
      "grad_norm": 0.9510783553123474,
      "learning_rate": 0.0001197246333433104,
      "loss": 0.5637,
      "step": 2688
    },
    {
      "epoch": 3.619111709286676,
      "grad_norm": 1.4027302265167236,
      "learning_rate": 0.00011969470218497456,
      "loss": 0.6099,
      "step": 2689
    },
    {
      "epoch": 3.620457604306864,
      "grad_norm": 1.1512000560760498,
      "learning_rate": 0.00011966477102663873,
      "loss": 0.5195,
      "step": 2690
    },
    {
      "epoch": 3.6218034993270525,
      "grad_norm": 1.15186607837677,
      "learning_rate": 0.0001196348398683029,
      "loss": 0.6385,
      "step": 2691
    },
    {
      "epoch": 3.623149394347241,
      "grad_norm": 1.162211537361145,
      "learning_rate": 0.00011960490870996708,
      "loss": 0.6541,
      "step": 2692
    },
    {
      "epoch": 3.624495289367429,
      "grad_norm": 1.1490068435668945,
      "learning_rate": 0.00011957497755163125,
      "loss": 0.6356,
      "step": 2693
    },
    {
      "epoch": 3.6258411843876175,
      "grad_norm": 1.1216741800308228,
      "learning_rate": 0.00011954504639329542,
      "loss": 0.6517,
      "step": 2694
    },
    {
      "epoch": 3.6271870794078063,
      "grad_norm": 1.1749681234359741,
      "learning_rate": 0.00011951511523495959,
      "loss": 0.5802,
      "step": 2695
    },
    {
      "epoch": 3.6285329744279946,
      "grad_norm": 0.9358469247817993,
      "learning_rate": 0.00011948518407662377,
      "loss": 0.555,
      "step": 2696
    },
    {
      "epoch": 3.629878869448183,
      "grad_norm": 1.0923995971679688,
      "learning_rate": 0.00011945525291828794,
      "loss": 0.5809,
      "step": 2697
    },
    {
      "epoch": 3.6312247644683717,
      "grad_norm": 1.5767016410827637,
      "learning_rate": 0.00011942532175995211,
      "loss": 0.5319,
      "step": 2698
    },
    {
      "epoch": 3.63257065948856,
      "grad_norm": 1.1945335865020752,
      "learning_rate": 0.00011939539060161628,
      "loss": 0.5132,
      "step": 2699
    },
    {
      "epoch": 3.6339165545087484,
      "grad_norm": 1.1632462739944458,
      "learning_rate": 0.00011936545944328046,
      "loss": 0.6015,
      "step": 2700
    },
    {
      "epoch": 3.6352624495289367,
      "grad_norm": 1.1689796447753906,
      "learning_rate": 0.00011933552828494463,
      "loss": 0.563,
      "step": 2701
    },
    {
      "epoch": 3.636608344549125,
      "grad_norm": 1.1778528690338135,
      "learning_rate": 0.0001193055971266088,
      "loss": 0.53,
      "step": 2702
    },
    {
      "epoch": 3.6379542395693134,
      "grad_norm": 1.0792980194091797,
      "learning_rate": 0.00011927566596827297,
      "loss": 0.6191,
      "step": 2703
    },
    {
      "epoch": 3.6393001345895017,
      "grad_norm": 1.1858549118041992,
      "learning_rate": 0.00011924573480993716,
      "loss": 0.5379,
      "step": 2704
    },
    {
      "epoch": 3.6406460296096905,
      "grad_norm": 1.1951593160629272,
      "learning_rate": 0.00011921580365160132,
      "loss": 0.8416,
      "step": 2705
    },
    {
      "epoch": 3.641991924629879,
      "grad_norm": 1.2311480045318604,
      "learning_rate": 0.0001191858724932655,
      "loss": 0.5906,
      "step": 2706
    },
    {
      "epoch": 3.643337819650067,
      "grad_norm": 1.0739142894744873,
      "learning_rate": 0.00011915594133492966,
      "loss": 0.7236,
      "step": 2707
    },
    {
      "epoch": 3.644683714670256,
      "grad_norm": 1.1547951698303223,
      "learning_rate": 0.00011912601017659385,
      "loss": 0.5956,
      "step": 2708
    },
    {
      "epoch": 3.6460296096904443,
      "grad_norm": 1.2556928396224976,
      "learning_rate": 0.00011909607901825801,
      "loss": 0.6255,
      "step": 2709
    },
    {
      "epoch": 3.6473755047106327,
      "grad_norm": 1.3214244842529297,
      "learning_rate": 0.00011906614785992218,
      "loss": 0.732,
      "step": 2710
    },
    {
      "epoch": 3.648721399730821,
      "grad_norm": 1.061922550201416,
      "learning_rate": 0.00011903621670158635,
      "loss": 0.5323,
      "step": 2711
    },
    {
      "epoch": 3.6500672947510093,
      "grad_norm": 1.1454713344573975,
      "learning_rate": 0.00011900628554325054,
      "loss": 0.6847,
      "step": 2712
    },
    {
      "epoch": 3.6514131897711977,
      "grad_norm": 1.4062778949737549,
      "learning_rate": 0.0001189763543849147,
      "loss": 0.776,
      "step": 2713
    },
    {
      "epoch": 3.652759084791386,
      "grad_norm": 1.5091248750686646,
      "learning_rate": 0.00011894642322657887,
      "loss": 0.7016,
      "step": 2714
    },
    {
      "epoch": 3.654104979811575,
      "grad_norm": 1.1317871809005737,
      "learning_rate": 0.00011891649206824304,
      "loss": 0.6854,
      "step": 2715
    },
    {
      "epoch": 3.655450874831763,
      "grad_norm": 1.2740482091903687,
      "learning_rate": 0.00011888656090990721,
      "loss": 0.7337,
      "step": 2716
    },
    {
      "epoch": 3.6567967698519515,
      "grad_norm": 1.0873740911483765,
      "learning_rate": 0.0001188566297515714,
      "loss": 0.5959,
      "step": 2717
    },
    {
      "epoch": 3.6581426648721402,
      "grad_norm": 1.1583915948867798,
      "learning_rate": 0.00011882669859323556,
      "loss": 0.5196,
      "step": 2718
    },
    {
      "epoch": 3.6594885598923286,
      "grad_norm": 1.0147699117660522,
      "learning_rate": 0.00011879676743489973,
      "loss": 0.5737,
      "step": 2719
    },
    {
      "epoch": 3.660834454912517,
      "grad_norm": 1.0265201330184937,
      "learning_rate": 0.0001187668362765639,
      "loss": 0.6592,
      "step": 2720
    },
    {
      "epoch": 3.6621803499327052,
      "grad_norm": 0.9890674352645874,
      "learning_rate": 0.00011873690511822808,
      "loss": 0.476,
      "step": 2721
    },
    {
      "epoch": 3.6635262449528936,
      "grad_norm": 1.4589881896972656,
      "learning_rate": 0.00011870697395989225,
      "loss": 0.6735,
      "step": 2722
    },
    {
      "epoch": 3.664872139973082,
      "grad_norm": 1.112719178199768,
      "learning_rate": 0.00011867704280155642,
      "loss": 0.78,
      "step": 2723
    },
    {
      "epoch": 3.6662180349932703,
      "grad_norm": 1.2591971158981323,
      "learning_rate": 0.00011864711164322059,
      "loss": 0.6177,
      "step": 2724
    },
    {
      "epoch": 3.667563930013459,
      "grad_norm": 1.04838228225708,
      "learning_rate": 0.00011861718048488477,
      "loss": 0.5866,
      "step": 2725
    },
    {
      "epoch": 3.6689098250336474,
      "grad_norm": 0.9866601228713989,
      "learning_rate": 0.00011858724932654894,
      "loss": 0.8203,
      "step": 2726
    },
    {
      "epoch": 3.6702557200538357,
      "grad_norm": 1.2442936897277832,
      "learning_rate": 0.00011855731816821311,
      "loss": 0.6397,
      "step": 2727
    },
    {
      "epoch": 3.6716016150740245,
      "grad_norm": 1.220618724822998,
      "learning_rate": 0.00011852738700987728,
      "loss": 0.5991,
      "step": 2728
    },
    {
      "epoch": 3.672947510094213,
      "grad_norm": 1.3875160217285156,
      "learning_rate": 0.00011849745585154147,
      "loss": 1.0292,
      "step": 2729
    },
    {
      "epoch": 3.674293405114401,
      "grad_norm": 1.2878626585006714,
      "learning_rate": 0.00011846752469320563,
      "loss": 0.5945,
      "step": 2730
    },
    {
      "epoch": 3.6756393001345895,
      "grad_norm": 1.1362640857696533,
      "learning_rate": 0.0001184375935348698,
      "loss": 0.6431,
      "step": 2731
    },
    {
      "epoch": 3.676985195154778,
      "grad_norm": 1.1921643018722534,
      "learning_rate": 0.00011840766237653397,
      "loss": 0.6269,
      "step": 2732
    },
    {
      "epoch": 3.678331090174966,
      "grad_norm": 1.154590129852295,
      "learning_rate": 0.00011837773121819816,
      "loss": 0.9189,
      "step": 2733
    },
    {
      "epoch": 3.6796769851951545,
      "grad_norm": 1.0186306238174438,
      "learning_rate": 0.00011834780005986232,
      "loss": 0.5727,
      "step": 2734
    },
    {
      "epoch": 3.6810228802153433,
      "grad_norm": 1.2849973440170288,
      "learning_rate": 0.0001183178689015265,
      "loss": 0.6511,
      "step": 2735
    },
    {
      "epoch": 3.6823687752355316,
      "grad_norm": 1.2833765745162964,
      "learning_rate": 0.00011828793774319066,
      "loss": 0.7866,
      "step": 2736
    },
    {
      "epoch": 3.68371467025572,
      "grad_norm": 1.3812546730041504,
      "learning_rate": 0.00011825800658485485,
      "loss": 0.7448,
      "step": 2737
    },
    {
      "epoch": 3.6850605652759088,
      "grad_norm": 1.2306113243103027,
      "learning_rate": 0.00011822807542651901,
      "loss": 0.5922,
      "step": 2738
    },
    {
      "epoch": 3.686406460296097,
      "grad_norm": 0.9975624680519104,
      "learning_rate": 0.00011819814426818318,
      "loss": 0.5933,
      "step": 2739
    },
    {
      "epoch": 3.6877523553162854,
      "grad_norm": 1.122323751449585,
      "learning_rate": 0.00011816821310984735,
      "loss": 0.676,
      "step": 2740
    },
    {
      "epoch": 3.6890982503364738,
      "grad_norm": 1.084343433380127,
      "learning_rate": 0.00011813828195151154,
      "loss": 0.4997,
      "step": 2741
    },
    {
      "epoch": 3.690444145356662,
      "grad_norm": 1.1672015190124512,
      "learning_rate": 0.0001181083507931757,
      "loss": 0.7544,
      "step": 2742
    },
    {
      "epoch": 3.6917900403768504,
      "grad_norm": 1.2362663745880127,
      "learning_rate": 0.00011807841963483987,
      "loss": 0.6823,
      "step": 2743
    },
    {
      "epoch": 3.6931359353970388,
      "grad_norm": 1.182132363319397,
      "learning_rate": 0.00011804848847650404,
      "loss": 0.6011,
      "step": 2744
    },
    {
      "epoch": 3.6944818304172276,
      "grad_norm": 1.3424034118652344,
      "learning_rate": 0.00011801855731816823,
      "loss": 0.7023,
      "step": 2745
    },
    {
      "epoch": 3.695827725437416,
      "grad_norm": 1.109892725944519,
      "learning_rate": 0.0001179886261598324,
      "loss": 0.6283,
      "step": 2746
    },
    {
      "epoch": 3.6971736204576042,
      "grad_norm": 1.2006036043167114,
      "learning_rate": 0.00011795869500149656,
      "loss": 0.6199,
      "step": 2747
    },
    {
      "epoch": 3.698519515477793,
      "grad_norm": 1.2820216417312622,
      "learning_rate": 0.00011792876384316073,
      "loss": 0.5321,
      "step": 2748
    },
    {
      "epoch": 3.6998654104979813,
      "grad_norm": 1.2411235570907593,
      "learning_rate": 0.00011789883268482492,
      "loss": 0.772,
      "step": 2749
    },
    {
      "epoch": 3.7012113055181697,
      "grad_norm": 1.210644245147705,
      "learning_rate": 0.00011786890152648908,
      "loss": 0.6364,
      "step": 2750
    },
    {
      "epoch": 3.702557200538358,
      "grad_norm": 1.1425342559814453,
      "learning_rate": 0.00011783897036815325,
      "loss": 0.7091,
      "step": 2751
    },
    {
      "epoch": 3.7039030955585464,
      "grad_norm": 0.9507461786270142,
      "learning_rate": 0.00011780903920981742,
      "loss": 0.4244,
      "step": 2752
    },
    {
      "epoch": 3.7052489905787347,
      "grad_norm": 1.2080540657043457,
      "learning_rate": 0.0001177791080514816,
      "loss": 0.5653,
      "step": 2753
    },
    {
      "epoch": 3.706594885598923,
      "grad_norm": 1.251452922821045,
      "learning_rate": 0.00011774917689314578,
      "loss": 0.5718,
      "step": 2754
    },
    {
      "epoch": 3.707940780619112,
      "grad_norm": 1.1610260009765625,
      "learning_rate": 0.00011771924573480994,
      "loss": 0.4816,
      "step": 2755
    },
    {
      "epoch": 3.7092866756393,
      "grad_norm": 1.1794493198394775,
      "learning_rate": 0.00011768931457647411,
      "loss": 0.7265,
      "step": 2756
    },
    {
      "epoch": 3.7106325706594885,
      "grad_norm": 1.2446093559265137,
      "learning_rate": 0.0001176593834181383,
      "loss": 0.4848,
      "step": 2757
    },
    {
      "epoch": 3.711978465679677,
      "grad_norm": 1.410474419593811,
      "learning_rate": 0.00011762945225980247,
      "loss": 0.7188,
      "step": 2758
    },
    {
      "epoch": 3.7133243606998656,
      "grad_norm": 1.1738202571868896,
      "learning_rate": 0.00011759952110146663,
      "loss": 0.7574,
      "step": 2759
    },
    {
      "epoch": 3.714670255720054,
      "grad_norm": 1.3544126749038696,
      "learning_rate": 0.0001175695899431308,
      "loss": 0.6351,
      "step": 2760
    },
    {
      "epoch": 3.7160161507402423,
      "grad_norm": 1.106383204460144,
      "learning_rate": 0.00011753965878479499,
      "loss": 0.5779,
      "step": 2761
    },
    {
      "epoch": 3.7173620457604306,
      "grad_norm": 1.0983963012695312,
      "learning_rate": 0.00011750972762645916,
      "loss": 0.5193,
      "step": 2762
    },
    {
      "epoch": 3.718707940780619,
      "grad_norm": 1.0308719873428345,
      "learning_rate": 0.00011747979646812332,
      "loss": 0.587,
      "step": 2763
    },
    {
      "epoch": 3.7200538358008073,
      "grad_norm": 1.2849159240722656,
      "learning_rate": 0.0001174498653097875,
      "loss": 0.6591,
      "step": 2764
    },
    {
      "epoch": 3.721399730820996,
      "grad_norm": 1.195667028427124,
      "learning_rate": 0.00011741993415145168,
      "loss": 0.6106,
      "step": 2765
    },
    {
      "epoch": 3.7227456258411844,
      "grad_norm": 1.1315698623657227,
      "learning_rate": 0.00011739000299311585,
      "loss": 0.5934,
      "step": 2766
    },
    {
      "epoch": 3.7240915208613727,
      "grad_norm": 1.2184510231018066,
      "learning_rate": 0.00011736007183478001,
      "loss": 0.6779,
      "step": 2767
    },
    {
      "epoch": 3.725437415881561,
      "grad_norm": 1.0092518329620361,
      "learning_rate": 0.00011733014067644418,
      "loss": 0.5805,
      "step": 2768
    },
    {
      "epoch": 3.72678331090175,
      "grad_norm": 1.0098155736923218,
      "learning_rate": 0.00011730020951810837,
      "loss": 0.4826,
      "step": 2769
    },
    {
      "epoch": 3.728129205921938,
      "grad_norm": 1.1495417356491089,
      "learning_rate": 0.00011727027835977254,
      "loss": 0.5896,
      "step": 2770
    },
    {
      "epoch": 3.7294751009421265,
      "grad_norm": 1.335418462753296,
      "learning_rate": 0.0001172403472014367,
      "loss": 0.6514,
      "step": 2771
    },
    {
      "epoch": 3.730820995962315,
      "grad_norm": 1.08863365650177,
      "learning_rate": 0.00011721041604310087,
      "loss": 0.742,
      "step": 2772
    },
    {
      "epoch": 3.732166890982503,
      "grad_norm": 1.198919653892517,
      "learning_rate": 0.00011718048488476506,
      "loss": 0.5989,
      "step": 2773
    },
    {
      "epoch": 3.7335127860026915,
      "grad_norm": 1.0789591073989868,
      "learning_rate": 0.00011715055372642923,
      "loss": 0.6601,
      "step": 2774
    },
    {
      "epoch": 3.7348586810228803,
      "grad_norm": 1.0670901536941528,
      "learning_rate": 0.0001171206225680934,
      "loss": 0.6166,
      "step": 2775
    },
    {
      "epoch": 3.7362045760430687,
      "grad_norm": 0.9722210168838501,
      "learning_rate": 0.00011709069140975756,
      "loss": 0.5888,
      "step": 2776
    },
    {
      "epoch": 3.737550471063257,
      "grad_norm": 1.3597712516784668,
      "learning_rate": 0.00011706076025142175,
      "loss": 0.7619,
      "step": 2777
    },
    {
      "epoch": 3.7388963660834453,
      "grad_norm": 1.0940123796463013,
      "learning_rate": 0.00011703082909308592,
      "loss": 0.5261,
      "step": 2778
    },
    {
      "epoch": 3.740242261103634,
      "grad_norm": 1.0299404859542847,
      "learning_rate": 0.00011700089793475009,
      "loss": 0.5938,
      "step": 2779
    },
    {
      "epoch": 3.7415881561238225,
      "grad_norm": 1.0763765573501587,
      "learning_rate": 0.00011697096677641425,
      "loss": 0.9184,
      "step": 2780
    },
    {
      "epoch": 3.742934051144011,
      "grad_norm": 1.0696169137954712,
      "learning_rate": 0.00011694103561807844,
      "loss": 0.5765,
      "step": 2781
    },
    {
      "epoch": 3.744279946164199,
      "grad_norm": 1.1173064708709717,
      "learning_rate": 0.0001169111044597426,
      "loss": 0.4984,
      "step": 2782
    },
    {
      "epoch": 3.7456258411843875,
      "grad_norm": 1.1366963386535645,
      "learning_rate": 0.00011688117330140678,
      "loss": 0.5738,
      "step": 2783
    },
    {
      "epoch": 3.746971736204576,
      "grad_norm": 0.9700462222099304,
      "learning_rate": 0.00011685124214307094,
      "loss": 0.6767,
      "step": 2784
    },
    {
      "epoch": 3.7483176312247646,
      "grad_norm": 1.2144063711166382,
      "learning_rate": 0.00011682131098473513,
      "loss": 0.8139,
      "step": 2785
    },
    {
      "epoch": 3.749663526244953,
      "grad_norm": 1.28984534740448,
      "learning_rate": 0.0001167913798263993,
      "loss": 0.6602,
      "step": 2786
    },
    {
      "epoch": 3.7510094212651413,
      "grad_norm": 1.2381420135498047,
      "learning_rate": 0.00011676144866806347,
      "loss": 0.5625,
      "step": 2787
    },
    {
      "epoch": 3.7523553162853296,
      "grad_norm": 1.0643612146377563,
      "learning_rate": 0.00011673151750972763,
      "loss": 0.6153,
      "step": 2788
    },
    {
      "epoch": 3.7537012113055184,
      "grad_norm": 1.1550050973892212,
      "learning_rate": 0.00011670158635139182,
      "loss": 0.7246,
      "step": 2789
    },
    {
      "epoch": 3.7550471063257067,
      "grad_norm": 1.0956088304519653,
      "learning_rate": 0.00011667165519305599,
      "loss": 0.529,
      "step": 2790
    },
    {
      "epoch": 3.756393001345895,
      "grad_norm": 1.1537758111953735,
      "learning_rate": 0.00011664172403472016,
      "loss": 0.5524,
      "step": 2791
    },
    {
      "epoch": 3.7577388963660834,
      "grad_norm": 1.1078660488128662,
      "learning_rate": 0.00011661179287638432,
      "loss": 0.7906,
      "step": 2792
    },
    {
      "epoch": 3.7590847913862717,
      "grad_norm": 1.1838587522506714,
      "learning_rate": 0.00011658186171804851,
      "loss": 0.7761,
      "step": 2793
    },
    {
      "epoch": 3.76043068640646,
      "grad_norm": 1.1506197452545166,
      "learning_rate": 0.00011655193055971268,
      "loss": 0.7726,
      "step": 2794
    },
    {
      "epoch": 3.761776581426649,
      "grad_norm": 1.162227988243103,
      "learning_rate": 0.00011652199940137685,
      "loss": 0.69,
      "step": 2795
    },
    {
      "epoch": 3.763122476446837,
      "grad_norm": 1.1464815139770508,
      "learning_rate": 0.00011649206824304101,
      "loss": 0.8159,
      "step": 2796
    },
    {
      "epoch": 3.7644683714670255,
      "grad_norm": 1.1190122365951538,
      "learning_rate": 0.0001164621370847052,
      "loss": 0.5947,
      "step": 2797
    },
    {
      "epoch": 3.765814266487214,
      "grad_norm": 1.1319782733917236,
      "learning_rate": 0.00011643220592636937,
      "loss": 0.4633,
      "step": 2798
    },
    {
      "epoch": 3.7671601615074026,
      "grad_norm": 1.1715842485427856,
      "learning_rate": 0.00011640227476803354,
      "loss": 0.5361,
      "step": 2799
    },
    {
      "epoch": 3.768506056527591,
      "grad_norm": 1.2927026748657227,
      "learning_rate": 0.0001163723436096977,
      "loss": 0.5591,
      "step": 2800
    },
    {
      "epoch": 3.7698519515477793,
      "grad_norm": 1.0724362134933472,
      "learning_rate": 0.00011634241245136189,
      "loss": 0.5463,
      "step": 2801
    },
    {
      "epoch": 3.7711978465679676,
      "grad_norm": 0.9897067546844482,
      "learning_rate": 0.00011631248129302606,
      "loss": 0.4317,
      "step": 2802
    },
    {
      "epoch": 3.772543741588156,
      "grad_norm": 0.9767532348632812,
      "learning_rate": 0.00011628255013469023,
      "loss": 0.4832,
      "step": 2803
    },
    {
      "epoch": 3.7738896366083443,
      "grad_norm": 1.2636008262634277,
      "learning_rate": 0.0001162526189763544,
      "loss": 0.622,
      "step": 2804
    },
    {
      "epoch": 3.775235531628533,
      "grad_norm": 1.1875842809677124,
      "learning_rate": 0.00011622268781801858,
      "loss": 0.5111,
      "step": 2805
    },
    {
      "epoch": 3.7765814266487214,
      "grad_norm": 1.5399835109710693,
      "learning_rate": 0.00011619275665968275,
      "loss": 0.594,
      "step": 2806
    },
    {
      "epoch": 3.7779273216689098,
      "grad_norm": 0.8909545540809631,
      "learning_rate": 0.00011616282550134692,
      "loss": 1.0246,
      "step": 2807
    },
    {
      "epoch": 3.779273216689098,
      "grad_norm": 0.9135534167289734,
      "learning_rate": 0.00011613289434301109,
      "loss": 0.6051,
      "step": 2808
    },
    {
      "epoch": 3.780619111709287,
      "grad_norm": 1.0308552980422974,
      "learning_rate": 0.00011610296318467527,
      "loss": 0.5989,
      "step": 2809
    },
    {
      "epoch": 3.781965006729475,
      "grad_norm": 1.2750165462493896,
      "learning_rate": 0.00011607303202633944,
      "loss": 0.5436,
      "step": 2810
    },
    {
      "epoch": 3.7833109017496636,
      "grad_norm": 1.055856466293335,
      "learning_rate": 0.0001160431008680036,
      "loss": 0.6827,
      "step": 2811
    },
    {
      "epoch": 3.784656796769852,
      "grad_norm": 1.1995255947113037,
      "learning_rate": 0.00011601316970966778,
      "loss": 0.7496,
      "step": 2812
    },
    {
      "epoch": 3.7860026917900402,
      "grad_norm": 1.3349766731262207,
      "learning_rate": 0.00011598323855133196,
      "loss": 0.6092,
      "step": 2813
    },
    {
      "epoch": 3.7873485868102286,
      "grad_norm": 1.2768560647964478,
      "learning_rate": 0.00011595330739299613,
      "loss": 0.7603,
      "step": 2814
    },
    {
      "epoch": 3.7886944818304173,
      "grad_norm": 1.0478423833847046,
      "learning_rate": 0.0001159233762346603,
      "loss": 0.5913,
      "step": 2815
    },
    {
      "epoch": 3.7900403768506057,
      "grad_norm": 1.2847352027893066,
      "learning_rate": 0.00011589344507632447,
      "loss": 0.6039,
      "step": 2816
    },
    {
      "epoch": 3.791386271870794,
      "grad_norm": 1.1200308799743652,
      "learning_rate": 0.00011586351391798865,
      "loss": 0.6288,
      "step": 2817
    },
    {
      "epoch": 3.7927321668909824,
      "grad_norm": 1.2030260562896729,
      "learning_rate": 0.00011583358275965282,
      "loss": 0.6878,
      "step": 2818
    },
    {
      "epoch": 3.794078061911171,
      "grad_norm": 1.1178392171859741,
      "learning_rate": 0.00011580365160131699,
      "loss": 0.6405,
      "step": 2819
    },
    {
      "epoch": 3.7954239569313595,
      "grad_norm": 1.0845463275909424,
      "learning_rate": 0.00011577372044298116,
      "loss": 0.4127,
      "step": 2820
    },
    {
      "epoch": 3.796769851951548,
      "grad_norm": 1.203805923461914,
      "learning_rate": 0.00011574378928464534,
      "loss": 0.7234,
      "step": 2821
    },
    {
      "epoch": 3.798115746971736,
      "grad_norm": 1.1485446691513062,
      "learning_rate": 0.00011571385812630951,
      "loss": 0.5086,
      "step": 2822
    },
    {
      "epoch": 3.7994616419919245,
      "grad_norm": 1.2080172300338745,
      "learning_rate": 0.00011568392696797368,
      "loss": 0.9385,
      "step": 2823
    },
    {
      "epoch": 3.800807537012113,
      "grad_norm": 1.313232421875,
      "learning_rate": 0.00011565399580963785,
      "loss": 0.8171,
      "step": 2824
    },
    {
      "epoch": 3.8021534320323016,
      "grad_norm": 1.072425365447998,
      "learning_rate": 0.000115624064651302,
      "loss": 0.7183,
      "step": 2825
    },
    {
      "epoch": 3.80349932705249,
      "grad_norm": 1.0740697383880615,
      "learning_rate": 0.00011559413349296617,
      "loss": 0.52,
      "step": 2826
    },
    {
      "epoch": 3.8048452220726783,
      "grad_norm": 1.2486469745635986,
      "learning_rate": 0.00011556420233463034,
      "loss": 0.6127,
      "step": 2827
    },
    {
      "epoch": 3.8061911170928666,
      "grad_norm": 1.315575361251831,
      "learning_rate": 0.00011553427117629452,
      "loss": 0.6653,
      "step": 2828
    },
    {
      "epoch": 3.8075370121130554,
      "grad_norm": 1.245076060295105,
      "learning_rate": 0.00011550434001795869,
      "loss": 0.5721,
      "step": 2829
    },
    {
      "epoch": 3.8088829071332437,
      "grad_norm": 1.1133564710617065,
      "learning_rate": 0.00011547440885962286,
      "loss": 0.5466,
      "step": 2830
    },
    {
      "epoch": 3.810228802153432,
      "grad_norm": 1.2063056230545044,
      "learning_rate": 0.00011544447770128703,
      "loss": 0.7559,
      "step": 2831
    },
    {
      "epoch": 3.8115746971736204,
      "grad_norm": 1.2919156551361084,
      "learning_rate": 0.00011541454654295121,
      "loss": 0.7458,
      "step": 2832
    },
    {
      "epoch": 3.8129205921938087,
      "grad_norm": 1.0883922576904297,
      "learning_rate": 0.00011538461538461538,
      "loss": 0.5724,
      "step": 2833
    },
    {
      "epoch": 3.814266487213997,
      "grad_norm": 1.2034395933151245,
      "learning_rate": 0.00011535468422627955,
      "loss": 0.6999,
      "step": 2834
    },
    {
      "epoch": 3.815612382234186,
      "grad_norm": 1.2030549049377441,
      "learning_rate": 0.00011532475306794372,
      "loss": 0.5734,
      "step": 2835
    },
    {
      "epoch": 3.816958277254374,
      "grad_norm": 1.1134979724884033,
      "learning_rate": 0.0001152948219096079,
      "loss": 0.5808,
      "step": 2836
    },
    {
      "epoch": 3.8183041722745625,
      "grad_norm": 1.2843437194824219,
      "learning_rate": 0.00011526489075127207,
      "loss": 0.6033,
      "step": 2837
    },
    {
      "epoch": 3.819650067294751,
      "grad_norm": 1.3415160179138184,
      "learning_rate": 0.00011523495959293624,
      "loss": 0.4613,
      "step": 2838
    },
    {
      "epoch": 3.8209959623149397,
      "grad_norm": 1.014951467514038,
      "learning_rate": 0.00011520502843460041,
      "loss": 0.9477,
      "step": 2839
    },
    {
      "epoch": 3.822341857335128,
      "grad_norm": 1.1019245386123657,
      "learning_rate": 0.00011517509727626459,
      "loss": 0.7588,
      "step": 2840
    },
    {
      "epoch": 3.8236877523553163,
      "grad_norm": 1.0210349559783936,
      "learning_rate": 0.00011514516611792876,
      "loss": 0.5429,
      "step": 2841
    },
    {
      "epoch": 3.8250336473755047,
      "grad_norm": 1.2539875507354736,
      "learning_rate": 0.00011511523495959293,
      "loss": 0.6293,
      "step": 2842
    },
    {
      "epoch": 3.826379542395693,
      "grad_norm": 1.0911102294921875,
      "learning_rate": 0.0001150853038012571,
      "loss": 0.7115,
      "step": 2843
    },
    {
      "epoch": 3.8277254374158813,
      "grad_norm": 1.18852961063385,
      "learning_rate": 0.00011505537264292128,
      "loss": 0.5128,
      "step": 2844
    },
    {
      "epoch": 3.82907133243607,
      "grad_norm": 1.2402375936508179,
      "learning_rate": 0.00011502544148458545,
      "loss": 0.671,
      "step": 2845
    },
    {
      "epoch": 3.8304172274562585,
      "grad_norm": 1.249024748802185,
      "learning_rate": 0.00011499551032624962,
      "loss": 0.616,
      "step": 2846
    },
    {
      "epoch": 3.831763122476447,
      "grad_norm": 1.2178224325180054,
      "learning_rate": 0.00011496557916791379,
      "loss": 0.4819,
      "step": 2847
    },
    {
      "epoch": 3.833109017496635,
      "grad_norm": 1.0457723140716553,
      "learning_rate": 0.00011493564800957797,
      "loss": 0.5095,
      "step": 2848
    },
    {
      "epoch": 3.834454912516824,
      "grad_norm": 1.3008571863174438,
      "learning_rate": 0.00011490571685124214,
      "loss": 0.5894,
      "step": 2849
    },
    {
      "epoch": 3.8358008075370122,
      "grad_norm": 1.2348273992538452,
      "learning_rate": 0.00011487578569290631,
      "loss": 0.5782,
      "step": 2850
    },
    {
      "epoch": 3.8371467025572006,
      "grad_norm": 1.1957604885101318,
      "learning_rate": 0.00011484585453457048,
      "loss": 0.5624,
      "step": 2851
    },
    {
      "epoch": 3.838492597577389,
      "grad_norm": 1.1718330383300781,
      "learning_rate": 0.00011481592337623466,
      "loss": 0.7873,
      "step": 2852
    },
    {
      "epoch": 3.8398384925975773,
      "grad_norm": 1.0823463201522827,
      "learning_rate": 0.00011478599221789883,
      "loss": 0.6533,
      "step": 2853
    },
    {
      "epoch": 3.8411843876177656,
      "grad_norm": 1.1034915447235107,
      "learning_rate": 0.000114756061059563,
      "loss": 0.646,
      "step": 2854
    },
    {
      "epoch": 3.8425302826379544,
      "grad_norm": 1.1073567867279053,
      "learning_rate": 0.00011472612990122717,
      "loss": 0.6322,
      "step": 2855
    },
    {
      "epoch": 3.8438761776581427,
      "grad_norm": 1.2767918109893799,
      "learning_rate": 0.00011469619874289135,
      "loss": 0.6852,
      "step": 2856
    },
    {
      "epoch": 3.845222072678331,
      "grad_norm": 1.1313793659210205,
      "learning_rate": 0.00011466626758455552,
      "loss": 0.6699,
      "step": 2857
    },
    {
      "epoch": 3.8465679676985194,
      "grad_norm": 1.1131333112716675,
      "learning_rate": 0.00011463633642621969,
      "loss": 0.747,
      "step": 2858
    },
    {
      "epoch": 3.847913862718708,
      "grad_norm": 1.4136830568313599,
      "learning_rate": 0.00011460640526788386,
      "loss": 0.6,
      "step": 2859
    },
    {
      "epoch": 3.8492597577388965,
      "grad_norm": 1.1374588012695312,
      "learning_rate": 0.00011457647410954804,
      "loss": 0.6182,
      "step": 2860
    },
    {
      "epoch": 3.850605652759085,
      "grad_norm": 1.3994802236557007,
      "learning_rate": 0.00011454654295121221,
      "loss": 0.9021,
      "step": 2861
    },
    {
      "epoch": 3.851951547779273,
      "grad_norm": 1.3249902725219727,
      "learning_rate": 0.00011451661179287638,
      "loss": 0.649,
      "step": 2862
    },
    {
      "epoch": 3.8532974427994615,
      "grad_norm": 1.4054834842681885,
      "learning_rate": 0.00011448668063454055,
      "loss": 0.6984,
      "step": 2863
    },
    {
      "epoch": 3.85464333781965,
      "grad_norm": 0.9218667149543762,
      "learning_rate": 0.00011445674947620473,
      "loss": 0.6454,
      "step": 2864
    },
    {
      "epoch": 3.8559892328398386,
      "grad_norm": 1.2141033411026,
      "learning_rate": 0.0001144268183178689,
      "loss": 0.7397,
      "step": 2865
    },
    {
      "epoch": 3.857335127860027,
      "grad_norm": 1.058666706085205,
      "learning_rate": 0.00011439688715953307,
      "loss": 0.4947,
      "step": 2866
    },
    {
      "epoch": 3.8586810228802153,
      "grad_norm": 1.211014747619629,
      "learning_rate": 0.00011436695600119724,
      "loss": 0.6662,
      "step": 2867
    },
    {
      "epoch": 3.8600269179004036,
      "grad_norm": 1.463300347328186,
      "learning_rate": 0.00011433702484286142,
      "loss": 0.6642,
      "step": 2868
    },
    {
      "epoch": 3.8613728129205924,
      "grad_norm": 1.1775673627853394,
      "learning_rate": 0.00011430709368452559,
      "loss": 0.5587,
      "step": 2869
    },
    {
      "epoch": 3.8627187079407808,
      "grad_norm": 0.9255548119544983,
      "learning_rate": 0.00011427716252618976,
      "loss": 0.5203,
      "step": 2870
    },
    {
      "epoch": 3.864064602960969,
      "grad_norm": 1.1705851554870605,
      "learning_rate": 0.00011424723136785393,
      "loss": 0.5785,
      "step": 2871
    },
    {
      "epoch": 3.8654104979811574,
      "grad_norm": 1.3060117959976196,
      "learning_rate": 0.00011421730020951811,
      "loss": 0.7271,
      "step": 2872
    },
    {
      "epoch": 3.8667563930013458,
      "grad_norm": 1.3030439615249634,
      "learning_rate": 0.00011418736905118228,
      "loss": 0.599,
      "step": 2873
    },
    {
      "epoch": 3.868102288021534,
      "grad_norm": 1.340171217918396,
      "learning_rate": 0.00011415743789284645,
      "loss": 0.77,
      "step": 2874
    },
    {
      "epoch": 3.869448183041723,
      "grad_norm": 0.9457437992095947,
      "learning_rate": 0.00011412750673451062,
      "loss": 0.4819,
      "step": 2875
    },
    {
      "epoch": 3.8707940780619112,
      "grad_norm": 1.194411277770996,
      "learning_rate": 0.0001140975755761748,
      "loss": 0.6774,
      "step": 2876
    },
    {
      "epoch": 3.8721399730820996,
      "grad_norm": 0.9555525779724121,
      "learning_rate": 0.00011406764441783897,
      "loss": 0.4417,
      "step": 2877
    },
    {
      "epoch": 3.873485868102288,
      "grad_norm": 1.1240681409835815,
      "learning_rate": 0.00011403771325950314,
      "loss": 0.8156,
      "step": 2878
    },
    {
      "epoch": 3.8748317631224767,
      "grad_norm": 1.1516225337982178,
      "learning_rate": 0.00011400778210116731,
      "loss": 0.5687,
      "step": 2879
    },
    {
      "epoch": 3.876177658142665,
      "grad_norm": 1.1230287551879883,
      "learning_rate": 0.0001139778509428315,
      "loss": 0.6173,
      "step": 2880
    },
    {
      "epoch": 3.8775235531628534,
      "grad_norm": 1.1224197149276733,
      "learning_rate": 0.00011394791978449566,
      "loss": 0.4942,
      "step": 2881
    },
    {
      "epoch": 3.8788694481830417,
      "grad_norm": 1.2281242609024048,
      "learning_rate": 0.00011391798862615983,
      "loss": 0.7552,
      "step": 2882
    },
    {
      "epoch": 3.88021534320323,
      "grad_norm": 1.1604629755020142,
      "learning_rate": 0.000113888057467824,
      "loss": 0.7487,
      "step": 2883
    },
    {
      "epoch": 3.8815612382234184,
      "grad_norm": 1.1149169206619263,
      "learning_rate": 0.00011385812630948818,
      "loss": 0.5768,
      "step": 2884
    },
    {
      "epoch": 3.882907133243607,
      "grad_norm": 1.0510106086730957,
      "learning_rate": 0.00011382819515115235,
      "loss": 0.5981,
      "step": 2885
    },
    {
      "epoch": 3.8842530282637955,
      "grad_norm": 1.3931314945220947,
      "learning_rate": 0.00011379826399281652,
      "loss": 0.7521,
      "step": 2886
    },
    {
      "epoch": 3.885598923283984,
      "grad_norm": 1.211598515510559,
      "learning_rate": 0.00011376833283448069,
      "loss": 0.674,
      "step": 2887
    },
    {
      "epoch": 3.886944818304172,
      "grad_norm": 1.3871791362762451,
      "learning_rate": 0.00011373840167614486,
      "loss": 0.6558,
      "step": 2888
    },
    {
      "epoch": 3.888290713324361,
      "grad_norm": 1.3984466791152954,
      "learning_rate": 0.00011370847051780904,
      "loss": 0.5967,
      "step": 2889
    },
    {
      "epoch": 3.8896366083445493,
      "grad_norm": 1.3865104913711548,
      "learning_rate": 0.00011367853935947321,
      "loss": 0.6938,
      "step": 2890
    },
    {
      "epoch": 3.8909825033647376,
      "grad_norm": 1.3783267736434937,
      "learning_rate": 0.00011364860820113738,
      "loss": 0.5896,
      "step": 2891
    },
    {
      "epoch": 3.892328398384926,
      "grad_norm": 1.1073851585388184,
      "learning_rate": 0.00011361867704280155,
      "loss": 0.5657,
      "step": 2892
    },
    {
      "epoch": 3.8936742934051143,
      "grad_norm": 1.1135282516479492,
      "learning_rate": 0.00011358874588446573,
      "loss": 0.4946,
      "step": 2893
    },
    {
      "epoch": 3.8950201884253026,
      "grad_norm": 1.4928395748138428,
      "learning_rate": 0.0001135588147261299,
      "loss": 0.6882,
      "step": 2894
    },
    {
      "epoch": 3.8963660834454914,
      "grad_norm": 1.1948119401931763,
      "learning_rate": 0.00011352888356779407,
      "loss": 0.5962,
      "step": 2895
    },
    {
      "epoch": 3.8977119784656797,
      "grad_norm": 1.1730523109436035,
      "learning_rate": 0.00011349895240945824,
      "loss": 0.5667,
      "step": 2896
    },
    {
      "epoch": 3.899057873485868,
      "grad_norm": 1.2726867198944092,
      "learning_rate": 0.00011346902125112242,
      "loss": 0.5438,
      "step": 2897
    },
    {
      "epoch": 3.9004037685060564,
      "grad_norm": 1.0579503774642944,
      "learning_rate": 0.0001134390900927866,
      "loss": 0.5193,
      "step": 2898
    },
    {
      "epoch": 3.901749663526245,
      "grad_norm": 1.1963163614273071,
      "learning_rate": 0.00011340915893445076,
      "loss": 0.7315,
      "step": 2899
    },
    {
      "epoch": 3.9030955585464335,
      "grad_norm": 1.1496354341506958,
      "learning_rate": 0.00011337922777611493,
      "loss": 0.475,
      "step": 2900
    },
    {
      "epoch": 3.904441453566622,
      "grad_norm": 1.038979411125183,
      "learning_rate": 0.00011334929661777911,
      "loss": 0.6912,
      "step": 2901
    },
    {
      "epoch": 3.90578734858681,
      "grad_norm": 1.336875081062317,
      "learning_rate": 0.00011331936545944328,
      "loss": 0.6291,
      "step": 2902
    },
    {
      "epoch": 3.9071332436069985,
      "grad_norm": 1.3733240365982056,
      "learning_rate": 0.00011328943430110745,
      "loss": 0.6218,
      "step": 2903
    },
    {
      "epoch": 3.908479138627187,
      "grad_norm": 1.2994393110275269,
      "learning_rate": 0.00011325950314277162,
      "loss": 0.6695,
      "step": 2904
    },
    {
      "epoch": 3.9098250336473757,
      "grad_norm": 1.3478353023529053,
      "learning_rate": 0.0001132295719844358,
      "loss": 0.5748,
      "step": 2905
    },
    {
      "epoch": 3.911170928667564,
      "grad_norm": 1.4451321363449097,
      "learning_rate": 0.00011319964082609997,
      "loss": 0.7732,
      "step": 2906
    },
    {
      "epoch": 3.9125168236877523,
      "grad_norm": 1.4225527048110962,
      "learning_rate": 0.00011316970966776414,
      "loss": 0.5962,
      "step": 2907
    },
    {
      "epoch": 3.9138627187079407,
      "grad_norm": 1.0984162092208862,
      "learning_rate": 0.00011313977850942831,
      "loss": 0.5581,
      "step": 2908
    },
    {
      "epoch": 3.9152086137281294,
      "grad_norm": 1.1538875102996826,
      "learning_rate": 0.0001131098473510925,
      "loss": 0.6614,
      "step": 2909
    },
    {
      "epoch": 3.916554508748318,
      "grad_norm": 1.2546509504318237,
      "learning_rate": 0.00011307991619275666,
      "loss": 0.5678,
      "step": 2910
    },
    {
      "epoch": 3.917900403768506,
      "grad_norm": 1.2908573150634766,
      "learning_rate": 0.00011304998503442083,
      "loss": 0.519,
      "step": 2911
    },
    {
      "epoch": 3.9192462987886945,
      "grad_norm": 1.2047103643417358,
      "learning_rate": 0.000113020053876085,
      "loss": 0.6483,
      "step": 2912
    },
    {
      "epoch": 3.920592193808883,
      "grad_norm": 1.1005232334136963,
      "learning_rate": 0.00011299012271774918,
      "loss": 0.5394,
      "step": 2913
    },
    {
      "epoch": 3.921938088829071,
      "grad_norm": 1.1215555667877197,
      "learning_rate": 0.00011296019155941335,
      "loss": 0.642,
      "step": 2914
    },
    {
      "epoch": 3.92328398384926,
      "grad_norm": 1.0618616342544556,
      "learning_rate": 0.00011293026040107752,
      "loss": 0.6066,
      "step": 2915
    },
    {
      "epoch": 3.9246298788694483,
      "grad_norm": 1.1407073736190796,
      "learning_rate": 0.00011290032924274169,
      "loss": 0.6088,
      "step": 2916
    },
    {
      "epoch": 3.9259757738896366,
      "grad_norm": 1.1032650470733643,
      "learning_rate": 0.00011287039808440587,
      "loss": 0.5457,
      "step": 2917
    },
    {
      "epoch": 3.927321668909825,
      "grad_norm": 1.2803635597229004,
      "learning_rate": 0.00011284046692607004,
      "loss": 0.6738,
      "step": 2918
    },
    {
      "epoch": 3.9286675639300137,
      "grad_norm": 1.488761067390442,
      "learning_rate": 0.00011281053576773421,
      "loss": 0.6843,
      "step": 2919
    },
    {
      "epoch": 3.930013458950202,
      "grad_norm": 1.2001519203186035,
      "learning_rate": 0.00011278060460939838,
      "loss": 0.5374,
      "step": 2920
    },
    {
      "epoch": 3.9313593539703904,
      "grad_norm": 1.0879672765731812,
      "learning_rate": 0.00011275067345106257,
      "loss": 0.5707,
      "step": 2921
    },
    {
      "epoch": 3.9327052489905787,
      "grad_norm": 1.068346619606018,
      "learning_rate": 0.00011272074229272673,
      "loss": 0.5013,
      "step": 2922
    },
    {
      "epoch": 3.934051144010767,
      "grad_norm": 1.1498916149139404,
      "learning_rate": 0.0001126908111343909,
      "loss": 0.587,
      "step": 2923
    },
    {
      "epoch": 3.9353970390309554,
      "grad_norm": 1.4180973768234253,
      "learning_rate": 0.00011266087997605507,
      "loss": 0.7037,
      "step": 2924
    },
    {
      "epoch": 3.936742934051144,
      "grad_norm": 1.2185887098312378,
      "learning_rate": 0.00011263094881771926,
      "loss": 0.6228,
      "step": 2925
    },
    {
      "epoch": 3.9380888290713325,
      "grad_norm": 1.2338358163833618,
      "learning_rate": 0.00011260101765938342,
      "loss": 0.5952,
      "step": 2926
    },
    {
      "epoch": 3.939434724091521,
      "grad_norm": 1.3531032800674438,
      "learning_rate": 0.0001125710865010476,
      "loss": 0.607,
      "step": 2927
    },
    {
      "epoch": 3.940780619111709,
      "grad_norm": 1.281791090965271,
      "learning_rate": 0.00011254115534271176,
      "loss": 0.6128,
      "step": 2928
    },
    {
      "epoch": 3.942126514131898,
      "grad_norm": 1.116727352142334,
      "learning_rate": 0.00011251122418437595,
      "loss": 0.5907,
      "step": 2929
    },
    {
      "epoch": 3.9434724091520863,
      "grad_norm": 1.1763449907302856,
      "learning_rate": 0.00011248129302604011,
      "loss": 0.8056,
      "step": 2930
    },
    {
      "epoch": 3.9448183041722746,
      "grad_norm": 1.1732267141342163,
      "learning_rate": 0.00011245136186770428,
      "loss": 0.5654,
      "step": 2931
    },
    {
      "epoch": 3.946164199192463,
      "grad_norm": 1.2433078289031982,
      "learning_rate": 0.00011242143070936845,
      "loss": 0.7941,
      "step": 2932
    },
    {
      "epoch": 3.9475100942126513,
      "grad_norm": 1.1260619163513184,
      "learning_rate": 0.00011239149955103264,
      "loss": 0.6085,
      "step": 2933
    },
    {
      "epoch": 3.9488559892328396,
      "grad_norm": 1.2854549884796143,
      "learning_rate": 0.0001123615683926968,
      "loss": 0.6633,
      "step": 2934
    },
    {
      "epoch": 3.9502018842530284,
      "grad_norm": 1.1608014106750488,
      "learning_rate": 0.00011233163723436097,
      "loss": 0.7186,
      "step": 2935
    },
    {
      "epoch": 3.9515477792732168,
      "grad_norm": 1.3572781085968018,
      "learning_rate": 0.00011230170607602514,
      "loss": 0.6327,
      "step": 2936
    },
    {
      "epoch": 3.952893674293405,
      "grad_norm": 1.2123689651489258,
      "learning_rate": 0.00011227177491768933,
      "loss": 0.5621,
      "step": 2937
    },
    {
      "epoch": 3.9542395693135934,
      "grad_norm": 1.1817744970321655,
      "learning_rate": 0.0001122418437593535,
      "loss": 0.7284,
      "step": 2938
    },
    {
      "epoch": 3.955585464333782,
      "grad_norm": 1.2559654712677002,
      "learning_rate": 0.00011221191260101766,
      "loss": 0.6051,
      "step": 2939
    },
    {
      "epoch": 3.9569313593539706,
      "grad_norm": 1.013961911201477,
      "learning_rate": 0.00011218198144268183,
      "loss": 0.5825,
      "step": 2940
    },
    {
      "epoch": 3.958277254374159,
      "grad_norm": 1.3732473850250244,
      "learning_rate": 0.00011215205028434602,
      "loss": 0.6472,
      "step": 2941
    },
    {
      "epoch": 3.9596231493943472,
      "grad_norm": 1.1441121101379395,
      "learning_rate": 0.00011212211912601018,
      "loss": 0.6176,
      "step": 2942
    },
    {
      "epoch": 3.9609690444145356,
      "grad_norm": 1.0390459299087524,
      "learning_rate": 0.00011209218796767435,
      "loss": 0.5473,
      "step": 2943
    },
    {
      "epoch": 3.962314939434724,
      "grad_norm": 1.0120152235031128,
      "learning_rate": 0.00011206225680933852,
      "loss": 0.6412,
      "step": 2944
    },
    {
      "epoch": 3.9636608344549122,
      "grad_norm": 0.9565226435661316,
      "learning_rate": 0.0001120323256510027,
      "loss": 0.639,
      "step": 2945
    },
    {
      "epoch": 3.965006729475101,
      "grad_norm": 1.0260990858078003,
      "learning_rate": 0.00011200239449266688,
      "loss": 0.4743,
      "step": 2946
    },
    {
      "epoch": 3.9663526244952894,
      "grad_norm": 1.2068676948547363,
      "learning_rate": 0.00011197246333433104,
      "loss": 0.671,
      "step": 2947
    },
    {
      "epoch": 3.9676985195154777,
      "grad_norm": 1.1454583406448364,
      "learning_rate": 0.00011194253217599521,
      "loss": 0.7188,
      "step": 2948
    },
    {
      "epoch": 3.9690444145356665,
      "grad_norm": 1.3792823553085327,
      "learning_rate": 0.0001119126010176594,
      "loss": 0.7105,
      "step": 2949
    },
    {
      "epoch": 3.970390309555855,
      "grad_norm": 1.200283408164978,
      "learning_rate": 0.00011188266985932357,
      "loss": 0.5856,
      "step": 2950
    },
    {
      "epoch": 3.971736204576043,
      "grad_norm": 1.1964349746704102,
      "learning_rate": 0.00011185273870098773,
      "loss": 0.7931,
      "step": 2951
    },
    {
      "epoch": 3.9730820995962315,
      "grad_norm": 1.176708459854126,
      "learning_rate": 0.0001118228075426519,
      "loss": 0.4613,
      "step": 2952
    },
    {
      "epoch": 3.97442799461642,
      "grad_norm": 1.4009360074996948,
      "learning_rate": 0.00011179287638431609,
      "loss": 0.6852,
      "step": 2953
    },
    {
      "epoch": 3.975773889636608,
      "grad_norm": 1.0009793043136597,
      "learning_rate": 0.00011176294522598026,
      "loss": 0.5283,
      "step": 2954
    },
    {
      "epoch": 3.9771197846567965,
      "grad_norm": 1.1975586414337158,
      "learning_rate": 0.00011173301406764442,
      "loss": 0.6341,
      "step": 2955
    },
    {
      "epoch": 3.9784656796769853,
      "grad_norm": 1.416436791419983,
      "learning_rate": 0.0001117030829093086,
      "loss": 0.6258,
      "step": 2956
    },
    {
      "epoch": 3.9798115746971736,
      "grad_norm": 1.0503591299057007,
      "learning_rate": 0.00011167315175097278,
      "loss": 0.5092,
      "step": 2957
    },
    {
      "epoch": 3.981157469717362,
      "grad_norm": 1.1643935441970825,
      "learning_rate": 0.00011164322059263695,
      "loss": 0.5845,
      "step": 2958
    },
    {
      "epoch": 3.9825033647375507,
      "grad_norm": 1.2164993286132812,
      "learning_rate": 0.00011161328943430111,
      "loss": 0.5776,
      "step": 2959
    },
    {
      "epoch": 3.983849259757739,
      "grad_norm": 1.0770326852798462,
      "learning_rate": 0.00011158335827596528,
      "loss": 0.5796,
      "step": 2960
    },
    {
      "epoch": 3.9851951547779274,
      "grad_norm": 1.1751587390899658,
      "learning_rate": 0.00011155342711762947,
      "loss": 0.5744,
      "step": 2961
    },
    {
      "epoch": 3.9865410497981157,
      "grad_norm": 1.0402337312698364,
      "learning_rate": 0.00011152349595929364,
      "loss": 0.6283,
      "step": 2962
    },
    {
      "epoch": 3.987886944818304,
      "grad_norm": 1.2353646755218506,
      "learning_rate": 0.0001114935648009578,
      "loss": 0.5788,
      "step": 2963
    },
    {
      "epoch": 3.9892328398384924,
      "grad_norm": 1.1339539289474487,
      "learning_rate": 0.00011146363364262197,
      "loss": 0.6602,
      "step": 2964
    },
    {
      "epoch": 3.9905787348586808,
      "grad_norm": 1.2189438343048096,
      "learning_rate": 0.00011143370248428616,
      "loss": 0.7494,
      "step": 2965
    },
    {
      "epoch": 3.9919246298788695,
      "grad_norm": 1.2807705402374268,
      "learning_rate": 0.00011140377132595033,
      "loss": 0.5639,
      "step": 2966
    },
    {
      "epoch": 3.993270524899058,
      "grad_norm": 1.208235263824463,
      "learning_rate": 0.0001113738401676145,
      "loss": 0.9362,
      "step": 2967
    },
    {
      "epoch": 3.994616419919246,
      "grad_norm": 1.3804779052734375,
      "learning_rate": 0.00011134390900927866,
      "loss": 0.5755,
      "step": 2968
    },
    {
      "epoch": 3.995962314939435,
      "grad_norm": 0.9844257831573486,
      "learning_rate": 0.00011131397785094285,
      "loss": 0.5844,
      "step": 2969
    },
    {
      "epoch": 3.9973082099596233,
      "grad_norm": 1.3004217147827148,
      "learning_rate": 0.00011128404669260702,
      "loss": 0.7115,
      "step": 2970
    },
    {
      "epoch": 3.9986541049798117,
      "grad_norm": 1.3359804153442383,
      "learning_rate": 0.00011125411553427119,
      "loss": 0.7594,
      "step": 2971
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.4072431325912476,
      "learning_rate": 0.00011122418437593535,
      "loss": 0.6219,
      "step": 2972
    },
    {
      "epoch": 4.001345895020188,
      "grad_norm": 0.9206676483154297,
      "learning_rate": 0.00011119425321759954,
      "loss": 0.4233,
      "step": 2973
    },
    {
      "epoch": 4.002691790040377,
      "grad_norm": 0.7367196083068848,
      "learning_rate": 0.0001111643220592637,
      "loss": 0.2566,
      "step": 2974
    },
    {
      "epoch": 4.004037685060565,
      "grad_norm": 0.9908972978591919,
      "learning_rate": 0.00011113439090092788,
      "loss": 0.3453,
      "step": 2975
    },
    {
      "epoch": 4.005383580080753,
      "grad_norm": 0.8469856977462769,
      "learning_rate": 0.00011110445974259204,
      "loss": 0.3215,
      "step": 2976
    },
    {
      "epoch": 4.006729475100942,
      "grad_norm": 0.829591691493988,
      "learning_rate": 0.00011107452858425623,
      "loss": 0.4309,
      "step": 2977
    },
    {
      "epoch": 4.008075370121131,
      "grad_norm": 0.9648606181144714,
      "learning_rate": 0.0001110445974259204,
      "loss": 0.5571,
      "step": 2978
    },
    {
      "epoch": 4.009421265141319,
      "grad_norm": 1.0422345399856567,
      "learning_rate": 0.00011101466626758457,
      "loss": 0.3899,
      "step": 2979
    },
    {
      "epoch": 4.010767160161508,
      "grad_norm": 1.1162444353103638,
      "learning_rate": 0.00011098473510924873,
      "loss": 0.4354,
      "step": 2980
    },
    {
      "epoch": 4.012113055181696,
      "grad_norm": 1.0604772567749023,
      "learning_rate": 0.00011095480395091292,
      "loss": 0.2967,
      "step": 2981
    },
    {
      "epoch": 4.013458950201884,
      "grad_norm": 2.0177042484283447,
      "learning_rate": 0.00011092487279257709,
      "loss": 0.3322,
      "step": 2982
    },
    {
      "epoch": 4.014804845222073,
      "grad_norm": 1.6596187353134155,
      "learning_rate": 0.00011089494163424126,
      "loss": 0.5866,
      "step": 2983
    },
    {
      "epoch": 4.016150740242261,
      "grad_norm": 1.2681635618209839,
      "learning_rate": 0.00011086501047590542,
      "loss": 0.5096,
      "step": 2984
    },
    {
      "epoch": 4.017496635262449,
      "grad_norm": 1.2880364656448364,
      "learning_rate": 0.00011083507931756961,
      "loss": 0.331,
      "step": 2985
    },
    {
      "epoch": 4.018842530282638,
      "grad_norm": 1.4398612976074219,
      "learning_rate": 0.00011080514815923378,
      "loss": 0.3381,
      "step": 2986
    },
    {
      "epoch": 4.020188425302826,
      "grad_norm": 1.2054554224014282,
      "learning_rate": 0.00011077521700089795,
      "loss": 0.3275,
      "step": 2987
    },
    {
      "epoch": 4.021534320323015,
      "grad_norm": 1.138914942741394,
      "learning_rate": 0.00011074528584256211,
      "loss": 0.3164,
      "step": 2988
    },
    {
      "epoch": 4.0228802153432035,
      "grad_norm": 1.3333323001861572,
      "learning_rate": 0.0001107153546842263,
      "loss": 0.3296,
      "step": 2989
    },
    {
      "epoch": 4.024226110363392,
      "grad_norm": 1.2347612380981445,
      "learning_rate": 0.00011068542352589047,
      "loss": 0.3081,
      "step": 2990
    },
    {
      "epoch": 4.02557200538358,
      "grad_norm": 1.1875813007354736,
      "learning_rate": 0.00011065549236755464,
      "loss": 0.3012,
      "step": 2991
    },
    {
      "epoch": 4.0269179004037685,
      "grad_norm": 1.5961207151412964,
      "learning_rate": 0.0001106255612092188,
      "loss": 0.3962,
      "step": 2992
    },
    {
      "epoch": 4.028263795423957,
      "grad_norm": 1.4542254209518433,
      "learning_rate": 0.00011059563005088299,
      "loss": 0.3385,
      "step": 2993
    },
    {
      "epoch": 4.029609690444145,
      "grad_norm": 1.1876561641693115,
      "learning_rate": 0.00011056569889254716,
      "loss": 0.3661,
      "step": 2994
    },
    {
      "epoch": 4.0309555854643335,
      "grad_norm": 1.182146668434143,
      "learning_rate": 0.00011053576773421133,
      "loss": 0.3073,
      "step": 2995
    },
    {
      "epoch": 4.032301480484522,
      "grad_norm": 1.162253975868225,
      "learning_rate": 0.0001105058365758755,
      "loss": 0.3178,
      "step": 2996
    },
    {
      "epoch": 4.03364737550471,
      "grad_norm": 1.1321345567703247,
      "learning_rate": 0.00011047590541753968,
      "loss": 0.3827,
      "step": 2997
    },
    {
      "epoch": 4.034993270524899,
      "grad_norm": 1.053877830505371,
      "learning_rate": 0.00011044597425920385,
      "loss": 0.2827,
      "step": 2998
    },
    {
      "epoch": 4.036339165545088,
      "grad_norm": 1.310479998588562,
      "learning_rate": 0.00011041604310086802,
      "loss": 0.3119,
      "step": 2999
    },
    {
      "epoch": 4.037685060565276,
      "grad_norm": 1.3255168199539185,
      "learning_rate": 0.00011038611194253219,
      "loss": 0.3242,
      "step": 3000
    },
    {
      "epoch": 4.039030955585464,
      "grad_norm": 1.4808924198150635,
      "learning_rate": 0.00011035618078419637,
      "loss": 0.4097,
      "step": 3001
    },
    {
      "epoch": 4.040376850605653,
      "grad_norm": 1.1192424297332764,
      "learning_rate": 0.00011032624962586054,
      "loss": 0.302,
      "step": 3002
    },
    {
      "epoch": 4.041722745625841,
      "grad_norm": 1.1722108125686646,
      "learning_rate": 0.0001102963184675247,
      "loss": 0.35,
      "step": 3003
    },
    {
      "epoch": 4.043068640646029,
      "grad_norm": 1.147770881652832,
      "learning_rate": 0.00011026638730918888,
      "loss": 0.417,
      "step": 3004
    },
    {
      "epoch": 4.044414535666218,
      "grad_norm": 1.0880284309387207,
      "learning_rate": 0.00011023645615085304,
      "loss": 0.3074,
      "step": 3005
    },
    {
      "epoch": 4.045760430686406,
      "grad_norm": 0.9600160717964172,
      "learning_rate": 0.00011020652499251723,
      "loss": 0.3123,
      "step": 3006
    },
    {
      "epoch": 4.0471063257065945,
      "grad_norm": 1.125440001487732,
      "learning_rate": 0.0001101765938341814,
      "loss": 0.3993,
      "step": 3007
    },
    {
      "epoch": 4.048452220726784,
      "grad_norm": 1.1163718700408936,
      "learning_rate": 0.00011014666267584557,
      "loss": 0.3108,
      "step": 3008
    },
    {
      "epoch": 4.049798115746972,
      "grad_norm": 1.094544529914856,
      "learning_rate": 0.00011011673151750973,
      "loss": 0.4433,
      "step": 3009
    },
    {
      "epoch": 4.05114401076716,
      "grad_norm": 1.4258049726486206,
      "learning_rate": 0.00011008680035917392,
      "loss": 0.3459,
      "step": 3010
    },
    {
      "epoch": 4.052489905787349,
      "grad_norm": 1.3081976175308228,
      "learning_rate": 0.00011005686920083809,
      "loss": 0.3688,
      "step": 3011
    },
    {
      "epoch": 4.053835800807537,
      "grad_norm": 1.0513951778411865,
      "learning_rate": 0.00011002693804250226,
      "loss": 0.3405,
      "step": 3012
    },
    {
      "epoch": 4.055181695827725,
      "grad_norm": 1.0480283498764038,
      "learning_rate": 0.00010999700688416642,
      "loss": 0.414,
      "step": 3013
    },
    {
      "epoch": 4.056527590847914,
      "grad_norm": 1.0759484767913818,
      "learning_rate": 0.00010996707572583061,
      "loss": 0.4117,
      "step": 3014
    },
    {
      "epoch": 4.057873485868102,
      "grad_norm": 1.1430611610412598,
      "learning_rate": 0.00010993714456749478,
      "loss": 0.3485,
      "step": 3015
    },
    {
      "epoch": 4.05921938088829,
      "grad_norm": 1.2418320178985596,
      "learning_rate": 0.00010990721340915895,
      "loss": 0.3296,
      "step": 3016
    },
    {
      "epoch": 4.060565275908479,
      "grad_norm": 1.1184874773025513,
      "learning_rate": 0.00010987728225082312,
      "loss": 0.3132,
      "step": 3017
    },
    {
      "epoch": 4.061911170928668,
      "grad_norm": 1.024931788444519,
      "learning_rate": 0.0001098473510924873,
      "loss": 0.2712,
      "step": 3018
    },
    {
      "epoch": 4.063257065948856,
      "grad_norm": 0.9656476974487305,
      "learning_rate": 0.00010981741993415147,
      "loss": 0.2822,
      "step": 3019
    },
    {
      "epoch": 4.064602960969045,
      "grad_norm": 1.4206374883651733,
      "learning_rate": 0.00010978748877581564,
      "loss": 0.5124,
      "step": 3020
    },
    {
      "epoch": 4.065948855989233,
      "grad_norm": 1.3690769672393799,
      "learning_rate": 0.0001097575576174798,
      "loss": 0.3776,
      "step": 3021
    },
    {
      "epoch": 4.067294751009421,
      "grad_norm": 1.1550312042236328,
      "learning_rate": 0.00010972762645914399,
      "loss": 0.3277,
      "step": 3022
    },
    {
      "epoch": 4.06864064602961,
      "grad_norm": 1.1166512966156006,
      "learning_rate": 0.00010969769530080816,
      "loss": 0.3335,
      "step": 3023
    },
    {
      "epoch": 4.069986541049798,
      "grad_norm": 1.2040356397628784,
      "learning_rate": 0.00010966776414247233,
      "loss": 0.3252,
      "step": 3024
    },
    {
      "epoch": 4.071332436069986,
      "grad_norm": 1.3059197664260864,
      "learning_rate": 0.0001096378329841365,
      "loss": 0.3145,
      "step": 3025
    },
    {
      "epoch": 4.072678331090175,
      "grad_norm": 1.440421462059021,
      "learning_rate": 0.00010960790182580068,
      "loss": 0.4051,
      "step": 3026
    },
    {
      "epoch": 4.074024226110363,
      "grad_norm": 1.2952816486358643,
      "learning_rate": 0.00010957797066746485,
      "loss": 0.401,
      "step": 3027
    },
    {
      "epoch": 4.075370121130552,
      "grad_norm": 1.2264940738677979,
      "learning_rate": 0.00010954803950912902,
      "loss": 0.3465,
      "step": 3028
    },
    {
      "epoch": 4.0767160161507405,
      "grad_norm": 1.329866886138916,
      "learning_rate": 0.00010951810835079319,
      "loss": 0.3542,
      "step": 3029
    },
    {
      "epoch": 4.078061911170929,
      "grad_norm": 1.3260295391082764,
      "learning_rate": 0.00010948817719245737,
      "loss": 0.4112,
      "step": 3030
    },
    {
      "epoch": 4.079407806191117,
      "grad_norm": 1.4225879907608032,
      "learning_rate": 0.00010945824603412154,
      "loss": 0.2955,
      "step": 3031
    },
    {
      "epoch": 4.0807537012113055,
      "grad_norm": 1.1015945672988892,
      "learning_rate": 0.0001094283148757857,
      "loss": 0.4326,
      "step": 3032
    },
    {
      "epoch": 4.082099596231494,
      "grad_norm": 1.183351993560791,
      "learning_rate": 0.00010939838371744988,
      "loss": 0.3787,
      "step": 3033
    },
    {
      "epoch": 4.083445491251682,
      "grad_norm": 1.1626601219177246,
      "learning_rate": 0.00010936845255911403,
      "loss": 0.3114,
      "step": 3034
    },
    {
      "epoch": 4.0847913862718706,
      "grad_norm": 1.0488842725753784,
      "learning_rate": 0.0001093385214007782,
      "loss": 0.3535,
      "step": 3035
    },
    {
      "epoch": 4.086137281292059,
      "grad_norm": 1.1975305080413818,
      "learning_rate": 0.00010930859024244238,
      "loss": 0.4075,
      "step": 3036
    },
    {
      "epoch": 4.087483176312247,
      "grad_norm": 0.9798936247825623,
      "learning_rate": 0.00010927865908410655,
      "loss": 0.249,
      "step": 3037
    },
    {
      "epoch": 4.0888290713324364,
      "grad_norm": 1.1747586727142334,
      "learning_rate": 0.00010924872792577072,
      "loss": 0.3101,
      "step": 3038
    },
    {
      "epoch": 4.090174966352625,
      "grad_norm": 1.286891222000122,
      "learning_rate": 0.00010921879676743489,
      "loss": 0.4493,
      "step": 3039
    },
    {
      "epoch": 4.091520861372813,
      "grad_norm": 1.1445561647415161,
      "learning_rate": 0.00010918886560909907,
      "loss": 0.3258,
      "step": 3040
    },
    {
      "epoch": 4.0928667563930015,
      "grad_norm": 1.1688202619552612,
      "learning_rate": 0.00010915893445076324,
      "loss": 0.4111,
      "step": 3041
    },
    {
      "epoch": 4.09421265141319,
      "grad_norm": 1.003142237663269,
      "learning_rate": 0.00010912900329242741,
      "loss": 0.3212,
      "step": 3042
    },
    {
      "epoch": 4.095558546433378,
      "grad_norm": 1.2524789571762085,
      "learning_rate": 0.00010909907213409158,
      "loss": 0.3348,
      "step": 3043
    },
    {
      "epoch": 4.0969044414535665,
      "grad_norm": 0.8938387632369995,
      "learning_rate": 0.00010906914097575576,
      "loss": 0.2018,
      "step": 3044
    },
    {
      "epoch": 4.098250336473755,
      "grad_norm": 1.0146371126174927,
      "learning_rate": 0.00010903920981741993,
      "loss": 0.3186,
      "step": 3045
    },
    {
      "epoch": 4.099596231493943,
      "grad_norm": 1.206270456314087,
      "learning_rate": 0.0001090092786590841,
      "loss": 0.3384,
      "step": 3046
    },
    {
      "epoch": 4.1009421265141315,
      "grad_norm": 1.3276525735855103,
      "learning_rate": 0.00010897934750074827,
      "loss": 0.3606,
      "step": 3047
    },
    {
      "epoch": 4.102288021534321,
      "grad_norm": 1.2292554378509521,
      "learning_rate": 0.00010894941634241245,
      "loss": 0.4308,
      "step": 3048
    },
    {
      "epoch": 4.103633916554509,
      "grad_norm": 1.164145588874817,
      "learning_rate": 0.00010891948518407662,
      "loss": 0.2869,
      "step": 3049
    },
    {
      "epoch": 4.104979811574697,
      "grad_norm": 1.3907957077026367,
      "learning_rate": 0.00010888955402574079,
      "loss": 0.4052,
      "step": 3050
    },
    {
      "epoch": 4.106325706594886,
      "grad_norm": 1.135653018951416,
      "learning_rate": 0.00010885962286740496,
      "loss": 0.3068,
      "step": 3051
    },
    {
      "epoch": 4.107671601615074,
      "grad_norm": 0.9677850008010864,
      "learning_rate": 0.00010882969170906914,
      "loss": 0.2683,
      "step": 3052
    },
    {
      "epoch": 4.109017496635262,
      "grad_norm": 0.8905746936798096,
      "learning_rate": 0.00010879976055073331,
      "loss": 0.2633,
      "step": 3053
    },
    {
      "epoch": 4.110363391655451,
      "grad_norm": 1.2265663146972656,
      "learning_rate": 0.00010876982939239748,
      "loss": 0.3738,
      "step": 3054
    },
    {
      "epoch": 4.111709286675639,
      "grad_norm": 1.1556367874145508,
      "learning_rate": 0.00010873989823406165,
      "loss": 0.3292,
      "step": 3055
    },
    {
      "epoch": 4.113055181695827,
      "grad_norm": 0.8171972632408142,
      "learning_rate": 0.00010870996707572583,
      "loss": 0.2431,
      "step": 3056
    },
    {
      "epoch": 4.114401076716016,
      "grad_norm": 1.2116727828979492,
      "learning_rate": 0.00010868003591739,
      "loss": 0.4357,
      "step": 3057
    },
    {
      "epoch": 4.115746971736205,
      "grad_norm": 1.1300535202026367,
      "learning_rate": 0.00010865010475905417,
      "loss": 0.3601,
      "step": 3058
    },
    {
      "epoch": 4.117092866756393,
      "grad_norm": 1.2413783073425293,
      "learning_rate": 0.00010862017360071834,
      "loss": 0.3509,
      "step": 3059
    },
    {
      "epoch": 4.118438761776582,
      "grad_norm": 1.2073160409927368,
      "learning_rate": 0.00010859024244238251,
      "loss": 0.3636,
      "step": 3060
    },
    {
      "epoch": 4.11978465679677,
      "grad_norm": 1.3687384128570557,
      "learning_rate": 0.00010856031128404669,
      "loss": 0.4108,
      "step": 3061
    },
    {
      "epoch": 4.121130551816958,
      "grad_norm": 1.3813279867172241,
      "learning_rate": 0.00010853038012571086,
      "loss": 0.6411,
      "step": 3062
    },
    {
      "epoch": 4.122476446837147,
      "grad_norm": 1.325521469116211,
      "learning_rate": 0.00010850044896737503,
      "loss": 0.4213,
      "step": 3063
    },
    {
      "epoch": 4.123822341857335,
      "grad_norm": 1.5156863927841187,
      "learning_rate": 0.0001084705178090392,
      "loss": 0.3583,
      "step": 3064
    },
    {
      "epoch": 4.125168236877523,
      "grad_norm": 1.2346093654632568,
      "learning_rate": 0.00010844058665070338,
      "loss": 0.3963,
      "step": 3065
    },
    {
      "epoch": 4.126514131897712,
      "grad_norm": 1.5173280239105225,
      "learning_rate": 0.00010841065549236755,
      "loss": 0.327,
      "step": 3066
    },
    {
      "epoch": 4.1278600269179,
      "grad_norm": 1.1878859996795654,
      "learning_rate": 0.00010838072433403172,
      "loss": 0.3858,
      "step": 3067
    },
    {
      "epoch": 4.129205921938089,
      "grad_norm": 1.1840238571166992,
      "learning_rate": 0.00010835079317569589,
      "loss": 0.3246,
      "step": 3068
    },
    {
      "epoch": 4.1305518169582776,
      "grad_norm": 0.9895729422569275,
      "learning_rate": 0.00010832086201736007,
      "loss": 0.2938,
      "step": 3069
    },
    {
      "epoch": 4.131897711978466,
      "grad_norm": 1.1105434894561768,
      "learning_rate": 0.00010829093085902424,
      "loss": 0.3067,
      "step": 3070
    },
    {
      "epoch": 4.133243606998654,
      "grad_norm": 1.453979730606079,
      "learning_rate": 0.00010826099970068841,
      "loss": 0.3503,
      "step": 3071
    },
    {
      "epoch": 4.134589502018843,
      "grad_norm": 1.1821033954620361,
      "learning_rate": 0.00010823106854235258,
      "loss": 0.4976,
      "step": 3072
    },
    {
      "epoch": 4.135935397039031,
      "grad_norm": 1.0681178569793701,
      "learning_rate": 0.00010820113738401676,
      "loss": 0.355,
      "step": 3073
    },
    {
      "epoch": 4.137281292059219,
      "grad_norm": 1.1115831136703491,
      "learning_rate": 0.00010817120622568093,
      "loss": 0.3297,
      "step": 3074
    },
    {
      "epoch": 4.138627187079408,
      "grad_norm": 1.1293913125991821,
      "learning_rate": 0.0001081412750673451,
      "loss": 0.439,
      "step": 3075
    },
    {
      "epoch": 4.139973082099596,
      "grad_norm": 1.0092233419418335,
      "learning_rate": 0.00010811134390900927,
      "loss": 0.2994,
      "step": 3076
    },
    {
      "epoch": 4.141318977119784,
      "grad_norm": 1.2615433931350708,
      "learning_rate": 0.00010808141275067345,
      "loss": 0.3502,
      "step": 3077
    },
    {
      "epoch": 4.1426648721399735,
      "grad_norm": 1.2786866426467896,
      "learning_rate": 0.00010805148159233762,
      "loss": 0.4855,
      "step": 3078
    },
    {
      "epoch": 4.144010767160162,
      "grad_norm": 1.2050522565841675,
      "learning_rate": 0.00010802155043400179,
      "loss": 0.3827,
      "step": 3079
    },
    {
      "epoch": 4.14535666218035,
      "grad_norm": 1.2321569919586182,
      "learning_rate": 0.00010799161927566596,
      "loss": 0.4019,
      "step": 3080
    },
    {
      "epoch": 4.1467025572005385,
      "grad_norm": 1.0967328548431396,
      "learning_rate": 0.00010796168811733014,
      "loss": 0.3349,
      "step": 3081
    },
    {
      "epoch": 4.148048452220727,
      "grad_norm": 1.1239135265350342,
      "learning_rate": 0.00010793175695899431,
      "loss": 0.3103,
      "step": 3082
    },
    {
      "epoch": 4.149394347240915,
      "grad_norm": 1.158369779586792,
      "learning_rate": 0.00010790182580065848,
      "loss": 0.3756,
      "step": 3083
    },
    {
      "epoch": 4.1507402422611035,
      "grad_norm": 1.2026387453079224,
      "learning_rate": 0.00010787189464232265,
      "loss": 0.3302,
      "step": 3084
    },
    {
      "epoch": 4.152086137281292,
      "grad_norm": 1.2122293710708618,
      "learning_rate": 0.00010784196348398683,
      "loss": 0.4159,
      "step": 3085
    },
    {
      "epoch": 4.15343203230148,
      "grad_norm": 1.1900904178619385,
      "learning_rate": 0.000107812032325651,
      "loss": 0.3248,
      "step": 3086
    },
    {
      "epoch": 4.1547779273216685,
      "grad_norm": 1.4054511785507202,
      "learning_rate": 0.00010778210116731517,
      "loss": 0.3226,
      "step": 3087
    },
    {
      "epoch": 4.156123822341858,
      "grad_norm": 1.3672429323196411,
      "learning_rate": 0.00010775217000897934,
      "loss": 0.3956,
      "step": 3088
    },
    {
      "epoch": 4.157469717362046,
      "grad_norm": 1.0873339176177979,
      "learning_rate": 0.00010772223885064352,
      "loss": 0.3405,
      "step": 3089
    },
    {
      "epoch": 4.158815612382234,
      "grad_norm": 1.0775823593139648,
      "learning_rate": 0.0001076923076923077,
      "loss": 0.37,
      "step": 3090
    },
    {
      "epoch": 4.160161507402423,
      "grad_norm": 1.1082408428192139,
      "learning_rate": 0.00010766237653397186,
      "loss": 0.267,
      "step": 3091
    },
    {
      "epoch": 4.161507402422611,
      "grad_norm": 1.4024484157562256,
      "learning_rate": 0.00010763244537563603,
      "loss": 0.3837,
      "step": 3092
    },
    {
      "epoch": 4.162853297442799,
      "grad_norm": 1.0318843126296997,
      "learning_rate": 0.00010760251421730021,
      "loss": 0.3106,
      "step": 3093
    },
    {
      "epoch": 4.164199192462988,
      "grad_norm": 1.4094349145889282,
      "learning_rate": 0.00010757258305896438,
      "loss": 0.4736,
      "step": 3094
    },
    {
      "epoch": 4.165545087483176,
      "grad_norm": 1.2862368822097778,
      "learning_rate": 0.00010754265190062855,
      "loss": 0.3407,
      "step": 3095
    },
    {
      "epoch": 4.166890982503364,
      "grad_norm": 1.1182982921600342,
      "learning_rate": 0.00010751272074229272,
      "loss": 0.2588,
      "step": 3096
    },
    {
      "epoch": 4.168236877523553,
      "grad_norm": 1.375314712524414,
      "learning_rate": 0.0001074827895839569,
      "loss": 0.3325,
      "step": 3097
    },
    {
      "epoch": 4.169582772543742,
      "grad_norm": 1.1277058124542236,
      "learning_rate": 0.00010745285842562107,
      "loss": 0.4286,
      "step": 3098
    },
    {
      "epoch": 4.17092866756393,
      "grad_norm": 1.266848087310791,
      "learning_rate": 0.00010742292726728524,
      "loss": 0.3547,
      "step": 3099
    },
    {
      "epoch": 4.172274562584119,
      "grad_norm": 1.1233325004577637,
      "learning_rate": 0.00010739299610894941,
      "loss": 0.2968,
      "step": 3100
    },
    {
      "epoch": 4.173620457604307,
      "grad_norm": 1.1995683908462524,
      "learning_rate": 0.0001073630649506136,
      "loss": 0.469,
      "step": 3101
    },
    {
      "epoch": 4.174966352624495,
      "grad_norm": 0.9871532917022705,
      "learning_rate": 0.00010733313379227776,
      "loss": 0.2865,
      "step": 3102
    },
    {
      "epoch": 4.176312247644684,
      "grad_norm": 1.1362037658691406,
      "learning_rate": 0.00010730320263394193,
      "loss": 0.3252,
      "step": 3103
    },
    {
      "epoch": 4.177658142664872,
      "grad_norm": 1.4126968383789062,
      "learning_rate": 0.0001072732714756061,
      "loss": 0.4666,
      "step": 3104
    },
    {
      "epoch": 4.17900403768506,
      "grad_norm": 1.0674066543579102,
      "learning_rate": 0.00010724334031727028,
      "loss": 0.2912,
      "step": 3105
    },
    {
      "epoch": 4.180349932705249,
      "grad_norm": 1.1059648990631104,
      "learning_rate": 0.00010721340915893445,
      "loss": 0.3336,
      "step": 3106
    },
    {
      "epoch": 4.181695827725437,
      "grad_norm": 1.0696001052856445,
      "learning_rate": 0.00010718347800059862,
      "loss": 0.3755,
      "step": 3107
    },
    {
      "epoch": 4.183041722745626,
      "grad_norm": 1.3646025657653809,
      "learning_rate": 0.00010715354684226279,
      "loss": 0.4034,
      "step": 3108
    },
    {
      "epoch": 4.184387617765815,
      "grad_norm": 1.160632610321045,
      "learning_rate": 0.00010712361568392697,
      "loss": 0.3103,
      "step": 3109
    },
    {
      "epoch": 4.185733512786003,
      "grad_norm": 1.4082386493682861,
      "learning_rate": 0.00010709368452559114,
      "loss": 0.3321,
      "step": 3110
    },
    {
      "epoch": 4.187079407806191,
      "grad_norm": 1.2670913934707642,
      "learning_rate": 0.00010706375336725531,
      "loss": 0.367,
      "step": 3111
    },
    {
      "epoch": 4.18842530282638,
      "grad_norm": 1.528681993484497,
      "learning_rate": 0.00010703382220891948,
      "loss": 0.3758,
      "step": 3112
    },
    {
      "epoch": 4.189771197846568,
      "grad_norm": 1.3111023902893066,
      "learning_rate": 0.00010700389105058367,
      "loss": 0.4012,
      "step": 3113
    },
    {
      "epoch": 4.191117092866756,
      "grad_norm": 1.2730128765106201,
      "learning_rate": 0.00010697395989224783,
      "loss": 0.3581,
      "step": 3114
    },
    {
      "epoch": 4.192462987886945,
      "grad_norm": 1.02515709400177,
      "learning_rate": 0.000106944028733912,
      "loss": 0.3598,
      "step": 3115
    },
    {
      "epoch": 4.193808882907133,
      "grad_norm": 1.5212401151657104,
      "learning_rate": 0.00010691409757557617,
      "loss": 0.3678,
      "step": 3116
    },
    {
      "epoch": 4.195154777927321,
      "grad_norm": 1.3177814483642578,
      "learning_rate": 0.00010688416641724036,
      "loss": 0.462,
      "step": 3117
    },
    {
      "epoch": 4.1965006729475105,
      "grad_norm": 1.0886911153793335,
      "learning_rate": 0.00010685423525890452,
      "loss": 0.2839,
      "step": 3118
    },
    {
      "epoch": 4.197846567967699,
      "grad_norm": 1.314301609992981,
      "learning_rate": 0.0001068243041005687,
      "loss": 0.3376,
      "step": 3119
    },
    {
      "epoch": 4.199192462987887,
      "grad_norm": 1.4615808725357056,
      "learning_rate": 0.00010679437294223286,
      "loss": 0.3742,
      "step": 3120
    },
    {
      "epoch": 4.2005383580080755,
      "grad_norm": 1.6341923475265503,
      "learning_rate": 0.00010676444178389705,
      "loss": 0.4316,
      "step": 3121
    },
    {
      "epoch": 4.201884253028264,
      "grad_norm": 1.4305062294006348,
      "learning_rate": 0.00010673451062556121,
      "loss": 0.3496,
      "step": 3122
    },
    {
      "epoch": 4.203230148048452,
      "grad_norm": 1.5284831523895264,
      "learning_rate": 0.00010670457946722538,
      "loss": 0.4521,
      "step": 3123
    },
    {
      "epoch": 4.2045760430686405,
      "grad_norm": 1.4774739742279053,
      "learning_rate": 0.00010667464830888955,
      "loss": 0.4661,
      "step": 3124
    },
    {
      "epoch": 4.205921938088829,
      "grad_norm": 1.1637191772460938,
      "learning_rate": 0.00010664471715055374,
      "loss": 0.3153,
      "step": 3125
    },
    {
      "epoch": 4.207267833109017,
      "grad_norm": 1.4664970636367798,
      "learning_rate": 0.0001066147859922179,
      "loss": 0.421,
      "step": 3126
    },
    {
      "epoch": 4.2086137281292055,
      "grad_norm": 1.5801538228988647,
      "learning_rate": 0.00010658485483388207,
      "loss": 0.7722,
      "step": 3127
    },
    {
      "epoch": 4.209959623149395,
      "grad_norm": 1.2752238512039185,
      "learning_rate": 0.00010655492367554624,
      "loss": 0.6373,
      "step": 3128
    },
    {
      "epoch": 4.211305518169583,
      "grad_norm": 1.1870061159133911,
      "learning_rate": 0.00010652499251721043,
      "loss": 0.3607,
      "step": 3129
    },
    {
      "epoch": 4.212651413189771,
      "grad_norm": 1.0958163738250732,
      "learning_rate": 0.0001064950613588746,
      "loss": 0.3302,
      "step": 3130
    },
    {
      "epoch": 4.21399730820996,
      "grad_norm": 0.921898365020752,
      "learning_rate": 0.00010646513020053876,
      "loss": 0.33,
      "step": 3131
    },
    {
      "epoch": 4.215343203230148,
      "grad_norm": 1.1397143602371216,
      "learning_rate": 0.00010643519904220293,
      "loss": 0.4153,
      "step": 3132
    },
    {
      "epoch": 4.216689098250336,
      "grad_norm": 1.307261347770691,
      "learning_rate": 0.00010640526788386712,
      "loss": 0.3194,
      "step": 3133
    },
    {
      "epoch": 4.218034993270525,
      "grad_norm": 1.0213466882705688,
      "learning_rate": 0.00010637533672553128,
      "loss": 0.2842,
      "step": 3134
    },
    {
      "epoch": 4.219380888290713,
      "grad_norm": 0.8990545868873596,
      "learning_rate": 0.00010634540556719545,
      "loss": 0.2635,
      "step": 3135
    },
    {
      "epoch": 4.2207267833109015,
      "grad_norm": 1.0983014106750488,
      "learning_rate": 0.00010631547440885962,
      "loss": 0.3926,
      "step": 3136
    },
    {
      "epoch": 4.22207267833109,
      "grad_norm": 1.1293562650680542,
      "learning_rate": 0.0001062855432505238,
      "loss": 0.2861,
      "step": 3137
    },
    {
      "epoch": 4.223418573351279,
      "grad_norm": 1.018525242805481,
      "learning_rate": 0.00010625561209218798,
      "loss": 0.3226,
      "step": 3138
    },
    {
      "epoch": 4.224764468371467,
      "grad_norm": 1.4411499500274658,
      "learning_rate": 0.00010622568093385214,
      "loss": 0.3998,
      "step": 3139
    },
    {
      "epoch": 4.226110363391656,
      "grad_norm": 1.1789442300796509,
      "learning_rate": 0.00010619574977551631,
      "loss": 0.2999,
      "step": 3140
    },
    {
      "epoch": 4.227456258411844,
      "grad_norm": 1.132942795753479,
      "learning_rate": 0.0001061658186171805,
      "loss": 0.3083,
      "step": 3141
    },
    {
      "epoch": 4.228802153432032,
      "grad_norm": 1.1482568979263306,
      "learning_rate": 0.00010613588745884467,
      "loss": 0.3529,
      "step": 3142
    },
    {
      "epoch": 4.230148048452221,
      "grad_norm": 1.114111304283142,
      "learning_rate": 0.00010610595630050883,
      "loss": 0.3453,
      "step": 3143
    },
    {
      "epoch": 4.231493943472409,
      "grad_norm": 1.4246634244918823,
      "learning_rate": 0.000106076025142173,
      "loss": 0.3736,
      "step": 3144
    },
    {
      "epoch": 4.232839838492597,
      "grad_norm": 1.1920608282089233,
      "learning_rate": 0.00010604609398383719,
      "loss": 0.3324,
      "step": 3145
    },
    {
      "epoch": 4.234185733512786,
      "grad_norm": 1.5564701557159424,
      "learning_rate": 0.00010601616282550136,
      "loss": 0.4682,
      "step": 3146
    },
    {
      "epoch": 4.235531628532974,
      "grad_norm": 1.2617522478103638,
      "learning_rate": 0.00010598623166716552,
      "loss": 0.3324,
      "step": 3147
    },
    {
      "epoch": 4.236877523553163,
      "grad_norm": 1.080952763557434,
      "learning_rate": 0.0001059563005088297,
      "loss": 0.3214,
      "step": 3148
    },
    {
      "epoch": 4.238223418573352,
      "grad_norm": 1.1269465684890747,
      "learning_rate": 0.00010592636935049388,
      "loss": 0.3312,
      "step": 3149
    },
    {
      "epoch": 4.23956931359354,
      "grad_norm": 1.3939132690429688,
      "learning_rate": 0.00010589643819215805,
      "loss": 0.3331,
      "step": 3150
    },
    {
      "epoch": 4.240915208613728,
      "grad_norm": 0.9486443996429443,
      "learning_rate": 0.00010586650703382221,
      "loss": 0.2864,
      "step": 3151
    },
    {
      "epoch": 4.242261103633917,
      "grad_norm": 1.4865652322769165,
      "learning_rate": 0.00010583657587548638,
      "loss": 0.4177,
      "step": 3152
    },
    {
      "epoch": 4.243606998654105,
      "grad_norm": 1.2270338535308838,
      "learning_rate": 0.00010580664471715057,
      "loss": 0.375,
      "step": 3153
    },
    {
      "epoch": 4.244952893674293,
      "grad_norm": 1.3357040882110596,
      "learning_rate": 0.00010577671355881474,
      "loss": 0.3935,
      "step": 3154
    },
    {
      "epoch": 4.246298788694482,
      "grad_norm": 1.333077073097229,
      "learning_rate": 0.0001057467824004789,
      "loss": 0.3108,
      "step": 3155
    },
    {
      "epoch": 4.24764468371467,
      "grad_norm": 1.2075790166854858,
      "learning_rate": 0.00010571685124214307,
      "loss": 0.3286,
      "step": 3156
    },
    {
      "epoch": 4.248990578734858,
      "grad_norm": 0.7039264440536499,
      "learning_rate": 0.00010568692008380726,
      "loss": 0.3226,
      "step": 3157
    },
    {
      "epoch": 4.250336473755047,
      "grad_norm": 1.259608268737793,
      "learning_rate": 0.00010565698892547143,
      "loss": 0.3684,
      "step": 3158
    },
    {
      "epoch": 4.251682368775236,
      "grad_norm": 1.3202441930770874,
      "learning_rate": 0.0001056270577671356,
      "loss": 0.3544,
      "step": 3159
    },
    {
      "epoch": 4.253028263795424,
      "grad_norm": 1.2690556049346924,
      "learning_rate": 0.00010559712660879976,
      "loss": 0.34,
      "step": 3160
    },
    {
      "epoch": 4.2543741588156125,
      "grad_norm": 1.3831310272216797,
      "learning_rate": 0.00010556719545046395,
      "loss": 0.4567,
      "step": 3161
    },
    {
      "epoch": 4.255720053835801,
      "grad_norm": 1.0512685775756836,
      "learning_rate": 0.00010553726429212812,
      "loss": 0.4014,
      "step": 3162
    },
    {
      "epoch": 4.257065948855989,
      "grad_norm": 1.2303122282028198,
      "learning_rate": 0.00010550733313379229,
      "loss": 0.4173,
      "step": 3163
    },
    {
      "epoch": 4.2584118438761775,
      "grad_norm": 1.3178507089614868,
      "learning_rate": 0.00010547740197545645,
      "loss": 0.4118,
      "step": 3164
    },
    {
      "epoch": 4.259757738896366,
      "grad_norm": 1.163437008857727,
      "learning_rate": 0.00010544747081712064,
      "loss": 0.3941,
      "step": 3165
    },
    {
      "epoch": 4.261103633916554,
      "grad_norm": 1.3225804567337036,
      "learning_rate": 0.0001054175396587848,
      "loss": 0.4483,
      "step": 3166
    },
    {
      "epoch": 4.262449528936743,
      "grad_norm": 1.1283442974090576,
      "learning_rate": 0.00010538760850044898,
      "loss": 0.3167,
      "step": 3167
    },
    {
      "epoch": 4.263795423956932,
      "grad_norm": 1.1557172536849976,
      "learning_rate": 0.00010535767734211314,
      "loss": 0.4968,
      "step": 3168
    },
    {
      "epoch": 4.26514131897712,
      "grad_norm": 1.4335553646087646,
      "learning_rate": 0.00010532774618377733,
      "loss": 0.368,
      "step": 3169
    },
    {
      "epoch": 4.2664872139973085,
      "grad_norm": 1.0185173749923706,
      "learning_rate": 0.0001052978150254415,
      "loss": 0.252,
      "step": 3170
    },
    {
      "epoch": 4.267833109017497,
      "grad_norm": 1.1952524185180664,
      "learning_rate": 0.00010526788386710567,
      "loss": 0.356,
      "step": 3171
    },
    {
      "epoch": 4.269179004037685,
      "grad_norm": 0.9147436618804932,
      "learning_rate": 0.00010523795270876983,
      "loss": 0.2805,
      "step": 3172
    },
    {
      "epoch": 4.2705248990578735,
      "grad_norm": 1.1285736560821533,
      "learning_rate": 0.00010520802155043402,
      "loss": 0.2841,
      "step": 3173
    },
    {
      "epoch": 4.271870794078062,
      "grad_norm": 1.2148395776748657,
      "learning_rate": 0.00010517809039209819,
      "loss": 0.3854,
      "step": 3174
    },
    {
      "epoch": 4.27321668909825,
      "grad_norm": 1.1964380741119385,
      "learning_rate": 0.00010514815923376236,
      "loss": 0.3783,
      "step": 3175
    },
    {
      "epoch": 4.2745625841184385,
      "grad_norm": 1.1960954666137695,
      "learning_rate": 0.00010511822807542652,
      "loss": 0.3434,
      "step": 3176
    },
    {
      "epoch": 4.275908479138627,
      "grad_norm": 1.2171406745910645,
      "learning_rate": 0.0001050882969170907,
      "loss": 0.4231,
      "step": 3177
    },
    {
      "epoch": 4.277254374158815,
      "grad_norm": 1.1826798915863037,
      "learning_rate": 0.00010505836575875488,
      "loss": 0.3363,
      "step": 3178
    },
    {
      "epoch": 4.278600269179004,
      "grad_norm": 1.1291224956512451,
      "learning_rate": 0.00010502843460041905,
      "loss": 0.272,
      "step": 3179
    },
    {
      "epoch": 4.279946164199193,
      "grad_norm": 1.3421645164489746,
      "learning_rate": 0.00010499850344208321,
      "loss": 0.3444,
      "step": 3180
    },
    {
      "epoch": 4.281292059219381,
      "grad_norm": 1.314671277999878,
      "learning_rate": 0.00010496857228374738,
      "loss": 0.3395,
      "step": 3181
    },
    {
      "epoch": 4.282637954239569,
      "grad_norm": 1.0086119174957275,
      "learning_rate": 0.00010493864112541157,
      "loss": 0.2717,
      "step": 3182
    },
    {
      "epoch": 4.283983849259758,
      "grad_norm": 1.2201977968215942,
      "learning_rate": 0.00010490870996707574,
      "loss": 0.3102,
      "step": 3183
    },
    {
      "epoch": 4.285329744279946,
      "grad_norm": 1.1202476024627686,
      "learning_rate": 0.0001048787788087399,
      "loss": 0.3107,
      "step": 3184
    },
    {
      "epoch": 4.286675639300134,
      "grad_norm": 1.2713791131973267,
      "learning_rate": 0.00010484884765040407,
      "loss": 0.278,
      "step": 3185
    },
    {
      "epoch": 4.288021534320323,
      "grad_norm": 1.267134428024292,
      "learning_rate": 0.00010481891649206826,
      "loss": 0.2713,
      "step": 3186
    },
    {
      "epoch": 4.289367429340511,
      "grad_norm": 1.1660890579223633,
      "learning_rate": 0.00010478898533373243,
      "loss": 0.2782,
      "step": 3187
    },
    {
      "epoch": 4.2907133243607,
      "grad_norm": 1.5327279567718506,
      "learning_rate": 0.0001047590541753966,
      "loss": 0.4294,
      "step": 3188
    },
    {
      "epoch": 4.292059219380889,
      "grad_norm": 1.0932252407073975,
      "learning_rate": 0.00010472912301706076,
      "loss": 0.3274,
      "step": 3189
    },
    {
      "epoch": 4.293405114401077,
      "grad_norm": 1.293061375617981,
      "learning_rate": 0.00010469919185872495,
      "loss": 0.3441,
      "step": 3190
    },
    {
      "epoch": 4.294751009421265,
      "grad_norm": 1.2580296993255615,
      "learning_rate": 0.00010466926070038912,
      "loss": 0.3779,
      "step": 3191
    },
    {
      "epoch": 4.296096904441454,
      "grad_norm": 1.3070114850997925,
      "learning_rate": 0.00010463932954205329,
      "loss": 0.3325,
      "step": 3192
    },
    {
      "epoch": 4.297442799461642,
      "grad_norm": 1.2893362045288086,
      "learning_rate": 0.00010460939838371745,
      "loss": 0.3599,
      "step": 3193
    },
    {
      "epoch": 4.29878869448183,
      "grad_norm": 1.1563313007354736,
      "learning_rate": 0.00010457946722538164,
      "loss": 0.3413,
      "step": 3194
    },
    {
      "epoch": 4.300134589502019,
      "grad_norm": 1.1674342155456543,
      "learning_rate": 0.0001045495360670458,
      "loss": 0.2737,
      "step": 3195
    },
    {
      "epoch": 4.301480484522207,
      "grad_norm": 1.399526834487915,
      "learning_rate": 0.00010451960490870998,
      "loss": 0.3731,
      "step": 3196
    },
    {
      "epoch": 4.302826379542395,
      "grad_norm": 1.2237848043441772,
      "learning_rate": 0.00010448967375037414,
      "loss": 0.3187,
      "step": 3197
    },
    {
      "epoch": 4.304172274562584,
      "grad_norm": 1.2571827173233032,
      "learning_rate": 0.00010445974259203833,
      "loss": 0.3691,
      "step": 3198
    },
    {
      "epoch": 4.305518169582773,
      "grad_norm": 1.2229540348052979,
      "learning_rate": 0.0001044298114337025,
      "loss": 0.3494,
      "step": 3199
    },
    {
      "epoch": 4.306864064602961,
      "grad_norm": 1.2207757234573364,
      "learning_rate": 0.00010439988027536667,
      "loss": 0.3334,
      "step": 3200
    },
    {
      "epoch": 4.30820995962315,
      "grad_norm": 1.196397304534912,
      "learning_rate": 0.00010436994911703083,
      "loss": 0.3951,
      "step": 3201
    },
    {
      "epoch": 4.309555854643338,
      "grad_norm": 2.2375667095184326,
      "learning_rate": 0.00010434001795869502,
      "loss": 0.4428,
      "step": 3202
    },
    {
      "epoch": 4.310901749663526,
      "grad_norm": 1.158267617225647,
      "learning_rate": 0.00010431008680035919,
      "loss": 0.3464,
      "step": 3203
    },
    {
      "epoch": 4.312247644683715,
      "grad_norm": 1.307918667793274,
      "learning_rate": 0.00010428015564202336,
      "loss": 0.2894,
      "step": 3204
    },
    {
      "epoch": 4.313593539703903,
      "grad_norm": 1.4991956949234009,
      "learning_rate": 0.00010425022448368752,
      "loss": 0.3753,
      "step": 3205
    },
    {
      "epoch": 4.314939434724091,
      "grad_norm": 0.939544677734375,
      "learning_rate": 0.00010422029332535171,
      "loss": 0.2545,
      "step": 3206
    },
    {
      "epoch": 4.31628532974428,
      "grad_norm": 1.348290205001831,
      "learning_rate": 0.00010419036216701588,
      "loss": 0.3342,
      "step": 3207
    },
    {
      "epoch": 4.317631224764469,
      "grad_norm": 1.0936154127120972,
      "learning_rate": 0.00010416043100868005,
      "loss": 0.2642,
      "step": 3208
    },
    {
      "epoch": 4.318977119784657,
      "grad_norm": 2.014683961868286,
      "learning_rate": 0.00010413049985034422,
      "loss": 0.3787,
      "step": 3209
    },
    {
      "epoch": 4.3203230148048455,
      "grad_norm": 1.3633607625961304,
      "learning_rate": 0.0001041005686920084,
      "loss": 0.4201,
      "step": 3210
    },
    {
      "epoch": 4.321668909825034,
      "grad_norm": 1.1090511083602905,
      "learning_rate": 0.00010407063753367257,
      "loss": 0.2835,
      "step": 3211
    },
    {
      "epoch": 4.323014804845222,
      "grad_norm": 1.25523042678833,
      "learning_rate": 0.00010404070637533674,
      "loss": 0.2629,
      "step": 3212
    },
    {
      "epoch": 4.3243606998654105,
      "grad_norm": 1.2316818237304688,
      "learning_rate": 0.0001040107752170009,
      "loss": 0.4004,
      "step": 3213
    },
    {
      "epoch": 4.325706594885599,
      "grad_norm": 1.2304250001907349,
      "learning_rate": 0.00010398084405866509,
      "loss": 0.324,
      "step": 3214
    },
    {
      "epoch": 4.327052489905787,
      "grad_norm": 1.489624261856079,
      "learning_rate": 0.00010395091290032926,
      "loss": 0.4102,
      "step": 3215
    },
    {
      "epoch": 4.3283983849259755,
      "grad_norm": 1.204696774482727,
      "learning_rate": 0.00010392098174199343,
      "loss": 0.4629,
      "step": 3216
    },
    {
      "epoch": 4.329744279946164,
      "grad_norm": 1.4924606084823608,
      "learning_rate": 0.0001038910505836576,
      "loss": 0.4088,
      "step": 3217
    },
    {
      "epoch": 4.331090174966352,
      "grad_norm": 1.3289334774017334,
      "learning_rate": 0.00010386111942532178,
      "loss": 0.4568,
      "step": 3218
    },
    {
      "epoch": 4.332436069986541,
      "grad_norm": 1.0006259679794312,
      "learning_rate": 0.00010383118826698595,
      "loss": 0.3004,
      "step": 3219
    },
    {
      "epoch": 4.33378196500673,
      "grad_norm": 1.5088142156600952,
      "learning_rate": 0.00010380125710865012,
      "loss": 0.5883,
      "step": 3220
    },
    {
      "epoch": 4.335127860026918,
      "grad_norm": 1.3006346225738525,
      "learning_rate": 0.00010377132595031429,
      "loss": 0.3886,
      "step": 3221
    },
    {
      "epoch": 4.336473755047106,
      "grad_norm": 1.2721303701400757,
      "learning_rate": 0.00010374139479197847,
      "loss": 0.2441,
      "step": 3222
    },
    {
      "epoch": 4.337819650067295,
      "grad_norm": 1.4713488817214966,
      "learning_rate": 0.00010371146363364264,
      "loss": 0.3507,
      "step": 3223
    },
    {
      "epoch": 4.339165545087483,
      "grad_norm": 1.1017260551452637,
      "learning_rate": 0.0001036815324753068,
      "loss": 0.3544,
      "step": 3224
    },
    {
      "epoch": 4.340511440107671,
      "grad_norm": 1.3157814741134644,
      "learning_rate": 0.00010365160131697098,
      "loss": 0.3707,
      "step": 3225
    },
    {
      "epoch": 4.34185733512786,
      "grad_norm": 1.1756706237792969,
      "learning_rate": 0.00010362167015863516,
      "loss": 0.3229,
      "step": 3226
    },
    {
      "epoch": 4.343203230148048,
      "grad_norm": 1.0998196601867676,
      "learning_rate": 0.00010359173900029933,
      "loss": 0.3329,
      "step": 3227
    },
    {
      "epoch": 4.344549125168237,
      "grad_norm": 1.0668106079101562,
      "learning_rate": 0.0001035618078419635,
      "loss": 0.2679,
      "step": 3228
    },
    {
      "epoch": 4.345895020188426,
      "grad_norm": 1.3132061958312988,
      "learning_rate": 0.00010353187668362767,
      "loss": 0.6022,
      "step": 3229
    },
    {
      "epoch": 4.347240915208614,
      "grad_norm": 1.012209415435791,
      "learning_rate": 0.00010350194552529185,
      "loss": 0.2955,
      "step": 3230
    },
    {
      "epoch": 4.348586810228802,
      "grad_norm": 1.2115660905838013,
      "learning_rate": 0.00010347201436695602,
      "loss": 0.3746,
      "step": 3231
    },
    {
      "epoch": 4.349932705248991,
      "grad_norm": 1.209862232208252,
      "learning_rate": 0.00010344208320862019,
      "loss": 0.3131,
      "step": 3232
    },
    {
      "epoch": 4.351278600269179,
      "grad_norm": 1.4005236625671387,
      "learning_rate": 0.00010341215205028436,
      "loss": 0.4962,
      "step": 3233
    },
    {
      "epoch": 4.352624495289367,
      "grad_norm": 1.3746323585510254,
      "learning_rate": 0.00010338222089194854,
      "loss": 0.4029,
      "step": 3234
    },
    {
      "epoch": 4.353970390309556,
      "grad_norm": 1.1219840049743652,
      "learning_rate": 0.00010335228973361271,
      "loss": 0.3346,
      "step": 3235
    },
    {
      "epoch": 4.355316285329744,
      "grad_norm": 1.3333265781402588,
      "learning_rate": 0.00010332235857527688,
      "loss": 0.3877,
      "step": 3236
    },
    {
      "epoch": 4.356662180349932,
      "grad_norm": 1.121410846710205,
      "learning_rate": 0.00010329242741694105,
      "loss": 0.3339,
      "step": 3237
    },
    {
      "epoch": 4.358008075370121,
      "grad_norm": 1.394727110862732,
      "learning_rate": 0.00010326249625860523,
      "loss": 0.4188,
      "step": 3238
    },
    {
      "epoch": 4.35935397039031,
      "grad_norm": 0.9480475187301636,
      "learning_rate": 0.0001032325651002694,
      "loss": 0.3309,
      "step": 3239
    },
    {
      "epoch": 4.360699865410498,
      "grad_norm": 1.2652394771575928,
      "learning_rate": 0.00010320263394193357,
      "loss": 0.2909,
      "step": 3240
    },
    {
      "epoch": 4.362045760430687,
      "grad_norm": 1.123089075088501,
      "learning_rate": 0.00010317270278359774,
      "loss": 0.2882,
      "step": 3241
    },
    {
      "epoch": 4.363391655450875,
      "grad_norm": 1.1285985708236694,
      "learning_rate": 0.00010314277162526192,
      "loss": 0.3264,
      "step": 3242
    },
    {
      "epoch": 4.364737550471063,
      "grad_norm": 0.7860589623451233,
      "learning_rate": 0.00010311284046692606,
      "loss": 0.2781,
      "step": 3243
    },
    {
      "epoch": 4.366083445491252,
      "grad_norm": 1.3678503036499023,
      "learning_rate": 0.00010308290930859023,
      "loss": 0.3897,
      "step": 3244
    },
    {
      "epoch": 4.36742934051144,
      "grad_norm": 0.9439470767974854,
      "learning_rate": 0.00010305297815025441,
      "loss": 0.3333,
      "step": 3245
    },
    {
      "epoch": 4.368775235531628,
      "grad_norm": 1.1262065172195435,
      "learning_rate": 0.00010302304699191858,
      "loss": 0.4616,
      "step": 3246
    },
    {
      "epoch": 4.370121130551817,
      "grad_norm": 1.0935158729553223,
      "learning_rate": 0.00010299311583358275,
      "loss": 0.4426,
      "step": 3247
    },
    {
      "epoch": 4.371467025572006,
      "grad_norm": 1.2463610172271729,
      "learning_rate": 0.00010296318467524692,
      "loss": 0.3158,
      "step": 3248
    },
    {
      "epoch": 4.372812920592194,
      "grad_norm": 1.542541265487671,
      "learning_rate": 0.0001029332535169111,
      "loss": 0.4474,
      "step": 3249
    },
    {
      "epoch": 4.3741588156123825,
      "grad_norm": 1.1026721000671387,
      "learning_rate": 0.00010290332235857527,
      "loss": 0.2926,
      "step": 3250
    },
    {
      "epoch": 4.375504710632571,
      "grad_norm": 1.15837562084198,
      "learning_rate": 0.00010287339120023944,
      "loss": 0.2996,
      "step": 3251
    },
    {
      "epoch": 4.376850605652759,
      "grad_norm": 1.2966357469558716,
      "learning_rate": 0.00010284346004190361,
      "loss": 0.3549,
      "step": 3252
    },
    {
      "epoch": 4.3781965006729475,
      "grad_norm": 1.3008097410202026,
      "learning_rate": 0.00010281352888356779,
      "loss": 0.365,
      "step": 3253
    },
    {
      "epoch": 4.379542395693136,
      "grad_norm": 1.5188833475112915,
      "learning_rate": 0.00010278359772523196,
      "loss": 0.3102,
      "step": 3254
    },
    {
      "epoch": 4.380888290713324,
      "grad_norm": 1.040854811668396,
      "learning_rate": 0.00010275366656689613,
      "loss": 0.3036,
      "step": 3255
    },
    {
      "epoch": 4.3822341857335125,
      "grad_norm": 1.4976133108139038,
      "learning_rate": 0.0001027237354085603,
      "loss": 0.3985,
      "step": 3256
    },
    {
      "epoch": 4.383580080753701,
      "grad_norm": 1.4772921800613403,
      "learning_rate": 0.00010269380425022448,
      "loss": 0.5384,
      "step": 3257
    },
    {
      "epoch": 4.384925975773889,
      "grad_norm": 1.1685402393341064,
      "learning_rate": 0.00010266387309188865,
      "loss": 0.3012,
      "step": 3258
    },
    {
      "epoch": 4.386271870794078,
      "grad_norm": 1.4724550247192383,
      "learning_rate": 0.00010263394193355282,
      "loss": 0.3995,
      "step": 3259
    },
    {
      "epoch": 4.387617765814267,
      "grad_norm": 1.266334056854248,
      "learning_rate": 0.00010260401077521699,
      "loss": 0.3399,
      "step": 3260
    },
    {
      "epoch": 4.388963660834455,
      "grad_norm": 1.225597858428955,
      "learning_rate": 0.00010257407961688117,
      "loss": 0.3412,
      "step": 3261
    },
    {
      "epoch": 4.390309555854643,
      "grad_norm": 1.4155609607696533,
      "learning_rate": 0.00010254414845854534,
      "loss": 0.3754,
      "step": 3262
    },
    {
      "epoch": 4.391655450874832,
      "grad_norm": 1.3331223726272583,
      "learning_rate": 0.00010251421730020951,
      "loss": 0.3621,
      "step": 3263
    },
    {
      "epoch": 4.39300134589502,
      "grad_norm": 1.2060455083847046,
      "learning_rate": 0.00010248428614187368,
      "loss": 0.3644,
      "step": 3264
    },
    {
      "epoch": 4.3943472409152085,
      "grad_norm": 1.1967213153839111,
      "learning_rate": 0.00010245435498353786,
      "loss": 0.4855,
      "step": 3265
    },
    {
      "epoch": 4.395693135935397,
      "grad_norm": 1.11040461063385,
      "learning_rate": 0.00010242442382520203,
      "loss": 0.3459,
      "step": 3266
    },
    {
      "epoch": 4.397039030955585,
      "grad_norm": 1.2837092876434326,
      "learning_rate": 0.0001023944926668662,
      "loss": 0.3803,
      "step": 3267
    },
    {
      "epoch": 4.398384925975774,
      "grad_norm": 1.1394833326339722,
      "learning_rate": 0.00010236456150853037,
      "loss": 0.5076,
      "step": 3268
    },
    {
      "epoch": 4.399730820995963,
      "grad_norm": 1.1207209825515747,
      "learning_rate": 0.00010233463035019455,
      "loss": 0.3394,
      "step": 3269
    },
    {
      "epoch": 4.401076716016151,
      "grad_norm": 1.1803275346755981,
      "learning_rate": 0.00010230469919185872,
      "loss": 0.3202,
      "step": 3270
    },
    {
      "epoch": 4.402422611036339,
      "grad_norm": 1.09395170211792,
      "learning_rate": 0.00010227476803352289,
      "loss": 0.3379,
      "step": 3271
    },
    {
      "epoch": 4.403768506056528,
      "grad_norm": 1.1814943552017212,
      "learning_rate": 0.00010224483687518706,
      "loss": 0.3327,
      "step": 3272
    },
    {
      "epoch": 4.405114401076716,
      "grad_norm": 1.4131187200546265,
      "learning_rate": 0.00010221490571685124,
      "loss": 0.3873,
      "step": 3273
    },
    {
      "epoch": 4.406460296096904,
      "grad_norm": 1.7263795137405396,
      "learning_rate": 0.00010218497455851541,
      "loss": 0.4892,
      "step": 3274
    },
    {
      "epoch": 4.407806191117093,
      "grad_norm": 1.2640377283096313,
      "learning_rate": 0.00010215504340017958,
      "loss": 0.3034,
      "step": 3275
    },
    {
      "epoch": 4.409152086137281,
      "grad_norm": 1.1297481060028076,
      "learning_rate": 0.00010212511224184375,
      "loss": 0.281,
      "step": 3276
    },
    {
      "epoch": 4.410497981157469,
      "grad_norm": 1.1114031076431274,
      "learning_rate": 0.00010209518108350793,
      "loss": 0.2619,
      "step": 3277
    },
    {
      "epoch": 4.411843876177658,
      "grad_norm": 1.0566012859344482,
      "learning_rate": 0.0001020652499251721,
      "loss": 0.3656,
      "step": 3278
    },
    {
      "epoch": 4.413189771197847,
      "grad_norm": 1.2379398345947266,
      "learning_rate": 0.00010203531876683627,
      "loss": 0.5887,
      "step": 3279
    },
    {
      "epoch": 4.414535666218035,
      "grad_norm": 1.3868845701217651,
      "learning_rate": 0.00010200538760850044,
      "loss": 0.4014,
      "step": 3280
    },
    {
      "epoch": 4.415881561238224,
      "grad_norm": 1.323502540588379,
      "learning_rate": 0.00010197545645016462,
      "loss": 0.3406,
      "step": 3281
    },
    {
      "epoch": 4.417227456258412,
      "grad_norm": 0.9737120866775513,
      "learning_rate": 0.0001019455252918288,
      "loss": 0.2869,
      "step": 3282
    },
    {
      "epoch": 4.4185733512786,
      "grad_norm": 1.0867304801940918,
      "learning_rate": 0.00010191559413349296,
      "loss": 0.2929,
      "step": 3283
    },
    {
      "epoch": 4.419919246298789,
      "grad_norm": 1.2539219856262207,
      "learning_rate": 0.00010188566297515713,
      "loss": 0.3697,
      "step": 3284
    },
    {
      "epoch": 4.421265141318977,
      "grad_norm": 1.1928801536560059,
      "learning_rate": 0.00010185573181682131,
      "loss": 0.3732,
      "step": 3285
    },
    {
      "epoch": 4.422611036339165,
      "grad_norm": 1.2505472898483276,
      "learning_rate": 0.00010182580065848548,
      "loss": 0.3009,
      "step": 3286
    },
    {
      "epoch": 4.423956931359354,
      "grad_norm": 0.9729588627815247,
      "learning_rate": 0.00010179586950014965,
      "loss": 0.4365,
      "step": 3287
    },
    {
      "epoch": 4.425302826379543,
      "grad_norm": 1.195332407951355,
      "learning_rate": 0.00010176593834181382,
      "loss": 0.3492,
      "step": 3288
    },
    {
      "epoch": 4.426648721399731,
      "grad_norm": 1.3499388694763184,
      "learning_rate": 0.000101736007183478,
      "loss": 0.3574,
      "step": 3289
    },
    {
      "epoch": 4.4279946164199195,
      "grad_norm": 1.565974473953247,
      "learning_rate": 0.00010170607602514217,
      "loss": 0.3996,
      "step": 3290
    },
    {
      "epoch": 4.429340511440108,
      "grad_norm": 1.2849520444869995,
      "learning_rate": 0.00010167614486680634,
      "loss": 0.3378,
      "step": 3291
    },
    {
      "epoch": 4.430686406460296,
      "grad_norm": 1.2683786153793335,
      "learning_rate": 0.00010164621370847051,
      "loss": 0.3386,
      "step": 3292
    },
    {
      "epoch": 4.4320323014804845,
      "grad_norm": 1.1434261798858643,
      "learning_rate": 0.0001016162825501347,
      "loss": 0.3542,
      "step": 3293
    },
    {
      "epoch": 4.433378196500673,
      "grad_norm": 1.1038602590560913,
      "learning_rate": 0.00010158635139179886,
      "loss": 0.2961,
      "step": 3294
    },
    {
      "epoch": 4.434724091520861,
      "grad_norm": 1.270165205001831,
      "learning_rate": 0.00010155642023346303,
      "loss": 0.4066,
      "step": 3295
    },
    {
      "epoch": 4.43606998654105,
      "grad_norm": 1.3636474609375,
      "learning_rate": 0.0001015264890751272,
      "loss": 0.4229,
      "step": 3296
    },
    {
      "epoch": 4.437415881561238,
      "grad_norm": 1.3737597465515137,
      "learning_rate": 0.00010149655791679138,
      "loss": 0.3429,
      "step": 3297
    },
    {
      "epoch": 4.438761776581426,
      "grad_norm": 1.241660237312317,
      "learning_rate": 0.00010146662675845555,
      "loss": 0.3295,
      "step": 3298
    },
    {
      "epoch": 4.4401076716016155,
      "grad_norm": 1.207600712776184,
      "learning_rate": 0.00010143669560011972,
      "loss": 0.2952,
      "step": 3299
    },
    {
      "epoch": 4.441453566621804,
      "grad_norm": 1.352013349533081,
      "learning_rate": 0.00010140676444178389,
      "loss": 0.4271,
      "step": 3300
    },
    {
      "epoch": 4.442799461641992,
      "grad_norm": 1.6130563020706177,
      "learning_rate": 0.00010137683328344807,
      "loss": 0.5385,
      "step": 3301
    },
    {
      "epoch": 4.4441453566621805,
      "grad_norm": 1.6128932237625122,
      "learning_rate": 0.00010134690212511224,
      "loss": 0.4718,
      "step": 3302
    },
    {
      "epoch": 4.445491251682369,
      "grad_norm": 2.074598789215088,
      "learning_rate": 0.00010131697096677641,
      "loss": 0.4668,
      "step": 3303
    },
    {
      "epoch": 4.446837146702557,
      "grad_norm": 1.0575512647628784,
      "learning_rate": 0.00010128703980844058,
      "loss": 0.2818,
      "step": 3304
    },
    {
      "epoch": 4.4481830417227455,
      "grad_norm": 1.357599139213562,
      "learning_rate": 0.00010125710865010477,
      "loss": 0.3907,
      "step": 3305
    },
    {
      "epoch": 4.449528936742934,
      "grad_norm": 1.127312183380127,
      "learning_rate": 0.00010122717749176893,
      "loss": 0.3253,
      "step": 3306
    },
    {
      "epoch": 4.450874831763122,
      "grad_norm": 1.3892780542373657,
      "learning_rate": 0.0001011972463334331,
      "loss": 0.4171,
      "step": 3307
    },
    {
      "epoch": 4.4522207267833105,
      "grad_norm": 1.358542561531067,
      "learning_rate": 0.00010116731517509727,
      "loss": 0.3895,
      "step": 3308
    },
    {
      "epoch": 4.4535666218035,
      "grad_norm": 1.1589999198913574,
      "learning_rate": 0.00010113738401676146,
      "loss": 0.3327,
      "step": 3309
    },
    {
      "epoch": 4.454912516823688,
      "grad_norm": 1.2172048091888428,
      "learning_rate": 0.00010110745285842562,
      "loss": 0.4322,
      "step": 3310
    },
    {
      "epoch": 4.456258411843876,
      "grad_norm": 1.0857315063476562,
      "learning_rate": 0.0001010775217000898,
      "loss": 0.2869,
      "step": 3311
    },
    {
      "epoch": 4.457604306864065,
      "grad_norm": 1.4035295248031616,
      "learning_rate": 0.00010104759054175396,
      "loss": 0.3953,
      "step": 3312
    },
    {
      "epoch": 4.458950201884253,
      "grad_norm": 0.8939598798751831,
      "learning_rate": 0.00010101765938341815,
      "loss": 0.2409,
      "step": 3313
    },
    {
      "epoch": 4.460296096904441,
      "grad_norm": 0.963318943977356,
      "learning_rate": 0.00010098772822508231,
      "loss": 0.3476,
      "step": 3314
    },
    {
      "epoch": 4.46164199192463,
      "grad_norm": 0.9316149950027466,
      "learning_rate": 0.00010095779706674648,
      "loss": 0.2837,
      "step": 3315
    },
    {
      "epoch": 4.462987886944818,
      "grad_norm": 1.1994539499282837,
      "learning_rate": 0.00010092786590841065,
      "loss": 0.3332,
      "step": 3316
    },
    {
      "epoch": 4.464333781965006,
      "grad_norm": 1.1453217267990112,
      "learning_rate": 0.00010089793475007484,
      "loss": 0.3319,
      "step": 3317
    },
    {
      "epoch": 4.465679676985195,
      "grad_norm": 1.3945075273513794,
      "learning_rate": 0.000100868003591739,
      "loss": 0.3379,
      "step": 3318
    },
    {
      "epoch": 4.467025572005384,
      "grad_norm": 1.2658473253250122,
      "learning_rate": 0.00010083807243340317,
      "loss": 0.2873,
      "step": 3319
    },
    {
      "epoch": 4.468371467025572,
      "grad_norm": 1.6887800693511963,
      "learning_rate": 0.00010080814127506734,
      "loss": 0.3691,
      "step": 3320
    },
    {
      "epoch": 4.469717362045761,
      "grad_norm": 1.4112663269042969,
      "learning_rate": 0.00010077821011673153,
      "loss": 0.4405,
      "step": 3321
    },
    {
      "epoch": 4.471063257065949,
      "grad_norm": 1.3454207181930542,
      "learning_rate": 0.0001007482789583957,
      "loss": 0.3903,
      "step": 3322
    },
    {
      "epoch": 4.472409152086137,
      "grad_norm": 1.4291781187057495,
      "learning_rate": 0.00010071834780005986,
      "loss": 0.3912,
      "step": 3323
    },
    {
      "epoch": 4.473755047106326,
      "grad_norm": 1.2102305889129639,
      "learning_rate": 0.00010068841664172403,
      "loss": 0.4476,
      "step": 3324
    },
    {
      "epoch": 4.475100942126514,
      "grad_norm": 1.4200665950775146,
      "learning_rate": 0.00010065848548338822,
      "loss": 0.6131,
      "step": 3325
    },
    {
      "epoch": 4.476446837146702,
      "grad_norm": 1.4454582929611206,
      "learning_rate": 0.00010062855432505238,
      "loss": 0.3604,
      "step": 3326
    },
    {
      "epoch": 4.477792732166891,
      "grad_norm": 1.2907716035842896,
      "learning_rate": 0.00010059862316671655,
      "loss": 0.3969,
      "step": 3327
    },
    {
      "epoch": 4.479138627187079,
      "grad_norm": 1.2444779872894287,
      "learning_rate": 0.00010056869200838072,
      "loss": 0.3213,
      "step": 3328
    },
    {
      "epoch": 4.480484522207268,
      "grad_norm": 1.2683161497116089,
      "learning_rate": 0.0001005387608500449,
      "loss": 0.3399,
      "step": 3329
    },
    {
      "epoch": 4.481830417227457,
      "grad_norm": 1.466913104057312,
      "learning_rate": 0.00010050882969170908,
      "loss": 0.3258,
      "step": 3330
    },
    {
      "epoch": 4.483176312247645,
      "grad_norm": 1.0341403484344482,
      "learning_rate": 0.00010047889853337324,
      "loss": 0.3734,
      "step": 3331
    },
    {
      "epoch": 4.484522207267833,
      "grad_norm": 1.0442835092544556,
      "learning_rate": 0.00010044896737503741,
      "loss": 0.2918,
      "step": 3332
    },
    {
      "epoch": 4.485868102288022,
      "grad_norm": 1.1967214345932007,
      "learning_rate": 0.0001004190362167016,
      "loss": 0.3614,
      "step": 3333
    },
    {
      "epoch": 4.48721399730821,
      "grad_norm": 1.0429484844207764,
      "learning_rate": 0.00010038910505836577,
      "loss": 0.3069,
      "step": 3334
    },
    {
      "epoch": 4.488559892328398,
      "grad_norm": 1.2011303901672363,
      "learning_rate": 0.00010035917390002993,
      "loss": 0.3615,
      "step": 3335
    },
    {
      "epoch": 4.489905787348587,
      "grad_norm": 1.1439056396484375,
      "learning_rate": 0.0001003292427416941,
      "loss": 0.3207,
      "step": 3336
    },
    {
      "epoch": 4.491251682368775,
      "grad_norm": 1.2933058738708496,
      "learning_rate": 0.00010029931158335829,
      "loss": 0.3088,
      "step": 3337
    },
    {
      "epoch": 4.492597577388963,
      "grad_norm": 1.2801674604415894,
      "learning_rate": 0.00010026938042502246,
      "loss": 0.311,
      "step": 3338
    },
    {
      "epoch": 4.4939434724091525,
      "grad_norm": 1.2471920251846313,
      "learning_rate": 0.00010023944926668662,
      "loss": 0.4488,
      "step": 3339
    },
    {
      "epoch": 4.495289367429341,
      "grad_norm": 1.7940318584442139,
      "learning_rate": 0.0001002095181083508,
      "loss": 0.3681,
      "step": 3340
    },
    {
      "epoch": 4.496635262449529,
      "grad_norm": 1.0271168947219849,
      "learning_rate": 0.00010017958695001498,
      "loss": 0.4954,
      "step": 3341
    },
    {
      "epoch": 4.4979811574697175,
      "grad_norm": 1.4328266382217407,
      "learning_rate": 0.00010014965579167915,
      "loss": 0.4693,
      "step": 3342
    },
    {
      "epoch": 4.499327052489906,
      "grad_norm": 1.1123427152633667,
      "learning_rate": 0.00010011972463334331,
      "loss": 0.475,
      "step": 3343
    },
    {
      "epoch": 4.500672947510094,
      "grad_norm": 1.1970996856689453,
      "learning_rate": 0.00010008979347500748,
      "loss": 0.3357,
      "step": 3344
    },
    {
      "epoch": 4.5020188425302825,
      "grad_norm": 1.3081557750701904,
      "learning_rate": 0.00010005986231667167,
      "loss": 0.4689,
      "step": 3345
    },
    {
      "epoch": 4.503364737550471,
      "grad_norm": 1.0183900594711304,
      "learning_rate": 0.00010002993115833584,
      "loss": 0.2863,
      "step": 3346
    },
    {
      "epoch": 4.504710632570659,
      "grad_norm": 1.2539050579071045,
      "learning_rate": 0.0001,
      "loss": 0.3867,
      "step": 3347
    },
    {
      "epoch": 4.506056527590848,
      "grad_norm": 1.0699810981750488,
      "learning_rate": 9.997006884166417e-05,
      "loss": 0.3453,
      "step": 3348
    },
    {
      "epoch": 4.507402422611037,
      "grad_norm": 1.4375317096710205,
      "learning_rate": 9.994013768332834e-05,
      "loss": 0.3767,
      "step": 3349
    },
    {
      "epoch": 4.508748317631225,
      "grad_norm": 1.2158690690994263,
      "learning_rate": 9.991020652499253e-05,
      "loss": 0.3437,
      "step": 3350
    },
    {
      "epoch": 4.510094212651413,
      "grad_norm": 1.1953532695770264,
      "learning_rate": 9.98802753666567e-05,
      "loss": 0.3418,
      "step": 3351
    },
    {
      "epoch": 4.511440107671602,
      "grad_norm": 1.230769395828247,
      "learning_rate": 9.985034420832086e-05,
      "loss": 0.3393,
      "step": 3352
    },
    {
      "epoch": 4.51278600269179,
      "grad_norm": 1.0023481845855713,
      "learning_rate": 9.982041304998503e-05,
      "loss": 0.3482,
      "step": 3353
    },
    {
      "epoch": 4.514131897711978,
      "grad_norm": 1.7045618295669556,
      "learning_rate": 9.979048189164922e-05,
      "loss": 0.4893,
      "step": 3354
    },
    {
      "epoch": 4.515477792732167,
      "grad_norm": 1.3240387439727783,
      "learning_rate": 9.976055073331339e-05,
      "loss": 0.3843,
      "step": 3355
    },
    {
      "epoch": 4.516823687752355,
      "grad_norm": 1.317381501197815,
      "learning_rate": 9.973061957497755e-05,
      "loss": 0.421,
      "step": 3356
    },
    {
      "epoch": 4.518169582772543,
      "grad_norm": 1.431136965751648,
      "learning_rate": 9.970068841664172e-05,
      "loss": 0.4129,
      "step": 3357
    },
    {
      "epoch": 4.519515477792732,
      "grad_norm": 1.0378894805908203,
      "learning_rate": 9.96707572583059e-05,
      "loss": 0.2948,
      "step": 3358
    },
    {
      "epoch": 4.52086137281292,
      "grad_norm": 1.5196845531463623,
      "learning_rate": 9.964082609997008e-05,
      "loss": 0.3928,
      "step": 3359
    },
    {
      "epoch": 4.522207267833109,
      "grad_norm": 1.5484671592712402,
      "learning_rate": 9.961089494163424e-05,
      "loss": 0.4883,
      "step": 3360
    },
    {
      "epoch": 4.523553162853298,
      "grad_norm": 1.4585604667663574,
      "learning_rate": 9.958096378329841e-05,
      "loss": 0.376,
      "step": 3361
    },
    {
      "epoch": 4.524899057873486,
      "grad_norm": 1.5081582069396973,
      "learning_rate": 9.95510326249626e-05,
      "loss": 0.4431,
      "step": 3362
    },
    {
      "epoch": 4.526244952893674,
      "grad_norm": 1.4273202419281006,
      "learning_rate": 9.952110146662677e-05,
      "loss": 0.4425,
      "step": 3363
    },
    {
      "epoch": 4.527590847913863,
      "grad_norm": 1.0429909229278564,
      "learning_rate": 9.949117030829093e-05,
      "loss": 0.3375,
      "step": 3364
    },
    {
      "epoch": 4.528936742934051,
      "grad_norm": 1.2418791055679321,
      "learning_rate": 9.94612391499551e-05,
      "loss": 0.3678,
      "step": 3365
    },
    {
      "epoch": 4.530282637954239,
      "grad_norm": 1.3395723104476929,
      "learning_rate": 9.943130799161929e-05,
      "loss": 0.4226,
      "step": 3366
    },
    {
      "epoch": 4.531628532974428,
      "grad_norm": 1.606894850730896,
      "learning_rate": 9.940137683328346e-05,
      "loss": 0.5059,
      "step": 3367
    },
    {
      "epoch": 4.532974427994617,
      "grad_norm": 1.464811086654663,
      "learning_rate": 9.937144567494762e-05,
      "loss": 0.4611,
      "step": 3368
    },
    {
      "epoch": 4.534320323014805,
      "grad_norm": 1.8152705430984497,
      "learning_rate": 9.93415145166118e-05,
      "loss": 0.5778,
      "step": 3369
    },
    {
      "epoch": 4.535666218034994,
      "grad_norm": 1.3316890001296997,
      "learning_rate": 9.931158335827598e-05,
      "loss": 0.5411,
      "step": 3370
    },
    {
      "epoch": 4.537012113055182,
      "grad_norm": 1.5006052255630493,
      "learning_rate": 9.928165219994015e-05,
      "loss": 0.4638,
      "step": 3371
    },
    {
      "epoch": 4.53835800807537,
      "grad_norm": 1.3210177421569824,
      "learning_rate": 9.925172104160431e-05,
      "loss": 0.4412,
      "step": 3372
    },
    {
      "epoch": 4.539703903095559,
      "grad_norm": 1.3862967491149902,
      "learning_rate": 9.922178988326848e-05,
      "loss": 0.3908,
      "step": 3373
    },
    {
      "epoch": 4.541049798115747,
      "grad_norm": 1.004048466682434,
      "learning_rate": 9.919185872493267e-05,
      "loss": 0.3199,
      "step": 3374
    },
    {
      "epoch": 4.542395693135935,
      "grad_norm": 1.5026969909667969,
      "learning_rate": 9.916192756659684e-05,
      "loss": 0.426,
      "step": 3375
    },
    {
      "epoch": 4.543741588156124,
      "grad_norm": 1.4568389654159546,
      "learning_rate": 9.9131996408261e-05,
      "loss": 0.4119,
      "step": 3376
    },
    {
      "epoch": 4.545087483176312,
      "grad_norm": 1.5385843515396118,
      "learning_rate": 9.910206524992517e-05,
      "loss": 0.5191,
      "step": 3377
    },
    {
      "epoch": 4.5464333781965,
      "grad_norm": 1.1385767459869385,
      "learning_rate": 9.907213409158936e-05,
      "loss": 0.3374,
      "step": 3378
    },
    {
      "epoch": 4.547779273216689,
      "grad_norm": 1.514986515045166,
      "learning_rate": 9.904220293325353e-05,
      "loss": 0.3668,
      "step": 3379
    },
    {
      "epoch": 4.549125168236878,
      "grad_norm": 1.200927734375,
      "learning_rate": 9.90122717749177e-05,
      "loss": 0.394,
      "step": 3380
    },
    {
      "epoch": 4.550471063257066,
      "grad_norm": 1.4184801578521729,
      "learning_rate": 9.898234061658186e-05,
      "loss": 0.4086,
      "step": 3381
    },
    {
      "epoch": 4.5518169582772545,
      "grad_norm": 1.3533324003219604,
      "learning_rate": 9.895240945824605e-05,
      "loss": 0.3465,
      "step": 3382
    },
    {
      "epoch": 4.553162853297443,
      "grad_norm": 1.1749130487442017,
      "learning_rate": 9.892247829991022e-05,
      "loss": 0.3704,
      "step": 3383
    },
    {
      "epoch": 4.554508748317631,
      "grad_norm": 1.2325042486190796,
      "learning_rate": 9.889254714157439e-05,
      "loss": 0.4331,
      "step": 3384
    },
    {
      "epoch": 4.5558546433378195,
      "grad_norm": 1.2928327322006226,
      "learning_rate": 9.886261598323855e-05,
      "loss": 0.3811,
      "step": 3385
    },
    {
      "epoch": 4.557200538358008,
      "grad_norm": 0.9806159138679504,
      "learning_rate": 9.883268482490274e-05,
      "loss": 0.3098,
      "step": 3386
    },
    {
      "epoch": 4.558546433378196,
      "grad_norm": 1.3224356174468994,
      "learning_rate": 9.88027536665669e-05,
      "loss": 0.4386,
      "step": 3387
    },
    {
      "epoch": 4.5598923283983845,
      "grad_norm": 1.1709671020507812,
      "learning_rate": 9.877282250823108e-05,
      "loss": 0.3643,
      "step": 3388
    },
    {
      "epoch": 4.561238223418574,
      "grad_norm": 1.223874568939209,
      "learning_rate": 9.874289134989524e-05,
      "loss": 0.4455,
      "step": 3389
    },
    {
      "epoch": 4.562584118438762,
      "grad_norm": 1.353196144104004,
      "learning_rate": 9.871296019155943e-05,
      "loss": 0.3842,
      "step": 3390
    },
    {
      "epoch": 4.56393001345895,
      "grad_norm": 1.3053252696990967,
      "learning_rate": 9.86830290332236e-05,
      "loss": 0.3774,
      "step": 3391
    },
    {
      "epoch": 4.565275908479139,
      "grad_norm": 1.4886606931686401,
      "learning_rate": 9.865309787488777e-05,
      "loss": 0.5171,
      "step": 3392
    },
    {
      "epoch": 4.566621803499327,
      "grad_norm": 1.0754142999649048,
      "learning_rate": 9.862316671655193e-05,
      "loss": 0.3397,
      "step": 3393
    },
    {
      "epoch": 4.5679676985195155,
      "grad_norm": 1.1832810640335083,
      "learning_rate": 9.859323555821612e-05,
      "loss": 0.4004,
      "step": 3394
    },
    {
      "epoch": 4.569313593539704,
      "grad_norm": 1.1578412055969238,
      "learning_rate": 9.856330439988029e-05,
      "loss": 0.2656,
      "step": 3395
    },
    {
      "epoch": 4.570659488559892,
      "grad_norm": 1.334542155265808,
      "learning_rate": 9.853337324154446e-05,
      "loss": 0.3428,
      "step": 3396
    },
    {
      "epoch": 4.5720053835800805,
      "grad_norm": 1.3724740743637085,
      "learning_rate": 9.850344208320862e-05,
      "loss": 0.4638,
      "step": 3397
    },
    {
      "epoch": 4.573351278600269,
      "grad_norm": 1.4408092498779297,
      "learning_rate": 9.847351092487281e-05,
      "loss": 0.4521,
      "step": 3398
    },
    {
      "epoch": 4.574697173620457,
      "grad_norm": 1.3393523693084717,
      "learning_rate": 9.844357976653698e-05,
      "loss": 0.3624,
      "step": 3399
    },
    {
      "epoch": 4.576043068640646,
      "grad_norm": 1.3654210567474365,
      "learning_rate": 9.841364860820113e-05,
      "loss": 0.4246,
      "step": 3400
    },
    {
      "epoch": 4.577388963660835,
      "grad_norm": 1.0608775615692139,
      "learning_rate": 9.838371744986532e-05,
      "loss": 0.2764,
      "step": 3401
    },
    {
      "epoch": 4.578734858681023,
      "grad_norm": 1.193401575088501,
      "learning_rate": 9.835378629152948e-05,
      "loss": 0.4301,
      "step": 3402
    },
    {
      "epoch": 4.580080753701211,
      "grad_norm": 1.3213229179382324,
      "learning_rate": 9.832385513319365e-05,
      "loss": 0.4011,
      "step": 3403
    },
    {
      "epoch": 4.5814266487214,
      "grad_norm": 1.1050530672073364,
      "learning_rate": 9.829392397485782e-05,
      "loss": 0.3974,
      "step": 3404
    },
    {
      "epoch": 4.582772543741588,
      "grad_norm": 1.6597797870635986,
      "learning_rate": 9.8263992816522e-05,
      "loss": 0.368,
      "step": 3405
    },
    {
      "epoch": 4.584118438761776,
      "grad_norm": 1.2429087162017822,
      "learning_rate": 9.823406165818617e-05,
      "loss": 0.3068,
      "step": 3406
    },
    {
      "epoch": 4.585464333781965,
      "grad_norm": 1.1794918775558472,
      "learning_rate": 9.820413049985034e-05,
      "loss": 0.3462,
      "step": 3407
    },
    {
      "epoch": 4.586810228802153,
      "grad_norm": 1.368541955947876,
      "learning_rate": 9.817419934151451e-05,
      "loss": 0.3917,
      "step": 3408
    },
    {
      "epoch": 4.588156123822342,
      "grad_norm": 1.124410629272461,
      "learning_rate": 9.81442681831787e-05,
      "loss": 0.3012,
      "step": 3409
    },
    {
      "epoch": 4.589502018842531,
      "grad_norm": 1.3642029762268066,
      "learning_rate": 9.811433702484286e-05,
      "loss": 0.3321,
      "step": 3410
    },
    {
      "epoch": 4.590847913862719,
      "grad_norm": 1.3122692108154297,
      "learning_rate": 9.808440586650703e-05,
      "loss": 0.378,
      "step": 3411
    },
    {
      "epoch": 4.592193808882907,
      "grad_norm": 1.186609148979187,
      "learning_rate": 9.80544747081712e-05,
      "loss": 0.3495,
      "step": 3412
    },
    {
      "epoch": 4.593539703903096,
      "grad_norm": 1.5332099199295044,
      "learning_rate": 9.802454354983539e-05,
      "loss": 0.364,
      "step": 3413
    },
    {
      "epoch": 4.594885598923284,
      "grad_norm": 1.4254754781723022,
      "learning_rate": 9.799461239149955e-05,
      "loss": 0.3281,
      "step": 3414
    },
    {
      "epoch": 4.596231493943472,
      "grad_norm": 1.1867045164108276,
      "learning_rate": 9.796468123316372e-05,
      "loss": 0.3037,
      "step": 3415
    },
    {
      "epoch": 4.597577388963661,
      "grad_norm": 1.3321735858917236,
      "learning_rate": 9.793475007482789e-05,
      "loss": 0.3041,
      "step": 3416
    },
    {
      "epoch": 4.598923283983849,
      "grad_norm": 1.2256314754486084,
      "learning_rate": 9.790481891649208e-05,
      "loss": 0.384,
      "step": 3417
    },
    {
      "epoch": 4.600269179004037,
      "grad_norm": 1.1980668306350708,
      "learning_rate": 9.787488775815624e-05,
      "loss": 0.4238,
      "step": 3418
    },
    {
      "epoch": 4.601615074024226,
      "grad_norm": 1.4098773002624512,
      "learning_rate": 9.784495659982041e-05,
      "loss": 0.3187,
      "step": 3419
    },
    {
      "epoch": 4.602960969044415,
      "grad_norm": 1.3533401489257812,
      "learning_rate": 9.781502544148458e-05,
      "loss": 0.4928,
      "step": 3420
    },
    {
      "epoch": 4.604306864064603,
      "grad_norm": 1.4368952512741089,
      "learning_rate": 9.778509428314877e-05,
      "loss": 0.4141,
      "step": 3421
    },
    {
      "epoch": 4.6056527590847915,
      "grad_norm": 1.4261481761932373,
      "learning_rate": 9.775516312481293e-05,
      "loss": 0.3868,
      "step": 3422
    },
    {
      "epoch": 4.60699865410498,
      "grad_norm": 1.081941843032837,
      "learning_rate": 9.77252319664771e-05,
      "loss": 0.4051,
      "step": 3423
    },
    {
      "epoch": 4.608344549125168,
      "grad_norm": 1.0527935028076172,
      "learning_rate": 9.769530080814127e-05,
      "loss": 0.3979,
      "step": 3424
    },
    {
      "epoch": 4.609690444145357,
      "grad_norm": 1.1897047758102417,
      "learning_rate": 9.766536964980546e-05,
      "loss": 0.4106,
      "step": 3425
    },
    {
      "epoch": 4.611036339165545,
      "grad_norm": 1.3435159921646118,
      "learning_rate": 9.763543849146963e-05,
      "loss": 0.3517,
      "step": 3426
    },
    {
      "epoch": 4.612382234185733,
      "grad_norm": 1.3628523349761963,
      "learning_rate": 9.76055073331338e-05,
      "loss": 0.3325,
      "step": 3427
    },
    {
      "epoch": 4.613728129205922,
      "grad_norm": 1.5115400552749634,
      "learning_rate": 9.757557617479796e-05,
      "loss": 0.404,
      "step": 3428
    },
    {
      "epoch": 4.615074024226111,
      "grad_norm": 1.2433987855911255,
      "learning_rate": 9.754564501646215e-05,
      "loss": 0.382,
      "step": 3429
    },
    {
      "epoch": 4.616419919246299,
      "grad_norm": 1.0057201385498047,
      "learning_rate": 9.751571385812632e-05,
      "loss": 0.3495,
      "step": 3430
    },
    {
      "epoch": 4.6177658142664875,
      "grad_norm": 1.4449400901794434,
      "learning_rate": 9.748578269979048e-05,
      "loss": 0.3534,
      "step": 3431
    },
    {
      "epoch": 4.619111709286676,
      "grad_norm": 1.138020634651184,
      "learning_rate": 9.745585154145465e-05,
      "loss": 0.4044,
      "step": 3432
    },
    {
      "epoch": 4.620457604306864,
      "grad_norm": 1.526986837387085,
      "learning_rate": 9.742592038311882e-05,
      "loss": 0.6034,
      "step": 3433
    },
    {
      "epoch": 4.6218034993270525,
      "grad_norm": 1.1971184015274048,
      "learning_rate": 9.7395989224783e-05,
      "loss": 0.3307,
      "step": 3434
    },
    {
      "epoch": 4.623149394347241,
      "grad_norm": 1.1449912786483765,
      "learning_rate": 9.736605806644717e-05,
      "loss": 0.389,
      "step": 3435
    },
    {
      "epoch": 4.624495289367429,
      "grad_norm": 1.164392352104187,
      "learning_rate": 9.733612690811134e-05,
      "loss": 0.331,
      "step": 3436
    },
    {
      "epoch": 4.6258411843876175,
      "grad_norm": 1.1950773000717163,
      "learning_rate": 9.730619574977551e-05,
      "loss": 0.3387,
      "step": 3437
    },
    {
      "epoch": 4.627187079407806,
      "grad_norm": 1.3485329151153564,
      "learning_rate": 9.72762645914397e-05,
      "loss": 0.4389,
      "step": 3438
    },
    {
      "epoch": 4.628532974427994,
      "grad_norm": 1.1702803373336792,
      "learning_rate": 9.724633343310386e-05,
      "loss": 0.3457,
      "step": 3439
    },
    {
      "epoch": 4.629878869448183,
      "grad_norm": 1.132692575454712,
      "learning_rate": 9.721640227476803e-05,
      "loss": 0.3364,
      "step": 3440
    },
    {
      "epoch": 4.631224764468372,
      "grad_norm": 1.1760834455490112,
      "learning_rate": 9.71864711164322e-05,
      "loss": 0.3817,
      "step": 3441
    },
    {
      "epoch": 4.63257065948856,
      "grad_norm": 1.6681913137435913,
      "learning_rate": 9.715653995809639e-05,
      "loss": 0.4298,
      "step": 3442
    },
    {
      "epoch": 4.633916554508748,
      "grad_norm": 0.9485131502151489,
      "learning_rate": 9.712660879976055e-05,
      "loss": 0.314,
      "step": 3443
    },
    {
      "epoch": 4.635262449528937,
      "grad_norm": 1.3819862604141235,
      "learning_rate": 9.709667764142472e-05,
      "loss": 0.3963,
      "step": 3444
    },
    {
      "epoch": 4.636608344549125,
      "grad_norm": 1.2440871000289917,
      "learning_rate": 9.706674648308889e-05,
      "loss": 0.3549,
      "step": 3445
    },
    {
      "epoch": 4.637954239569313,
      "grad_norm": 1.1209800243377686,
      "learning_rate": 9.703681532475308e-05,
      "loss": 0.3239,
      "step": 3446
    },
    {
      "epoch": 4.639300134589502,
      "grad_norm": 1.1033049821853638,
      "learning_rate": 9.700688416641724e-05,
      "loss": 0.5085,
      "step": 3447
    },
    {
      "epoch": 4.64064602960969,
      "grad_norm": 1.1015018224716187,
      "learning_rate": 9.697695300808141e-05,
      "loss": 0.436,
      "step": 3448
    },
    {
      "epoch": 4.641991924629879,
      "grad_norm": 0.9860460162162781,
      "learning_rate": 9.694702184974558e-05,
      "loss": 0.4065,
      "step": 3449
    },
    {
      "epoch": 4.643337819650068,
      "grad_norm": 1.345384120941162,
      "learning_rate": 9.691709069140977e-05,
      "loss": 0.3482,
      "step": 3450
    },
    {
      "epoch": 4.644683714670256,
      "grad_norm": 1.1338646411895752,
      "learning_rate": 9.688715953307394e-05,
      "loss": 0.3388,
      "step": 3451
    },
    {
      "epoch": 4.646029609690444,
      "grad_norm": 1.1882110834121704,
      "learning_rate": 9.68572283747381e-05,
      "loss": 0.3244,
      "step": 3452
    },
    {
      "epoch": 4.647375504710633,
      "grad_norm": 1.1753995418548584,
      "learning_rate": 9.682729721640227e-05,
      "loss": 0.4087,
      "step": 3453
    },
    {
      "epoch": 4.648721399730821,
      "grad_norm": 1.4238122701644897,
      "learning_rate": 9.679736605806646e-05,
      "loss": 0.3742,
      "step": 3454
    },
    {
      "epoch": 4.650067294751009,
      "grad_norm": 1.242775321006775,
      "learning_rate": 9.676743489973063e-05,
      "loss": 0.3722,
      "step": 3455
    },
    {
      "epoch": 4.651413189771198,
      "grad_norm": 1.438185214996338,
      "learning_rate": 9.67375037413948e-05,
      "loss": 0.387,
      "step": 3456
    },
    {
      "epoch": 4.652759084791386,
      "grad_norm": 1.2372788190841675,
      "learning_rate": 9.670757258305896e-05,
      "loss": 0.3397,
      "step": 3457
    },
    {
      "epoch": 4.654104979811574,
      "grad_norm": 1.022605538368225,
      "learning_rate": 9.667764142472315e-05,
      "loss": 0.3344,
      "step": 3458
    },
    {
      "epoch": 4.655450874831763,
      "grad_norm": 1.2813770771026611,
      "learning_rate": 9.664771026638732e-05,
      "loss": 0.3448,
      "step": 3459
    },
    {
      "epoch": 4.656796769851952,
      "grad_norm": 1.3994169235229492,
      "learning_rate": 9.661777910805148e-05,
      "loss": 0.5219,
      "step": 3460
    },
    {
      "epoch": 4.65814266487214,
      "grad_norm": 1.3978263139724731,
      "learning_rate": 9.658784794971565e-05,
      "loss": 0.3884,
      "step": 3461
    },
    {
      "epoch": 4.659488559892329,
      "grad_norm": 1.15680992603302,
      "learning_rate": 9.655791679137984e-05,
      "loss": 0.3872,
      "step": 3462
    },
    {
      "epoch": 4.660834454912517,
      "grad_norm": 1.2113436460494995,
      "learning_rate": 9.6527985633044e-05,
      "loss": 0.3477,
      "step": 3463
    },
    {
      "epoch": 4.662180349932705,
      "grad_norm": 1.4404667615890503,
      "learning_rate": 9.649805447470817e-05,
      "loss": 0.3875,
      "step": 3464
    },
    {
      "epoch": 4.663526244952894,
      "grad_norm": 1.3838623762130737,
      "learning_rate": 9.646812331637234e-05,
      "loss": 0.3253,
      "step": 3465
    },
    {
      "epoch": 4.664872139973082,
      "grad_norm": 1.261170744895935,
      "learning_rate": 9.643819215803653e-05,
      "loss": 0.4122,
      "step": 3466
    },
    {
      "epoch": 4.66621803499327,
      "grad_norm": 1.3396835327148438,
      "learning_rate": 9.64082609997007e-05,
      "loss": 0.3858,
      "step": 3467
    },
    {
      "epoch": 4.667563930013459,
      "grad_norm": 1.2991663217544556,
      "learning_rate": 9.637832984136486e-05,
      "loss": 0.335,
      "step": 3468
    },
    {
      "epoch": 4.668909825033648,
      "grad_norm": 1.1710894107818604,
      "learning_rate": 9.634839868302903e-05,
      "loss": 0.4921,
      "step": 3469
    },
    {
      "epoch": 4.670255720053836,
      "grad_norm": 1.532094955444336,
      "learning_rate": 9.631846752469322e-05,
      "loss": 0.6211,
      "step": 3470
    },
    {
      "epoch": 4.6716016150740245,
      "grad_norm": 1.1466290950775146,
      "learning_rate": 9.628853636635739e-05,
      "loss": 0.4277,
      "step": 3471
    },
    {
      "epoch": 4.672947510094213,
      "grad_norm": 1.5972858667373657,
      "learning_rate": 9.625860520802155e-05,
      "loss": 0.3839,
      "step": 3472
    },
    {
      "epoch": 4.674293405114401,
      "grad_norm": 1.6590558290481567,
      "learning_rate": 9.622867404968572e-05,
      "loss": 0.4238,
      "step": 3473
    },
    {
      "epoch": 4.6756393001345895,
      "grad_norm": 1.562896966934204,
      "learning_rate": 9.619874289134991e-05,
      "loss": 0.5046,
      "step": 3474
    },
    {
      "epoch": 4.676985195154778,
      "grad_norm": 1.0765056610107422,
      "learning_rate": 9.616881173301408e-05,
      "loss": 0.2812,
      "step": 3475
    },
    {
      "epoch": 4.678331090174966,
      "grad_norm": 1.3223459720611572,
      "learning_rate": 9.613888057467825e-05,
      "loss": 0.3446,
      "step": 3476
    },
    {
      "epoch": 4.6796769851951545,
      "grad_norm": 1.4212757349014282,
      "learning_rate": 9.610894941634241e-05,
      "loss": 0.4747,
      "step": 3477
    },
    {
      "epoch": 4.681022880215343,
      "grad_norm": 1.3788018226623535,
      "learning_rate": 9.60790182580066e-05,
      "loss": 0.3866,
      "step": 3478
    },
    {
      "epoch": 4.682368775235531,
      "grad_norm": 1.251113772392273,
      "learning_rate": 9.604908709967077e-05,
      "loss": 0.368,
      "step": 3479
    },
    {
      "epoch": 4.68371467025572,
      "grad_norm": 1.139406442642212,
      "learning_rate": 9.601915594133494e-05,
      "loss": 0.2981,
      "step": 3480
    },
    {
      "epoch": 4.685060565275909,
      "grad_norm": 1.3471680879592896,
      "learning_rate": 9.59892247829991e-05,
      "loss": 0.4,
      "step": 3481
    },
    {
      "epoch": 4.686406460296097,
      "grad_norm": 1.3794044256210327,
      "learning_rate": 9.595929362466329e-05,
      "loss": 0.4791,
      "step": 3482
    },
    {
      "epoch": 4.687752355316285,
      "grad_norm": 1.6808631420135498,
      "learning_rate": 9.592936246632746e-05,
      "loss": 0.4336,
      "step": 3483
    },
    {
      "epoch": 4.689098250336474,
      "grad_norm": 1.4041707515716553,
      "learning_rate": 9.589943130799163e-05,
      "loss": 0.3314,
      "step": 3484
    },
    {
      "epoch": 4.690444145356662,
      "grad_norm": 1.320034384727478,
      "learning_rate": 9.58695001496558e-05,
      "loss": 0.3495,
      "step": 3485
    },
    {
      "epoch": 4.69179004037685,
      "grad_norm": 1.0293102264404297,
      "learning_rate": 9.583956899131998e-05,
      "loss": 0.3478,
      "step": 3486
    },
    {
      "epoch": 4.693135935397039,
      "grad_norm": 1.1873860359191895,
      "learning_rate": 9.580963783298415e-05,
      "loss": 0.3062,
      "step": 3487
    },
    {
      "epoch": 4.694481830417227,
      "grad_norm": 1.282400131225586,
      "learning_rate": 9.577970667464832e-05,
      "loss": 0.422,
      "step": 3488
    },
    {
      "epoch": 4.695827725437416,
      "grad_norm": 1.426352858543396,
      "learning_rate": 9.574977551631248e-05,
      "loss": 0.3433,
      "step": 3489
    },
    {
      "epoch": 4.697173620457605,
      "grad_norm": 1.5443954467773438,
      "learning_rate": 9.571984435797667e-05,
      "loss": 0.4264,
      "step": 3490
    },
    {
      "epoch": 4.698519515477793,
      "grad_norm": 1.0666085481643677,
      "learning_rate": 9.568991319964084e-05,
      "loss": 0.3954,
      "step": 3491
    },
    {
      "epoch": 4.699865410497981,
      "grad_norm": 1.0213521718978882,
      "learning_rate": 9.5659982041305e-05,
      "loss": 0.254,
      "step": 3492
    },
    {
      "epoch": 4.70121130551817,
      "grad_norm": 1.055202603340149,
      "learning_rate": 9.563005088296917e-05,
      "loss": 0.3502,
      "step": 3493
    },
    {
      "epoch": 4.702557200538358,
      "grad_norm": 1.3692158460617065,
      "learning_rate": 9.560011972463336e-05,
      "loss": 0.425,
      "step": 3494
    },
    {
      "epoch": 4.703903095558546,
      "grad_norm": 1.4253252744674683,
      "learning_rate": 9.557018856629753e-05,
      "loss": 0.4549,
      "step": 3495
    },
    {
      "epoch": 4.705248990578735,
      "grad_norm": 1.138706922531128,
      "learning_rate": 9.55402574079617e-05,
      "loss": 0.3077,
      "step": 3496
    },
    {
      "epoch": 4.706594885598923,
      "grad_norm": 1.3427590131759644,
      "learning_rate": 9.551032624962587e-05,
      "loss": 0.3359,
      "step": 3497
    },
    {
      "epoch": 4.707940780619111,
      "grad_norm": 1.2698473930358887,
      "learning_rate": 9.548039509129005e-05,
      "loss": 0.4009,
      "step": 3498
    },
    {
      "epoch": 4.7092866756393,
      "grad_norm": 1.6357921361923218,
      "learning_rate": 9.545046393295422e-05,
      "loss": 0.4435,
      "step": 3499
    },
    {
      "epoch": 4.710632570659489,
      "grad_norm": 1.2995190620422363,
      "learning_rate": 9.542053277461839e-05,
      "loss": 0.379,
      "step": 3500
    },
    {
      "epoch": 4.711978465679677,
      "grad_norm": 1.505208969116211,
      "learning_rate": 9.539060161628256e-05,
      "loss": 0.4661,
      "step": 3501
    },
    {
      "epoch": 4.713324360699866,
      "grad_norm": 1.1802895069122314,
      "learning_rate": 9.536067045794674e-05,
      "loss": 0.3311,
      "step": 3502
    },
    {
      "epoch": 4.714670255720054,
      "grad_norm": 1.271093487739563,
      "learning_rate": 9.533073929961091e-05,
      "loss": 0.3652,
      "step": 3503
    },
    {
      "epoch": 4.716016150740242,
      "grad_norm": 1.171005129814148,
      "learning_rate": 9.530080814127506e-05,
      "loss": 0.3109,
      "step": 3504
    },
    {
      "epoch": 4.717362045760431,
      "grad_norm": 1.4745550155639648,
      "learning_rate": 9.527087698293925e-05,
      "loss": 0.3658,
      "step": 3505
    },
    {
      "epoch": 4.718707940780619,
      "grad_norm": 1.032963752746582,
      "learning_rate": 9.524094582460341e-05,
      "loss": 0.3686,
      "step": 3506
    },
    {
      "epoch": 4.720053835800807,
      "grad_norm": 1.046947717666626,
      "learning_rate": 9.521101466626758e-05,
      "loss": 0.3695,
      "step": 3507
    },
    {
      "epoch": 4.721399730820996,
      "grad_norm": 1.3687608242034912,
      "learning_rate": 9.518108350793175e-05,
      "loss": 0.3104,
      "step": 3508
    },
    {
      "epoch": 4.722745625841185,
      "grad_norm": 1.4882365465164185,
      "learning_rate": 9.515115234959594e-05,
      "loss": 0.48,
      "step": 3509
    },
    {
      "epoch": 4.724091520861373,
      "grad_norm": 1.3399080038070679,
      "learning_rate": 9.51212211912601e-05,
      "loss": 0.3839,
      "step": 3510
    },
    {
      "epoch": 4.7254374158815615,
      "grad_norm": 1.6074798107147217,
      "learning_rate": 9.509129003292427e-05,
      "loss": 0.4044,
      "step": 3511
    },
    {
      "epoch": 4.72678331090175,
      "grad_norm": 1.1016395092010498,
      "learning_rate": 9.506135887458844e-05,
      "loss": 0.4078,
      "step": 3512
    },
    {
      "epoch": 4.728129205921938,
      "grad_norm": 1.2098466157913208,
      "learning_rate": 9.503142771625263e-05,
      "loss": 0.3924,
      "step": 3513
    },
    {
      "epoch": 4.7294751009421265,
      "grad_norm": 1.0761443376541138,
      "learning_rate": 9.50014965579168e-05,
      "loss": 0.2717,
      "step": 3514
    },
    {
      "epoch": 4.730820995962315,
      "grad_norm": 1.0929596424102783,
      "learning_rate": 9.497156539958096e-05,
      "loss": 0.354,
      "step": 3515
    },
    {
      "epoch": 4.732166890982503,
      "grad_norm": 1.7566182613372803,
      "learning_rate": 9.494163424124513e-05,
      "loss": 0.4318,
      "step": 3516
    },
    {
      "epoch": 4.7335127860026915,
      "grad_norm": 1.3772083520889282,
      "learning_rate": 9.491170308290932e-05,
      "loss": 0.4482,
      "step": 3517
    },
    {
      "epoch": 4.73485868102288,
      "grad_norm": 1.1486488580703735,
      "learning_rate": 9.488177192457348e-05,
      "loss": 0.3984,
      "step": 3518
    },
    {
      "epoch": 4.736204576043068,
      "grad_norm": 1.232344388961792,
      "learning_rate": 9.485184076623765e-05,
      "loss": 0.4372,
      "step": 3519
    },
    {
      "epoch": 4.737550471063257,
      "grad_norm": 1.239515781402588,
      "learning_rate": 9.482190960790182e-05,
      "loss": 0.3633,
      "step": 3520
    },
    {
      "epoch": 4.738896366083446,
      "grad_norm": 1.3607563972473145,
      "learning_rate": 9.479197844956599e-05,
      "loss": 0.3675,
      "step": 3521
    },
    {
      "epoch": 4.740242261103634,
      "grad_norm": 1.1195852756500244,
      "learning_rate": 9.476204729123018e-05,
      "loss": 0.3255,
      "step": 3522
    },
    {
      "epoch": 4.7415881561238225,
      "grad_norm": 1.2810947895050049,
      "learning_rate": 9.473211613289434e-05,
      "loss": 0.3618,
      "step": 3523
    },
    {
      "epoch": 4.742934051144011,
      "grad_norm": 1.6497238874435425,
      "learning_rate": 9.470218497455851e-05,
      "loss": 0.4305,
      "step": 3524
    },
    {
      "epoch": 4.744279946164199,
      "grad_norm": 1.4229873418807983,
      "learning_rate": 9.467225381622268e-05,
      "loss": 0.3402,
      "step": 3525
    },
    {
      "epoch": 4.7456258411843875,
      "grad_norm": 1.4222170114517212,
      "learning_rate": 9.464232265788687e-05,
      "loss": 0.4388,
      "step": 3526
    },
    {
      "epoch": 4.746971736204576,
      "grad_norm": 1.5839020013809204,
      "learning_rate": 9.461239149955103e-05,
      "loss": 0.4611,
      "step": 3527
    },
    {
      "epoch": 4.748317631224764,
      "grad_norm": 1.1325865983963013,
      "learning_rate": 9.45824603412152e-05,
      "loss": 0.3379,
      "step": 3528
    },
    {
      "epoch": 4.749663526244953,
      "grad_norm": 1.2597728967666626,
      "learning_rate": 9.455252918287937e-05,
      "loss": 0.41,
      "step": 3529
    },
    {
      "epoch": 4.751009421265142,
      "grad_norm": 1.2770792245864868,
      "learning_rate": 9.452259802454356e-05,
      "loss": 0.3389,
      "step": 3530
    },
    {
      "epoch": 4.75235531628533,
      "grad_norm": 1.3682866096496582,
      "learning_rate": 9.449266686620772e-05,
      "loss": 0.4159,
      "step": 3531
    },
    {
      "epoch": 4.753701211305518,
      "grad_norm": 1.0015419721603394,
      "learning_rate": 9.44627357078719e-05,
      "loss": 0.3375,
      "step": 3532
    },
    {
      "epoch": 4.755047106325707,
      "grad_norm": 1.4408291578292847,
      "learning_rate": 9.443280454953606e-05,
      "loss": 0.4231,
      "step": 3533
    },
    {
      "epoch": 4.756393001345895,
      "grad_norm": 1.6086310148239136,
      "learning_rate": 9.440287339120025e-05,
      "loss": 0.5273,
      "step": 3534
    },
    {
      "epoch": 4.757738896366083,
      "grad_norm": 1.277316927909851,
      "learning_rate": 9.437294223286441e-05,
      "loss": 0.3837,
      "step": 3535
    },
    {
      "epoch": 4.759084791386272,
      "grad_norm": 1.348690390586853,
      "learning_rate": 9.434301107452858e-05,
      "loss": 0.5549,
      "step": 3536
    },
    {
      "epoch": 4.76043068640646,
      "grad_norm": 1.504167914390564,
      "learning_rate": 9.431307991619275e-05,
      "loss": 0.4304,
      "step": 3537
    },
    {
      "epoch": 4.761776581426648,
      "grad_norm": 1.3320695161819458,
      "learning_rate": 9.428314875785694e-05,
      "loss": 0.4633,
      "step": 3538
    },
    {
      "epoch": 4.763122476446837,
      "grad_norm": 1.3533190488815308,
      "learning_rate": 9.42532175995211e-05,
      "loss": 0.5204,
      "step": 3539
    },
    {
      "epoch": 4.764468371467026,
      "grad_norm": 1.1148810386657715,
      "learning_rate": 9.422328644118527e-05,
      "loss": 0.2936,
      "step": 3540
    },
    {
      "epoch": 4.765814266487214,
      "grad_norm": 1.4966102838516235,
      "learning_rate": 9.419335528284944e-05,
      "loss": 0.3813,
      "step": 3541
    },
    {
      "epoch": 4.767160161507403,
      "grad_norm": 1.505810260772705,
      "learning_rate": 9.416342412451363e-05,
      "loss": 0.4847,
      "step": 3542
    },
    {
      "epoch": 4.768506056527591,
      "grad_norm": 1.307665467262268,
      "learning_rate": 9.41334929661778e-05,
      "loss": 0.3757,
      "step": 3543
    },
    {
      "epoch": 4.769851951547779,
      "grad_norm": 1.065724492073059,
      "learning_rate": 9.410356180784196e-05,
      "loss": 0.2879,
      "step": 3544
    },
    {
      "epoch": 4.771197846567968,
      "grad_norm": 0.9864640831947327,
      "learning_rate": 9.407363064950613e-05,
      "loss": 0.3322,
      "step": 3545
    },
    {
      "epoch": 4.772543741588156,
      "grad_norm": 0.9240162372589111,
      "learning_rate": 9.404369949117032e-05,
      "loss": 0.3242,
      "step": 3546
    },
    {
      "epoch": 4.773889636608344,
      "grad_norm": 1.120603322982788,
      "learning_rate": 9.401376833283449e-05,
      "loss": 0.3779,
      "step": 3547
    },
    {
      "epoch": 4.775235531628533,
      "grad_norm": 1.2189786434173584,
      "learning_rate": 9.398383717449865e-05,
      "loss": 0.3804,
      "step": 3548
    },
    {
      "epoch": 4.776581426648722,
      "grad_norm": 1.1846519708633423,
      "learning_rate": 9.395390601616282e-05,
      "loss": 0.3962,
      "step": 3549
    },
    {
      "epoch": 4.77792732166891,
      "grad_norm": 1.2422397136688232,
      "learning_rate": 9.3923974857827e-05,
      "loss": 0.3533,
      "step": 3550
    },
    {
      "epoch": 4.7792732166890985,
      "grad_norm": 1.5639371871948242,
      "learning_rate": 9.389404369949118e-05,
      "loss": 0.5003,
      "step": 3551
    },
    {
      "epoch": 4.780619111709287,
      "grad_norm": 1.0788921117782593,
      "learning_rate": 9.386411254115534e-05,
      "loss": 0.4113,
      "step": 3552
    },
    {
      "epoch": 4.781965006729475,
      "grad_norm": 1.306480884552002,
      "learning_rate": 9.383418138281951e-05,
      "loss": 0.3824,
      "step": 3553
    },
    {
      "epoch": 4.783310901749664,
      "grad_norm": 1.3522484302520752,
      "learning_rate": 9.38042502244837e-05,
      "loss": 0.4134,
      "step": 3554
    },
    {
      "epoch": 4.784656796769852,
      "grad_norm": 1.0180782079696655,
      "learning_rate": 9.377431906614787e-05,
      "loss": 0.3365,
      "step": 3555
    },
    {
      "epoch": 4.78600269179004,
      "grad_norm": 1.2988979816436768,
      "learning_rate": 9.374438790781203e-05,
      "loss": 0.3788,
      "step": 3556
    },
    {
      "epoch": 4.787348586810229,
      "grad_norm": 1.5033141374588013,
      "learning_rate": 9.37144567494762e-05,
      "loss": 0.3973,
      "step": 3557
    },
    {
      "epoch": 4.788694481830417,
      "grad_norm": 1.2396260499954224,
      "learning_rate": 9.368452559114039e-05,
      "loss": 0.35,
      "step": 3558
    },
    {
      "epoch": 4.790040376850605,
      "grad_norm": 1.2634673118591309,
      "learning_rate": 9.365459443280456e-05,
      "loss": 0.3318,
      "step": 3559
    },
    {
      "epoch": 4.7913862718707945,
      "grad_norm": 1.2769176959991455,
      "learning_rate": 9.362466327446872e-05,
      "loss": 0.3089,
      "step": 3560
    },
    {
      "epoch": 4.792732166890983,
      "grad_norm": 1.179714322090149,
      "learning_rate": 9.35947321161329e-05,
      "loss": 0.2882,
      "step": 3561
    },
    {
      "epoch": 4.794078061911171,
      "grad_norm": 1.207602620124817,
      "learning_rate": 9.356480095779708e-05,
      "loss": 0.3387,
      "step": 3562
    },
    {
      "epoch": 4.7954239569313595,
      "grad_norm": 1.14773428440094,
      "learning_rate": 9.353486979946125e-05,
      "loss": 0.5097,
      "step": 3563
    },
    {
      "epoch": 4.796769851951548,
      "grad_norm": 1.500012755393982,
      "learning_rate": 9.350493864112541e-05,
      "loss": 0.4834,
      "step": 3564
    },
    {
      "epoch": 4.798115746971736,
      "grad_norm": 1.1021548509597778,
      "learning_rate": 9.347500748278958e-05,
      "loss": 0.309,
      "step": 3565
    },
    {
      "epoch": 4.7994616419919245,
      "grad_norm": 1.4011256694793701,
      "learning_rate": 9.344507632445377e-05,
      "loss": 0.445,
      "step": 3566
    },
    {
      "epoch": 4.800807537012113,
      "grad_norm": 1.3566159009933472,
      "learning_rate": 9.341514516611794e-05,
      "loss": 0.382,
      "step": 3567
    },
    {
      "epoch": 4.802153432032301,
      "grad_norm": 1.0772188901901245,
      "learning_rate": 9.33852140077821e-05,
      "loss": 0.4697,
      "step": 3568
    },
    {
      "epoch": 4.80349932705249,
      "grad_norm": 1.182493805885315,
      "learning_rate": 9.335528284944627e-05,
      "loss": 0.4856,
      "step": 3569
    },
    {
      "epoch": 4.804845222072679,
      "grad_norm": 1.037613034248352,
      "learning_rate": 9.332535169111046e-05,
      "loss": 0.3546,
      "step": 3570
    },
    {
      "epoch": 4.806191117092867,
      "grad_norm": 1.0079174041748047,
      "learning_rate": 9.329542053277463e-05,
      "loss": 0.3536,
      "step": 3571
    },
    {
      "epoch": 4.807537012113055,
      "grad_norm": 1.2769545316696167,
      "learning_rate": 9.32654893744388e-05,
      "loss": 0.3915,
      "step": 3572
    },
    {
      "epoch": 4.808882907133244,
      "grad_norm": 1.3463371992111206,
      "learning_rate": 9.323555821610296e-05,
      "loss": 0.4666,
      "step": 3573
    },
    {
      "epoch": 4.810228802153432,
      "grad_norm": 1.2639449834823608,
      "learning_rate": 9.320562705776715e-05,
      "loss": 0.3898,
      "step": 3574
    },
    {
      "epoch": 4.81157469717362,
      "grad_norm": 1.2739741802215576,
      "learning_rate": 9.317569589943132e-05,
      "loss": 0.4295,
      "step": 3575
    },
    {
      "epoch": 4.812920592193809,
      "grad_norm": 1.518438696861267,
      "learning_rate": 9.314576474109549e-05,
      "loss": 0.478,
      "step": 3576
    },
    {
      "epoch": 4.814266487213997,
      "grad_norm": 1.344735026359558,
      "learning_rate": 9.311583358275965e-05,
      "loss": 0.3487,
      "step": 3577
    },
    {
      "epoch": 4.815612382234185,
      "grad_norm": 1.4742400646209717,
      "learning_rate": 9.308590242442384e-05,
      "loss": 0.3614,
      "step": 3578
    },
    {
      "epoch": 4.816958277254374,
      "grad_norm": 1.3630757331848145,
      "learning_rate": 9.3055971266088e-05,
      "loss": 0.3672,
      "step": 3579
    },
    {
      "epoch": 4.818304172274562,
      "grad_norm": 1.2586027383804321,
      "learning_rate": 9.302604010775218e-05,
      "loss": 0.3712,
      "step": 3580
    },
    {
      "epoch": 4.819650067294751,
      "grad_norm": 1.239931583404541,
      "learning_rate": 9.299610894941634e-05,
      "loss": 0.2952,
      "step": 3581
    },
    {
      "epoch": 4.82099596231494,
      "grad_norm": 1.0455048084259033,
      "learning_rate": 9.296617779108053e-05,
      "loss": 0.422,
      "step": 3582
    },
    {
      "epoch": 4.822341857335128,
      "grad_norm": 1.2156144380569458,
      "learning_rate": 9.29362466327447e-05,
      "loss": 0.33,
      "step": 3583
    },
    {
      "epoch": 4.823687752355316,
      "grad_norm": 1.2241787910461426,
      "learning_rate": 9.290631547440887e-05,
      "loss": 0.3714,
      "step": 3584
    },
    {
      "epoch": 4.825033647375505,
      "grad_norm": 1.3963003158569336,
      "learning_rate": 9.287638431607303e-05,
      "loss": 0.3049,
      "step": 3585
    },
    {
      "epoch": 4.826379542395693,
      "grad_norm": 1.0550705194473267,
      "learning_rate": 9.284645315773722e-05,
      "loss": 0.307,
      "step": 3586
    },
    {
      "epoch": 4.827725437415881,
      "grad_norm": 1.0433841943740845,
      "learning_rate": 9.281652199940139e-05,
      "loss": 0.386,
      "step": 3587
    },
    {
      "epoch": 4.82907133243607,
      "grad_norm": 1.057794213294983,
      "learning_rate": 9.278659084106556e-05,
      "loss": 0.4667,
      "step": 3588
    },
    {
      "epoch": 4.830417227456259,
      "grad_norm": 1.551173448562622,
      "learning_rate": 9.275665968272972e-05,
      "loss": 0.408,
      "step": 3589
    },
    {
      "epoch": 4.831763122476447,
      "grad_norm": 1.1996592283248901,
      "learning_rate": 9.272672852439391e-05,
      "loss": 0.3776,
      "step": 3590
    },
    {
      "epoch": 4.833109017496636,
      "grad_norm": 1.2956641912460327,
      "learning_rate": 9.269679736605808e-05,
      "loss": 0.4027,
      "step": 3591
    },
    {
      "epoch": 4.834454912516824,
      "grad_norm": 1.0810472965240479,
      "learning_rate": 9.266686620772225e-05,
      "loss": 0.3331,
      "step": 3592
    },
    {
      "epoch": 4.835800807537012,
      "grad_norm": 1.435310959815979,
      "learning_rate": 9.263693504938641e-05,
      "loss": 0.4163,
      "step": 3593
    },
    {
      "epoch": 4.837146702557201,
      "grad_norm": 1.2788002490997314,
      "learning_rate": 9.26070038910506e-05,
      "loss": 0.32,
      "step": 3594
    },
    {
      "epoch": 4.838492597577389,
      "grad_norm": 1.662509560585022,
      "learning_rate": 9.257707273271477e-05,
      "loss": 0.5736,
      "step": 3595
    },
    {
      "epoch": 4.839838492597577,
      "grad_norm": 1.1440789699554443,
      "learning_rate": 9.254714157437894e-05,
      "loss": 0.3355,
      "step": 3596
    },
    {
      "epoch": 4.841184387617766,
      "grad_norm": 1.3738003969192505,
      "learning_rate": 9.25172104160431e-05,
      "loss": 0.4142,
      "step": 3597
    },
    {
      "epoch": 4.842530282637954,
      "grad_norm": 1.2712349891662598,
      "learning_rate": 9.248727925770729e-05,
      "loss": 0.553,
      "step": 3598
    },
    {
      "epoch": 4.843876177658142,
      "grad_norm": 1.6305797100067139,
      "learning_rate": 9.245734809937146e-05,
      "loss": 0.4757,
      "step": 3599
    },
    {
      "epoch": 4.845222072678331,
      "grad_norm": 1.3320202827453613,
      "learning_rate": 9.242741694103563e-05,
      "loss": 0.4238,
      "step": 3600
    },
    {
      "epoch": 4.84656796769852,
      "grad_norm": 1.4088488817214966,
      "learning_rate": 9.23974857826998e-05,
      "loss": 0.3313,
      "step": 3601
    },
    {
      "epoch": 4.847913862718708,
      "grad_norm": 1.2348252534866333,
      "learning_rate": 9.236755462436398e-05,
      "loss": 0.3525,
      "step": 3602
    },
    {
      "epoch": 4.8492597577388965,
      "grad_norm": 0.8724386096000671,
      "learning_rate": 9.233762346602815e-05,
      "loss": 0.2706,
      "step": 3603
    },
    {
      "epoch": 4.850605652759085,
      "grad_norm": 1.479872465133667,
      "learning_rate": 9.230769230769232e-05,
      "loss": 0.4173,
      "step": 3604
    },
    {
      "epoch": 4.851951547779273,
      "grad_norm": 1.2774009704589844,
      "learning_rate": 9.227776114935649e-05,
      "loss": 0.4343,
      "step": 3605
    },
    {
      "epoch": 4.8532974427994615,
      "grad_norm": 1.164992094039917,
      "learning_rate": 9.224782999102067e-05,
      "loss": 0.3114,
      "step": 3606
    },
    {
      "epoch": 4.85464333781965,
      "grad_norm": 1.807407259941101,
      "learning_rate": 9.221789883268484e-05,
      "loss": 0.4451,
      "step": 3607
    },
    {
      "epoch": 4.855989232839838,
      "grad_norm": 1.2471345663070679,
      "learning_rate": 9.2187967674349e-05,
      "loss": 0.3436,
      "step": 3608
    },
    {
      "epoch": 4.857335127860027,
      "grad_norm": 1.302197813987732,
      "learning_rate": 9.215803651601316e-05,
      "loss": 0.374,
      "step": 3609
    },
    {
      "epoch": 4.858681022880216,
      "grad_norm": 1.1065524816513062,
      "learning_rate": 9.212810535767734e-05,
      "loss": 0.3032,
      "step": 3610
    },
    {
      "epoch": 4.860026917900404,
      "grad_norm": 1.1595101356506348,
      "learning_rate": 9.209817419934151e-05,
      "loss": 0.2805,
      "step": 3611
    },
    {
      "epoch": 4.861372812920592,
      "grad_norm": 1.9338880777359009,
      "learning_rate": 9.206824304100568e-05,
      "loss": 0.4608,
      "step": 3612
    },
    {
      "epoch": 4.862718707940781,
      "grad_norm": 1.2987966537475586,
      "learning_rate": 9.203831188266985e-05,
      "loss": 0.3535,
      "step": 3613
    },
    {
      "epoch": 4.864064602960969,
      "grad_norm": 1.2893640995025635,
      "learning_rate": 9.200838072433403e-05,
      "loss": 0.3116,
      "step": 3614
    },
    {
      "epoch": 4.865410497981157,
      "grad_norm": 1.3032240867614746,
      "learning_rate": 9.19784495659982e-05,
      "loss": 0.3554,
      "step": 3615
    },
    {
      "epoch": 4.866756393001346,
      "grad_norm": 1.2324353456497192,
      "learning_rate": 9.194851840766237e-05,
      "loss": 0.3502,
      "step": 3616
    },
    {
      "epoch": 4.868102288021534,
      "grad_norm": 1.7750755548477173,
      "learning_rate": 9.191858724932654e-05,
      "loss": 0.3668,
      "step": 3617
    },
    {
      "epoch": 4.8694481830417224,
      "grad_norm": 1.4054261445999146,
      "learning_rate": 9.188865609099073e-05,
      "loss": 0.5089,
      "step": 3618
    },
    {
      "epoch": 4.870794078061911,
      "grad_norm": 1.2591240406036377,
      "learning_rate": 9.18587249326549e-05,
      "loss": 0.3299,
      "step": 3619
    },
    {
      "epoch": 4.872139973082099,
      "grad_norm": 1.3209621906280518,
      "learning_rate": 9.182879377431906e-05,
      "loss": 0.3726,
      "step": 3620
    },
    {
      "epoch": 4.873485868102288,
      "grad_norm": 1.2272368669509888,
      "learning_rate": 9.179886261598323e-05,
      "loss": 0.3311,
      "step": 3621
    },
    {
      "epoch": 4.874831763122477,
      "grad_norm": 1.167388677597046,
      "learning_rate": 9.176893145764742e-05,
      "loss": 0.2887,
      "step": 3622
    },
    {
      "epoch": 4.876177658142665,
      "grad_norm": 1.2564160823822021,
      "learning_rate": 9.173900029931158e-05,
      "loss": 0.4161,
      "step": 3623
    },
    {
      "epoch": 4.877523553162853,
      "grad_norm": 1.247149109840393,
      "learning_rate": 9.170906914097575e-05,
      "loss": 0.5474,
      "step": 3624
    },
    {
      "epoch": 4.878869448183042,
      "grad_norm": 1.1876665353775024,
      "learning_rate": 9.167913798263992e-05,
      "loss": 0.3592,
      "step": 3625
    },
    {
      "epoch": 4.88021534320323,
      "grad_norm": 1.725640892982483,
      "learning_rate": 9.16492068243041e-05,
      "loss": 0.3747,
      "step": 3626
    },
    {
      "epoch": 4.881561238223418,
      "grad_norm": 1.3899681568145752,
      "learning_rate": 9.161927566596827e-05,
      "loss": 0.3824,
      "step": 3627
    },
    {
      "epoch": 4.882907133243607,
      "grad_norm": 1.4023290872573853,
      "learning_rate": 9.158934450763244e-05,
      "loss": 0.3909,
      "step": 3628
    },
    {
      "epoch": 4.884253028263795,
      "grad_norm": 1.1379802227020264,
      "learning_rate": 9.155941334929661e-05,
      "loss": 0.8082,
      "step": 3629
    },
    {
      "epoch": 4.885598923283984,
      "grad_norm": 1.3955698013305664,
      "learning_rate": 9.15294821909608e-05,
      "loss": 0.3798,
      "step": 3630
    },
    {
      "epoch": 4.886944818304173,
      "grad_norm": 1.1916362047195435,
      "learning_rate": 9.149955103262496e-05,
      "loss": 0.3431,
      "step": 3631
    },
    {
      "epoch": 4.888290713324361,
      "grad_norm": 1.6836897134780884,
      "learning_rate": 9.146961987428913e-05,
      "loss": 0.4282,
      "step": 3632
    },
    {
      "epoch": 4.889636608344549,
      "grad_norm": 1.3475537300109863,
      "learning_rate": 9.14396887159533e-05,
      "loss": 0.3168,
      "step": 3633
    },
    {
      "epoch": 4.890982503364738,
      "grad_norm": 1.432096242904663,
      "learning_rate": 9.140975755761749e-05,
      "loss": 0.4479,
      "step": 3634
    },
    {
      "epoch": 4.892328398384926,
      "grad_norm": 1.0948184728622437,
      "learning_rate": 9.137982639928165e-05,
      "loss": 0.3298,
      "step": 3635
    },
    {
      "epoch": 4.893674293405114,
      "grad_norm": 1.134434700012207,
      "learning_rate": 9.134989524094582e-05,
      "loss": 0.3471,
      "step": 3636
    },
    {
      "epoch": 4.895020188425303,
      "grad_norm": 1.3782769441604614,
      "learning_rate": 9.131996408260999e-05,
      "loss": 0.4892,
      "step": 3637
    },
    {
      "epoch": 4.896366083445491,
      "grad_norm": 1.122022032737732,
      "learning_rate": 9.129003292427418e-05,
      "loss": 0.4165,
      "step": 3638
    },
    {
      "epoch": 4.897711978465679,
      "grad_norm": 1.523837924003601,
      "learning_rate": 9.126010176593834e-05,
      "loss": 0.4051,
      "step": 3639
    },
    {
      "epoch": 4.899057873485868,
      "grad_norm": 1.077833652496338,
      "learning_rate": 9.123017060760251e-05,
      "loss": 0.3216,
      "step": 3640
    },
    {
      "epoch": 4.900403768506057,
      "grad_norm": 1.39272940158844,
      "learning_rate": 9.120023944926668e-05,
      "loss": 0.4675,
      "step": 3641
    },
    {
      "epoch": 4.901749663526245,
      "grad_norm": 1.1747829914093018,
      "learning_rate": 9.117030829093087e-05,
      "loss": 0.3204,
      "step": 3642
    },
    {
      "epoch": 4.9030955585464335,
      "grad_norm": 1.4906290769577026,
      "learning_rate": 9.114037713259504e-05,
      "loss": 0.311,
      "step": 3643
    },
    {
      "epoch": 4.904441453566622,
      "grad_norm": 1.0732941627502441,
      "learning_rate": 9.11104459742592e-05,
      "loss": 0.3126,
      "step": 3644
    },
    {
      "epoch": 4.90578734858681,
      "grad_norm": 1.4892494678497314,
      "learning_rate": 9.108051481592337e-05,
      "loss": 0.3569,
      "step": 3645
    },
    {
      "epoch": 4.9071332436069985,
      "grad_norm": 1.1687086820602417,
      "learning_rate": 9.105058365758756e-05,
      "loss": 0.3322,
      "step": 3646
    },
    {
      "epoch": 4.908479138627187,
      "grad_norm": 1.3352702856063843,
      "learning_rate": 9.102065249925173e-05,
      "loss": 0.3406,
      "step": 3647
    },
    {
      "epoch": 4.909825033647375,
      "grad_norm": 1.2540180683135986,
      "learning_rate": 9.09907213409159e-05,
      "loss": 0.3577,
      "step": 3648
    },
    {
      "epoch": 4.9111709286675636,
      "grad_norm": 1.0067569017410278,
      "learning_rate": 9.096079018258006e-05,
      "loss": 0.2977,
      "step": 3649
    },
    {
      "epoch": 4.912516823687753,
      "grad_norm": 1.6158453226089478,
      "learning_rate": 9.093085902424425e-05,
      "loss": 0.3611,
      "step": 3650
    },
    {
      "epoch": 4.913862718707941,
      "grad_norm": 1.1917721033096313,
      "learning_rate": 9.090092786590842e-05,
      "loss": 0.3588,
      "step": 3651
    },
    {
      "epoch": 4.9152086137281294,
      "grad_norm": 1.0232200622558594,
      "learning_rate": 9.087099670757258e-05,
      "loss": 0.2941,
      "step": 3652
    },
    {
      "epoch": 4.916554508748318,
      "grad_norm": 1.2852602005004883,
      "learning_rate": 9.084106554923675e-05,
      "loss": 0.4077,
      "step": 3653
    },
    {
      "epoch": 4.917900403768506,
      "grad_norm": 1.3489221334457397,
      "learning_rate": 9.081113439090094e-05,
      "loss": 0.419,
      "step": 3654
    },
    {
      "epoch": 4.9192462987886945,
      "grad_norm": 1.118131399154663,
      "learning_rate": 9.07812032325651e-05,
      "loss": 0.3066,
      "step": 3655
    },
    {
      "epoch": 4.920592193808883,
      "grad_norm": 1.4879409074783325,
      "learning_rate": 9.075127207422927e-05,
      "loss": 0.4167,
      "step": 3656
    },
    {
      "epoch": 4.921938088829071,
      "grad_norm": 1.3361692428588867,
      "learning_rate": 9.072134091589344e-05,
      "loss": 0.3708,
      "step": 3657
    },
    {
      "epoch": 4.9232839838492595,
      "grad_norm": 1.418859601020813,
      "learning_rate": 9.069140975755763e-05,
      "loss": 0.3702,
      "step": 3658
    },
    {
      "epoch": 4.924629878869448,
      "grad_norm": 1.3542773723602295,
      "learning_rate": 9.06614785992218e-05,
      "loss": 0.3158,
      "step": 3659
    },
    {
      "epoch": 4.925975773889636,
      "grad_norm": 1.153370976448059,
      "learning_rate": 9.063154744088596e-05,
      "loss": 0.3753,
      "step": 3660
    },
    {
      "epoch": 4.927321668909825,
      "grad_norm": 1.330319881439209,
      "learning_rate": 9.060161628255013e-05,
      "loss": 0.3724,
      "step": 3661
    },
    {
      "epoch": 4.928667563930014,
      "grad_norm": 1.4321470260620117,
      "learning_rate": 9.057168512421432e-05,
      "loss": 0.4662,
      "step": 3662
    },
    {
      "epoch": 4.930013458950202,
      "grad_norm": 1.1727522611618042,
      "learning_rate": 9.054175396587849e-05,
      "loss": 0.31,
      "step": 3663
    },
    {
      "epoch": 4.93135935397039,
      "grad_norm": 1.3414329290390015,
      "learning_rate": 9.051182280754265e-05,
      "loss": 0.4089,
      "step": 3664
    },
    {
      "epoch": 4.932705248990579,
      "grad_norm": 1.3731504678726196,
      "learning_rate": 9.048189164920682e-05,
      "loss": 0.3711,
      "step": 3665
    },
    {
      "epoch": 4.934051144010767,
      "grad_norm": 1.407033920288086,
      "learning_rate": 9.045196049087101e-05,
      "loss": 0.3456,
      "step": 3666
    },
    {
      "epoch": 4.935397039030955,
      "grad_norm": 1.3081525564193726,
      "learning_rate": 9.042202933253518e-05,
      "loss": 0.4007,
      "step": 3667
    },
    {
      "epoch": 4.936742934051144,
      "grad_norm": 1.2246414422988892,
      "learning_rate": 9.039209817419935e-05,
      "loss": 0.2939,
      "step": 3668
    },
    {
      "epoch": 4.938088829071332,
      "grad_norm": 1.156322956085205,
      "learning_rate": 9.036216701586351e-05,
      "loss": 0.3341,
      "step": 3669
    },
    {
      "epoch": 4.939434724091521,
      "grad_norm": 1.1517997980117798,
      "learning_rate": 9.03322358575277e-05,
      "loss": 0.3636,
      "step": 3670
    },
    {
      "epoch": 4.94078061911171,
      "grad_norm": 1.0147464275360107,
      "learning_rate": 9.030230469919187e-05,
      "loss": 0.2839,
      "step": 3671
    },
    {
      "epoch": 4.942126514131898,
      "grad_norm": 1.4314204454421997,
      "learning_rate": 9.027237354085604e-05,
      "loss": 0.3609,
      "step": 3672
    },
    {
      "epoch": 4.943472409152086,
      "grad_norm": 1.1566632986068726,
      "learning_rate": 9.02424423825202e-05,
      "loss": 0.4132,
      "step": 3673
    },
    {
      "epoch": 4.944818304172275,
      "grad_norm": 1.1858611106872559,
      "learning_rate": 9.021251122418439e-05,
      "loss": 0.3236,
      "step": 3674
    },
    {
      "epoch": 4.946164199192463,
      "grad_norm": 1.1406700611114502,
      "learning_rate": 9.018258006584856e-05,
      "loss": 0.3402,
      "step": 3675
    },
    {
      "epoch": 4.947510094212651,
      "grad_norm": 1.3717695474624634,
      "learning_rate": 9.015264890751273e-05,
      "loss": 0.3766,
      "step": 3676
    },
    {
      "epoch": 4.94885598923284,
      "grad_norm": 1.2391972541809082,
      "learning_rate": 9.01227177491769e-05,
      "loss": 0.3331,
      "step": 3677
    },
    {
      "epoch": 4.950201884253028,
      "grad_norm": 1.2756938934326172,
      "learning_rate": 9.009278659084108e-05,
      "loss": 0.4813,
      "step": 3678
    },
    {
      "epoch": 4.951547779273216,
      "grad_norm": 1.3023217916488647,
      "learning_rate": 9.006285543250525e-05,
      "loss": 0.3711,
      "step": 3679
    },
    {
      "epoch": 4.952893674293405,
      "grad_norm": 1.1317600011825562,
      "learning_rate": 9.003292427416942e-05,
      "loss": 0.3568,
      "step": 3680
    },
    {
      "epoch": 4.954239569313594,
      "grad_norm": 1.7623647451400757,
      "learning_rate": 9.000299311583358e-05,
      "loss": 0.4771,
      "step": 3681
    },
    {
      "epoch": 4.955585464333782,
      "grad_norm": 1.0348697900772095,
      "learning_rate": 8.997306195749777e-05,
      "loss": 0.2427,
      "step": 3682
    },
    {
      "epoch": 4.956931359353971,
      "grad_norm": 1.1993623971939087,
      "learning_rate": 8.994313079916194e-05,
      "loss": 0.3031,
      "step": 3683
    },
    {
      "epoch": 4.958277254374159,
      "grad_norm": 1.1996335983276367,
      "learning_rate": 8.99131996408261e-05,
      "loss": 0.329,
      "step": 3684
    },
    {
      "epoch": 4.959623149394347,
      "grad_norm": 1.1775506734848022,
      "learning_rate": 8.988326848249027e-05,
      "loss": 0.3665,
      "step": 3685
    },
    {
      "epoch": 4.960969044414536,
      "grad_norm": 1.4385994672775269,
      "learning_rate": 8.985333732415446e-05,
      "loss": 0.407,
      "step": 3686
    },
    {
      "epoch": 4.962314939434724,
      "grad_norm": 1.3226698637008667,
      "learning_rate": 8.982340616581863e-05,
      "loss": 0.4325,
      "step": 3687
    },
    {
      "epoch": 4.963660834454912,
      "grad_norm": 1.2615324258804321,
      "learning_rate": 8.97934750074828e-05,
      "loss": 0.4784,
      "step": 3688
    },
    {
      "epoch": 4.965006729475101,
      "grad_norm": 1.2071386575698853,
      "learning_rate": 8.976354384914696e-05,
      "loss": 0.2875,
      "step": 3689
    },
    {
      "epoch": 4.96635262449529,
      "grad_norm": 1.4222404956817627,
      "learning_rate": 8.973361269081115e-05,
      "loss": 0.3915,
      "step": 3690
    },
    {
      "epoch": 4.967698519515478,
      "grad_norm": 1.1900711059570312,
      "learning_rate": 8.970368153247532e-05,
      "loss": 0.3291,
      "step": 3691
    },
    {
      "epoch": 4.9690444145356665,
      "grad_norm": 1.2467989921569824,
      "learning_rate": 8.967375037413949e-05,
      "loss": 0.3081,
      "step": 3692
    },
    {
      "epoch": 4.970390309555855,
      "grad_norm": 1.2083231210708618,
      "learning_rate": 8.964381921580366e-05,
      "loss": 0.3276,
      "step": 3693
    },
    {
      "epoch": 4.971736204576043,
      "grad_norm": 1.3026446104049683,
      "learning_rate": 8.961388805746784e-05,
      "loss": 0.4518,
      "step": 3694
    },
    {
      "epoch": 4.9730820995962315,
      "grad_norm": 1.4399656057357788,
      "learning_rate": 8.958395689913201e-05,
      "loss": 0.4055,
      "step": 3695
    },
    {
      "epoch": 4.97442799461642,
      "grad_norm": 1.336289644241333,
      "learning_rate": 8.955402574079618e-05,
      "loss": 0.3817,
      "step": 3696
    },
    {
      "epoch": 4.975773889636608,
      "grad_norm": 1.532446265220642,
      "learning_rate": 8.952409458246035e-05,
      "loss": 0.3344,
      "step": 3697
    },
    {
      "epoch": 4.9771197846567965,
      "grad_norm": 1.2152650356292725,
      "learning_rate": 8.949416342412453e-05,
      "loss": 0.3487,
      "step": 3698
    },
    {
      "epoch": 4.978465679676985,
      "grad_norm": 1.33163583278656,
      "learning_rate": 8.94642322657887e-05,
      "loss": 0.2954,
      "step": 3699
    },
    {
      "epoch": 4.979811574697173,
      "grad_norm": 1.2888531684875488,
      "learning_rate": 8.943430110745287e-05,
      "loss": 0.389,
      "step": 3700
    },
    {
      "epoch": 4.981157469717362,
      "grad_norm": 1.492135763168335,
      "learning_rate": 8.940436994911704e-05,
      "loss": 0.4934,
      "step": 3701
    },
    {
      "epoch": 4.982503364737551,
      "grad_norm": 1.377390742301941,
      "learning_rate": 8.937443879078122e-05,
      "loss": 0.3651,
      "step": 3702
    },
    {
      "epoch": 4.983849259757739,
      "grad_norm": 1.0941753387451172,
      "learning_rate": 8.934450763244539e-05,
      "loss": 0.2901,
      "step": 3703
    },
    {
      "epoch": 4.985195154777927,
      "grad_norm": 1.2751349210739136,
      "learning_rate": 8.931457647410956e-05,
      "loss": 0.3387,
      "step": 3704
    },
    {
      "epoch": 4.986541049798116,
      "grad_norm": 1.31637442111969,
      "learning_rate": 8.928464531577373e-05,
      "loss": 0.4668,
      "step": 3705
    },
    {
      "epoch": 4.987886944818304,
      "grad_norm": 1.1758335828781128,
      "learning_rate": 8.925471415743791e-05,
      "loss": 0.3106,
      "step": 3706
    },
    {
      "epoch": 4.989232839838492,
      "grad_norm": 1.173819899559021,
      "learning_rate": 8.922478299910208e-05,
      "loss": 0.3916,
      "step": 3707
    },
    {
      "epoch": 4.990578734858681,
      "grad_norm": 1.0693159103393555,
      "learning_rate": 8.919485184076625e-05,
      "loss": 0.2846,
      "step": 3708
    },
    {
      "epoch": 4.991924629878869,
      "grad_norm": 1.1814854145050049,
      "learning_rate": 8.916492068243042e-05,
      "loss": 0.4188,
      "step": 3709
    },
    {
      "epoch": 4.993270524899058,
      "grad_norm": 1.3425315618515015,
      "learning_rate": 8.91349895240946e-05,
      "loss": 0.3563,
      "step": 3710
    },
    {
      "epoch": 4.994616419919247,
      "grad_norm": 1.4603488445281982,
      "learning_rate": 8.910505836575877e-05,
      "loss": 0.4524,
      "step": 3711
    },
    {
      "epoch": 4.995962314939435,
      "grad_norm": 1.2764887809753418,
      "learning_rate": 8.907512720742294e-05,
      "loss": 0.3824,
      "step": 3712
    },
    {
      "epoch": 4.997308209959623,
      "grad_norm": 1.1730765104293823,
      "learning_rate": 8.904519604908709e-05,
      "loss": 0.4565,
      "step": 3713
    },
    {
      "epoch": 4.998654104979812,
      "grad_norm": 1.2825218439102173,
      "learning_rate": 8.901526489075128e-05,
      "loss": 0.3496,
      "step": 3714
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.5173285007476807,
      "learning_rate": 8.898533373241544e-05,
      "loss": 0.3251,
      "step": 3715
    },
    {
      "epoch": 5.001345895020188,
      "grad_norm": 0.8158367276191711,
      "learning_rate": 8.895540257407961e-05,
      "loss": 0.189,
      "step": 3716
    },
    {
      "epoch": 5.002691790040377,
      "grad_norm": 0.6978240609169006,
      "learning_rate": 8.892547141574378e-05,
      "loss": 0.1807,
      "step": 3717
    },
    {
      "epoch": 5.004037685060565,
      "grad_norm": 0.7833775877952576,
      "learning_rate": 8.889554025740797e-05,
      "loss": 0.1992,
      "step": 3718
    },
    {
      "epoch": 5.005383580080753,
      "grad_norm": 0.6827622652053833,
      "learning_rate": 8.886560909907213e-05,
      "loss": 0.1806,
      "step": 3719
    },
    {
      "epoch": 5.006729475100942,
      "grad_norm": 0.8476518392562866,
      "learning_rate": 8.88356779407363e-05,
      "loss": 0.2098,
      "step": 3720
    },
    {
      "epoch": 5.008075370121131,
      "grad_norm": 0.9363707900047302,
      "learning_rate": 8.880574678240047e-05,
      "loss": 0.2084,
      "step": 3721
    },
    {
      "epoch": 5.009421265141319,
      "grad_norm": 0.8510527610778809,
      "learning_rate": 8.877581562406466e-05,
      "loss": 0.1999,
      "step": 3722
    },
    {
      "epoch": 5.010767160161508,
      "grad_norm": 0.8134601712226868,
      "learning_rate": 8.874588446572882e-05,
      "loss": 0.2084,
      "step": 3723
    },
    {
      "epoch": 5.012113055181696,
      "grad_norm": 0.8794556856155396,
      "learning_rate": 8.8715953307393e-05,
      "loss": 0.2,
      "step": 3724
    },
    {
      "epoch": 5.013458950201884,
      "grad_norm": 1.1921405792236328,
      "learning_rate": 8.868602214905716e-05,
      "loss": 0.2594,
      "step": 3725
    },
    {
      "epoch": 5.014804845222073,
      "grad_norm": 0.8962361812591553,
      "learning_rate": 8.865609099072135e-05,
      "loss": 0.1926,
      "step": 3726
    },
    {
      "epoch": 5.016150740242261,
      "grad_norm": 0.8501212000846863,
      "learning_rate": 8.862615983238551e-05,
      "loss": 0.1994,
      "step": 3727
    },
    {
      "epoch": 5.017496635262449,
      "grad_norm": 1.0787169933319092,
      "learning_rate": 8.859622867404968e-05,
      "loss": 0.2269,
      "step": 3728
    },
    {
      "epoch": 5.018842530282638,
      "grad_norm": 1.1801663637161255,
      "learning_rate": 8.856629751571385e-05,
      "loss": 0.2421,
      "step": 3729
    },
    {
      "epoch": 5.020188425302826,
      "grad_norm": 0.9138762950897217,
      "learning_rate": 8.853636635737804e-05,
      "loss": 0.1794,
      "step": 3730
    },
    {
      "epoch": 5.021534320323015,
      "grad_norm": 0.6757158637046814,
      "learning_rate": 8.85064351990422e-05,
      "loss": 0.1287,
      "step": 3731
    },
    {
      "epoch": 5.0228802153432035,
      "grad_norm": 1.239216685295105,
      "learning_rate": 8.847650404070637e-05,
      "loss": 0.2754,
      "step": 3732
    },
    {
      "epoch": 5.024226110363392,
      "grad_norm": 0.9195122718811035,
      "learning_rate": 8.844657288237054e-05,
      "loss": 0.186,
      "step": 3733
    },
    {
      "epoch": 5.02557200538358,
      "grad_norm": 1.1364063024520874,
      "learning_rate": 8.841664172403473e-05,
      "loss": 0.2629,
      "step": 3734
    },
    {
      "epoch": 5.0269179004037685,
      "grad_norm": 1.018784523010254,
      "learning_rate": 8.83867105656989e-05,
      "loss": 0.1838,
      "step": 3735
    },
    {
      "epoch": 5.028263795423957,
      "grad_norm": 0.9361119866371155,
      "learning_rate": 8.835677940736306e-05,
      "loss": 0.1749,
      "step": 3736
    },
    {
      "epoch": 5.029609690444145,
      "grad_norm": 0.9142611026763916,
      "learning_rate": 8.832684824902723e-05,
      "loss": 0.1891,
      "step": 3737
    },
    {
      "epoch": 5.0309555854643335,
      "grad_norm": 0.846008837223053,
      "learning_rate": 8.829691709069142e-05,
      "loss": 0.298,
      "step": 3738
    },
    {
      "epoch": 5.032301480484522,
      "grad_norm": 0.9340632557868958,
      "learning_rate": 8.826698593235559e-05,
      "loss": 0.1671,
      "step": 3739
    },
    {
      "epoch": 5.03364737550471,
      "grad_norm": 1.1880083084106445,
      "learning_rate": 8.823705477401975e-05,
      "loss": 0.1724,
      "step": 3740
    },
    {
      "epoch": 5.034993270524899,
      "grad_norm": 1.144951581954956,
      "learning_rate": 8.820712361568392e-05,
      "loss": 0.2188,
      "step": 3741
    },
    {
      "epoch": 5.036339165545088,
      "grad_norm": 1.2987427711486816,
      "learning_rate": 8.81771924573481e-05,
      "loss": 0.2254,
      "step": 3742
    },
    {
      "epoch": 5.037685060565276,
      "grad_norm": 1.166407585144043,
      "learning_rate": 8.814726129901228e-05,
      "loss": 0.2408,
      "step": 3743
    },
    {
      "epoch": 5.039030955585464,
      "grad_norm": 0.8779162168502808,
      "learning_rate": 8.811733014067644e-05,
      "loss": 0.1619,
      "step": 3744
    },
    {
      "epoch": 5.040376850605653,
      "grad_norm": 1.324898362159729,
      "learning_rate": 8.808739898234061e-05,
      "loss": 0.2893,
      "step": 3745
    },
    {
      "epoch": 5.041722745625841,
      "grad_norm": 1.2211787700653076,
      "learning_rate": 8.80574678240048e-05,
      "loss": 0.2775,
      "step": 3746
    },
    {
      "epoch": 5.043068640646029,
      "grad_norm": 1.094520926475525,
      "learning_rate": 8.802753666566897e-05,
      "loss": 0.2286,
      "step": 3747
    },
    {
      "epoch": 5.044414535666218,
      "grad_norm": 1.0361340045928955,
      "learning_rate": 8.799760550733313e-05,
      "loss": 0.212,
      "step": 3748
    },
    {
      "epoch": 5.045760430686406,
      "grad_norm": 1.1718354225158691,
      "learning_rate": 8.79676743489973e-05,
      "loss": 0.1733,
      "step": 3749
    },
    {
      "epoch": 5.0471063257065945,
      "grad_norm": 0.7358759641647339,
      "learning_rate": 8.793774319066149e-05,
      "loss": 0.1509,
      "step": 3750
    },
    {
      "epoch": 5.048452220726784,
      "grad_norm": 0.8860839605331421,
      "learning_rate": 8.790781203232566e-05,
      "loss": 0.1986,
      "step": 3751
    },
    {
      "epoch": 5.049798115746972,
      "grad_norm": 1.3005191087722778,
      "learning_rate": 8.787788087398982e-05,
      "loss": 0.1786,
      "step": 3752
    },
    {
      "epoch": 5.05114401076716,
      "grad_norm": 0.9961234331130981,
      "learning_rate": 8.7847949715654e-05,
      "loss": 0.198,
      "step": 3753
    },
    {
      "epoch": 5.052489905787349,
      "grad_norm": 1.078492522239685,
      "learning_rate": 8.781801855731818e-05,
      "loss": 0.262,
      "step": 3754
    },
    {
      "epoch": 5.053835800807537,
      "grad_norm": 0.7698878645896912,
      "learning_rate": 8.778808739898235e-05,
      "loss": 0.1567,
      "step": 3755
    },
    {
      "epoch": 5.055181695827725,
      "grad_norm": 1.308925747871399,
      "learning_rate": 8.775815624064651e-05,
      "loss": 0.2412,
      "step": 3756
    },
    {
      "epoch": 5.056527590847914,
      "grad_norm": 0.9431397318840027,
      "learning_rate": 8.772822508231068e-05,
      "loss": 0.2119,
      "step": 3757
    },
    {
      "epoch": 5.057873485868102,
      "grad_norm": 1.0398985147476196,
      "learning_rate": 8.769829392397487e-05,
      "loss": 0.2132,
      "step": 3758
    },
    {
      "epoch": 5.05921938088829,
      "grad_norm": 0.8957598805427551,
      "learning_rate": 8.766836276563904e-05,
      "loss": 0.1999,
      "step": 3759
    },
    {
      "epoch": 5.060565275908479,
      "grad_norm": 0.9265396595001221,
      "learning_rate": 8.76384316073032e-05,
      "loss": 0.192,
      "step": 3760
    },
    {
      "epoch": 5.061911170928668,
      "grad_norm": 0.9746903777122498,
      "learning_rate": 8.760850044896737e-05,
      "loss": 0.2105,
      "step": 3761
    },
    {
      "epoch": 5.063257065948856,
      "grad_norm": 1.0310081243515015,
      "learning_rate": 8.757856929063156e-05,
      "loss": 0.2263,
      "step": 3762
    },
    {
      "epoch": 5.064602960969045,
      "grad_norm": 1.1941803693771362,
      "learning_rate": 8.754863813229573e-05,
      "loss": 0.1649,
      "step": 3763
    },
    {
      "epoch": 5.065948855989233,
      "grad_norm": 1.114940881729126,
      "learning_rate": 8.75187069739599e-05,
      "loss": 0.1768,
      "step": 3764
    },
    {
      "epoch": 5.067294751009421,
      "grad_norm": 1.1363266706466675,
      "learning_rate": 8.748877581562406e-05,
      "loss": 0.2187,
      "step": 3765
    },
    {
      "epoch": 5.06864064602961,
      "grad_norm": 1.023970603942871,
      "learning_rate": 8.745884465728825e-05,
      "loss": 0.1846,
      "step": 3766
    },
    {
      "epoch": 5.069986541049798,
      "grad_norm": 0.9617955684661865,
      "learning_rate": 8.742891349895242e-05,
      "loss": 0.1829,
      "step": 3767
    },
    {
      "epoch": 5.071332436069986,
      "grad_norm": 0.7867661118507385,
      "learning_rate": 8.739898234061659e-05,
      "loss": 0.1863,
      "step": 3768
    },
    {
      "epoch": 5.072678331090175,
      "grad_norm": 0.8512422442436218,
      "learning_rate": 8.736905118228075e-05,
      "loss": 0.1763,
      "step": 3769
    },
    {
      "epoch": 5.074024226110363,
      "grad_norm": 1.591101884841919,
      "learning_rate": 8.733912002394494e-05,
      "loss": 0.3667,
      "step": 3770
    },
    {
      "epoch": 5.075370121130552,
      "grad_norm": 0.930686354637146,
      "learning_rate": 8.73091888656091e-05,
      "loss": 0.1599,
      "step": 3771
    },
    {
      "epoch": 5.0767160161507405,
      "grad_norm": 0.8253734707832336,
      "learning_rate": 8.727925770727328e-05,
      "loss": 0.1283,
      "step": 3772
    },
    {
      "epoch": 5.078061911170929,
      "grad_norm": 0.7693127393722534,
      "learning_rate": 8.724932654893744e-05,
      "loss": 0.1726,
      "step": 3773
    },
    {
      "epoch": 5.079407806191117,
      "grad_norm": 1.1632120609283447,
      "learning_rate": 8.721939539060163e-05,
      "loss": 0.2515,
      "step": 3774
    },
    {
      "epoch": 5.0807537012113055,
      "grad_norm": 1.3521180152893066,
      "learning_rate": 8.71894642322658e-05,
      "loss": 0.2694,
      "step": 3775
    },
    {
      "epoch": 5.082099596231494,
      "grad_norm": 1.4227005243301392,
      "learning_rate": 8.715953307392997e-05,
      "loss": 0.2184,
      "step": 3776
    },
    {
      "epoch": 5.083445491251682,
      "grad_norm": 1.0704256296157837,
      "learning_rate": 8.712960191559413e-05,
      "loss": 0.2259,
      "step": 3777
    },
    {
      "epoch": 5.0847913862718706,
      "grad_norm": 1.01570463180542,
      "learning_rate": 8.709967075725832e-05,
      "loss": 0.2054,
      "step": 3778
    },
    {
      "epoch": 5.086137281292059,
      "grad_norm": 1.2481921911239624,
      "learning_rate": 8.706973959892249e-05,
      "loss": 0.2766,
      "step": 3779
    },
    {
      "epoch": 5.087483176312247,
      "grad_norm": 1.0474745035171509,
      "learning_rate": 8.703980844058666e-05,
      "loss": 0.2479,
      "step": 3780
    },
    {
      "epoch": 5.0888290713324364,
      "grad_norm": 1.0940766334533691,
      "learning_rate": 8.700987728225082e-05,
      "loss": 0.2498,
      "step": 3781
    },
    {
      "epoch": 5.090174966352625,
      "grad_norm": 1.3644447326660156,
      "learning_rate": 8.697994612391501e-05,
      "loss": 0.4185,
      "step": 3782
    },
    {
      "epoch": 5.091520861372813,
      "grad_norm": 1.2536977529525757,
      "learning_rate": 8.695001496557918e-05,
      "loss": 0.2031,
      "step": 3783
    },
    {
      "epoch": 5.0928667563930015,
      "grad_norm": 0.8090847730636597,
      "learning_rate": 8.692008380724335e-05,
      "loss": 0.1597,
      "step": 3784
    },
    {
      "epoch": 5.09421265141319,
      "grad_norm": 1.1547895669937134,
      "learning_rate": 8.689015264890751e-05,
      "loss": 0.1576,
      "step": 3785
    },
    {
      "epoch": 5.095558546433378,
      "grad_norm": 0.9682356119155884,
      "learning_rate": 8.68602214905717e-05,
      "loss": 0.1777,
      "step": 3786
    },
    {
      "epoch": 5.0969044414535665,
      "grad_norm": 1.6777098178863525,
      "learning_rate": 8.683029033223587e-05,
      "loss": 0.2306,
      "step": 3787
    },
    {
      "epoch": 5.098250336473755,
      "grad_norm": 1.281055212020874,
      "learning_rate": 8.680035917390004e-05,
      "loss": 0.2199,
      "step": 3788
    },
    {
      "epoch": 5.099596231493943,
      "grad_norm": 1.1484150886535645,
      "learning_rate": 8.67704280155642e-05,
      "loss": 0.2204,
      "step": 3789
    },
    {
      "epoch": 5.1009421265141315,
      "grad_norm": 0.8070133924484253,
      "learning_rate": 8.674049685722839e-05,
      "loss": 0.1822,
      "step": 3790
    },
    {
      "epoch": 5.102288021534321,
      "grad_norm": 1.0127413272857666,
      "learning_rate": 8.671056569889256e-05,
      "loss": 0.1996,
      "step": 3791
    },
    {
      "epoch": 5.103633916554509,
      "grad_norm": 1.1030093431472778,
      "learning_rate": 8.668063454055673e-05,
      "loss": 0.2276,
      "step": 3792
    },
    {
      "epoch": 5.104979811574697,
      "grad_norm": 1.1048367023468018,
      "learning_rate": 8.66507033822209e-05,
      "loss": 0.2404,
      "step": 3793
    },
    {
      "epoch": 5.106325706594886,
      "grad_norm": 0.7448629140853882,
      "learning_rate": 8.662077222388508e-05,
      "loss": 0.1516,
      "step": 3794
    },
    {
      "epoch": 5.107671601615074,
      "grad_norm": 1.050215482711792,
      "learning_rate": 8.659084106554925e-05,
      "loss": 0.2124,
      "step": 3795
    },
    {
      "epoch": 5.109017496635262,
      "grad_norm": 1.3107787370681763,
      "learning_rate": 8.656090990721342e-05,
      "loss": 0.2271,
      "step": 3796
    },
    {
      "epoch": 5.110363391655451,
      "grad_norm": 1.190890908241272,
      "learning_rate": 8.653097874887759e-05,
      "loss": 0.1509,
      "step": 3797
    },
    {
      "epoch": 5.111709286675639,
      "grad_norm": 0.8433790802955627,
      "learning_rate": 8.650104759054177e-05,
      "loss": 0.1366,
      "step": 3798
    },
    {
      "epoch": 5.113055181695827,
      "grad_norm": 0.920356810092926,
      "learning_rate": 8.647111643220594e-05,
      "loss": 0.2138,
      "step": 3799
    },
    {
      "epoch": 5.114401076716016,
      "grad_norm": 0.8410487771034241,
      "learning_rate": 8.64411852738701e-05,
      "loss": 0.1621,
      "step": 3800
    },
    {
      "epoch": 5.115746971736205,
      "grad_norm": 1.3643468618392944,
      "learning_rate": 8.641125411553428e-05,
      "loss": 0.1886,
      "step": 3801
    },
    {
      "epoch": 5.117092866756393,
      "grad_norm": 1.1407567262649536,
      "learning_rate": 8.638132295719846e-05,
      "loss": 0.2049,
      "step": 3802
    },
    {
      "epoch": 5.118438761776582,
      "grad_norm": 0.7455899715423584,
      "learning_rate": 8.635139179886263e-05,
      "loss": 0.174,
      "step": 3803
    },
    {
      "epoch": 5.11978465679677,
      "grad_norm": 1.0824949741363525,
      "learning_rate": 8.63214606405268e-05,
      "loss": 0.2158,
      "step": 3804
    },
    {
      "epoch": 5.121130551816958,
      "grad_norm": 1.1771246194839478,
      "learning_rate": 8.629152948219097e-05,
      "loss": 0.2016,
      "step": 3805
    },
    {
      "epoch": 5.122476446837147,
      "grad_norm": 0.8885634541511536,
      "learning_rate": 8.626159832385515e-05,
      "loss": 0.212,
      "step": 3806
    },
    {
      "epoch": 5.123822341857335,
      "grad_norm": 1.273654818534851,
      "learning_rate": 8.623166716551932e-05,
      "loss": 0.2353,
      "step": 3807
    },
    {
      "epoch": 5.125168236877523,
      "grad_norm": 1.0318328142166138,
      "learning_rate": 8.620173600718349e-05,
      "loss": 0.2414,
      "step": 3808
    },
    {
      "epoch": 5.126514131897712,
      "grad_norm": 1.0794841051101685,
      "learning_rate": 8.617180484884766e-05,
      "loss": 0.1812,
      "step": 3809
    },
    {
      "epoch": 5.1278600269179,
      "grad_norm": 0.9338566660881042,
      "learning_rate": 8.614187369051183e-05,
      "loss": 0.1796,
      "step": 3810
    },
    {
      "epoch": 5.129205921938089,
      "grad_norm": 1.04623544216156,
      "learning_rate": 8.611194253217601e-05,
      "loss": 0.1934,
      "step": 3811
    },
    {
      "epoch": 5.1305518169582776,
      "grad_norm": 1.1928924322128296,
      "learning_rate": 8.608201137384018e-05,
      "loss": 0.2055,
      "step": 3812
    },
    {
      "epoch": 5.131897711978466,
      "grad_norm": 0.9804511070251465,
      "learning_rate": 8.605208021550435e-05,
      "loss": 0.2035,
      "step": 3813
    },
    {
      "epoch": 5.133243606998654,
      "grad_norm": 1.0341427326202393,
      "learning_rate": 8.602214905716852e-05,
      "loss": 0.2741,
      "step": 3814
    },
    {
      "epoch": 5.134589502018843,
      "grad_norm": 1.3265591859817505,
      "learning_rate": 8.59922178988327e-05,
      "loss": 0.2308,
      "step": 3815
    },
    {
      "epoch": 5.135935397039031,
      "grad_norm": 0.8503767848014832,
      "learning_rate": 8.596228674049687e-05,
      "loss": 0.1886,
      "step": 3816
    },
    {
      "epoch": 5.137281292059219,
      "grad_norm": 1.03486168384552,
      "learning_rate": 8.593235558216102e-05,
      "loss": 0.2409,
      "step": 3817
    },
    {
      "epoch": 5.138627187079408,
      "grad_norm": 1.1007862091064453,
      "learning_rate": 8.59024244238252e-05,
      "loss": 0.2301,
      "step": 3818
    },
    {
      "epoch": 5.139973082099596,
      "grad_norm": 1.2535555362701416,
      "learning_rate": 8.587249326548937e-05,
      "loss": 0.2593,
      "step": 3819
    },
    {
      "epoch": 5.141318977119784,
      "grad_norm": 1.1293470859527588,
      "learning_rate": 8.584256210715354e-05,
      "loss": 0.2389,
      "step": 3820
    },
    {
      "epoch": 5.1426648721399735,
      "grad_norm": 0.996663510799408,
      "learning_rate": 8.581263094881771e-05,
      "loss": 0.2159,
      "step": 3821
    },
    {
      "epoch": 5.144010767160162,
      "grad_norm": 0.9080471992492676,
      "learning_rate": 8.57826997904819e-05,
      "loss": 0.2413,
      "step": 3822
    },
    {
      "epoch": 5.14535666218035,
      "grad_norm": 0.9144138693809509,
      "learning_rate": 8.575276863214606e-05,
      "loss": 0.1754,
      "step": 3823
    },
    {
      "epoch": 5.1467025572005385,
      "grad_norm": 0.8630090355873108,
      "learning_rate": 8.572283747381023e-05,
      "loss": 0.16,
      "step": 3824
    },
    {
      "epoch": 5.148048452220727,
      "grad_norm": 1.014219045639038,
      "learning_rate": 8.56929063154744e-05,
      "loss": 0.1763,
      "step": 3825
    },
    {
      "epoch": 5.149394347240915,
      "grad_norm": 0.9615554213523865,
      "learning_rate": 8.566297515713859e-05,
      "loss": 0.2235,
      "step": 3826
    },
    {
      "epoch": 5.1507402422611035,
      "grad_norm": 0.9699686169624329,
      "learning_rate": 8.563304399880275e-05,
      "loss": 0.2013,
      "step": 3827
    },
    {
      "epoch": 5.152086137281292,
      "grad_norm": 1.1326261758804321,
      "learning_rate": 8.560311284046692e-05,
      "loss": 0.1698,
      "step": 3828
    },
    {
      "epoch": 5.15343203230148,
      "grad_norm": 1.301005482673645,
      "learning_rate": 8.557318168213109e-05,
      "loss": 0.1923,
      "step": 3829
    },
    {
      "epoch": 5.1547779273216685,
      "grad_norm": 1.101765513420105,
      "learning_rate": 8.554325052379528e-05,
      "loss": 0.1999,
      "step": 3830
    },
    {
      "epoch": 5.156123822341858,
      "grad_norm": 0.6412386894226074,
      "learning_rate": 8.551331936545944e-05,
      "loss": 0.1471,
      "step": 3831
    },
    {
      "epoch": 5.157469717362046,
      "grad_norm": 0.9539312720298767,
      "learning_rate": 8.548338820712361e-05,
      "loss": 0.1577,
      "step": 3832
    },
    {
      "epoch": 5.158815612382234,
      "grad_norm": 1.0865916013717651,
      "learning_rate": 8.545345704878778e-05,
      "loss": 0.2557,
      "step": 3833
    },
    {
      "epoch": 5.160161507402423,
      "grad_norm": 1.0383738279342651,
      "learning_rate": 8.542352589045197e-05,
      "loss": 0.1877,
      "step": 3834
    },
    {
      "epoch": 5.161507402422611,
      "grad_norm": 0.9563828706741333,
      "learning_rate": 8.539359473211614e-05,
      "loss": 0.1758,
      "step": 3835
    },
    {
      "epoch": 5.162853297442799,
      "grad_norm": 1.6079647541046143,
      "learning_rate": 8.53636635737803e-05,
      "loss": 0.3248,
      "step": 3836
    },
    {
      "epoch": 5.164199192462988,
      "grad_norm": 1.3861393928527832,
      "learning_rate": 8.533373241544447e-05,
      "loss": 0.2217,
      "step": 3837
    },
    {
      "epoch": 5.165545087483176,
      "grad_norm": 0.7815966606140137,
      "learning_rate": 8.530380125710866e-05,
      "loss": 0.1682,
      "step": 3838
    },
    {
      "epoch": 5.166890982503364,
      "grad_norm": 1.3232717514038086,
      "learning_rate": 8.527387009877283e-05,
      "loss": 0.1976,
      "step": 3839
    },
    {
      "epoch": 5.168236877523553,
      "grad_norm": 1.350302815437317,
      "learning_rate": 8.5243938940437e-05,
      "loss": 0.3016,
      "step": 3840
    },
    {
      "epoch": 5.169582772543742,
      "grad_norm": 1.081318974494934,
      "learning_rate": 8.521400778210116e-05,
      "loss": 0.2092,
      "step": 3841
    },
    {
      "epoch": 5.17092866756393,
      "grad_norm": 1.3162156343460083,
      "learning_rate": 8.518407662376535e-05,
      "loss": 0.215,
      "step": 3842
    },
    {
      "epoch": 5.172274562584119,
      "grad_norm": 1.193488359451294,
      "learning_rate": 8.515414546542952e-05,
      "loss": 0.2127,
      "step": 3843
    },
    {
      "epoch": 5.173620457604307,
      "grad_norm": 1.140220284461975,
      "learning_rate": 8.512421430709368e-05,
      "loss": 0.2004,
      "step": 3844
    },
    {
      "epoch": 5.174966352624495,
      "grad_norm": 0.9188832640647888,
      "learning_rate": 8.509428314875785e-05,
      "loss": 0.1809,
      "step": 3845
    },
    {
      "epoch": 5.176312247644684,
      "grad_norm": 1.3124914169311523,
      "learning_rate": 8.506435199042204e-05,
      "loss": 0.2988,
      "step": 3846
    },
    {
      "epoch": 5.177658142664872,
      "grad_norm": 1.0203168392181396,
      "learning_rate": 8.50344208320862e-05,
      "loss": 0.1872,
      "step": 3847
    },
    {
      "epoch": 5.17900403768506,
      "grad_norm": 0.8533086776733398,
      "learning_rate": 8.500448967375037e-05,
      "loss": 0.1782,
      "step": 3848
    },
    {
      "epoch": 5.180349932705249,
      "grad_norm": 1.0828561782836914,
      "learning_rate": 8.497455851541454e-05,
      "loss": 0.181,
      "step": 3849
    },
    {
      "epoch": 5.181695827725437,
      "grad_norm": 0.983704686164856,
      "learning_rate": 8.494462735707873e-05,
      "loss": 0.1943,
      "step": 3850
    },
    {
      "epoch": 5.183041722745626,
      "grad_norm": 0.9286051988601685,
      "learning_rate": 8.49146961987429e-05,
      "loss": 0.2067,
      "step": 3851
    },
    {
      "epoch": 5.184387617765815,
      "grad_norm": 0.9252648949623108,
      "learning_rate": 8.488476504040706e-05,
      "loss": 0.1857,
      "step": 3852
    },
    {
      "epoch": 5.185733512786003,
      "grad_norm": 1.1212315559387207,
      "learning_rate": 8.485483388207123e-05,
      "loss": 0.2518,
      "step": 3853
    },
    {
      "epoch": 5.187079407806191,
      "grad_norm": 1.065990924835205,
      "learning_rate": 8.482490272373542e-05,
      "loss": 0.1907,
      "step": 3854
    },
    {
      "epoch": 5.18842530282638,
      "grad_norm": 1.0060253143310547,
      "learning_rate": 8.479497156539959e-05,
      "loss": 0.1822,
      "step": 3855
    },
    {
      "epoch": 5.189771197846568,
      "grad_norm": 1.026412844657898,
      "learning_rate": 8.476504040706375e-05,
      "loss": 0.2103,
      "step": 3856
    },
    {
      "epoch": 5.191117092866756,
      "grad_norm": 1.3271266222000122,
      "learning_rate": 8.473510924872792e-05,
      "loss": 0.2444,
      "step": 3857
    },
    {
      "epoch": 5.192462987886945,
      "grad_norm": 1.047810435295105,
      "learning_rate": 8.470517809039211e-05,
      "loss": 0.1746,
      "step": 3858
    },
    {
      "epoch": 5.193808882907133,
      "grad_norm": 0.9324247241020203,
      "learning_rate": 8.467524693205628e-05,
      "loss": 0.3173,
      "step": 3859
    },
    {
      "epoch": 5.195154777927321,
      "grad_norm": 0.9131499528884888,
      "learning_rate": 8.464531577372045e-05,
      "loss": 0.1871,
      "step": 3860
    },
    {
      "epoch": 5.1965006729475105,
      "grad_norm": 0.9993782639503479,
      "learning_rate": 8.461538461538461e-05,
      "loss": 0.2277,
      "step": 3861
    },
    {
      "epoch": 5.197846567967699,
      "grad_norm": 1.0890215635299683,
      "learning_rate": 8.45854534570488e-05,
      "loss": 0.209,
      "step": 3862
    },
    {
      "epoch": 5.199192462987887,
      "grad_norm": 1.1458745002746582,
      "learning_rate": 8.455552229871297e-05,
      "loss": 0.2475,
      "step": 3863
    },
    {
      "epoch": 5.2005383580080755,
      "grad_norm": 0.9443506002426147,
      "learning_rate": 8.452559114037714e-05,
      "loss": 0.1817,
      "step": 3864
    },
    {
      "epoch": 5.201884253028264,
      "grad_norm": 0.9996106028556824,
      "learning_rate": 8.44956599820413e-05,
      "loss": 0.2014,
      "step": 3865
    },
    {
      "epoch": 5.203230148048452,
      "grad_norm": 1.3081773519515991,
      "learning_rate": 8.446572882370549e-05,
      "loss": 0.2455,
      "step": 3866
    },
    {
      "epoch": 5.2045760430686405,
      "grad_norm": 1.1284626722335815,
      "learning_rate": 8.443579766536966e-05,
      "loss": 0.2009,
      "step": 3867
    },
    {
      "epoch": 5.205921938088829,
      "grad_norm": 1.1450587511062622,
      "learning_rate": 8.440586650703383e-05,
      "loss": 0.2181,
      "step": 3868
    },
    {
      "epoch": 5.207267833109017,
      "grad_norm": 0.9017307758331299,
      "learning_rate": 8.4375935348698e-05,
      "loss": 0.193,
      "step": 3869
    },
    {
      "epoch": 5.2086137281292055,
      "grad_norm": 1.1106717586517334,
      "learning_rate": 8.434600419036218e-05,
      "loss": 0.2039,
      "step": 3870
    },
    {
      "epoch": 5.209959623149395,
      "grad_norm": 0.9839627146720886,
      "learning_rate": 8.431607303202635e-05,
      "loss": 0.166,
      "step": 3871
    },
    {
      "epoch": 5.211305518169583,
      "grad_norm": 1.2724889516830444,
      "learning_rate": 8.428614187369052e-05,
      "loss": 0.2414,
      "step": 3872
    },
    {
      "epoch": 5.212651413189771,
      "grad_norm": 0.8495454788208008,
      "learning_rate": 8.425621071535468e-05,
      "loss": 0.1791,
      "step": 3873
    },
    {
      "epoch": 5.21399730820996,
      "grad_norm": 1.0671594142913818,
      "learning_rate": 8.422627955701887e-05,
      "loss": 0.2281,
      "step": 3874
    },
    {
      "epoch": 5.215343203230148,
      "grad_norm": 0.8534552454948425,
      "learning_rate": 8.419634839868304e-05,
      "loss": 0.1439,
      "step": 3875
    },
    {
      "epoch": 5.216689098250336,
      "grad_norm": 1.070542335510254,
      "learning_rate": 8.41664172403472e-05,
      "loss": 0.2298,
      "step": 3876
    },
    {
      "epoch": 5.218034993270525,
      "grad_norm": 0.8468745350837708,
      "learning_rate": 8.413648608201137e-05,
      "loss": 0.1622,
      "step": 3877
    },
    {
      "epoch": 5.219380888290713,
      "grad_norm": 1.1113618612289429,
      "learning_rate": 8.410655492367556e-05,
      "loss": 0.2166,
      "step": 3878
    },
    {
      "epoch": 5.2207267833109015,
      "grad_norm": 1.137252688407898,
      "learning_rate": 8.407662376533973e-05,
      "loss": 0.225,
      "step": 3879
    },
    {
      "epoch": 5.22207267833109,
      "grad_norm": 1.2043445110321045,
      "learning_rate": 8.40466926070039e-05,
      "loss": 0.3035,
      "step": 3880
    },
    {
      "epoch": 5.223418573351279,
      "grad_norm": 1.097172737121582,
      "learning_rate": 8.401676144866806e-05,
      "loss": 0.2351,
      "step": 3881
    },
    {
      "epoch": 5.224764468371467,
      "grad_norm": 1.1013524532318115,
      "learning_rate": 8.398683029033225e-05,
      "loss": 0.1916,
      "step": 3882
    },
    {
      "epoch": 5.226110363391656,
      "grad_norm": 1.2002902030944824,
      "learning_rate": 8.395689913199642e-05,
      "loss": 0.1938,
      "step": 3883
    },
    {
      "epoch": 5.227456258411844,
      "grad_norm": 0.9978522658348083,
      "learning_rate": 8.392696797366059e-05,
      "loss": 0.2016,
      "step": 3884
    },
    {
      "epoch": 5.228802153432032,
      "grad_norm": 1.1173843145370483,
      "learning_rate": 8.389703681532476e-05,
      "loss": 0.2154,
      "step": 3885
    },
    {
      "epoch": 5.230148048452221,
      "grad_norm": 0.6512889266014099,
      "learning_rate": 8.386710565698894e-05,
      "loss": 0.1308,
      "step": 3886
    },
    {
      "epoch": 5.231493943472409,
      "grad_norm": 1.1942881345748901,
      "learning_rate": 8.383717449865311e-05,
      "loss": 0.3285,
      "step": 3887
    },
    {
      "epoch": 5.232839838492597,
      "grad_norm": 1.4443577527999878,
      "learning_rate": 8.380724334031728e-05,
      "loss": 0.3198,
      "step": 3888
    },
    {
      "epoch": 5.234185733512786,
      "grad_norm": 0.9710310697555542,
      "learning_rate": 8.377731218198145e-05,
      "loss": 0.2086,
      "step": 3889
    },
    {
      "epoch": 5.235531628532974,
      "grad_norm": 1.1395436525344849,
      "learning_rate": 8.374738102364563e-05,
      "loss": 0.2466,
      "step": 3890
    },
    {
      "epoch": 5.236877523553163,
      "grad_norm": 0.9314771890640259,
      "learning_rate": 8.37174498653098e-05,
      "loss": 0.1825,
      "step": 3891
    },
    {
      "epoch": 5.238223418573352,
      "grad_norm": 1.127444863319397,
      "learning_rate": 8.368751870697397e-05,
      "loss": 0.228,
      "step": 3892
    },
    {
      "epoch": 5.23956931359354,
      "grad_norm": 1.0434023141860962,
      "learning_rate": 8.365758754863814e-05,
      "loss": 0.2455,
      "step": 3893
    },
    {
      "epoch": 5.240915208613728,
      "grad_norm": 1.2145575284957886,
      "learning_rate": 8.362765639030232e-05,
      "loss": 0.1989,
      "step": 3894
    },
    {
      "epoch": 5.242261103633917,
      "grad_norm": 0.9630274176597595,
      "learning_rate": 8.359772523196649e-05,
      "loss": 0.1779,
      "step": 3895
    },
    {
      "epoch": 5.243606998654105,
      "grad_norm": 0.9726566672325134,
      "learning_rate": 8.356779407363066e-05,
      "loss": 0.1812,
      "step": 3896
    },
    {
      "epoch": 5.244952893674293,
      "grad_norm": 1.0440409183502197,
      "learning_rate": 8.353786291529483e-05,
      "loss": 0.198,
      "step": 3897
    },
    {
      "epoch": 5.246298788694482,
      "grad_norm": 0.8826577663421631,
      "learning_rate": 8.3507931756959e-05,
      "loss": 0.1643,
      "step": 3898
    },
    {
      "epoch": 5.24764468371467,
      "grad_norm": 1.2712979316711426,
      "learning_rate": 8.347800059862318e-05,
      "loss": 0.2703,
      "step": 3899
    },
    {
      "epoch": 5.248990578734858,
      "grad_norm": 1.1111632585525513,
      "learning_rate": 8.344806944028735e-05,
      "loss": 0.2285,
      "step": 3900
    },
    {
      "epoch": 5.250336473755047,
      "grad_norm": 1.0152592658996582,
      "learning_rate": 8.341813828195152e-05,
      "loss": 0.1782,
      "step": 3901
    },
    {
      "epoch": 5.251682368775236,
      "grad_norm": 1.245161533355713,
      "learning_rate": 8.338820712361568e-05,
      "loss": 0.1999,
      "step": 3902
    },
    {
      "epoch": 5.253028263795424,
      "grad_norm": 0.9944461584091187,
      "learning_rate": 8.335827596527987e-05,
      "loss": 0.2221,
      "step": 3903
    },
    {
      "epoch": 5.2543741588156125,
      "grad_norm": 1.411789894104004,
      "learning_rate": 8.332834480694404e-05,
      "loss": 0.2183,
      "step": 3904
    },
    {
      "epoch": 5.255720053835801,
      "grad_norm": 1.2713202238082886,
      "learning_rate": 8.32984136486082e-05,
      "loss": 0.215,
      "step": 3905
    },
    {
      "epoch": 5.257065948855989,
      "grad_norm": 1.1178982257843018,
      "learning_rate": 8.326848249027238e-05,
      "loss": 0.2374,
      "step": 3906
    },
    {
      "epoch": 5.2584118438761775,
      "grad_norm": 0.9322090744972229,
      "learning_rate": 8.323855133193656e-05,
      "loss": 0.1937,
      "step": 3907
    },
    {
      "epoch": 5.259757738896366,
      "grad_norm": 1.2354249954223633,
      "learning_rate": 8.320862017360073e-05,
      "loss": 0.2228,
      "step": 3908
    },
    {
      "epoch": 5.261103633916554,
      "grad_norm": 1.2025209665298462,
      "learning_rate": 8.31786890152649e-05,
      "loss": 0.2317,
      "step": 3909
    },
    {
      "epoch": 5.262449528936743,
      "grad_norm": 1.422691822052002,
      "learning_rate": 8.314875785692907e-05,
      "loss": 0.293,
      "step": 3910
    },
    {
      "epoch": 5.263795423956932,
      "grad_norm": 0.7826690673828125,
      "learning_rate": 8.311882669859325e-05,
      "loss": 0.1448,
      "step": 3911
    },
    {
      "epoch": 5.26514131897712,
      "grad_norm": 1.1696619987487793,
      "learning_rate": 8.308889554025742e-05,
      "loss": 0.2189,
      "step": 3912
    },
    {
      "epoch": 5.2664872139973085,
      "grad_norm": 0.9829529523849487,
      "learning_rate": 8.305896438192159e-05,
      "loss": 0.2702,
      "step": 3913
    },
    {
      "epoch": 5.267833109017497,
      "grad_norm": 1.2848387956619263,
      "learning_rate": 8.302903322358576e-05,
      "loss": 0.2313,
      "step": 3914
    },
    {
      "epoch": 5.269179004037685,
      "grad_norm": 1.5339341163635254,
      "learning_rate": 8.299910206524994e-05,
      "loss": 0.302,
      "step": 3915
    },
    {
      "epoch": 5.2705248990578735,
      "grad_norm": 0.9796574115753174,
      "learning_rate": 8.296917090691411e-05,
      "loss": 0.1828,
      "step": 3916
    },
    {
      "epoch": 5.271870794078062,
      "grad_norm": 1.0307235717773438,
      "learning_rate": 8.293923974857828e-05,
      "loss": 0.2058,
      "step": 3917
    },
    {
      "epoch": 5.27321668909825,
      "grad_norm": 1.03696870803833,
      "learning_rate": 8.290930859024245e-05,
      "loss": 0.1823,
      "step": 3918
    },
    {
      "epoch": 5.2745625841184385,
      "grad_norm": 1.3427156209945679,
      "learning_rate": 8.287937743190663e-05,
      "loss": 0.2591,
      "step": 3919
    },
    {
      "epoch": 5.275908479138627,
      "grad_norm": 0.9470914006233215,
      "learning_rate": 8.28494462735708e-05,
      "loss": 0.1794,
      "step": 3920
    },
    {
      "epoch": 5.277254374158815,
      "grad_norm": 1.1214027404785156,
      "learning_rate": 8.281951511523497e-05,
      "loss": 0.2261,
      "step": 3921
    },
    {
      "epoch": 5.278600269179004,
      "grad_norm": 0.9948038458824158,
      "learning_rate": 8.278958395689914e-05,
      "loss": 0.1923,
      "step": 3922
    },
    {
      "epoch": 5.279946164199193,
      "grad_norm": 1.241540551185608,
      "learning_rate": 8.27596527985633e-05,
      "loss": 0.2909,
      "step": 3923
    },
    {
      "epoch": 5.281292059219381,
      "grad_norm": 1.1142070293426514,
      "learning_rate": 8.272972164022747e-05,
      "loss": 0.2361,
      "step": 3924
    },
    {
      "epoch": 5.282637954239569,
      "grad_norm": 1.369653344154358,
      "learning_rate": 8.269979048189164e-05,
      "loss": 0.2657,
      "step": 3925
    },
    {
      "epoch": 5.283983849259758,
      "grad_norm": 1.1146676540374756,
      "learning_rate": 8.266985932355583e-05,
      "loss": 0.1967,
      "step": 3926
    },
    {
      "epoch": 5.285329744279946,
      "grad_norm": 1.3053910732269287,
      "learning_rate": 8.263992816522e-05,
      "loss": 0.2359,
      "step": 3927
    },
    {
      "epoch": 5.286675639300134,
      "grad_norm": 1.4280791282653809,
      "learning_rate": 8.260999700688416e-05,
      "loss": 0.2904,
      "step": 3928
    },
    {
      "epoch": 5.288021534320323,
      "grad_norm": 1.0337913036346436,
      "learning_rate": 8.258006584854833e-05,
      "loss": 0.196,
      "step": 3929
    },
    {
      "epoch": 5.289367429340511,
      "grad_norm": 1.0030696392059326,
      "learning_rate": 8.255013469021252e-05,
      "loss": 0.1676,
      "step": 3930
    },
    {
      "epoch": 5.2907133243607,
      "grad_norm": 1.093195915222168,
      "learning_rate": 8.252020353187669e-05,
      "loss": 0.2139,
      "step": 3931
    },
    {
      "epoch": 5.292059219380889,
      "grad_norm": 1.0478023290634155,
      "learning_rate": 8.249027237354085e-05,
      "loss": 0.2209,
      "step": 3932
    },
    {
      "epoch": 5.293405114401077,
      "grad_norm": 1.0781489610671997,
      "learning_rate": 8.246034121520502e-05,
      "loss": 0.2703,
      "step": 3933
    },
    {
      "epoch": 5.294751009421265,
      "grad_norm": 1.1360152959823608,
      "learning_rate": 8.24304100568692e-05,
      "loss": 0.1782,
      "step": 3934
    },
    {
      "epoch": 5.296096904441454,
      "grad_norm": 1.0922168493270874,
      "learning_rate": 8.240047889853338e-05,
      "loss": 0.2058,
      "step": 3935
    },
    {
      "epoch": 5.297442799461642,
      "grad_norm": 1.0340838432312012,
      "learning_rate": 8.237054774019754e-05,
      "loss": 0.2167,
      "step": 3936
    },
    {
      "epoch": 5.29878869448183,
      "grad_norm": 1.375401496887207,
      "learning_rate": 8.234061658186171e-05,
      "loss": 0.2168,
      "step": 3937
    },
    {
      "epoch": 5.300134589502019,
      "grad_norm": 1.1026664972305298,
      "learning_rate": 8.23106854235259e-05,
      "loss": 0.2859,
      "step": 3938
    },
    {
      "epoch": 5.301480484522207,
      "grad_norm": 1.0835137367248535,
      "learning_rate": 8.228075426519007e-05,
      "loss": 0.2145,
      "step": 3939
    },
    {
      "epoch": 5.302826379542395,
      "grad_norm": 0.9411798715591431,
      "learning_rate": 8.225082310685423e-05,
      "loss": 0.1781,
      "step": 3940
    },
    {
      "epoch": 5.304172274562584,
      "grad_norm": 1.0734621286392212,
      "learning_rate": 8.22208919485184e-05,
      "loss": 0.2394,
      "step": 3941
    },
    {
      "epoch": 5.305518169582773,
      "grad_norm": 1.0690293312072754,
      "learning_rate": 8.219096079018259e-05,
      "loss": 0.2111,
      "step": 3942
    },
    {
      "epoch": 5.306864064602961,
      "grad_norm": 1.081031322479248,
      "learning_rate": 8.216102963184676e-05,
      "loss": 0.2382,
      "step": 3943
    },
    {
      "epoch": 5.30820995962315,
      "grad_norm": 1.1219916343688965,
      "learning_rate": 8.213109847351092e-05,
      "loss": 0.4237,
      "step": 3944
    },
    {
      "epoch": 5.309555854643338,
      "grad_norm": 0.9029479622840881,
      "learning_rate": 8.21011673151751e-05,
      "loss": 0.1663,
      "step": 3945
    },
    {
      "epoch": 5.310901749663526,
      "grad_norm": 0.9905092716217041,
      "learning_rate": 8.207123615683928e-05,
      "loss": 0.1863,
      "step": 3946
    },
    {
      "epoch": 5.312247644683715,
      "grad_norm": 1.1372753381729126,
      "learning_rate": 8.204130499850345e-05,
      "loss": 0.1952,
      "step": 3947
    },
    {
      "epoch": 5.313593539703903,
      "grad_norm": 0.9424579739570618,
      "learning_rate": 8.201137384016761e-05,
      "loss": 0.1723,
      "step": 3948
    },
    {
      "epoch": 5.314939434724091,
      "grad_norm": 1.1006728410720825,
      "learning_rate": 8.198144268183178e-05,
      "loss": 0.1812,
      "step": 3949
    },
    {
      "epoch": 5.31628532974428,
      "grad_norm": 1.0667352676391602,
      "learning_rate": 8.195151152349597e-05,
      "loss": 0.2182,
      "step": 3950
    },
    {
      "epoch": 5.317631224764469,
      "grad_norm": 0.828580379486084,
      "learning_rate": 8.192158036516014e-05,
      "loss": 0.1396,
      "step": 3951
    },
    {
      "epoch": 5.318977119784657,
      "grad_norm": 0.9637138247489929,
      "learning_rate": 8.18916492068243e-05,
      "loss": 0.2101,
      "step": 3952
    },
    {
      "epoch": 5.3203230148048455,
      "grad_norm": 1.0735058784484863,
      "learning_rate": 8.186171804848847e-05,
      "loss": 0.1937,
      "step": 3953
    },
    {
      "epoch": 5.321668909825034,
      "grad_norm": 1.6529359817504883,
      "learning_rate": 8.183178689015266e-05,
      "loss": 0.2381,
      "step": 3954
    },
    {
      "epoch": 5.323014804845222,
      "grad_norm": 1.1060054302215576,
      "learning_rate": 8.180185573181683e-05,
      "loss": 0.2943,
      "step": 3955
    },
    {
      "epoch": 5.3243606998654105,
      "grad_norm": 0.8595532774925232,
      "learning_rate": 8.1771924573481e-05,
      "loss": 0.1796,
      "step": 3956
    },
    {
      "epoch": 5.325706594885599,
      "grad_norm": 1.5163743495941162,
      "learning_rate": 8.174199341514516e-05,
      "loss": 0.2474,
      "step": 3957
    },
    {
      "epoch": 5.327052489905787,
      "grad_norm": 1.6238787174224854,
      "learning_rate": 8.171206225680935e-05,
      "loss": 0.3074,
      "step": 3958
    },
    {
      "epoch": 5.3283983849259755,
      "grad_norm": 1.2279095649719238,
      "learning_rate": 8.168213109847352e-05,
      "loss": 0.256,
      "step": 3959
    },
    {
      "epoch": 5.329744279946164,
      "grad_norm": 1.1266965866088867,
      "learning_rate": 8.165219994013769e-05,
      "loss": 0.2272,
      "step": 3960
    },
    {
      "epoch": 5.331090174966352,
      "grad_norm": 1.3997876644134521,
      "learning_rate": 8.162226878180185e-05,
      "loss": 0.21,
      "step": 3961
    },
    {
      "epoch": 5.332436069986541,
      "grad_norm": 1.1090593338012695,
      "learning_rate": 8.159233762346604e-05,
      "loss": 0.1904,
      "step": 3962
    },
    {
      "epoch": 5.33378196500673,
      "grad_norm": 1.3450723886489868,
      "learning_rate": 8.15624064651302e-05,
      "loss": 0.2637,
      "step": 3963
    },
    {
      "epoch": 5.335127860026918,
      "grad_norm": 1.0124973058700562,
      "learning_rate": 8.153247530679438e-05,
      "loss": 0.215,
      "step": 3964
    },
    {
      "epoch": 5.336473755047106,
      "grad_norm": 0.9304026961326599,
      "learning_rate": 8.150254414845854e-05,
      "loss": 0.1984,
      "step": 3965
    },
    {
      "epoch": 5.337819650067295,
      "grad_norm": 1.3314921855926514,
      "learning_rate": 8.147261299012273e-05,
      "loss": 0.2887,
      "step": 3966
    },
    {
      "epoch": 5.339165545087483,
      "grad_norm": 0.8311031460762024,
      "learning_rate": 8.14426818317869e-05,
      "loss": 0.1807,
      "step": 3967
    },
    {
      "epoch": 5.340511440107671,
      "grad_norm": 0.8413747549057007,
      "learning_rate": 8.141275067345107e-05,
      "loss": 0.1934,
      "step": 3968
    },
    {
      "epoch": 5.34185733512786,
      "grad_norm": 1.252656102180481,
      "learning_rate": 8.138281951511523e-05,
      "loss": 0.2017,
      "step": 3969
    },
    {
      "epoch": 5.343203230148048,
      "grad_norm": 1.0410425662994385,
      "learning_rate": 8.135288835677942e-05,
      "loss": 0.2492,
      "step": 3970
    },
    {
      "epoch": 5.344549125168237,
      "grad_norm": 0.9171389937400818,
      "learning_rate": 8.132295719844359e-05,
      "loss": 0.226,
      "step": 3971
    },
    {
      "epoch": 5.345895020188426,
      "grad_norm": 1.0380754470825195,
      "learning_rate": 8.129302604010776e-05,
      "loss": 0.2461,
      "step": 3972
    },
    {
      "epoch": 5.347240915208614,
      "grad_norm": 1.1907535791397095,
      "learning_rate": 8.126309488177192e-05,
      "loss": 0.195,
      "step": 3973
    },
    {
      "epoch": 5.348586810228802,
      "grad_norm": 1.0101875066757202,
      "learning_rate": 8.123316372343611e-05,
      "loss": 0.2022,
      "step": 3974
    },
    {
      "epoch": 5.349932705248991,
      "grad_norm": 1.1310542821884155,
      "learning_rate": 8.120323256510028e-05,
      "loss": 0.2355,
      "step": 3975
    },
    {
      "epoch": 5.351278600269179,
      "grad_norm": 1.2155563831329346,
      "learning_rate": 8.117330140676445e-05,
      "loss": 0.2043,
      "step": 3976
    },
    {
      "epoch": 5.352624495289367,
      "grad_norm": 1.1040185689926147,
      "learning_rate": 8.114337024842861e-05,
      "loss": 0.2082,
      "step": 3977
    },
    {
      "epoch": 5.353970390309556,
      "grad_norm": 1.4210871458053589,
      "learning_rate": 8.11134390900928e-05,
      "loss": 0.2783,
      "step": 3978
    },
    {
      "epoch": 5.355316285329744,
      "grad_norm": 0.9465250968933105,
      "learning_rate": 8.108350793175697e-05,
      "loss": 0.2147,
      "step": 3979
    },
    {
      "epoch": 5.356662180349932,
      "grad_norm": 1.0279486179351807,
      "learning_rate": 8.105357677342114e-05,
      "loss": 0.2343,
      "step": 3980
    },
    {
      "epoch": 5.358008075370121,
      "grad_norm": 1.3473817110061646,
      "learning_rate": 8.10236456150853e-05,
      "loss": 0.2156,
      "step": 3981
    },
    {
      "epoch": 5.35935397039031,
      "grad_norm": 0.9064136743545532,
      "learning_rate": 8.099371445674947e-05,
      "loss": 0.1717,
      "step": 3982
    },
    {
      "epoch": 5.360699865410498,
      "grad_norm": 0.8143244385719299,
      "learning_rate": 8.096378329841366e-05,
      "loss": 0.161,
      "step": 3983
    },
    {
      "epoch": 5.362045760430687,
      "grad_norm": 1.0479894876480103,
      "learning_rate": 8.093385214007783e-05,
      "loss": 0.1941,
      "step": 3984
    },
    {
      "epoch": 5.363391655450875,
      "grad_norm": 1.3899638652801514,
      "learning_rate": 8.0903920981742e-05,
      "loss": 0.3004,
      "step": 3985
    },
    {
      "epoch": 5.364737550471063,
      "grad_norm": 1.322224736213684,
      "learning_rate": 8.087398982340616e-05,
      "loss": 0.3108,
      "step": 3986
    },
    {
      "epoch": 5.366083445491252,
      "grad_norm": 0.952221155166626,
      "learning_rate": 8.084405866507035e-05,
      "loss": 0.2582,
      "step": 3987
    },
    {
      "epoch": 5.36742934051144,
      "grad_norm": 1.154595971107483,
      "learning_rate": 8.081412750673452e-05,
      "loss": 0.2165,
      "step": 3988
    },
    {
      "epoch": 5.368775235531628,
      "grad_norm": 1.2667157649993896,
      "learning_rate": 8.078419634839869e-05,
      "loss": 0.2758,
      "step": 3989
    },
    {
      "epoch": 5.370121130551817,
      "grad_norm": 1.0235508680343628,
      "learning_rate": 8.075426519006285e-05,
      "loss": 0.221,
      "step": 3990
    },
    {
      "epoch": 5.371467025572006,
      "grad_norm": 1.218403935432434,
      "learning_rate": 8.072433403172704e-05,
      "loss": 0.2379,
      "step": 3991
    },
    {
      "epoch": 5.372812920592194,
      "grad_norm": 0.9407192468643188,
      "learning_rate": 8.06944028733912e-05,
      "loss": 0.1741,
      "step": 3992
    },
    {
      "epoch": 5.3741588156123825,
      "grad_norm": 1.0562070608139038,
      "learning_rate": 8.066447171505538e-05,
      "loss": 0.2427,
      "step": 3993
    },
    {
      "epoch": 5.375504710632571,
      "grad_norm": 0.9126486778259277,
      "learning_rate": 8.063454055671954e-05,
      "loss": 0.1661,
      "step": 3994
    },
    {
      "epoch": 5.376850605652759,
      "grad_norm": 0.9042807221412659,
      "learning_rate": 8.060460939838373e-05,
      "loss": 0.188,
      "step": 3995
    },
    {
      "epoch": 5.3781965006729475,
      "grad_norm": 0.9401246905326843,
      "learning_rate": 8.05746782400479e-05,
      "loss": 0.1876,
      "step": 3996
    },
    {
      "epoch": 5.379542395693136,
      "grad_norm": 1.0358150005340576,
      "learning_rate": 8.054474708171207e-05,
      "loss": 0.2326,
      "step": 3997
    },
    {
      "epoch": 5.380888290713324,
      "grad_norm": 0.9733290672302246,
      "learning_rate": 8.051481592337623e-05,
      "loss": 0.1849,
      "step": 3998
    },
    {
      "epoch": 5.3822341857335125,
      "grad_norm": 0.7776737809181213,
      "learning_rate": 8.048488476504042e-05,
      "loss": 0.1564,
      "step": 3999
    },
    {
      "epoch": 5.383580080753701,
      "grad_norm": 0.9498118162155151,
      "learning_rate": 8.045495360670459e-05,
      "loss": 0.2023,
      "step": 4000
    },
    {
      "epoch": 5.384925975773889,
      "grad_norm": 1.1095988750457764,
      "learning_rate": 8.042502244836876e-05,
      "loss": 0.27,
      "step": 4001
    },
    {
      "epoch": 5.386271870794078,
      "grad_norm": 0.8813433647155762,
      "learning_rate": 8.039509129003292e-05,
      "loss": 0.1636,
      "step": 4002
    },
    {
      "epoch": 5.387617765814267,
      "grad_norm": 1.2668309211730957,
      "learning_rate": 8.036516013169711e-05,
      "loss": 0.3037,
      "step": 4003
    },
    {
      "epoch": 5.388963660834455,
      "grad_norm": 1.156801700592041,
      "learning_rate": 8.033522897336128e-05,
      "loss": 0.2195,
      "step": 4004
    },
    {
      "epoch": 5.390309555854643,
      "grad_norm": 1.5465151071548462,
      "learning_rate": 8.030529781502545e-05,
      "loss": 0.298,
      "step": 4005
    },
    {
      "epoch": 5.391655450874832,
      "grad_norm": 1.045281171798706,
      "learning_rate": 8.027536665668962e-05,
      "loss": 0.2164,
      "step": 4006
    },
    {
      "epoch": 5.39300134589502,
      "grad_norm": 0.9333127737045288,
      "learning_rate": 8.02454354983538e-05,
      "loss": 0.1924,
      "step": 4007
    },
    {
      "epoch": 5.3943472409152085,
      "grad_norm": 0.6567441821098328,
      "learning_rate": 8.021550434001797e-05,
      "loss": 0.1589,
      "step": 4008
    },
    {
      "epoch": 5.395693135935397,
      "grad_norm": 0.9957659840583801,
      "learning_rate": 8.018557318168214e-05,
      "loss": 0.1992,
      "step": 4009
    },
    {
      "epoch": 5.397039030955585,
      "grad_norm": 1.2690626382827759,
      "learning_rate": 8.01556420233463e-05,
      "loss": 0.2524,
      "step": 4010
    },
    {
      "epoch": 5.398384925975774,
      "grad_norm": 0.8703166246414185,
      "learning_rate": 8.012571086501049e-05,
      "loss": 0.1824,
      "step": 4011
    },
    {
      "epoch": 5.399730820995963,
      "grad_norm": 0.9288082122802734,
      "learning_rate": 8.009577970667466e-05,
      "loss": 0.1848,
      "step": 4012
    },
    {
      "epoch": 5.401076716016151,
      "grad_norm": 1.0579862594604492,
      "learning_rate": 8.006584854833883e-05,
      "loss": 0.1909,
      "step": 4013
    },
    {
      "epoch": 5.402422611036339,
      "grad_norm": 1.0626287460327148,
      "learning_rate": 8.0035917390003e-05,
      "loss": 0.1688,
      "step": 4014
    },
    {
      "epoch": 5.403768506056528,
      "grad_norm": 1.1124964952468872,
      "learning_rate": 8.000598623166718e-05,
      "loss": 0.1906,
      "step": 4015
    },
    {
      "epoch": 5.405114401076716,
      "grad_norm": 1.0747525691986084,
      "learning_rate": 7.997605507333135e-05,
      "loss": 0.2285,
      "step": 4016
    },
    {
      "epoch": 5.406460296096904,
      "grad_norm": 1.2131778001785278,
      "learning_rate": 7.994612391499552e-05,
      "loss": 0.2588,
      "step": 4017
    },
    {
      "epoch": 5.407806191117093,
      "grad_norm": 1.0335253477096558,
      "learning_rate": 7.991619275665969e-05,
      "loss": 0.1889,
      "step": 4018
    },
    {
      "epoch": 5.409152086137281,
      "grad_norm": 1.1718766689300537,
      "learning_rate": 7.988626159832387e-05,
      "loss": 0.2579,
      "step": 4019
    },
    {
      "epoch": 5.410497981157469,
      "grad_norm": 0.8724368810653687,
      "learning_rate": 7.985633043998804e-05,
      "loss": 0.2317,
      "step": 4020
    },
    {
      "epoch": 5.411843876177658,
      "grad_norm": 0.7962177991867065,
      "learning_rate": 7.98263992816522e-05,
      "loss": 0.1798,
      "step": 4021
    },
    {
      "epoch": 5.413189771197847,
      "grad_norm": 1.482831597328186,
      "learning_rate": 7.979646812331638e-05,
      "loss": 0.264,
      "step": 4022
    },
    {
      "epoch": 5.414535666218035,
      "grad_norm": 1.2574445009231567,
      "learning_rate": 7.976653696498056e-05,
      "loss": 0.2363,
      "step": 4023
    },
    {
      "epoch": 5.415881561238224,
      "grad_norm": 1.1268413066864014,
      "learning_rate": 7.973660580664473e-05,
      "loss": 0.1958,
      "step": 4024
    },
    {
      "epoch": 5.417227456258412,
      "grad_norm": 0.8533938527107239,
      "learning_rate": 7.97066746483089e-05,
      "loss": 0.2609,
      "step": 4025
    },
    {
      "epoch": 5.4185733512786,
      "grad_norm": 0.9436639547348022,
      "learning_rate": 7.967674348997307e-05,
      "loss": 0.1694,
      "step": 4026
    },
    {
      "epoch": 5.419919246298789,
      "grad_norm": 1.0226874351501465,
      "learning_rate": 7.964681233163724e-05,
      "loss": 0.1981,
      "step": 4027
    },
    {
      "epoch": 5.421265141318977,
      "grad_norm": 0.9657869338989258,
      "learning_rate": 7.96168811733014e-05,
      "loss": 0.187,
      "step": 4028
    },
    {
      "epoch": 5.422611036339165,
      "grad_norm": 1.1136499643325806,
      "learning_rate": 7.958695001496557e-05,
      "loss": 0.1958,
      "step": 4029
    },
    {
      "epoch": 5.423956931359354,
      "grad_norm": 1.2511478662490845,
      "learning_rate": 7.955701885662976e-05,
      "loss": 0.3048,
      "step": 4030
    },
    {
      "epoch": 5.425302826379543,
      "grad_norm": 1.2407408952713013,
      "learning_rate": 7.952708769829393e-05,
      "loss": 0.2508,
      "step": 4031
    },
    {
      "epoch": 5.426648721399731,
      "grad_norm": 1.283726692199707,
      "learning_rate": 7.94971565399581e-05,
      "loss": 0.2306,
      "step": 4032
    },
    {
      "epoch": 5.4279946164199195,
      "grad_norm": 1.0620182752609253,
      "learning_rate": 7.946722538162226e-05,
      "loss": 0.2099,
      "step": 4033
    },
    {
      "epoch": 5.429340511440108,
      "grad_norm": 1.1142927408218384,
      "learning_rate": 7.943729422328645e-05,
      "loss": 0.2164,
      "step": 4034
    },
    {
      "epoch": 5.430686406460296,
      "grad_norm": 1.2715657949447632,
      "learning_rate": 7.940736306495062e-05,
      "loss": 0.2556,
      "step": 4035
    },
    {
      "epoch": 5.4320323014804845,
      "grad_norm": 1.4027501344680786,
      "learning_rate": 7.937743190661478e-05,
      "loss": 0.2289,
      "step": 4036
    },
    {
      "epoch": 5.433378196500673,
      "grad_norm": 1.2918277978897095,
      "learning_rate": 7.934750074827895e-05,
      "loss": 0.2808,
      "step": 4037
    },
    {
      "epoch": 5.434724091520861,
      "grad_norm": 1.0968436002731323,
      "learning_rate": 7.931756958994314e-05,
      "loss": 0.1965,
      "step": 4038
    },
    {
      "epoch": 5.43606998654105,
      "grad_norm": 1.3224537372589111,
      "learning_rate": 7.92876384316073e-05,
      "loss": 0.2263,
      "step": 4039
    },
    {
      "epoch": 5.437415881561238,
      "grad_norm": 0.9794450402259827,
      "learning_rate": 7.925770727327147e-05,
      "loss": 0.1799,
      "step": 4040
    },
    {
      "epoch": 5.438761776581426,
      "grad_norm": 0.901435136795044,
      "learning_rate": 7.922777611493564e-05,
      "loss": 0.2136,
      "step": 4041
    },
    {
      "epoch": 5.4401076716016155,
      "grad_norm": 1.030617356300354,
      "learning_rate": 7.919784495659983e-05,
      "loss": 0.1789,
      "step": 4042
    },
    {
      "epoch": 5.441453566621804,
      "grad_norm": 1.3615835905075073,
      "learning_rate": 7.9167913798264e-05,
      "loss": 0.277,
      "step": 4043
    },
    {
      "epoch": 5.442799461641992,
      "grad_norm": 1.1553454399108887,
      "learning_rate": 7.913798263992816e-05,
      "loss": 0.2264,
      "step": 4044
    },
    {
      "epoch": 5.4441453566621805,
      "grad_norm": 1.2573318481445312,
      "learning_rate": 7.910805148159233e-05,
      "loss": 0.3773,
      "step": 4045
    },
    {
      "epoch": 5.445491251682369,
      "grad_norm": 1.1328802108764648,
      "learning_rate": 7.907812032325652e-05,
      "loss": 0.2384,
      "step": 4046
    },
    {
      "epoch": 5.446837146702557,
      "grad_norm": 1.3088438510894775,
      "learning_rate": 7.904818916492069e-05,
      "loss": 0.2529,
      "step": 4047
    },
    {
      "epoch": 5.4481830417227455,
      "grad_norm": 1.0108672380447388,
      "learning_rate": 7.901825800658485e-05,
      "loss": 0.2365,
      "step": 4048
    },
    {
      "epoch": 5.449528936742934,
      "grad_norm": 1.0570985078811646,
      "learning_rate": 7.898832684824902e-05,
      "loss": 0.229,
      "step": 4049
    },
    {
      "epoch": 5.450874831763122,
      "grad_norm": 1.2906534671783447,
      "learning_rate": 7.89583956899132e-05,
      "loss": 0.2143,
      "step": 4050
    },
    {
      "epoch": 5.4522207267833105,
      "grad_norm": 0.985768735408783,
      "learning_rate": 7.892846453157738e-05,
      "loss": 0.2054,
      "step": 4051
    },
    {
      "epoch": 5.4535666218035,
      "grad_norm": 1.204594373703003,
      "learning_rate": 7.889853337324155e-05,
      "loss": 0.2571,
      "step": 4052
    },
    {
      "epoch": 5.454912516823688,
      "grad_norm": 1.0740702152252197,
      "learning_rate": 7.886860221490571e-05,
      "loss": 0.304,
      "step": 4053
    },
    {
      "epoch": 5.456258411843876,
      "grad_norm": 0.9079054594039917,
      "learning_rate": 7.88386710565699e-05,
      "loss": 0.1451,
      "step": 4054
    },
    {
      "epoch": 5.457604306864065,
      "grad_norm": 1.0331130027770996,
      "learning_rate": 7.880873989823407e-05,
      "loss": 0.187,
      "step": 4055
    },
    {
      "epoch": 5.458950201884253,
      "grad_norm": 0.9832082390785217,
      "learning_rate": 7.877880873989824e-05,
      "loss": 0.1816,
      "step": 4056
    },
    {
      "epoch": 5.460296096904441,
      "grad_norm": 1.1830732822418213,
      "learning_rate": 7.87488775815624e-05,
      "loss": 0.2769,
      "step": 4057
    },
    {
      "epoch": 5.46164199192463,
      "grad_norm": 1.2227674722671509,
      "learning_rate": 7.871894642322659e-05,
      "loss": 0.2448,
      "step": 4058
    },
    {
      "epoch": 5.462987886944818,
      "grad_norm": 0.8801345229148865,
      "learning_rate": 7.868901526489076e-05,
      "loss": 0.1976,
      "step": 4059
    },
    {
      "epoch": 5.464333781965006,
      "grad_norm": 1.1779727935791016,
      "learning_rate": 7.865908410655493e-05,
      "loss": 0.2014,
      "step": 4060
    },
    {
      "epoch": 5.465679676985195,
      "grad_norm": 1.0169793367385864,
      "learning_rate": 7.86291529482191e-05,
      "loss": 0.1771,
      "step": 4061
    },
    {
      "epoch": 5.467025572005384,
      "grad_norm": 0.823897123336792,
      "learning_rate": 7.859922178988328e-05,
      "loss": 0.163,
      "step": 4062
    },
    {
      "epoch": 5.468371467025572,
      "grad_norm": 0.9787765145301819,
      "learning_rate": 7.856929063154745e-05,
      "loss": 0.2089,
      "step": 4063
    },
    {
      "epoch": 5.469717362045761,
      "grad_norm": 1.4452548027038574,
      "learning_rate": 7.853935947321162e-05,
      "loss": 0.2738,
      "step": 4064
    },
    {
      "epoch": 5.471063257065949,
      "grad_norm": 1.0412566661834717,
      "learning_rate": 7.850942831487578e-05,
      "loss": 0.1825,
      "step": 4065
    },
    {
      "epoch": 5.472409152086137,
      "grad_norm": 1.20563805103302,
      "learning_rate": 7.847949715653997e-05,
      "loss": 0.2013,
      "step": 4066
    },
    {
      "epoch": 5.473755047106326,
      "grad_norm": 0.9849705696105957,
      "learning_rate": 7.844956599820414e-05,
      "loss": 0.203,
      "step": 4067
    },
    {
      "epoch": 5.475100942126514,
      "grad_norm": 1.071954369544983,
      "learning_rate": 7.84196348398683e-05,
      "loss": 0.2374,
      "step": 4068
    },
    {
      "epoch": 5.476446837146702,
      "grad_norm": 0.8540495038032532,
      "learning_rate": 7.838970368153247e-05,
      "loss": 0.1664,
      "step": 4069
    },
    {
      "epoch": 5.477792732166891,
      "grad_norm": 0.9020435810089111,
      "learning_rate": 7.835977252319664e-05,
      "loss": 0.1838,
      "step": 4070
    },
    {
      "epoch": 5.479138627187079,
      "grad_norm": 1.0487327575683594,
      "learning_rate": 7.832984136486083e-05,
      "loss": 0.1959,
      "step": 4071
    },
    {
      "epoch": 5.480484522207268,
      "grad_norm": 1.2656865119934082,
      "learning_rate": 7.8299910206525e-05,
      "loss": 0.2027,
      "step": 4072
    },
    {
      "epoch": 5.481830417227457,
      "grad_norm": 0.9585423469543457,
      "learning_rate": 7.826997904818916e-05,
      "loss": 0.196,
      "step": 4073
    },
    {
      "epoch": 5.483176312247645,
      "grad_norm": 1.313162088394165,
      "learning_rate": 7.824004788985333e-05,
      "loss": 0.2719,
      "step": 4074
    },
    {
      "epoch": 5.484522207267833,
      "grad_norm": 1.0811784267425537,
      "learning_rate": 7.821011673151752e-05,
      "loss": 0.2223,
      "step": 4075
    },
    {
      "epoch": 5.485868102288022,
      "grad_norm": 0.9758499264717102,
      "learning_rate": 7.818018557318169e-05,
      "loss": 0.2286,
      "step": 4076
    },
    {
      "epoch": 5.48721399730821,
      "grad_norm": 1.1456286907196045,
      "learning_rate": 7.815025441484586e-05,
      "loss": 0.2028,
      "step": 4077
    },
    {
      "epoch": 5.488559892328398,
      "grad_norm": 0.9542264938354492,
      "learning_rate": 7.812032325651002e-05,
      "loss": 0.1732,
      "step": 4078
    },
    {
      "epoch": 5.489905787348587,
      "grad_norm": 1.3318829536437988,
      "learning_rate": 7.809039209817421e-05,
      "loss": 0.2449,
      "step": 4079
    },
    {
      "epoch": 5.491251682368775,
      "grad_norm": 1.0762563943862915,
      "learning_rate": 7.806046093983838e-05,
      "loss": 0.1853,
      "step": 4080
    },
    {
      "epoch": 5.492597577388963,
      "grad_norm": 1.0480964183807373,
      "learning_rate": 7.803052978150255e-05,
      "loss": 0.1912,
      "step": 4081
    },
    {
      "epoch": 5.4939434724091525,
      "grad_norm": 1.0100548267364502,
      "learning_rate": 7.800059862316671e-05,
      "loss": 0.2178,
      "step": 4082
    },
    {
      "epoch": 5.495289367429341,
      "grad_norm": 1.093520998954773,
      "learning_rate": 7.79706674648309e-05,
      "loss": 0.2375,
      "step": 4083
    },
    {
      "epoch": 5.496635262449529,
      "grad_norm": 1.2010217905044556,
      "learning_rate": 7.794073630649507e-05,
      "loss": 0.1973,
      "step": 4084
    },
    {
      "epoch": 5.4979811574697175,
      "grad_norm": 1.0690864324569702,
      "learning_rate": 7.791080514815924e-05,
      "loss": 0.2419,
      "step": 4085
    },
    {
      "epoch": 5.499327052489906,
      "grad_norm": 1.4513025283813477,
      "learning_rate": 7.78808739898234e-05,
      "loss": 0.3088,
      "step": 4086
    },
    {
      "epoch": 5.500672947510094,
      "grad_norm": 1.0024425983428955,
      "learning_rate": 7.785094283148759e-05,
      "loss": 0.2026,
      "step": 4087
    },
    {
      "epoch": 5.5020188425302825,
      "grad_norm": 1.0035858154296875,
      "learning_rate": 7.782101167315176e-05,
      "loss": 0.1741,
      "step": 4088
    },
    {
      "epoch": 5.503364737550471,
      "grad_norm": 0.9935075044631958,
      "learning_rate": 7.779108051481593e-05,
      "loss": 0.1609,
      "step": 4089
    },
    {
      "epoch": 5.504710632570659,
      "grad_norm": 0.9338830709457397,
      "learning_rate": 7.77611493564801e-05,
      "loss": 0.1733,
      "step": 4090
    },
    {
      "epoch": 5.506056527590848,
      "grad_norm": 1.0112979412078857,
      "learning_rate": 7.773121819814428e-05,
      "loss": 0.2027,
      "step": 4091
    },
    {
      "epoch": 5.507402422611037,
      "grad_norm": 0.8441640734672546,
      "learning_rate": 7.770128703980845e-05,
      "loss": 0.2505,
      "step": 4092
    },
    {
      "epoch": 5.508748317631225,
      "grad_norm": 1.0952054262161255,
      "learning_rate": 7.767135588147262e-05,
      "loss": 0.5166,
      "step": 4093
    },
    {
      "epoch": 5.510094212651413,
      "grad_norm": 1.1220341920852661,
      "learning_rate": 7.764142472313678e-05,
      "loss": 0.1905,
      "step": 4094
    },
    {
      "epoch": 5.511440107671602,
      "grad_norm": 0.8827632665634155,
      "learning_rate": 7.761149356480097e-05,
      "loss": 0.1578,
      "step": 4095
    },
    {
      "epoch": 5.51278600269179,
      "grad_norm": 1.4266304969787598,
      "learning_rate": 7.758156240646514e-05,
      "loss": 0.2622,
      "step": 4096
    },
    {
      "epoch": 5.514131897711978,
      "grad_norm": 1.2556811571121216,
      "learning_rate": 7.75516312481293e-05,
      "loss": 0.2119,
      "step": 4097
    },
    {
      "epoch": 5.515477792732167,
      "grad_norm": 1.03278386592865,
      "learning_rate": 7.752170008979347e-05,
      "loss": 0.2017,
      "step": 4098
    },
    {
      "epoch": 5.516823687752355,
      "grad_norm": 1.2789207696914673,
      "learning_rate": 7.749176893145766e-05,
      "loss": 0.2082,
      "step": 4099
    },
    {
      "epoch": 5.518169582772543,
      "grad_norm": 1.3401105403900146,
      "learning_rate": 7.746183777312183e-05,
      "loss": 0.255,
      "step": 4100
    },
    {
      "epoch": 5.519515477792732,
      "grad_norm": 1.286623239517212,
      "learning_rate": 7.7431906614786e-05,
      "loss": 0.2477,
      "step": 4101
    },
    {
      "epoch": 5.52086137281292,
      "grad_norm": 1.117810606956482,
      "learning_rate": 7.740197545645017e-05,
      "loss": 0.232,
      "step": 4102
    },
    {
      "epoch": 5.522207267833109,
      "grad_norm": 1.1811435222625732,
      "learning_rate": 7.737204429811435e-05,
      "loss": 0.2857,
      "step": 4103
    },
    {
      "epoch": 5.523553162853298,
      "grad_norm": 1.551769495010376,
      "learning_rate": 7.734211313977852e-05,
      "loss": 0.2827,
      "step": 4104
    },
    {
      "epoch": 5.524899057873486,
      "grad_norm": 1.1850987672805786,
      "learning_rate": 7.731218198144269e-05,
      "loss": 0.3167,
      "step": 4105
    },
    {
      "epoch": 5.526244952893674,
      "grad_norm": 1.0037561655044556,
      "learning_rate": 7.728225082310686e-05,
      "loss": 0.1786,
      "step": 4106
    },
    {
      "epoch": 5.527590847913863,
      "grad_norm": 1.1046375036239624,
      "learning_rate": 7.725231966477104e-05,
      "loss": 0.187,
      "step": 4107
    },
    {
      "epoch": 5.528936742934051,
      "grad_norm": 1.3727214336395264,
      "learning_rate": 7.722238850643521e-05,
      "loss": 0.2357,
      "step": 4108
    },
    {
      "epoch": 5.530282637954239,
      "grad_norm": 1.0449926853179932,
      "learning_rate": 7.719245734809938e-05,
      "loss": 0.3237,
      "step": 4109
    },
    {
      "epoch": 5.531628532974428,
      "grad_norm": 1.10434889793396,
      "learning_rate": 7.716252618976355e-05,
      "loss": 0.2426,
      "step": 4110
    },
    {
      "epoch": 5.532974427994617,
      "grad_norm": 1.1209843158721924,
      "learning_rate": 7.713259503142773e-05,
      "loss": 0.2308,
      "step": 4111
    },
    {
      "epoch": 5.534320323014805,
      "grad_norm": 1.206751823425293,
      "learning_rate": 7.71026638730919e-05,
      "loss": 0.2269,
      "step": 4112
    },
    {
      "epoch": 5.535666218034994,
      "grad_norm": 1.1564487218856812,
      "learning_rate": 7.707273271475607e-05,
      "loss": 0.2348,
      "step": 4113
    },
    {
      "epoch": 5.537012113055182,
      "grad_norm": 1.1330832242965698,
      "learning_rate": 7.704280155642024e-05,
      "loss": 0.2199,
      "step": 4114
    },
    {
      "epoch": 5.53835800807537,
      "grad_norm": 1.0131756067276,
      "learning_rate": 7.701287039808442e-05,
      "loss": 0.2137,
      "step": 4115
    },
    {
      "epoch": 5.539703903095559,
      "grad_norm": 1.0920369625091553,
      "learning_rate": 7.698293923974859e-05,
      "loss": 0.1812,
      "step": 4116
    },
    {
      "epoch": 5.541049798115747,
      "grad_norm": 0.9986361861228943,
      "learning_rate": 7.695300808141276e-05,
      "loss": 0.1841,
      "step": 4117
    },
    {
      "epoch": 5.542395693135935,
      "grad_norm": 1.0423558950424194,
      "learning_rate": 7.692307692307693e-05,
      "loss": 0.241,
      "step": 4118
    },
    {
      "epoch": 5.543741588156124,
      "grad_norm": 1.155505657196045,
      "learning_rate": 7.689314576474111e-05,
      "loss": 0.226,
      "step": 4119
    },
    {
      "epoch": 5.545087483176312,
      "grad_norm": 0.9619414210319519,
      "learning_rate": 7.686321460640528e-05,
      "loss": 0.1903,
      "step": 4120
    },
    {
      "epoch": 5.5464333781965,
      "grad_norm": 1.400488257408142,
      "learning_rate": 7.683328344806945e-05,
      "loss": 0.2631,
      "step": 4121
    },
    {
      "epoch": 5.547779273216689,
      "grad_norm": 1.276301383972168,
      "learning_rate": 7.680335228973362e-05,
      "loss": 0.2614,
      "step": 4122
    },
    {
      "epoch": 5.549125168236878,
      "grad_norm": 1.034141182899475,
      "learning_rate": 7.67734211313978e-05,
      "loss": 0.2213,
      "step": 4123
    },
    {
      "epoch": 5.550471063257066,
      "grad_norm": 1.1647348403930664,
      "learning_rate": 7.674348997306197e-05,
      "loss": 0.2274,
      "step": 4124
    },
    {
      "epoch": 5.5518169582772545,
      "grad_norm": 1.1856521368026733,
      "learning_rate": 7.671355881472614e-05,
      "loss": 0.3452,
      "step": 4125
    },
    {
      "epoch": 5.553162853297443,
      "grad_norm": 1.1276897192001343,
      "learning_rate": 7.66836276563903e-05,
      "loss": 0.2281,
      "step": 4126
    },
    {
      "epoch": 5.554508748317631,
      "grad_norm": 0.8840770721435547,
      "learning_rate": 7.665369649805449e-05,
      "loss": 0.1701,
      "step": 4127
    },
    {
      "epoch": 5.5558546433378195,
      "grad_norm": 1.0222762823104858,
      "learning_rate": 7.662376533971866e-05,
      "loss": 0.2089,
      "step": 4128
    },
    {
      "epoch": 5.557200538358008,
      "grad_norm": 1.0065001249313354,
      "learning_rate": 7.659383418138283e-05,
      "loss": 0.1889,
      "step": 4129
    },
    {
      "epoch": 5.558546433378196,
      "grad_norm": 1.3755193948745728,
      "learning_rate": 7.6563903023047e-05,
      "loss": 0.2306,
      "step": 4130
    },
    {
      "epoch": 5.5598923283983845,
      "grad_norm": 1.0531103610992432,
      "learning_rate": 7.653397186471117e-05,
      "loss": 0.2076,
      "step": 4131
    },
    {
      "epoch": 5.561238223418574,
      "grad_norm": 0.8512094020843506,
      "learning_rate": 7.650404070637533e-05,
      "loss": 0.2425,
      "step": 4132
    },
    {
      "epoch": 5.562584118438762,
      "grad_norm": 1.1792702674865723,
      "learning_rate": 7.64741095480395e-05,
      "loss": 0.1967,
      "step": 4133
    },
    {
      "epoch": 5.56393001345895,
      "grad_norm": 1.2744098901748657,
      "learning_rate": 7.644417838970369e-05,
      "loss": 0.2072,
      "step": 4134
    },
    {
      "epoch": 5.565275908479139,
      "grad_norm": 1.196474552154541,
      "learning_rate": 7.641424723136786e-05,
      "loss": 0.215,
      "step": 4135
    },
    {
      "epoch": 5.566621803499327,
      "grad_norm": 1.1211538314819336,
      "learning_rate": 7.638431607303202e-05,
      "loss": 0.2383,
      "step": 4136
    },
    {
      "epoch": 5.5679676985195155,
      "grad_norm": 0.9589577913284302,
      "learning_rate": 7.63543849146962e-05,
      "loss": 0.1935,
      "step": 4137
    },
    {
      "epoch": 5.569313593539704,
      "grad_norm": 0.8913475275039673,
      "learning_rate": 7.632445375636038e-05,
      "loss": 0.1855,
      "step": 4138
    },
    {
      "epoch": 5.570659488559892,
      "grad_norm": 1.1250783205032349,
      "learning_rate": 7.629452259802455e-05,
      "loss": 0.2146,
      "step": 4139
    },
    {
      "epoch": 5.5720053835800805,
      "grad_norm": 1.3137335777282715,
      "learning_rate": 7.626459143968871e-05,
      "loss": 0.2822,
      "step": 4140
    },
    {
      "epoch": 5.573351278600269,
      "grad_norm": 0.9637765884399414,
      "learning_rate": 7.623466028135288e-05,
      "loss": 0.1756,
      "step": 4141
    },
    {
      "epoch": 5.574697173620457,
      "grad_norm": 1.1667267084121704,
      "learning_rate": 7.620472912301707e-05,
      "loss": 0.2389,
      "step": 4142
    },
    {
      "epoch": 5.576043068640646,
      "grad_norm": 1.2947160005569458,
      "learning_rate": 7.617479796468124e-05,
      "loss": 0.2071,
      "step": 4143
    },
    {
      "epoch": 5.577388963660835,
      "grad_norm": 1.2236307859420776,
      "learning_rate": 7.61448668063454e-05,
      "loss": 0.2453,
      "step": 4144
    },
    {
      "epoch": 5.578734858681023,
      "grad_norm": 1.0233919620513916,
      "learning_rate": 7.611493564800957e-05,
      "loss": 0.1955,
      "step": 4145
    },
    {
      "epoch": 5.580080753701211,
      "grad_norm": 1.221099615097046,
      "learning_rate": 7.608500448967376e-05,
      "loss": 0.2143,
      "step": 4146
    },
    {
      "epoch": 5.5814266487214,
      "grad_norm": 1.089084267616272,
      "learning_rate": 7.605507333133793e-05,
      "loss": 0.2292,
      "step": 4147
    },
    {
      "epoch": 5.582772543741588,
      "grad_norm": 1.0516761541366577,
      "learning_rate": 7.60251421730021e-05,
      "loss": 0.202,
      "step": 4148
    },
    {
      "epoch": 5.584118438761776,
      "grad_norm": 1.1691114902496338,
      "learning_rate": 7.599521101466626e-05,
      "loss": 0.2418,
      "step": 4149
    },
    {
      "epoch": 5.585464333781965,
      "grad_norm": 1.7140495777130127,
      "learning_rate": 7.596527985633045e-05,
      "loss": 0.244,
      "step": 4150
    },
    {
      "epoch": 5.586810228802153,
      "grad_norm": 1.1721824407577515,
      "learning_rate": 7.593534869799462e-05,
      "loss": 0.2267,
      "step": 4151
    },
    {
      "epoch": 5.588156123822342,
      "grad_norm": 1.0658893585205078,
      "learning_rate": 7.590541753965879e-05,
      "loss": 0.2158,
      "step": 4152
    },
    {
      "epoch": 5.589502018842531,
      "grad_norm": 0.8211358785629272,
      "learning_rate": 7.587548638132295e-05,
      "loss": 0.1604,
      "step": 4153
    },
    {
      "epoch": 5.590847913862719,
      "grad_norm": 0.7978762984275818,
      "learning_rate": 7.584555522298712e-05,
      "loss": 0.1947,
      "step": 4154
    },
    {
      "epoch": 5.592193808882907,
      "grad_norm": 1.002860188484192,
      "learning_rate": 7.58156240646513e-05,
      "loss": 0.2151,
      "step": 4155
    },
    {
      "epoch": 5.593539703903096,
      "grad_norm": 1.5054880380630493,
      "learning_rate": 7.578569290631548e-05,
      "loss": 0.3244,
      "step": 4156
    },
    {
      "epoch": 5.594885598923284,
      "grad_norm": 1.1496957540512085,
      "learning_rate": 7.575576174797964e-05,
      "loss": 0.2623,
      "step": 4157
    },
    {
      "epoch": 5.596231493943472,
      "grad_norm": 1.1519101858139038,
      "learning_rate": 7.572583058964381e-05,
      "loss": 0.2749,
      "step": 4158
    },
    {
      "epoch": 5.597577388963661,
      "grad_norm": 0.9590771198272705,
      "learning_rate": 7.5695899431308e-05,
      "loss": 0.1964,
      "step": 4159
    },
    {
      "epoch": 5.598923283983849,
      "grad_norm": 1.287279486656189,
      "learning_rate": 7.566596827297217e-05,
      "loss": 0.2473,
      "step": 4160
    },
    {
      "epoch": 5.600269179004037,
      "grad_norm": 1.036779522895813,
      "learning_rate": 7.563603711463633e-05,
      "loss": 0.2129,
      "step": 4161
    },
    {
      "epoch": 5.601615074024226,
      "grad_norm": 1.1525551080703735,
      "learning_rate": 7.56061059563005e-05,
      "loss": 0.2858,
      "step": 4162
    },
    {
      "epoch": 5.602960969044415,
      "grad_norm": 1.2750130891799927,
      "learning_rate": 7.557617479796469e-05,
      "loss": 0.2099,
      "step": 4163
    },
    {
      "epoch": 5.604306864064603,
      "grad_norm": 1.0138081312179565,
      "learning_rate": 7.554624363962886e-05,
      "loss": 0.2198,
      "step": 4164
    },
    {
      "epoch": 5.6056527590847915,
      "grad_norm": 1.1668506860733032,
      "learning_rate": 7.551631248129302e-05,
      "loss": 0.2292,
      "step": 4165
    },
    {
      "epoch": 5.60699865410498,
      "grad_norm": 1.4290159940719604,
      "learning_rate": 7.54863813229572e-05,
      "loss": 0.297,
      "step": 4166
    },
    {
      "epoch": 5.608344549125168,
      "grad_norm": 1.290606141090393,
      "learning_rate": 7.545645016462138e-05,
      "loss": 0.3155,
      "step": 4167
    },
    {
      "epoch": 5.609690444145357,
      "grad_norm": 1.1621724367141724,
      "learning_rate": 7.542651900628555e-05,
      "loss": 0.2079,
      "step": 4168
    },
    {
      "epoch": 5.611036339165545,
      "grad_norm": 1.2688062191009521,
      "learning_rate": 7.539658784794971e-05,
      "loss": 0.2206,
      "step": 4169
    },
    {
      "epoch": 5.612382234185733,
      "grad_norm": 1.4024709463119507,
      "learning_rate": 7.536665668961388e-05,
      "loss": 0.3257,
      "step": 4170
    },
    {
      "epoch": 5.613728129205922,
      "grad_norm": 0.847643256187439,
      "learning_rate": 7.533672553127807e-05,
      "loss": 0.1971,
      "step": 4171
    },
    {
      "epoch": 5.615074024226111,
      "grad_norm": 0.9080542922019958,
      "learning_rate": 7.530679437294224e-05,
      "loss": 0.1932,
      "step": 4172
    },
    {
      "epoch": 5.616419919246299,
      "grad_norm": 1.5559169054031372,
      "learning_rate": 7.52768632146064e-05,
      "loss": 0.2692,
      "step": 4173
    },
    {
      "epoch": 5.6177658142664875,
      "grad_norm": 1.4632970094680786,
      "learning_rate": 7.524693205627057e-05,
      "loss": 0.2971,
      "step": 4174
    },
    {
      "epoch": 5.619111709286676,
      "grad_norm": 1.0872946977615356,
      "learning_rate": 7.521700089793476e-05,
      "loss": 0.2083,
      "step": 4175
    },
    {
      "epoch": 5.620457604306864,
      "grad_norm": 0.9259426593780518,
      "learning_rate": 7.518706973959893e-05,
      "loss": 0.2108,
      "step": 4176
    },
    {
      "epoch": 5.6218034993270525,
      "grad_norm": 0.9958670139312744,
      "learning_rate": 7.51571385812631e-05,
      "loss": 0.1833,
      "step": 4177
    },
    {
      "epoch": 5.623149394347241,
      "grad_norm": 1.468787431716919,
      "learning_rate": 7.512720742292726e-05,
      "loss": 0.213,
      "step": 4178
    },
    {
      "epoch": 5.624495289367429,
      "grad_norm": 1.043878436088562,
      "learning_rate": 7.509727626459145e-05,
      "loss": 0.185,
      "step": 4179
    },
    {
      "epoch": 5.6258411843876175,
      "grad_norm": 1.3224273920059204,
      "learning_rate": 7.506734510625562e-05,
      "loss": 0.2792,
      "step": 4180
    },
    {
      "epoch": 5.627187079407806,
      "grad_norm": 1.2637914419174194,
      "learning_rate": 7.503741394791979e-05,
      "loss": 0.2257,
      "step": 4181
    },
    {
      "epoch": 5.628532974427994,
      "grad_norm": 1.0546990633010864,
      "learning_rate": 7.500748278958395e-05,
      "loss": 0.3181,
      "step": 4182
    },
    {
      "epoch": 5.629878869448183,
      "grad_norm": 0.9562848210334778,
      "learning_rate": 7.497755163124814e-05,
      "loss": 0.2083,
      "step": 4183
    },
    {
      "epoch": 5.631224764468372,
      "grad_norm": 0.9765194058418274,
      "learning_rate": 7.49476204729123e-05,
      "loss": 0.1987,
      "step": 4184
    },
    {
      "epoch": 5.63257065948856,
      "grad_norm": 0.9744883179664612,
      "learning_rate": 7.491768931457648e-05,
      "loss": 0.206,
      "step": 4185
    },
    {
      "epoch": 5.633916554508748,
      "grad_norm": 0.9235139489173889,
      "learning_rate": 7.488775815624064e-05,
      "loss": 0.1845,
      "step": 4186
    },
    {
      "epoch": 5.635262449528937,
      "grad_norm": 1.0769859552383423,
      "learning_rate": 7.485782699790483e-05,
      "loss": 0.2137,
      "step": 4187
    },
    {
      "epoch": 5.636608344549125,
      "grad_norm": 0.9754256010055542,
      "learning_rate": 7.4827895839569e-05,
      "loss": 0.1966,
      "step": 4188
    },
    {
      "epoch": 5.637954239569313,
      "grad_norm": 0.9020510315895081,
      "learning_rate": 7.479796468123317e-05,
      "loss": 0.1739,
      "step": 4189
    },
    {
      "epoch": 5.639300134589502,
      "grad_norm": 1.628953218460083,
      "learning_rate": 7.476803352289733e-05,
      "loss": 0.2729,
      "step": 4190
    },
    {
      "epoch": 5.64064602960969,
      "grad_norm": 1.3685271739959717,
      "learning_rate": 7.473810236456152e-05,
      "loss": 0.279,
      "step": 4191
    },
    {
      "epoch": 5.641991924629879,
      "grad_norm": 0.8082294464111328,
      "learning_rate": 7.470817120622569e-05,
      "loss": 0.1619,
      "step": 4192
    },
    {
      "epoch": 5.643337819650068,
      "grad_norm": 1.0912489891052246,
      "learning_rate": 7.467824004788986e-05,
      "loss": 0.2005,
      "step": 4193
    },
    {
      "epoch": 5.644683714670256,
      "grad_norm": 1.5388436317443848,
      "learning_rate": 7.464830888955402e-05,
      "loss": 0.2776,
      "step": 4194
    },
    {
      "epoch": 5.646029609690444,
      "grad_norm": 1.1603834629058838,
      "learning_rate": 7.461837773121821e-05,
      "loss": 0.2027,
      "step": 4195
    },
    {
      "epoch": 5.647375504710633,
      "grad_norm": 1.4513040781021118,
      "learning_rate": 7.458844657288238e-05,
      "loss": 0.2749,
      "step": 4196
    },
    {
      "epoch": 5.648721399730821,
      "grad_norm": 1.3789935111999512,
      "learning_rate": 7.455851541454655e-05,
      "loss": 0.2715,
      "step": 4197
    },
    {
      "epoch": 5.650067294751009,
      "grad_norm": 0.8729038834571838,
      "learning_rate": 7.452858425621072e-05,
      "loss": 0.1603,
      "step": 4198
    },
    {
      "epoch": 5.651413189771198,
      "grad_norm": 1.6346707344055176,
      "learning_rate": 7.44986530978749e-05,
      "loss": 0.2997,
      "step": 4199
    },
    {
      "epoch": 5.652759084791386,
      "grad_norm": 1.4291270971298218,
      "learning_rate": 7.446872193953907e-05,
      "loss": 0.268,
      "step": 4200
    },
    {
      "epoch": 5.654104979811574,
      "grad_norm": 1.0814450979232788,
      "learning_rate": 7.443879078120324e-05,
      "loss": 0.2472,
      "step": 4201
    },
    {
      "epoch": 5.655450874831763,
      "grad_norm": 1.5948797464370728,
      "learning_rate": 7.44088596228674e-05,
      "loss": 0.2418,
      "step": 4202
    },
    {
      "epoch": 5.656796769851952,
      "grad_norm": 1.1195390224456787,
      "learning_rate": 7.437892846453159e-05,
      "loss": 0.2332,
      "step": 4203
    },
    {
      "epoch": 5.65814266487214,
      "grad_norm": 1.655995488166809,
      "learning_rate": 7.434899730619576e-05,
      "loss": 0.2852,
      "step": 4204
    },
    {
      "epoch": 5.659488559892329,
      "grad_norm": 1.2448862791061401,
      "learning_rate": 7.431906614785993e-05,
      "loss": 0.2143,
      "step": 4205
    },
    {
      "epoch": 5.660834454912517,
      "grad_norm": 1.2369567155838013,
      "learning_rate": 7.42891349895241e-05,
      "loss": 0.216,
      "step": 4206
    },
    {
      "epoch": 5.662180349932705,
      "grad_norm": 1.4502960443496704,
      "learning_rate": 7.425920383118828e-05,
      "loss": 0.2629,
      "step": 4207
    },
    {
      "epoch": 5.663526244952894,
      "grad_norm": 1.3279438018798828,
      "learning_rate": 7.422927267285245e-05,
      "loss": 0.1916,
      "step": 4208
    },
    {
      "epoch": 5.664872139973082,
      "grad_norm": 1.0504599809646606,
      "learning_rate": 7.419934151451662e-05,
      "loss": 0.1978,
      "step": 4209
    },
    {
      "epoch": 5.66621803499327,
      "grad_norm": 1.055625319480896,
      "learning_rate": 7.416941035618079e-05,
      "loss": 0.1871,
      "step": 4210
    },
    {
      "epoch": 5.667563930013459,
      "grad_norm": 1.3283655643463135,
      "learning_rate": 7.413947919784497e-05,
      "loss": 0.245,
      "step": 4211
    },
    {
      "epoch": 5.668909825033648,
      "grad_norm": 0.9109307527542114,
      "learning_rate": 7.410954803950914e-05,
      "loss": 0.3362,
      "step": 4212
    },
    {
      "epoch": 5.670255720053836,
      "grad_norm": 1.0967358350753784,
      "learning_rate": 7.40796168811733e-05,
      "loss": 0.1824,
      "step": 4213
    },
    {
      "epoch": 5.6716016150740245,
      "grad_norm": 1.0447158813476562,
      "learning_rate": 7.404968572283748e-05,
      "loss": 0.1889,
      "step": 4214
    },
    {
      "epoch": 5.672947510094213,
      "grad_norm": 1.1113228797912598,
      "learning_rate": 7.401975456450166e-05,
      "loss": 0.1908,
      "step": 4215
    },
    {
      "epoch": 5.674293405114401,
      "grad_norm": 0.8447422385215759,
      "learning_rate": 7.398982340616583e-05,
      "loss": 0.1822,
      "step": 4216
    },
    {
      "epoch": 5.6756393001345895,
      "grad_norm": 0.8514620661735535,
      "learning_rate": 7.395989224783e-05,
      "loss": 0.1982,
      "step": 4217
    },
    {
      "epoch": 5.676985195154778,
      "grad_norm": 1.247456431388855,
      "learning_rate": 7.392996108949417e-05,
      "loss": 0.2285,
      "step": 4218
    },
    {
      "epoch": 5.678331090174966,
      "grad_norm": 0.9483583569526672,
      "learning_rate": 7.390002993115835e-05,
      "loss": 0.2308,
      "step": 4219
    },
    {
      "epoch": 5.6796769851951545,
      "grad_norm": 0.97907954454422,
      "learning_rate": 7.387009877282252e-05,
      "loss": 0.201,
      "step": 4220
    },
    {
      "epoch": 5.681022880215343,
      "grad_norm": 0.872755765914917,
      "learning_rate": 7.384016761448669e-05,
      "loss": 0.1868,
      "step": 4221
    },
    {
      "epoch": 5.682368775235531,
      "grad_norm": 1.102216124534607,
      "learning_rate": 7.381023645615086e-05,
      "loss": 0.2804,
      "step": 4222
    },
    {
      "epoch": 5.68371467025572,
      "grad_norm": 1.3783965110778809,
      "learning_rate": 7.378030529781504e-05,
      "loss": 0.2526,
      "step": 4223
    },
    {
      "epoch": 5.685060565275909,
      "grad_norm": 1.1840585470199585,
      "learning_rate": 7.375037413947921e-05,
      "loss": 0.2044,
      "step": 4224
    },
    {
      "epoch": 5.686406460296097,
      "grad_norm": 1.0755081176757812,
      "learning_rate": 7.372044298114338e-05,
      "loss": 0.2223,
      "step": 4225
    },
    {
      "epoch": 5.687752355316285,
      "grad_norm": 1.4224096536636353,
      "learning_rate": 7.369051182280755e-05,
      "loss": 0.2668,
      "step": 4226
    },
    {
      "epoch": 5.689098250336474,
      "grad_norm": 1.2063757181167603,
      "learning_rate": 7.366058066447173e-05,
      "loss": 0.241,
      "step": 4227
    },
    {
      "epoch": 5.690444145356662,
      "grad_norm": 1.1570380926132202,
      "learning_rate": 7.36306495061359e-05,
      "loss": 0.2352,
      "step": 4228
    },
    {
      "epoch": 5.69179004037685,
      "grad_norm": 1.0580095052719116,
      "learning_rate": 7.360071834780007e-05,
      "loss": 0.2195,
      "step": 4229
    },
    {
      "epoch": 5.693135935397039,
      "grad_norm": 0.963086724281311,
      "learning_rate": 7.357078718946424e-05,
      "loss": 0.1898,
      "step": 4230
    },
    {
      "epoch": 5.694481830417227,
      "grad_norm": 1.1307258605957031,
      "learning_rate": 7.354085603112842e-05,
      "loss": 0.2733,
      "step": 4231
    },
    {
      "epoch": 5.695827725437416,
      "grad_norm": 1.0137702226638794,
      "learning_rate": 7.351092487279259e-05,
      "loss": 0.2212,
      "step": 4232
    },
    {
      "epoch": 5.697173620457605,
      "grad_norm": 1.0899966955184937,
      "learning_rate": 7.348099371445676e-05,
      "loss": 0.2263,
      "step": 4233
    },
    {
      "epoch": 5.698519515477793,
      "grad_norm": 0.8246158361434937,
      "learning_rate": 7.345106255612093e-05,
      "loss": 0.1656,
      "step": 4234
    },
    {
      "epoch": 5.699865410497981,
      "grad_norm": 1.1317481994628906,
      "learning_rate": 7.34211313977851e-05,
      "loss": 0.1916,
      "step": 4235
    },
    {
      "epoch": 5.70121130551817,
      "grad_norm": 1.1935466527938843,
      "learning_rate": 7.339120023944926e-05,
      "loss": 0.2212,
      "step": 4236
    },
    {
      "epoch": 5.702557200538358,
      "grad_norm": 1.0875403881072998,
      "learning_rate": 7.336126908111343e-05,
      "loss": 0.2615,
      "step": 4237
    },
    {
      "epoch": 5.703903095558546,
      "grad_norm": 1.234135389328003,
      "learning_rate": 7.333133792277762e-05,
      "loss": 0.2016,
      "step": 4238
    },
    {
      "epoch": 5.705248990578735,
      "grad_norm": 1.030755877494812,
      "learning_rate": 7.330140676444179e-05,
      "loss": 0.2085,
      "step": 4239
    },
    {
      "epoch": 5.706594885598923,
      "grad_norm": 1.1582121849060059,
      "learning_rate": 7.327147560610595e-05,
      "loss": 0.2724,
      "step": 4240
    },
    {
      "epoch": 5.707940780619111,
      "grad_norm": 0.8756378889083862,
      "learning_rate": 7.324154444777012e-05,
      "loss": 0.1876,
      "step": 4241
    },
    {
      "epoch": 5.7092866756393,
      "grad_norm": 0.8589973449707031,
      "learning_rate": 7.321161328943429e-05,
      "loss": 0.1923,
      "step": 4242
    },
    {
      "epoch": 5.710632570659489,
      "grad_norm": 1.1012415885925293,
      "learning_rate": 7.318168213109848e-05,
      "loss": 0.2282,
      "step": 4243
    },
    {
      "epoch": 5.711978465679677,
      "grad_norm": 1.0041571855545044,
      "learning_rate": 7.315175097276265e-05,
      "loss": 0.1804,
      "step": 4244
    },
    {
      "epoch": 5.713324360699866,
      "grad_norm": 1.2801133394241333,
      "learning_rate": 7.312181981442681e-05,
      "loss": 0.2213,
      "step": 4245
    },
    {
      "epoch": 5.714670255720054,
      "grad_norm": 1.0456947088241577,
      "learning_rate": 7.309188865609098e-05,
      "loss": 0.2137,
      "step": 4246
    },
    {
      "epoch": 5.716016150740242,
      "grad_norm": 1.098741888999939,
      "learning_rate": 7.306195749775517e-05,
      "loss": 0.2352,
      "step": 4247
    },
    {
      "epoch": 5.717362045760431,
      "grad_norm": 0.9835174679756165,
      "learning_rate": 7.303202633941934e-05,
      "loss": 0.1972,
      "step": 4248
    },
    {
      "epoch": 5.718707940780619,
      "grad_norm": 1.0409948825836182,
      "learning_rate": 7.30020951810835e-05,
      "loss": 0.2122,
      "step": 4249
    },
    {
      "epoch": 5.720053835800807,
      "grad_norm": 1.1248908042907715,
      "learning_rate": 7.297216402274767e-05,
      "loss": 0.2044,
      "step": 4250
    },
    {
      "epoch": 5.721399730820996,
      "grad_norm": 1.1021342277526855,
      "learning_rate": 7.294223286441186e-05,
      "loss": 0.2438,
      "step": 4251
    },
    {
      "epoch": 5.722745625841185,
      "grad_norm": 1.1546608209609985,
      "learning_rate": 7.291230170607603e-05,
      "loss": 0.195,
      "step": 4252
    },
    {
      "epoch": 5.724091520861373,
      "grad_norm": 0.9061903953552246,
      "learning_rate": 7.28823705477402e-05,
      "loss": 0.1825,
      "step": 4253
    },
    {
      "epoch": 5.7254374158815615,
      "grad_norm": 1.1384650468826294,
      "learning_rate": 7.285243938940436e-05,
      "loss": 0.2177,
      "step": 4254
    },
    {
      "epoch": 5.72678331090175,
      "grad_norm": 1.1875641345977783,
      "learning_rate": 7.282250823106855e-05,
      "loss": 0.2684,
      "step": 4255
    },
    {
      "epoch": 5.728129205921938,
      "grad_norm": 1.190933346748352,
      "learning_rate": 7.279257707273272e-05,
      "loss": 0.3389,
      "step": 4256
    },
    {
      "epoch": 5.7294751009421265,
      "grad_norm": 1.1853684186935425,
      "learning_rate": 7.276264591439688e-05,
      "loss": 0.2132,
      "step": 4257
    },
    {
      "epoch": 5.730820995962315,
      "grad_norm": 1.3105875253677368,
      "learning_rate": 7.273271475606105e-05,
      "loss": 0.2444,
      "step": 4258
    },
    {
      "epoch": 5.732166890982503,
      "grad_norm": 1.1863508224487305,
      "learning_rate": 7.270278359772524e-05,
      "loss": 0.1889,
      "step": 4259
    },
    {
      "epoch": 5.7335127860026915,
      "grad_norm": 1.1795179843902588,
      "learning_rate": 7.26728524393894e-05,
      "loss": 0.233,
      "step": 4260
    },
    {
      "epoch": 5.73485868102288,
      "grad_norm": 1.1002922058105469,
      "learning_rate": 7.264292128105357e-05,
      "loss": 0.2265,
      "step": 4261
    },
    {
      "epoch": 5.736204576043068,
      "grad_norm": 1.292248010635376,
      "learning_rate": 7.261299012271774e-05,
      "loss": 0.3034,
      "step": 4262
    },
    {
      "epoch": 5.737550471063257,
      "grad_norm": 1.1839476823806763,
      "learning_rate": 7.258305896438193e-05,
      "loss": 0.2344,
      "step": 4263
    },
    {
      "epoch": 5.738896366083446,
      "grad_norm": 0.9933472871780396,
      "learning_rate": 7.25531278060461e-05,
      "loss": 0.1997,
      "step": 4264
    },
    {
      "epoch": 5.740242261103634,
      "grad_norm": 0.9648587703704834,
      "learning_rate": 7.252319664771026e-05,
      "loss": 0.1816,
      "step": 4265
    },
    {
      "epoch": 5.7415881561238225,
      "grad_norm": 0.813449501991272,
      "learning_rate": 7.249326548937443e-05,
      "loss": 0.1794,
      "step": 4266
    },
    {
      "epoch": 5.742934051144011,
      "grad_norm": 1.0870872735977173,
      "learning_rate": 7.246333433103862e-05,
      "loss": 0.2524,
      "step": 4267
    },
    {
      "epoch": 5.744279946164199,
      "grad_norm": 1.0900709629058838,
      "learning_rate": 7.243340317270279e-05,
      "loss": 0.1954,
      "step": 4268
    },
    {
      "epoch": 5.7456258411843875,
      "grad_norm": 1.137424111366272,
      "learning_rate": 7.240347201436696e-05,
      "loss": 0.1917,
      "step": 4269
    },
    {
      "epoch": 5.746971736204576,
      "grad_norm": 1.4064414501190186,
      "learning_rate": 7.237354085603112e-05,
      "loss": 0.2224,
      "step": 4270
    },
    {
      "epoch": 5.748317631224764,
      "grad_norm": 1.089621901512146,
      "learning_rate": 7.234360969769531e-05,
      "loss": 0.2109,
      "step": 4271
    },
    {
      "epoch": 5.749663526244953,
      "grad_norm": 1.0201752185821533,
      "learning_rate": 7.231367853935948e-05,
      "loss": 0.1884,
      "step": 4272
    },
    {
      "epoch": 5.751009421265142,
      "grad_norm": 1.1320924758911133,
      "learning_rate": 7.228374738102365e-05,
      "loss": 0.2459,
      "step": 4273
    },
    {
      "epoch": 5.75235531628533,
      "grad_norm": 1.0691912174224854,
      "learning_rate": 7.225381622268781e-05,
      "loss": 0.1907,
      "step": 4274
    },
    {
      "epoch": 5.753701211305518,
      "grad_norm": 1.1112415790557861,
      "learning_rate": 7.2223885064352e-05,
      "loss": 0.2279,
      "step": 4275
    },
    {
      "epoch": 5.755047106325707,
      "grad_norm": 0.689238429069519,
      "learning_rate": 7.219395390601617e-05,
      "loss": 0.2726,
      "step": 4276
    },
    {
      "epoch": 5.756393001345895,
      "grad_norm": 1.271786093711853,
      "learning_rate": 7.216402274768034e-05,
      "loss": 0.2519,
      "step": 4277
    },
    {
      "epoch": 5.757738896366083,
      "grad_norm": 1.1857547760009766,
      "learning_rate": 7.21340915893445e-05,
      "loss": 0.2409,
      "step": 4278
    },
    {
      "epoch": 5.759084791386272,
      "grad_norm": 1.2787327766418457,
      "learning_rate": 7.210416043100869e-05,
      "loss": 0.2374,
      "step": 4279
    },
    {
      "epoch": 5.76043068640646,
      "grad_norm": 1.0611006021499634,
      "learning_rate": 7.207422927267286e-05,
      "loss": 0.2104,
      "step": 4280
    },
    {
      "epoch": 5.761776581426648,
      "grad_norm": 0.8457815647125244,
      "learning_rate": 7.204429811433703e-05,
      "loss": 0.1714,
      "step": 4281
    },
    {
      "epoch": 5.763122476446837,
      "grad_norm": 0.8032166957855225,
      "learning_rate": 7.20143669560012e-05,
      "loss": 0.1634,
      "step": 4282
    },
    {
      "epoch": 5.764468371467026,
      "grad_norm": 1.098471760749817,
      "learning_rate": 7.198443579766538e-05,
      "loss": 0.1724,
      "step": 4283
    },
    {
      "epoch": 5.765814266487214,
      "grad_norm": 1.3391149044036865,
      "learning_rate": 7.195450463932955e-05,
      "loss": 0.2697,
      "step": 4284
    },
    {
      "epoch": 5.767160161507403,
      "grad_norm": 1.0517184734344482,
      "learning_rate": 7.192457348099372e-05,
      "loss": 0.21,
      "step": 4285
    },
    {
      "epoch": 5.768506056527591,
      "grad_norm": 1.0131268501281738,
      "learning_rate": 7.189464232265788e-05,
      "loss": 0.1598,
      "step": 4286
    },
    {
      "epoch": 5.769851951547779,
      "grad_norm": 1.3643807172775269,
      "learning_rate": 7.186471116432207e-05,
      "loss": 0.2635,
      "step": 4287
    },
    {
      "epoch": 5.771197846567968,
      "grad_norm": 1.1599538326263428,
      "learning_rate": 7.183478000598624e-05,
      "loss": 0.2217,
      "step": 4288
    },
    {
      "epoch": 5.772543741588156,
      "grad_norm": 1.350632905960083,
      "learning_rate": 7.18048488476504e-05,
      "loss": 0.2506,
      "step": 4289
    },
    {
      "epoch": 5.773889636608344,
      "grad_norm": 1.2771743535995483,
      "learning_rate": 7.177491768931457e-05,
      "loss": 0.2225,
      "step": 4290
    },
    {
      "epoch": 5.775235531628533,
      "grad_norm": 1.000991702079773,
      "learning_rate": 7.174498653097876e-05,
      "loss": 0.2181,
      "step": 4291
    },
    {
      "epoch": 5.776581426648722,
      "grad_norm": 1.0727561712265015,
      "learning_rate": 7.171505537264293e-05,
      "loss": 0.2179,
      "step": 4292
    },
    {
      "epoch": 5.77792732166891,
      "grad_norm": 1.245425820350647,
      "learning_rate": 7.16851242143071e-05,
      "loss": 0.2515,
      "step": 4293
    },
    {
      "epoch": 5.7792732166890985,
      "grad_norm": 1.4635239839553833,
      "learning_rate": 7.165519305597127e-05,
      "loss": 0.2355,
      "step": 4294
    },
    {
      "epoch": 5.780619111709287,
      "grad_norm": 1.428907871246338,
      "learning_rate": 7.162526189763545e-05,
      "loss": 0.2105,
      "step": 4295
    },
    {
      "epoch": 5.781965006729475,
      "grad_norm": 1.0190088748931885,
      "learning_rate": 7.159533073929962e-05,
      "loss": 0.1943,
      "step": 4296
    },
    {
      "epoch": 5.783310901749664,
      "grad_norm": 0.8189781308174133,
      "learning_rate": 7.156539958096379e-05,
      "loss": 0.2152,
      "step": 4297
    },
    {
      "epoch": 5.784656796769852,
      "grad_norm": 1.428054928779602,
      "learning_rate": 7.153546842262796e-05,
      "loss": 0.313,
      "step": 4298
    },
    {
      "epoch": 5.78600269179004,
      "grad_norm": 1.2901166677474976,
      "learning_rate": 7.150553726429214e-05,
      "loss": 0.2313,
      "step": 4299
    },
    {
      "epoch": 5.787348586810229,
      "grad_norm": 1.0520857572555542,
      "learning_rate": 7.147560610595631e-05,
      "loss": 0.1898,
      "step": 4300
    },
    {
      "epoch": 5.788694481830417,
      "grad_norm": 1.1103484630584717,
      "learning_rate": 7.144567494762048e-05,
      "loss": 0.2213,
      "step": 4301
    },
    {
      "epoch": 5.790040376850605,
      "grad_norm": 0.9217361807823181,
      "learning_rate": 7.141574378928465e-05,
      "loss": 0.2225,
      "step": 4302
    },
    {
      "epoch": 5.7913862718707945,
      "grad_norm": 1.141947865486145,
      "learning_rate": 7.138581263094883e-05,
      "loss": 0.2205,
      "step": 4303
    },
    {
      "epoch": 5.792732166890983,
      "grad_norm": 1.099619746208191,
      "learning_rate": 7.1355881472613e-05,
      "loss": 0.1955,
      "step": 4304
    },
    {
      "epoch": 5.794078061911171,
      "grad_norm": 1.0348042249679565,
      "learning_rate": 7.132595031427717e-05,
      "loss": 0.2496,
      "step": 4305
    },
    {
      "epoch": 5.7954239569313595,
      "grad_norm": 1.1643083095550537,
      "learning_rate": 7.129601915594134e-05,
      "loss": 0.2472,
      "step": 4306
    },
    {
      "epoch": 5.796769851951548,
      "grad_norm": 1.08002769947052,
      "learning_rate": 7.126608799760552e-05,
      "loss": 0.2246,
      "step": 4307
    },
    {
      "epoch": 5.798115746971736,
      "grad_norm": 0.93989497423172,
      "learning_rate": 7.123615683926969e-05,
      "loss": 0.2076,
      "step": 4308
    },
    {
      "epoch": 5.7994616419919245,
      "grad_norm": 0.9887457489967346,
      "learning_rate": 7.120622568093386e-05,
      "loss": 0.2158,
      "step": 4309
    },
    {
      "epoch": 5.800807537012113,
      "grad_norm": 1.2379156351089478,
      "learning_rate": 7.117629452259803e-05,
      "loss": 0.208,
      "step": 4310
    },
    {
      "epoch": 5.802153432032301,
      "grad_norm": 1.5817238092422485,
      "learning_rate": 7.114636336426221e-05,
      "loss": 0.3978,
      "step": 4311
    },
    {
      "epoch": 5.80349932705249,
      "grad_norm": 1.106859803199768,
      "learning_rate": 7.111643220592638e-05,
      "loss": 0.1948,
      "step": 4312
    },
    {
      "epoch": 5.804845222072679,
      "grad_norm": 1.1983078718185425,
      "learning_rate": 7.108650104759055e-05,
      "loss": 0.2169,
      "step": 4313
    },
    {
      "epoch": 5.806191117092867,
      "grad_norm": 1.1915124654769897,
      "learning_rate": 7.105656988925472e-05,
      "loss": 0.2255,
      "step": 4314
    },
    {
      "epoch": 5.807537012113055,
      "grad_norm": 1.254549503326416,
      "learning_rate": 7.10266387309189e-05,
      "loss": 0.2297,
      "step": 4315
    },
    {
      "epoch": 5.808882907133244,
      "grad_norm": 1.1522051095962524,
      "learning_rate": 7.099670757258307e-05,
      "loss": 0.2263,
      "step": 4316
    },
    {
      "epoch": 5.810228802153432,
      "grad_norm": 1.3048279285430908,
      "learning_rate": 7.096677641424724e-05,
      "loss": 0.2733,
      "step": 4317
    },
    {
      "epoch": 5.81157469717362,
      "grad_norm": 1.0123432874679565,
      "learning_rate": 7.09368452559114e-05,
      "loss": 0.2194,
      "step": 4318
    },
    {
      "epoch": 5.812920592193809,
      "grad_norm": 1.4050482511520386,
      "learning_rate": 7.090691409757559e-05,
      "loss": 0.2892,
      "step": 4319
    },
    {
      "epoch": 5.814266487213997,
      "grad_norm": 1.11635160446167,
      "learning_rate": 7.087698293923976e-05,
      "loss": 0.2413,
      "step": 4320
    },
    {
      "epoch": 5.815612382234185,
      "grad_norm": 1.2475619316101074,
      "learning_rate": 7.084705178090393e-05,
      "loss": 0.2652,
      "step": 4321
    },
    {
      "epoch": 5.816958277254374,
      "grad_norm": 1.2196877002716064,
      "learning_rate": 7.08171206225681e-05,
      "loss": 0.2451,
      "step": 4322
    },
    {
      "epoch": 5.818304172274562,
      "grad_norm": 1.0279608964920044,
      "learning_rate": 7.078718946423228e-05,
      "loss": 0.1953,
      "step": 4323
    },
    {
      "epoch": 5.819650067294751,
      "grad_norm": 1.0674402713775635,
      "learning_rate": 7.075725830589645e-05,
      "loss": 0.243,
      "step": 4324
    },
    {
      "epoch": 5.82099596231494,
      "grad_norm": 0.8559880256652832,
      "learning_rate": 7.072732714756062e-05,
      "loss": 0.1663,
      "step": 4325
    },
    {
      "epoch": 5.822341857335128,
      "grad_norm": 1.1197150945663452,
      "learning_rate": 7.069739598922479e-05,
      "loss": 0.2092,
      "step": 4326
    },
    {
      "epoch": 5.823687752355316,
      "grad_norm": 1.2941508293151855,
      "learning_rate": 7.066746483088897e-05,
      "loss": 0.2182,
      "step": 4327
    },
    {
      "epoch": 5.825033647375505,
      "grad_norm": 1.1503320932388306,
      "learning_rate": 7.063753367255314e-05,
      "loss": 0.2057,
      "step": 4328
    },
    {
      "epoch": 5.826379542395693,
      "grad_norm": 1.3522430658340454,
      "learning_rate": 7.060760251421731e-05,
      "loss": 0.2138,
      "step": 4329
    },
    {
      "epoch": 5.827725437415881,
      "grad_norm": 0.9562138319015503,
      "learning_rate": 7.057767135588148e-05,
      "loss": 0.1833,
      "step": 4330
    },
    {
      "epoch": 5.82907133243607,
      "grad_norm": 0.9477855563163757,
      "learning_rate": 7.054774019754566e-05,
      "loss": 0.2071,
      "step": 4331
    },
    {
      "epoch": 5.830417227456259,
      "grad_norm": 1.126621127128601,
      "learning_rate": 7.051780903920983e-05,
      "loss": 0.2319,
      "step": 4332
    },
    {
      "epoch": 5.831763122476447,
      "grad_norm": 1.1348650455474854,
      "learning_rate": 7.0487877880874e-05,
      "loss": 0.2294,
      "step": 4333
    },
    {
      "epoch": 5.833109017496636,
      "grad_norm": 1.1843476295471191,
      "learning_rate": 7.045794672253817e-05,
      "loss": 0.2277,
      "step": 4334
    },
    {
      "epoch": 5.834454912516824,
      "grad_norm": 0.8427169322967529,
      "learning_rate": 7.042801556420235e-05,
      "loss": 0.21,
      "step": 4335
    },
    {
      "epoch": 5.835800807537012,
      "grad_norm": 1.13516104221344,
      "learning_rate": 7.039808440586652e-05,
      "loss": 0.2538,
      "step": 4336
    },
    {
      "epoch": 5.837146702557201,
      "grad_norm": 0.9161860942840576,
      "learning_rate": 7.036815324753069e-05,
      "loss": 0.1827,
      "step": 4337
    },
    {
      "epoch": 5.838492597577389,
      "grad_norm": 0.9801563620567322,
      "learning_rate": 7.033822208919486e-05,
      "loss": 0.2332,
      "step": 4338
    },
    {
      "epoch": 5.839838492597577,
      "grad_norm": 1.0233150720596313,
      "learning_rate": 7.030829093085903e-05,
      "loss": 0.1865,
      "step": 4339
    },
    {
      "epoch": 5.841184387617766,
      "grad_norm": 1.2363450527191162,
      "learning_rate": 7.02783597725232e-05,
      "loss": 0.2155,
      "step": 4340
    },
    {
      "epoch": 5.842530282637954,
      "grad_norm": 0.9989772439002991,
      "learning_rate": 7.024842861418736e-05,
      "loss": 0.1805,
      "step": 4341
    },
    {
      "epoch": 5.843876177658142,
      "grad_norm": 1.0477294921875,
      "learning_rate": 7.021849745585153e-05,
      "loss": 0.1979,
      "step": 4342
    },
    {
      "epoch": 5.845222072678331,
      "grad_norm": 1.168289065361023,
      "learning_rate": 7.018856629751572e-05,
      "loss": 0.2253,
      "step": 4343
    },
    {
      "epoch": 5.84656796769852,
      "grad_norm": 1.2959818840026855,
      "learning_rate": 7.015863513917989e-05,
      "loss": 0.2429,
      "step": 4344
    },
    {
      "epoch": 5.847913862718708,
      "grad_norm": 0.8938281536102295,
      "learning_rate": 7.012870398084405e-05,
      "loss": 0.1943,
      "step": 4345
    },
    {
      "epoch": 5.8492597577388965,
      "grad_norm": 1.150927186012268,
      "learning_rate": 7.009877282250822e-05,
      "loss": 0.2364,
      "step": 4346
    },
    {
      "epoch": 5.850605652759085,
      "grad_norm": 1.0785276889801025,
      "learning_rate": 7.00688416641724e-05,
      "loss": 0.201,
      "step": 4347
    },
    {
      "epoch": 5.851951547779273,
      "grad_norm": 1.4172605276107788,
      "learning_rate": 7.003891050583658e-05,
      "loss": 0.2863,
      "step": 4348
    },
    {
      "epoch": 5.8532974427994615,
      "grad_norm": 0.9321974515914917,
      "learning_rate": 7.000897934750074e-05,
      "loss": 0.1831,
      "step": 4349
    },
    {
      "epoch": 5.85464333781965,
      "grad_norm": 1.1069303750991821,
      "learning_rate": 6.997904818916491e-05,
      "loss": 0.2491,
      "step": 4350
    },
    {
      "epoch": 5.855989232839838,
      "grad_norm": 1.1657110452651978,
      "learning_rate": 6.99491170308291e-05,
      "loss": 0.2303,
      "step": 4351
    },
    {
      "epoch": 5.857335127860027,
      "grad_norm": 0.9885339736938477,
      "learning_rate": 6.991918587249327e-05,
      "loss": 0.2148,
      "step": 4352
    },
    {
      "epoch": 5.858681022880216,
      "grad_norm": 1.255765438079834,
      "learning_rate": 6.988925471415743e-05,
      "loss": 0.3288,
      "step": 4353
    },
    {
      "epoch": 5.860026917900404,
      "grad_norm": 1.2649208307266235,
      "learning_rate": 6.98593235558216e-05,
      "loss": 0.2548,
      "step": 4354
    },
    {
      "epoch": 5.861372812920592,
      "grad_norm": 1.1218173503875732,
      "learning_rate": 6.982939239748579e-05,
      "loss": 0.2163,
      "step": 4355
    },
    {
      "epoch": 5.862718707940781,
      "grad_norm": 1.0414215326309204,
      "learning_rate": 6.979946123914996e-05,
      "loss": 0.1936,
      "step": 4356
    },
    {
      "epoch": 5.864064602960969,
      "grad_norm": 1.0728180408477783,
      "learning_rate": 6.976953008081412e-05,
      "loss": 0.2106,
      "step": 4357
    },
    {
      "epoch": 5.865410497981157,
      "grad_norm": 1.2222387790679932,
      "learning_rate": 6.97395989224783e-05,
      "loss": 0.2556,
      "step": 4358
    },
    {
      "epoch": 5.866756393001346,
      "grad_norm": 0.9644279479980469,
      "learning_rate": 6.970966776414248e-05,
      "loss": 0.24,
      "step": 4359
    },
    {
      "epoch": 5.868102288021534,
      "grad_norm": 1.1085765361785889,
      "learning_rate": 6.967973660580665e-05,
      "loss": 0.1964,
      "step": 4360
    },
    {
      "epoch": 5.8694481830417224,
      "grad_norm": 1.096503496170044,
      "learning_rate": 6.964980544747081e-05,
      "loss": 0.204,
      "step": 4361
    },
    {
      "epoch": 5.870794078061911,
      "grad_norm": 1.0230915546417236,
      "learning_rate": 6.961987428913498e-05,
      "loss": 0.195,
      "step": 4362
    },
    {
      "epoch": 5.872139973082099,
      "grad_norm": 1.03167724609375,
      "learning_rate": 6.958994313079917e-05,
      "loss": 0.1876,
      "step": 4363
    },
    {
      "epoch": 5.873485868102288,
      "grad_norm": 0.8630223870277405,
      "learning_rate": 6.956001197246334e-05,
      "loss": 0.2039,
      "step": 4364
    },
    {
      "epoch": 5.874831763122477,
      "grad_norm": 0.8826595544815063,
      "learning_rate": 6.95300808141275e-05,
      "loss": 0.1775,
      "step": 4365
    },
    {
      "epoch": 5.876177658142665,
      "grad_norm": 0.8569809198379517,
      "learning_rate": 6.950014965579167e-05,
      "loss": 0.1671,
      "step": 4366
    },
    {
      "epoch": 5.877523553162853,
      "grad_norm": 1.4148616790771484,
      "learning_rate": 6.947021849745586e-05,
      "loss": 0.3105,
      "step": 4367
    },
    {
      "epoch": 5.878869448183042,
      "grad_norm": 0.9784875512123108,
      "learning_rate": 6.944028733912003e-05,
      "loss": 0.2076,
      "step": 4368
    },
    {
      "epoch": 5.88021534320323,
      "grad_norm": 1.2649492025375366,
      "learning_rate": 6.94103561807842e-05,
      "loss": 0.2153,
      "step": 4369
    },
    {
      "epoch": 5.881561238223418,
      "grad_norm": 1.1100764274597168,
      "learning_rate": 6.938042502244836e-05,
      "loss": 0.2276,
      "step": 4370
    },
    {
      "epoch": 5.882907133243607,
      "grad_norm": 1.134616732597351,
      "learning_rate": 6.935049386411255e-05,
      "loss": 0.2336,
      "step": 4371
    },
    {
      "epoch": 5.884253028263795,
      "grad_norm": 0.825225293636322,
      "learning_rate": 6.932056270577672e-05,
      "loss": 0.1636,
      "step": 4372
    },
    {
      "epoch": 5.885598923283984,
      "grad_norm": 1.137460470199585,
      "learning_rate": 6.929063154744089e-05,
      "loss": 0.2446,
      "step": 4373
    },
    {
      "epoch": 5.886944818304173,
      "grad_norm": 1.3650504350662231,
      "learning_rate": 6.926070038910505e-05,
      "loss": 0.2169,
      "step": 4374
    },
    {
      "epoch": 5.888290713324361,
      "grad_norm": 1.375162959098816,
      "learning_rate": 6.923076923076924e-05,
      "loss": 0.2247,
      "step": 4375
    },
    {
      "epoch": 5.889636608344549,
      "grad_norm": 1.1114298105239868,
      "learning_rate": 6.92008380724334e-05,
      "loss": 0.2015,
      "step": 4376
    },
    {
      "epoch": 5.890982503364738,
      "grad_norm": 1.09794282913208,
      "learning_rate": 6.917090691409758e-05,
      "loss": 0.2118,
      "step": 4377
    },
    {
      "epoch": 5.892328398384926,
      "grad_norm": 1.0816620588302612,
      "learning_rate": 6.914097575576174e-05,
      "loss": 0.2352,
      "step": 4378
    },
    {
      "epoch": 5.893674293405114,
      "grad_norm": 1.301486611366272,
      "learning_rate": 6.911104459742593e-05,
      "loss": 0.2133,
      "step": 4379
    },
    {
      "epoch": 5.895020188425303,
      "grad_norm": 1.2229564189910889,
      "learning_rate": 6.90811134390901e-05,
      "loss": 0.2638,
      "step": 4380
    },
    {
      "epoch": 5.896366083445491,
      "grad_norm": 1.3520002365112305,
      "learning_rate": 6.905118228075427e-05,
      "loss": 0.2738,
      "step": 4381
    },
    {
      "epoch": 5.897711978465679,
      "grad_norm": 1.2536245584487915,
      "learning_rate": 6.902125112241843e-05,
      "loss": 0.2643,
      "step": 4382
    },
    {
      "epoch": 5.899057873485868,
      "grad_norm": 1.1266287565231323,
      "learning_rate": 6.899131996408262e-05,
      "loss": 0.2122,
      "step": 4383
    },
    {
      "epoch": 5.900403768506057,
      "grad_norm": 0.930358350276947,
      "learning_rate": 6.896138880574679e-05,
      "loss": 0.2105,
      "step": 4384
    },
    {
      "epoch": 5.901749663526245,
      "grad_norm": 1.157990574836731,
      "learning_rate": 6.893145764741096e-05,
      "loss": 0.2409,
      "step": 4385
    },
    {
      "epoch": 5.9030955585464335,
      "grad_norm": 0.9699758887290955,
      "learning_rate": 6.890152648907512e-05,
      "loss": 0.1989,
      "step": 4386
    },
    {
      "epoch": 5.904441453566622,
      "grad_norm": 1.1066725254058838,
      "learning_rate": 6.887159533073931e-05,
      "loss": 0.1818,
      "step": 4387
    },
    {
      "epoch": 5.90578734858681,
      "grad_norm": 0.9507357478141785,
      "learning_rate": 6.884166417240348e-05,
      "loss": 0.192,
      "step": 4388
    },
    {
      "epoch": 5.9071332436069985,
      "grad_norm": 0.9998635649681091,
      "learning_rate": 6.881173301406765e-05,
      "loss": 0.226,
      "step": 4389
    },
    {
      "epoch": 5.908479138627187,
      "grad_norm": 0.9251704216003418,
      "learning_rate": 6.878180185573182e-05,
      "loss": 0.213,
      "step": 4390
    },
    {
      "epoch": 5.909825033647375,
      "grad_norm": 1.1452745199203491,
      "learning_rate": 6.8751870697396e-05,
      "loss": 0.2653,
      "step": 4391
    },
    {
      "epoch": 5.9111709286675636,
      "grad_norm": 1.3012152910232544,
      "learning_rate": 6.872193953906017e-05,
      "loss": 0.2706,
      "step": 4392
    },
    {
      "epoch": 5.912516823687753,
      "grad_norm": 1.4897798299789429,
      "learning_rate": 6.869200838072434e-05,
      "loss": 0.3315,
      "step": 4393
    },
    {
      "epoch": 5.913862718707941,
      "grad_norm": 0.9021219611167908,
      "learning_rate": 6.86620772223885e-05,
      "loss": 0.1606,
      "step": 4394
    },
    {
      "epoch": 5.9152086137281294,
      "grad_norm": 1.1274669170379639,
      "learning_rate": 6.863214606405269e-05,
      "loss": 0.243,
      "step": 4395
    },
    {
      "epoch": 5.916554508748318,
      "grad_norm": 0.9381576180458069,
      "learning_rate": 6.860221490571686e-05,
      "loss": 0.1849,
      "step": 4396
    },
    {
      "epoch": 5.917900403768506,
      "grad_norm": 1.0053763389587402,
      "learning_rate": 6.857228374738103e-05,
      "loss": 0.1832,
      "step": 4397
    },
    {
      "epoch": 5.9192462987886945,
      "grad_norm": 1.2636743783950806,
      "learning_rate": 6.85423525890452e-05,
      "loss": 0.2145,
      "step": 4398
    },
    {
      "epoch": 5.920592193808883,
      "grad_norm": 1.7816616296768188,
      "learning_rate": 6.851242143070938e-05,
      "loss": 0.3183,
      "step": 4399
    },
    {
      "epoch": 5.921938088829071,
      "grad_norm": 0.8456140756607056,
      "learning_rate": 6.848249027237355e-05,
      "loss": 0.1656,
      "step": 4400
    },
    {
      "epoch": 5.9232839838492595,
      "grad_norm": 1.0551105737686157,
      "learning_rate": 6.845255911403772e-05,
      "loss": 0.2322,
      "step": 4401
    },
    {
      "epoch": 5.924629878869448,
      "grad_norm": 1.138122797012329,
      "learning_rate": 6.842262795570189e-05,
      "loss": 0.1993,
      "step": 4402
    },
    {
      "epoch": 5.925975773889636,
      "grad_norm": 1.3104530572891235,
      "learning_rate": 6.839269679736607e-05,
      "loss": 0.2254,
      "step": 4403
    },
    {
      "epoch": 5.927321668909825,
      "grad_norm": 0.9576018452644348,
      "learning_rate": 6.836276563903024e-05,
      "loss": 0.2269,
      "step": 4404
    },
    {
      "epoch": 5.928667563930014,
      "grad_norm": 1.3361507654190063,
      "learning_rate": 6.83328344806944e-05,
      "loss": 0.3016,
      "step": 4405
    },
    {
      "epoch": 5.930013458950202,
      "grad_norm": 1.0082236528396606,
      "learning_rate": 6.830290332235858e-05,
      "loss": 0.202,
      "step": 4406
    },
    {
      "epoch": 5.93135935397039,
      "grad_norm": 0.9885491132736206,
      "learning_rate": 6.827297216402276e-05,
      "loss": 0.2231,
      "step": 4407
    },
    {
      "epoch": 5.932705248990579,
      "grad_norm": 0.7636922597885132,
      "learning_rate": 6.824304100568693e-05,
      "loss": 0.246,
      "step": 4408
    },
    {
      "epoch": 5.934051144010767,
      "grad_norm": 1.0493968725204468,
      "learning_rate": 6.82131098473511e-05,
      "loss": 0.2035,
      "step": 4409
    },
    {
      "epoch": 5.935397039030955,
      "grad_norm": 1.056498408317566,
      "learning_rate": 6.818317868901527e-05,
      "loss": 0.2586,
      "step": 4410
    },
    {
      "epoch": 5.936742934051144,
      "grad_norm": 0.9840744137763977,
      "learning_rate": 6.815324753067945e-05,
      "loss": 0.2023,
      "step": 4411
    },
    {
      "epoch": 5.938088829071332,
      "grad_norm": 1.2864211797714233,
      "learning_rate": 6.812331637234362e-05,
      "loss": 0.2469,
      "step": 4412
    },
    {
      "epoch": 5.939434724091521,
      "grad_norm": 1.0459506511688232,
      "learning_rate": 6.809338521400779e-05,
      "loss": 0.2191,
      "step": 4413
    },
    {
      "epoch": 5.94078061911171,
      "grad_norm": 1.1862105131149292,
      "learning_rate": 6.806345405567196e-05,
      "loss": 0.221,
      "step": 4414
    },
    {
      "epoch": 5.942126514131898,
      "grad_norm": 1.5700699090957642,
      "learning_rate": 6.803352289733614e-05,
      "loss": 0.5064,
      "step": 4415
    },
    {
      "epoch": 5.943472409152086,
      "grad_norm": 1.0932163000106812,
      "learning_rate": 6.800359173900031e-05,
      "loss": 0.1848,
      "step": 4416
    },
    {
      "epoch": 5.944818304172275,
      "grad_norm": 1.231641173362732,
      "learning_rate": 6.797366058066448e-05,
      "loss": 0.2504,
      "step": 4417
    },
    {
      "epoch": 5.946164199192463,
      "grad_norm": 1.1881203651428223,
      "learning_rate": 6.794372942232865e-05,
      "loss": 0.2081,
      "step": 4418
    },
    {
      "epoch": 5.947510094212651,
      "grad_norm": 1.5384085178375244,
      "learning_rate": 6.791379826399283e-05,
      "loss": 0.2471,
      "step": 4419
    },
    {
      "epoch": 5.94885598923284,
      "grad_norm": 0.7727330923080444,
      "learning_rate": 6.7883867105657e-05,
      "loss": 0.1815,
      "step": 4420
    },
    {
      "epoch": 5.950201884253028,
      "grad_norm": 1.1369227170944214,
      "learning_rate": 6.785393594732117e-05,
      "loss": 0.2569,
      "step": 4421
    },
    {
      "epoch": 5.951547779273216,
      "grad_norm": 0.9903686046600342,
      "learning_rate": 6.782400478898534e-05,
      "loss": 0.2077,
      "step": 4422
    },
    {
      "epoch": 5.952893674293405,
      "grad_norm": 1.2818225622177124,
      "learning_rate": 6.779407363064952e-05,
      "loss": 0.2608,
      "step": 4423
    },
    {
      "epoch": 5.954239569313594,
      "grad_norm": 1.4492489099502563,
      "learning_rate": 6.776414247231369e-05,
      "loss": 0.2282,
      "step": 4424
    },
    {
      "epoch": 5.955585464333782,
      "grad_norm": 1.2003144025802612,
      "learning_rate": 6.773421131397786e-05,
      "loss": 0.2798,
      "step": 4425
    },
    {
      "epoch": 5.956931359353971,
      "grad_norm": 0.9099695086479187,
      "learning_rate": 6.770428015564203e-05,
      "loss": 0.22,
      "step": 4426
    },
    {
      "epoch": 5.958277254374159,
      "grad_norm": 1.0726596117019653,
      "learning_rate": 6.767434899730621e-05,
      "loss": 0.2257,
      "step": 4427
    },
    {
      "epoch": 5.959623149394347,
      "grad_norm": 0.9273098111152649,
      "learning_rate": 6.764441783897038e-05,
      "loss": 0.195,
      "step": 4428
    },
    {
      "epoch": 5.960969044414536,
      "grad_norm": 1.0512336492538452,
      "learning_rate": 6.761448668063455e-05,
      "loss": 0.2024,
      "step": 4429
    },
    {
      "epoch": 5.962314939434724,
      "grad_norm": 1.3198801279067993,
      "learning_rate": 6.758455552229872e-05,
      "loss": 0.2508,
      "step": 4430
    },
    {
      "epoch": 5.963660834454912,
      "grad_norm": 1.2569819688796997,
      "learning_rate": 6.75546243639629e-05,
      "loss": 0.2884,
      "step": 4431
    },
    {
      "epoch": 5.965006729475101,
      "grad_norm": 1.2890511751174927,
      "learning_rate": 6.752469320562707e-05,
      "loss": 0.2244,
      "step": 4432
    },
    {
      "epoch": 5.96635262449529,
      "grad_norm": 0.9913404583930969,
      "learning_rate": 6.749476204729124e-05,
      "loss": 0.1963,
      "step": 4433
    },
    {
      "epoch": 5.967698519515478,
      "grad_norm": 1.28629469871521,
      "learning_rate": 6.74648308889554e-05,
      "loss": 0.21,
      "step": 4434
    },
    {
      "epoch": 5.9690444145356665,
      "grad_norm": 1.6171761751174927,
      "learning_rate": 6.743489973061959e-05,
      "loss": 0.3322,
      "step": 4435
    },
    {
      "epoch": 5.970390309555855,
      "grad_norm": 1.3211554288864136,
      "learning_rate": 6.740496857228376e-05,
      "loss": 0.2395,
      "step": 4436
    },
    {
      "epoch": 5.971736204576043,
      "grad_norm": 1.1405587196350098,
      "learning_rate": 6.737503741394793e-05,
      "loss": 0.2342,
      "step": 4437
    },
    {
      "epoch": 5.9730820995962315,
      "grad_norm": 0.8774842619895935,
      "learning_rate": 6.73451062556121e-05,
      "loss": 0.1895,
      "step": 4438
    },
    {
      "epoch": 5.97442799461642,
      "grad_norm": 1.6280955076217651,
      "learning_rate": 6.731517509727628e-05,
      "loss": 0.3741,
      "step": 4439
    },
    {
      "epoch": 5.975773889636608,
      "grad_norm": 0.9972068667411804,
      "learning_rate": 6.728524393894045e-05,
      "loss": 0.202,
      "step": 4440
    },
    {
      "epoch": 5.9771197846567965,
      "grad_norm": 1.1659621000289917,
      "learning_rate": 6.725531278060462e-05,
      "loss": 0.1974,
      "step": 4441
    },
    {
      "epoch": 5.978465679676985,
      "grad_norm": 1.1442296504974365,
      "learning_rate": 6.722538162226879e-05,
      "loss": 0.2309,
      "step": 4442
    },
    {
      "epoch": 5.979811574697173,
      "grad_norm": 1.143853783607483,
      "learning_rate": 6.719545046393297e-05,
      "loss": 0.2413,
      "step": 4443
    },
    {
      "epoch": 5.981157469717362,
      "grad_norm": 0.8941206932067871,
      "learning_rate": 6.716551930559713e-05,
      "loss": 0.175,
      "step": 4444
    },
    {
      "epoch": 5.982503364737551,
      "grad_norm": 0.9597800374031067,
      "learning_rate": 6.71355881472613e-05,
      "loss": 0.185,
      "step": 4445
    },
    {
      "epoch": 5.983849259757739,
      "grad_norm": 1.1500474214553833,
      "learning_rate": 6.710565698892546e-05,
      "loss": 0.268,
      "step": 4446
    },
    {
      "epoch": 5.985195154777927,
      "grad_norm": 1.4527997970581055,
      "learning_rate": 6.707572583058965e-05,
      "loss": 0.2534,
      "step": 4447
    },
    {
      "epoch": 5.986541049798116,
      "grad_norm": 1.2904592752456665,
      "learning_rate": 6.704579467225382e-05,
      "loss": 0.2838,
      "step": 4448
    },
    {
      "epoch": 5.987886944818304,
      "grad_norm": 1.0503369569778442,
      "learning_rate": 6.701586351391798e-05,
      "loss": 0.1719,
      "step": 4449
    },
    {
      "epoch": 5.989232839838492,
      "grad_norm": 1.2071518898010254,
      "learning_rate": 6.698593235558215e-05,
      "loss": 0.2514,
      "step": 4450
    },
    {
      "epoch": 5.990578734858681,
      "grad_norm": 0.7902654409408569,
      "learning_rate": 6.695600119724634e-05,
      "loss": 0.1578,
      "step": 4451
    },
    {
      "epoch": 5.991924629878869,
      "grad_norm": 1.2147111892700195,
      "learning_rate": 6.69260700389105e-05,
      "loss": 0.2248,
      "step": 4452
    },
    {
      "epoch": 5.993270524899058,
      "grad_norm": 0.9616194367408752,
      "learning_rate": 6.689613888057467e-05,
      "loss": 0.1804,
      "step": 4453
    },
    {
      "epoch": 5.994616419919247,
      "grad_norm": 1.0764038562774658,
      "learning_rate": 6.686620772223884e-05,
      "loss": 0.2095,
      "step": 4454
    },
    {
      "epoch": 5.995962314939435,
      "grad_norm": 1.1422210931777954,
      "learning_rate": 6.683627656390303e-05,
      "loss": 0.2027,
      "step": 4455
    },
    {
      "epoch": 5.997308209959623,
      "grad_norm": 1.237196922302246,
      "learning_rate": 6.68063454055672e-05,
      "loss": 0.2072,
      "step": 4456
    },
    {
      "epoch": 5.998654104979812,
      "grad_norm": 0.9350237250328064,
      "learning_rate": 6.677641424723136e-05,
      "loss": 0.1929,
      "step": 4457
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.0386275053024292,
      "learning_rate": 6.674648308889553e-05,
      "loss": 0.1709,
      "step": 4458
    },
    {
      "epoch": 6.001345895020188,
      "grad_norm": 0.750362753868103,
      "learning_rate": 6.671655193055972e-05,
      "loss": 0.1497,
      "step": 4459
    },
    {
      "epoch": 6.002691790040377,
      "grad_norm": 0.5805119872093201,
      "learning_rate": 6.668662077222389e-05,
      "loss": 0.1448,
      "step": 4460
    },
    {
      "epoch": 6.004037685060565,
      "grad_norm": 0.5352798104286194,
      "learning_rate": 6.665668961388806e-05,
      "loss": 0.1218,
      "step": 4461
    },
    {
      "epoch": 6.005383580080753,
      "grad_norm": 0.5742934942245483,
      "learning_rate": 6.662675845555222e-05,
      "loss": 0.147,
      "step": 4462
    },
    {
      "epoch": 6.006729475100942,
      "grad_norm": 0.6017671823501587,
      "learning_rate": 6.659682729721641e-05,
      "loss": 0.1507,
      "step": 4463
    },
    {
      "epoch": 6.008075370121131,
      "grad_norm": 0.821385383605957,
      "learning_rate": 6.656689613888058e-05,
      "loss": 0.1748,
      "step": 4464
    },
    {
      "epoch": 6.009421265141319,
      "grad_norm": 0.8308049440383911,
      "learning_rate": 6.653696498054475e-05,
      "loss": 0.1648,
      "step": 4465
    },
    {
      "epoch": 6.010767160161508,
      "grad_norm": 0.6306377649307251,
      "learning_rate": 6.650703382220891e-05,
      "loss": 0.1298,
      "step": 4466
    },
    {
      "epoch": 6.012113055181696,
      "grad_norm": 0.573488175868988,
      "learning_rate": 6.64771026638731e-05,
      "loss": 0.129,
      "step": 4467
    },
    {
      "epoch": 6.013458950201884,
      "grad_norm": 1.1087861061096191,
      "learning_rate": 6.644717150553727e-05,
      "loss": 0.1709,
      "step": 4468
    },
    {
      "epoch": 6.014804845222073,
      "grad_norm": 0.6848665475845337,
      "learning_rate": 6.641724034720144e-05,
      "loss": 0.1339,
      "step": 4469
    },
    {
      "epoch": 6.016150740242261,
      "grad_norm": 0.6704927086830139,
      "learning_rate": 6.63873091888656e-05,
      "loss": 0.1291,
      "step": 4470
    },
    {
      "epoch": 6.017496635262449,
      "grad_norm": 0.8589305281639099,
      "learning_rate": 6.635737803052979e-05,
      "loss": 0.1573,
      "step": 4471
    },
    {
      "epoch": 6.018842530282638,
      "grad_norm": 0.6228564977645874,
      "learning_rate": 6.632744687219396e-05,
      "loss": 0.1235,
      "step": 4472
    },
    {
      "epoch": 6.020188425302826,
      "grad_norm": 0.7478146553039551,
      "learning_rate": 6.629751571385813e-05,
      "loss": 0.1312,
      "step": 4473
    },
    {
      "epoch": 6.021534320323015,
      "grad_norm": 0.8959635496139526,
      "learning_rate": 6.62675845555223e-05,
      "loss": 0.1746,
      "step": 4474
    },
    {
      "epoch": 6.0228802153432035,
      "grad_norm": 0.5723633170127869,
      "learning_rate": 6.623765339718648e-05,
      "loss": 0.1029,
      "step": 4475
    },
    {
      "epoch": 6.024226110363392,
      "grad_norm": 0.7523148655891418,
      "learning_rate": 6.620772223885065e-05,
      "loss": 0.1545,
      "step": 4476
    },
    {
      "epoch": 6.02557200538358,
      "grad_norm": 0.6397901177406311,
      "learning_rate": 6.617779108051482e-05,
      "loss": 0.1708,
      "step": 4477
    },
    {
      "epoch": 6.0269179004037685,
      "grad_norm": 0.5187839865684509,
      "learning_rate": 6.614785992217898e-05,
      "loss": 0.1152,
      "step": 4478
    },
    {
      "epoch": 6.028263795423957,
      "grad_norm": 0.750318169593811,
      "learning_rate": 6.611792876384317e-05,
      "loss": 0.1304,
      "step": 4479
    },
    {
      "epoch": 6.029609690444145,
      "grad_norm": 0.9095929265022278,
      "learning_rate": 6.608799760550734e-05,
      "loss": 0.1496,
      "step": 4480
    },
    {
      "epoch": 6.0309555854643335,
      "grad_norm": 1.2042014598846436,
      "learning_rate": 6.60580664471715e-05,
      "loss": 0.1962,
      "step": 4481
    },
    {
      "epoch": 6.032301480484522,
      "grad_norm": 0.7093381285667419,
      "learning_rate": 6.602813528883567e-05,
      "loss": 0.1149,
      "step": 4482
    },
    {
      "epoch": 6.03364737550471,
      "grad_norm": 0.5787877440452576,
      "learning_rate": 6.599820413049986e-05,
      "loss": 0.1163,
      "step": 4483
    },
    {
      "epoch": 6.034993270524899,
      "grad_norm": 0.9017354846000671,
      "learning_rate": 6.596827297216403e-05,
      "loss": 0.1973,
      "step": 4484
    },
    {
      "epoch": 6.036339165545088,
      "grad_norm": 1.0494383573532104,
      "learning_rate": 6.59383418138282e-05,
      "loss": 0.1742,
      "step": 4485
    },
    {
      "epoch": 6.037685060565276,
      "grad_norm": 0.5991629958152771,
      "learning_rate": 6.590841065549237e-05,
      "loss": 0.1344,
      "step": 4486
    },
    {
      "epoch": 6.039030955585464,
      "grad_norm": 0.7148439288139343,
      "learning_rate": 6.587847949715655e-05,
      "loss": 0.1392,
      "step": 4487
    },
    {
      "epoch": 6.040376850605653,
      "grad_norm": 0.7606369256973267,
      "learning_rate": 6.584854833882072e-05,
      "loss": 0.1179,
      "step": 4488
    },
    {
      "epoch": 6.041722745625841,
      "grad_norm": 0.7421452403068542,
      "learning_rate": 6.581861718048489e-05,
      "loss": 0.1372,
      "step": 4489
    },
    {
      "epoch": 6.043068640646029,
      "grad_norm": 0.765080988407135,
      "learning_rate": 6.578868602214906e-05,
      "loss": 0.1254,
      "step": 4490
    },
    {
      "epoch": 6.044414535666218,
      "grad_norm": 0.725483238697052,
      "learning_rate": 6.575875486381324e-05,
      "loss": 0.1223,
      "step": 4491
    },
    {
      "epoch": 6.045760430686406,
      "grad_norm": 1.043940782546997,
      "learning_rate": 6.572882370547741e-05,
      "loss": 0.1564,
      "step": 4492
    },
    {
      "epoch": 6.0471063257065945,
      "grad_norm": 1.1363682746887207,
      "learning_rate": 6.569889254714158e-05,
      "loss": 0.1872,
      "step": 4493
    },
    {
      "epoch": 6.048452220726784,
      "grad_norm": 0.9217888116836548,
      "learning_rate": 6.566896138880575e-05,
      "loss": 0.1283,
      "step": 4494
    },
    {
      "epoch": 6.049798115746972,
      "grad_norm": 0.8341182470321655,
      "learning_rate": 6.563903023046993e-05,
      "loss": 0.1602,
      "step": 4495
    },
    {
      "epoch": 6.05114401076716,
      "grad_norm": 0.5341888070106506,
      "learning_rate": 6.56090990721341e-05,
      "loss": 0.1159,
      "step": 4496
    },
    {
      "epoch": 6.052489905787349,
      "grad_norm": 0.9501197934150696,
      "learning_rate": 6.557916791379827e-05,
      "loss": 0.2443,
      "step": 4497
    },
    {
      "epoch": 6.053835800807537,
      "grad_norm": 0.6868422627449036,
      "learning_rate": 6.554923675546244e-05,
      "loss": 0.1289,
      "step": 4498
    },
    {
      "epoch": 6.055181695827725,
      "grad_norm": 0.6688122153282166,
      "learning_rate": 6.551930559712662e-05,
      "loss": 0.1173,
      "step": 4499
    },
    {
      "epoch": 6.056527590847914,
      "grad_norm": 1.1259464025497437,
      "learning_rate": 6.548937443879079e-05,
      "loss": 0.1815,
      "step": 4500
    },
    {
      "epoch": 6.057873485868102,
      "grad_norm": 1.318751335144043,
      "learning_rate": 6.545944328045496e-05,
      "loss": 0.1408,
      "step": 4501
    },
    {
      "epoch": 6.05921938088829,
      "grad_norm": 0.9134578704833984,
      "learning_rate": 6.542951212211913e-05,
      "loss": 0.1743,
      "step": 4502
    },
    {
      "epoch": 6.060565275908479,
      "grad_norm": 0.9475511312484741,
      "learning_rate": 6.539958096378331e-05,
      "loss": 0.1752,
      "step": 4503
    },
    {
      "epoch": 6.061911170928668,
      "grad_norm": 0.8088759183883667,
      "learning_rate": 6.536964980544748e-05,
      "loss": 0.126,
      "step": 4504
    },
    {
      "epoch": 6.063257065948856,
      "grad_norm": 0.6168215274810791,
      "learning_rate": 6.533971864711165e-05,
      "loss": 0.1262,
      "step": 4505
    },
    {
      "epoch": 6.064602960969045,
      "grad_norm": 0.524235725402832,
      "learning_rate": 6.530978748877582e-05,
      "loss": 0.1109,
      "step": 4506
    },
    {
      "epoch": 6.065948855989233,
      "grad_norm": 0.8551616668701172,
      "learning_rate": 6.527985633044e-05,
      "loss": 0.1275,
      "step": 4507
    },
    {
      "epoch": 6.067294751009421,
      "grad_norm": 0.8306909799575806,
      "learning_rate": 6.524992517210417e-05,
      "loss": 0.134,
      "step": 4508
    },
    {
      "epoch": 6.06864064602961,
      "grad_norm": 0.5759070515632629,
      "learning_rate": 6.521999401376834e-05,
      "loss": 0.1279,
      "step": 4509
    },
    {
      "epoch": 6.069986541049798,
      "grad_norm": 0.8659780621528625,
      "learning_rate": 6.51900628554325e-05,
      "loss": 0.1321,
      "step": 4510
    },
    {
      "epoch": 6.071332436069986,
      "grad_norm": 0.722405731678009,
      "learning_rate": 6.516013169709669e-05,
      "loss": 0.1176,
      "step": 4511
    },
    {
      "epoch": 6.072678331090175,
      "grad_norm": 0.4950915277004242,
      "learning_rate": 6.513020053876086e-05,
      "loss": 0.1075,
      "step": 4512
    },
    {
      "epoch": 6.074024226110363,
      "grad_norm": 1.0680999755859375,
      "learning_rate": 6.510026938042503e-05,
      "loss": 0.1317,
      "step": 4513
    },
    {
      "epoch": 6.075370121130552,
      "grad_norm": 0.8846058249473572,
      "learning_rate": 6.50703382220892e-05,
      "loss": 0.191,
      "step": 4514
    },
    {
      "epoch": 6.0767160161507405,
      "grad_norm": 1.0398831367492676,
      "learning_rate": 6.504040706375338e-05,
      "loss": 0.117,
      "step": 4515
    },
    {
      "epoch": 6.078061911170929,
      "grad_norm": 1.2967374324798584,
      "learning_rate": 6.501047590541755e-05,
      "loss": 0.2155,
      "step": 4516
    },
    {
      "epoch": 6.079407806191117,
      "grad_norm": 0.7652207016944885,
      "learning_rate": 6.498054474708172e-05,
      "loss": 0.1551,
      "step": 4517
    },
    {
      "epoch": 6.0807537012113055,
      "grad_norm": 1.1315596103668213,
      "learning_rate": 6.495061358874589e-05,
      "loss": 0.164,
      "step": 4518
    },
    {
      "epoch": 6.082099596231494,
      "grad_norm": 1.0819151401519775,
      "learning_rate": 6.492068243041007e-05,
      "loss": 0.1535,
      "step": 4519
    },
    {
      "epoch": 6.083445491251682,
      "grad_norm": 0.8547624349594116,
      "learning_rate": 6.489075127207424e-05,
      "loss": 0.1685,
      "step": 4520
    },
    {
      "epoch": 6.0847913862718706,
      "grad_norm": 0.7848299145698547,
      "learning_rate": 6.486082011373841e-05,
      "loss": 0.1474,
      "step": 4521
    },
    {
      "epoch": 6.086137281292059,
      "grad_norm": 0.7996109127998352,
      "learning_rate": 6.483088895540258e-05,
      "loss": 0.1279,
      "step": 4522
    },
    {
      "epoch": 6.087483176312247,
      "grad_norm": 0.716584324836731,
      "learning_rate": 6.480095779706676e-05,
      "loss": 0.1278,
      "step": 4523
    },
    {
      "epoch": 6.0888290713324364,
      "grad_norm": 0.5697702169418335,
      "learning_rate": 6.477102663873093e-05,
      "loss": 0.1295,
      "step": 4524
    },
    {
      "epoch": 6.090174966352625,
      "grad_norm": 1.126629114151001,
      "learning_rate": 6.47410954803951e-05,
      "loss": 0.1548,
      "step": 4525
    },
    {
      "epoch": 6.091520861372813,
      "grad_norm": 0.8049165606498718,
      "learning_rate": 6.471116432205927e-05,
      "loss": 0.1539,
      "step": 4526
    },
    {
      "epoch": 6.0928667563930015,
      "grad_norm": 1.0296815633773804,
      "learning_rate": 6.468123316372345e-05,
      "loss": 0.1367,
      "step": 4527
    },
    {
      "epoch": 6.09421265141319,
      "grad_norm": 1.0114442110061646,
      "learning_rate": 6.465130200538762e-05,
      "loss": 0.1719,
      "step": 4528
    },
    {
      "epoch": 6.095558546433378,
      "grad_norm": 0.49782687425613403,
      "learning_rate": 6.462137084705179e-05,
      "loss": 0.0987,
      "step": 4529
    },
    {
      "epoch": 6.0969044414535665,
      "grad_norm": 0.7290263772010803,
      "learning_rate": 6.459143968871596e-05,
      "loss": 0.1547,
      "step": 4530
    },
    {
      "epoch": 6.098250336473755,
      "grad_norm": 0.7927989959716797,
      "learning_rate": 6.456150853038013e-05,
      "loss": 0.137,
      "step": 4531
    },
    {
      "epoch": 6.099596231493943,
      "grad_norm": 0.9795776009559631,
      "learning_rate": 6.453157737204431e-05,
      "loss": 0.1681,
      "step": 4532
    },
    {
      "epoch": 6.1009421265141315,
      "grad_norm": 0.9333860278129578,
      "learning_rate": 6.450164621370848e-05,
      "loss": 0.1514,
      "step": 4533
    },
    {
      "epoch": 6.102288021534321,
      "grad_norm": 0.5843372344970703,
      "learning_rate": 6.447171505537265e-05,
      "loss": 0.1116,
      "step": 4534
    },
    {
      "epoch": 6.103633916554509,
      "grad_norm": 0.750097930431366,
      "learning_rate": 6.444178389703682e-05,
      "loss": 0.1356,
      "step": 4535
    },
    {
      "epoch": 6.104979811574697,
      "grad_norm": 0.6431787610054016,
      "learning_rate": 6.4411852738701e-05,
      "loss": 0.1186,
      "step": 4536
    },
    {
      "epoch": 6.106325706594886,
      "grad_norm": 0.9250704646110535,
      "learning_rate": 6.438192158036517e-05,
      "loss": 0.1825,
      "step": 4537
    },
    {
      "epoch": 6.107671601615074,
      "grad_norm": 1.186703085899353,
      "learning_rate": 6.435199042202934e-05,
      "loss": 0.1742,
      "step": 4538
    },
    {
      "epoch": 6.109017496635262,
      "grad_norm": 0.712583065032959,
      "learning_rate": 6.43220592636935e-05,
      "loss": 0.1163,
      "step": 4539
    },
    {
      "epoch": 6.110363391655451,
      "grad_norm": 1.0735807418823242,
      "learning_rate": 6.429212810535769e-05,
      "loss": 0.1239,
      "step": 4540
    },
    {
      "epoch": 6.111709286675639,
      "grad_norm": 0.8574972748756409,
      "learning_rate": 6.426219694702186e-05,
      "loss": 0.1265,
      "step": 4541
    },
    {
      "epoch": 6.113055181695827,
      "grad_norm": 1.0841965675354004,
      "learning_rate": 6.423226578868603e-05,
      "loss": 0.1758,
      "step": 4542
    },
    {
      "epoch": 6.114401076716016,
      "grad_norm": 1.3450806140899658,
      "learning_rate": 6.42023346303502e-05,
      "loss": 0.1728,
      "step": 4543
    },
    {
      "epoch": 6.115746971736205,
      "grad_norm": 0.6768481731414795,
      "learning_rate": 6.417240347201438e-05,
      "loss": 0.1318,
      "step": 4544
    },
    {
      "epoch": 6.117092866756393,
      "grad_norm": 0.6091039776802063,
      "learning_rate": 6.414247231367855e-05,
      "loss": 0.126,
      "step": 4545
    },
    {
      "epoch": 6.118438761776582,
      "grad_norm": 0.6403982639312744,
      "learning_rate": 6.411254115534272e-05,
      "loss": 0.1277,
      "step": 4546
    },
    {
      "epoch": 6.11978465679677,
      "grad_norm": 0.9395982027053833,
      "learning_rate": 6.408260999700689e-05,
      "loss": 0.1481,
      "step": 4547
    },
    {
      "epoch": 6.121130551816958,
      "grad_norm": 0.6594138145446777,
      "learning_rate": 6.405267883867106e-05,
      "loss": 0.1418,
      "step": 4548
    },
    {
      "epoch": 6.122476446837147,
      "grad_norm": 0.6938663125038147,
      "learning_rate": 6.402274768033522e-05,
      "loss": 0.1081,
      "step": 4549
    },
    {
      "epoch": 6.123822341857335,
      "grad_norm": 0.815055251121521,
      "learning_rate": 6.39928165219994e-05,
      "loss": 0.1417,
      "step": 4550
    },
    {
      "epoch": 6.125168236877523,
      "grad_norm": 0.8684921264648438,
      "learning_rate": 6.396288536366358e-05,
      "loss": 0.1422,
      "step": 4551
    },
    {
      "epoch": 6.126514131897712,
      "grad_norm": 0.5624045729637146,
      "learning_rate": 6.393295420532775e-05,
      "loss": 0.1145,
      "step": 4552
    },
    {
      "epoch": 6.1278600269179,
      "grad_norm": 1.4502760171890259,
      "learning_rate": 6.390302304699191e-05,
      "loss": 0.2144,
      "step": 4553
    },
    {
      "epoch": 6.129205921938089,
      "grad_norm": 0.9715522527694702,
      "learning_rate": 6.387309188865608e-05,
      "loss": 0.1415,
      "step": 4554
    },
    {
      "epoch": 6.1305518169582776,
      "grad_norm": 0.832987368106842,
      "learning_rate": 6.384316073032027e-05,
      "loss": 0.1321,
      "step": 4555
    },
    {
      "epoch": 6.131897711978466,
      "grad_norm": 0.649066686630249,
      "learning_rate": 6.381322957198444e-05,
      "loss": 0.1286,
      "step": 4556
    },
    {
      "epoch": 6.133243606998654,
      "grad_norm": 0.6321640014648438,
      "learning_rate": 6.37832984136486e-05,
      "loss": 0.1175,
      "step": 4557
    },
    {
      "epoch": 6.134589502018843,
      "grad_norm": 0.8408671617507935,
      "learning_rate": 6.375336725531277e-05,
      "loss": 0.1428,
      "step": 4558
    },
    {
      "epoch": 6.135935397039031,
      "grad_norm": 0.7317038774490356,
      "learning_rate": 6.372343609697696e-05,
      "loss": 0.116,
      "step": 4559
    },
    {
      "epoch": 6.137281292059219,
      "grad_norm": 1.192395806312561,
      "learning_rate": 6.369350493864113e-05,
      "loss": 0.1411,
      "step": 4560
    },
    {
      "epoch": 6.138627187079408,
      "grad_norm": 0.6777478456497192,
      "learning_rate": 6.36635737803053e-05,
      "loss": 0.0949,
      "step": 4561
    },
    {
      "epoch": 6.139973082099596,
      "grad_norm": 1.111274242401123,
      "learning_rate": 6.363364262196946e-05,
      "loss": 0.1474,
      "step": 4562
    },
    {
      "epoch": 6.141318977119784,
      "grad_norm": 0.8376325964927673,
      "learning_rate": 6.360371146363365e-05,
      "loss": 0.1458,
      "step": 4563
    },
    {
      "epoch": 6.1426648721399735,
      "grad_norm": 1.02157723903656,
      "learning_rate": 6.357378030529782e-05,
      "loss": 0.1405,
      "step": 4564
    },
    {
      "epoch": 6.144010767160162,
      "grad_norm": 0.6097263693809509,
      "learning_rate": 6.354384914696199e-05,
      "loss": 0.0993,
      "step": 4565
    },
    {
      "epoch": 6.14535666218035,
      "grad_norm": 0.909247636795044,
      "learning_rate": 6.351391798862615e-05,
      "loss": 0.1317,
      "step": 4566
    },
    {
      "epoch": 6.1467025572005385,
      "grad_norm": 1.6220849752426147,
      "learning_rate": 6.348398683029034e-05,
      "loss": 0.1969,
      "step": 4567
    },
    {
      "epoch": 6.148048452220727,
      "grad_norm": 0.8553150296211243,
      "learning_rate": 6.34540556719545e-05,
      "loss": 0.1791,
      "step": 4568
    },
    {
      "epoch": 6.149394347240915,
      "grad_norm": 0.646131694316864,
      "learning_rate": 6.342412451361868e-05,
      "loss": 0.1194,
      "step": 4569
    },
    {
      "epoch": 6.1507402422611035,
      "grad_norm": 1.060051679611206,
      "learning_rate": 6.339419335528284e-05,
      "loss": 0.1667,
      "step": 4570
    },
    {
      "epoch": 6.152086137281292,
      "grad_norm": 0.875359296798706,
      "learning_rate": 6.336426219694703e-05,
      "loss": 0.1485,
      "step": 4571
    },
    {
      "epoch": 6.15343203230148,
      "grad_norm": 0.7411397695541382,
      "learning_rate": 6.33343310386112e-05,
      "loss": 0.1314,
      "step": 4572
    },
    {
      "epoch": 6.1547779273216685,
      "grad_norm": 0.8965743780136108,
      "learning_rate": 6.330439988027537e-05,
      "loss": 0.1063,
      "step": 4573
    },
    {
      "epoch": 6.156123822341858,
      "grad_norm": 0.7787559628486633,
      "learning_rate": 6.327446872193953e-05,
      "loss": 0.1272,
      "step": 4574
    },
    {
      "epoch": 6.157469717362046,
      "grad_norm": 1.2872058153152466,
      "learning_rate": 6.324453756360372e-05,
      "loss": 0.1654,
      "step": 4575
    },
    {
      "epoch": 6.158815612382234,
      "grad_norm": 0.6703606843948364,
      "learning_rate": 6.321460640526789e-05,
      "loss": 0.145,
      "step": 4576
    },
    {
      "epoch": 6.160161507402423,
      "grad_norm": 0.9539833664894104,
      "learning_rate": 6.318467524693206e-05,
      "loss": 0.1918,
      "step": 4577
    },
    {
      "epoch": 6.161507402422611,
      "grad_norm": 0.879885733127594,
      "learning_rate": 6.315474408859622e-05,
      "loss": 0.1526,
      "step": 4578
    },
    {
      "epoch": 6.162853297442799,
      "grad_norm": 0.6400516033172607,
      "learning_rate": 6.312481293026041e-05,
      "loss": 0.1273,
      "step": 4579
    },
    {
      "epoch": 6.164199192462988,
      "grad_norm": 0.7825109362602234,
      "learning_rate": 6.309488177192458e-05,
      "loss": 0.1223,
      "step": 4580
    },
    {
      "epoch": 6.165545087483176,
      "grad_norm": 0.681125283241272,
      "learning_rate": 6.306495061358875e-05,
      "loss": 0.1494,
      "step": 4581
    },
    {
      "epoch": 6.166890982503364,
      "grad_norm": 0.7830687761306763,
      "learning_rate": 6.303501945525292e-05,
      "loss": 0.1956,
      "step": 4582
    },
    {
      "epoch": 6.168236877523553,
      "grad_norm": 1.2313587665557861,
      "learning_rate": 6.30050882969171e-05,
      "loss": 0.2204,
      "step": 4583
    },
    {
      "epoch": 6.169582772543742,
      "grad_norm": 0.9883022904396057,
      "learning_rate": 6.297515713858127e-05,
      "loss": 0.1094,
      "step": 4584
    },
    {
      "epoch": 6.17092866756393,
      "grad_norm": 0.7229222655296326,
      "learning_rate": 6.294522598024544e-05,
      "loss": 0.1346,
      "step": 4585
    },
    {
      "epoch": 6.172274562584119,
      "grad_norm": 0.8168614506721497,
      "learning_rate": 6.29152948219096e-05,
      "loss": 0.1421,
      "step": 4586
    },
    {
      "epoch": 6.173620457604307,
      "grad_norm": 0.8676519989967346,
      "learning_rate": 6.288536366357379e-05,
      "loss": 0.112,
      "step": 4587
    },
    {
      "epoch": 6.174966352624495,
      "grad_norm": 0.7112467885017395,
      "learning_rate": 6.285543250523796e-05,
      "loss": 0.1345,
      "step": 4588
    },
    {
      "epoch": 6.176312247644684,
      "grad_norm": 0.7807027697563171,
      "learning_rate": 6.282550134690213e-05,
      "loss": 0.1466,
      "step": 4589
    },
    {
      "epoch": 6.177658142664872,
      "grad_norm": 0.8828235864639282,
      "learning_rate": 6.27955701885663e-05,
      "loss": 0.1787,
      "step": 4590
    },
    {
      "epoch": 6.17900403768506,
      "grad_norm": 0.8391311168670654,
      "learning_rate": 6.276563903023048e-05,
      "loss": 0.1707,
      "step": 4591
    },
    {
      "epoch": 6.180349932705249,
      "grad_norm": 0.809633195400238,
      "learning_rate": 6.273570787189465e-05,
      "loss": 0.1634,
      "step": 4592
    },
    {
      "epoch": 6.181695827725437,
      "grad_norm": 0.8314540386199951,
      "learning_rate": 6.270577671355882e-05,
      "loss": 0.163,
      "step": 4593
    },
    {
      "epoch": 6.183041722745626,
      "grad_norm": 0.7440269589424133,
      "learning_rate": 6.267584555522299e-05,
      "loss": 0.1744,
      "step": 4594
    },
    {
      "epoch": 6.184387617765815,
      "grad_norm": 0.8698445558547974,
      "learning_rate": 6.264591439688717e-05,
      "loss": 0.1544,
      "step": 4595
    },
    {
      "epoch": 6.185733512786003,
      "grad_norm": 0.7441799640655518,
      "learning_rate": 6.261598323855134e-05,
      "loss": 0.1601,
      "step": 4596
    },
    {
      "epoch": 6.187079407806191,
      "grad_norm": 0.5431576371192932,
      "learning_rate": 6.25860520802155e-05,
      "loss": 0.1174,
      "step": 4597
    },
    {
      "epoch": 6.18842530282638,
      "grad_norm": 0.665742814540863,
      "learning_rate": 6.255612092187968e-05,
      "loss": 0.1328,
      "step": 4598
    },
    {
      "epoch": 6.189771197846568,
      "grad_norm": 0.8546058535575867,
      "learning_rate": 6.252618976354386e-05,
      "loss": 0.1652,
      "step": 4599
    },
    {
      "epoch": 6.191117092866756,
      "grad_norm": 0.6094647645950317,
      "learning_rate": 6.249625860520803e-05,
      "loss": 0.1334,
      "step": 4600
    },
    {
      "epoch": 6.192462987886945,
      "grad_norm": 0.9274560213088989,
      "learning_rate": 6.24663274468722e-05,
      "loss": 0.1626,
      "step": 4601
    },
    {
      "epoch": 6.193808882907133,
      "grad_norm": 0.9044516682624817,
      "learning_rate": 6.243639628853637e-05,
      "loss": 0.1391,
      "step": 4602
    },
    {
      "epoch": 6.195154777927321,
      "grad_norm": 0.8289530873298645,
      "learning_rate": 6.240646513020055e-05,
      "loss": 0.1533,
      "step": 4603
    },
    {
      "epoch": 6.1965006729475105,
      "grad_norm": 0.8622075915336609,
      "learning_rate": 6.237653397186472e-05,
      "loss": 0.1245,
      "step": 4604
    },
    {
      "epoch": 6.197846567967699,
      "grad_norm": 0.6075546741485596,
      "learning_rate": 6.234660281352889e-05,
      "loss": 0.1354,
      "step": 4605
    },
    {
      "epoch": 6.199192462987887,
      "grad_norm": 0.9087105989456177,
      "learning_rate": 6.231667165519306e-05,
      "loss": 0.1381,
      "step": 4606
    },
    {
      "epoch": 6.2005383580080755,
      "grad_norm": 0.7754583954811096,
      "learning_rate": 6.228674049685724e-05,
      "loss": 0.1397,
      "step": 4607
    },
    {
      "epoch": 6.201884253028264,
      "grad_norm": 0.6407707333564758,
      "learning_rate": 6.225680933852141e-05,
      "loss": 0.1321,
      "step": 4608
    },
    {
      "epoch": 6.203230148048452,
      "grad_norm": 0.682227373123169,
      "learning_rate": 6.222687818018558e-05,
      "loss": 0.1255,
      "step": 4609
    },
    {
      "epoch": 6.2045760430686405,
      "grad_norm": 1.2548141479492188,
      "learning_rate": 6.219694702184975e-05,
      "loss": 0.2057,
      "step": 4610
    },
    {
      "epoch": 6.205921938088829,
      "grad_norm": 0.9775466918945312,
      "learning_rate": 6.216701586351393e-05,
      "loss": 0.1663,
      "step": 4611
    },
    {
      "epoch": 6.207267833109017,
      "grad_norm": 0.722588837146759,
      "learning_rate": 6.21370847051781e-05,
      "loss": 0.1372,
      "step": 4612
    },
    {
      "epoch": 6.2086137281292055,
      "grad_norm": 0.9735605120658875,
      "learning_rate": 6.210715354684227e-05,
      "loss": 0.1652,
      "step": 4613
    },
    {
      "epoch": 6.209959623149395,
      "grad_norm": 0.612492024898529,
      "learning_rate": 6.207722238850644e-05,
      "loss": 0.1144,
      "step": 4614
    },
    {
      "epoch": 6.211305518169583,
      "grad_norm": 0.8279536366462708,
      "learning_rate": 6.20472912301706e-05,
      "loss": 0.1432,
      "step": 4615
    },
    {
      "epoch": 6.212651413189771,
      "grad_norm": 0.9056232571601868,
      "learning_rate": 6.201736007183479e-05,
      "loss": 0.1438,
      "step": 4616
    },
    {
      "epoch": 6.21399730820996,
      "grad_norm": 0.9267968535423279,
      "learning_rate": 6.198742891349896e-05,
      "loss": 0.2655,
      "step": 4617
    },
    {
      "epoch": 6.215343203230148,
      "grad_norm": 0.7351073622703552,
      "learning_rate": 6.195749775516313e-05,
      "loss": 0.164,
      "step": 4618
    },
    {
      "epoch": 6.216689098250336,
      "grad_norm": 0.8418672680854797,
      "learning_rate": 6.19275665968273e-05,
      "loss": 0.1579,
      "step": 4619
    },
    {
      "epoch": 6.218034993270525,
      "grad_norm": 0.5275110006332397,
      "learning_rate": 6.189763543849148e-05,
      "loss": 0.1221,
      "step": 4620
    },
    {
      "epoch": 6.219380888290713,
      "grad_norm": 0.7199177742004395,
      "learning_rate": 6.186770428015565e-05,
      "loss": 0.1414,
      "step": 4621
    },
    {
      "epoch": 6.2207267833109015,
      "grad_norm": 1.2253307104110718,
      "learning_rate": 6.183777312181982e-05,
      "loss": 0.1857,
      "step": 4622
    },
    {
      "epoch": 6.22207267833109,
      "grad_norm": 1.4286657571792603,
      "learning_rate": 6.180784196348399e-05,
      "loss": 0.2124,
      "step": 4623
    },
    {
      "epoch": 6.223418573351279,
      "grad_norm": 0.7314109206199646,
      "learning_rate": 6.177791080514817e-05,
      "loss": 0.1329,
      "step": 4624
    },
    {
      "epoch": 6.224764468371467,
      "grad_norm": 1.0527578592300415,
      "learning_rate": 6.174797964681234e-05,
      "loss": 0.1555,
      "step": 4625
    },
    {
      "epoch": 6.226110363391656,
      "grad_norm": 1.0059586763381958,
      "learning_rate": 6.17180484884765e-05,
      "loss": 0.1723,
      "step": 4626
    },
    {
      "epoch": 6.227456258411844,
      "grad_norm": 0.9236509203910828,
      "learning_rate": 6.168811733014068e-05,
      "loss": 0.1388,
      "step": 4627
    },
    {
      "epoch": 6.228802153432032,
      "grad_norm": 1.2223678827285767,
      "learning_rate": 6.165818617180486e-05,
      "loss": 0.201,
      "step": 4628
    },
    {
      "epoch": 6.230148048452221,
      "grad_norm": 0.8749555349349976,
      "learning_rate": 6.162825501346903e-05,
      "loss": 0.1252,
      "step": 4629
    },
    {
      "epoch": 6.231493943472409,
      "grad_norm": 0.4623048007488251,
      "learning_rate": 6.15983238551332e-05,
      "loss": 0.0982,
      "step": 4630
    },
    {
      "epoch": 6.232839838492597,
      "grad_norm": 0.9598312973976135,
      "learning_rate": 6.156839269679737e-05,
      "loss": 0.1437,
      "step": 4631
    },
    {
      "epoch": 6.234185733512786,
      "grad_norm": 0.7653378248214722,
      "learning_rate": 6.153846153846155e-05,
      "loss": 0.1382,
      "step": 4632
    },
    {
      "epoch": 6.235531628532974,
      "grad_norm": 0.679930567741394,
      "learning_rate": 6.150853038012572e-05,
      "loss": 0.101,
      "step": 4633
    },
    {
      "epoch": 6.236877523553163,
      "grad_norm": 0.785219669342041,
      "learning_rate": 6.147859922178989e-05,
      "loss": 0.1444,
      "step": 4634
    },
    {
      "epoch": 6.238223418573352,
      "grad_norm": 0.9873150587081909,
      "learning_rate": 6.144866806345406e-05,
      "loss": 0.1789,
      "step": 4635
    },
    {
      "epoch": 6.23956931359354,
      "grad_norm": 0.8658066987991333,
      "learning_rate": 6.141873690511824e-05,
      "loss": 0.1475,
      "step": 4636
    },
    {
      "epoch": 6.240915208613728,
      "grad_norm": 0.6686791777610779,
      "learning_rate": 6.138880574678241e-05,
      "loss": 0.1409,
      "step": 4637
    },
    {
      "epoch": 6.242261103633917,
      "grad_norm": 1.0266613960266113,
      "learning_rate": 6.135887458844658e-05,
      "loss": 0.1502,
      "step": 4638
    },
    {
      "epoch": 6.243606998654105,
      "grad_norm": 0.6504769921302795,
      "learning_rate": 6.132894343011075e-05,
      "loss": 0.1397,
      "step": 4639
    },
    {
      "epoch": 6.244952893674293,
      "grad_norm": 0.4464414715766907,
      "learning_rate": 6.129901227177493e-05,
      "loss": 0.0934,
      "step": 4640
    },
    {
      "epoch": 6.246298788694482,
      "grad_norm": 0.7185971736907959,
      "learning_rate": 6.12690811134391e-05,
      "loss": 0.1204,
      "step": 4641
    },
    {
      "epoch": 6.24764468371467,
      "grad_norm": 1.005241870880127,
      "learning_rate": 6.123914995510327e-05,
      "loss": 0.1416,
      "step": 4642
    },
    {
      "epoch": 6.248990578734858,
      "grad_norm": 0.7599879503250122,
      "learning_rate": 6.120921879676744e-05,
      "loss": 0.167,
      "step": 4643
    },
    {
      "epoch": 6.250336473755047,
      "grad_norm": 0.803786039352417,
      "learning_rate": 6.117928763843162e-05,
      "loss": 0.1438,
      "step": 4644
    },
    {
      "epoch": 6.251682368775236,
      "grad_norm": 0.8699011206626892,
      "learning_rate": 6.114935648009579e-05,
      "loss": 0.1354,
      "step": 4645
    },
    {
      "epoch": 6.253028263795424,
      "grad_norm": 0.9274969100952148,
      "learning_rate": 6.111942532175996e-05,
      "loss": 0.1499,
      "step": 4646
    },
    {
      "epoch": 6.2543741588156125,
      "grad_norm": 0.9006862044334412,
      "learning_rate": 6.108949416342413e-05,
      "loss": 0.1367,
      "step": 4647
    },
    {
      "epoch": 6.255720053835801,
      "grad_norm": 0.6800037026405334,
      "learning_rate": 6.105956300508831e-05,
      "loss": 0.138,
      "step": 4648
    },
    {
      "epoch": 6.257065948855989,
      "grad_norm": 0.845837414264679,
      "learning_rate": 6.102963184675248e-05,
      "loss": 0.122,
      "step": 4649
    },
    {
      "epoch": 6.2584118438761775,
      "grad_norm": 0.7150436043739319,
      "learning_rate": 6.099970068841665e-05,
      "loss": 0.1446,
      "step": 4650
    },
    {
      "epoch": 6.259757738896366,
      "grad_norm": 1.114107370376587,
      "learning_rate": 6.0969769530080824e-05,
      "loss": 0.1531,
      "step": 4651
    },
    {
      "epoch": 6.261103633916554,
      "grad_norm": 0.9198892712593079,
      "learning_rate": 6.093983837174499e-05,
      "loss": 0.1666,
      "step": 4652
    },
    {
      "epoch": 6.262449528936743,
      "grad_norm": 1.3772532939910889,
      "learning_rate": 6.0909907213409155e-05,
      "loss": 0.1569,
      "step": 4653
    },
    {
      "epoch": 6.263795423956932,
      "grad_norm": 0.6763873100280762,
      "learning_rate": 6.087997605507333e-05,
      "loss": 0.1346,
      "step": 4654
    },
    {
      "epoch": 6.26514131897712,
      "grad_norm": 0.6700016856193542,
      "learning_rate": 6.08500448967375e-05,
      "loss": 0.1568,
      "step": 4655
    },
    {
      "epoch": 6.2664872139973085,
      "grad_norm": 1.1292730569839478,
      "learning_rate": 6.0820113738401676e-05,
      "loss": 0.1535,
      "step": 4656
    },
    {
      "epoch": 6.267833109017497,
      "grad_norm": 1.0223201513290405,
      "learning_rate": 6.0790182580065845e-05,
      "loss": 0.1359,
      "step": 4657
    },
    {
      "epoch": 6.269179004037685,
      "grad_norm": 0.45929279923439026,
      "learning_rate": 6.076025142173002e-05,
      "loss": 0.0978,
      "step": 4658
    },
    {
      "epoch": 6.2705248990578735,
      "grad_norm": 0.763739287853241,
      "learning_rate": 6.073032026339419e-05,
      "loss": 0.1437,
      "step": 4659
    },
    {
      "epoch": 6.271870794078062,
      "grad_norm": 0.840420663356781,
      "learning_rate": 6.0700389105058366e-05,
      "loss": 0.1473,
      "step": 4660
    },
    {
      "epoch": 6.27321668909825,
      "grad_norm": 0.9835050702095032,
      "learning_rate": 6.0670457946722535e-05,
      "loss": 0.184,
      "step": 4661
    },
    {
      "epoch": 6.2745625841184385,
      "grad_norm": 0.9913572072982788,
      "learning_rate": 6.064052678838671e-05,
      "loss": 0.1597,
      "step": 4662
    },
    {
      "epoch": 6.275908479138627,
      "grad_norm": 0.7518450021743774,
      "learning_rate": 6.061059563005088e-05,
      "loss": 0.1492,
      "step": 4663
    },
    {
      "epoch": 6.277254374158815,
      "grad_norm": 0.5691529512405396,
      "learning_rate": 6.0580664471715056e-05,
      "loss": 0.1179,
      "step": 4664
    },
    {
      "epoch": 6.278600269179004,
      "grad_norm": 0.8003506064414978,
      "learning_rate": 6.0550733313379225e-05,
      "loss": 0.1661,
      "step": 4665
    },
    {
      "epoch": 6.279946164199193,
      "grad_norm": 0.5502784848213196,
      "learning_rate": 6.05208021550434e-05,
      "loss": 0.1238,
      "step": 4666
    },
    {
      "epoch": 6.281292059219381,
      "grad_norm": 0.9850059151649475,
      "learning_rate": 6.049087099670757e-05,
      "loss": 0.1483,
      "step": 4667
    },
    {
      "epoch": 6.282637954239569,
      "grad_norm": 0.812926709651947,
      "learning_rate": 6.0460939838371746e-05,
      "loss": 0.1332,
      "step": 4668
    },
    {
      "epoch": 6.283983849259758,
      "grad_norm": 0.8056701421737671,
      "learning_rate": 6.0431008680035916e-05,
      "loss": 0.1244,
      "step": 4669
    },
    {
      "epoch": 6.285329744279946,
      "grad_norm": 0.7680186629295349,
      "learning_rate": 6.040107752170009e-05,
      "loss": 0.1229,
      "step": 4670
    },
    {
      "epoch": 6.286675639300134,
      "grad_norm": 1.0033949613571167,
      "learning_rate": 6.037114636336426e-05,
      "loss": 0.1769,
      "step": 4671
    },
    {
      "epoch": 6.288021534320323,
      "grad_norm": 0.6886230111122131,
      "learning_rate": 6.034121520502844e-05,
      "loss": 0.1281,
      "step": 4672
    },
    {
      "epoch": 6.289367429340511,
      "grad_norm": 0.7183333039283752,
      "learning_rate": 6.0311284046692606e-05,
      "loss": 0.1539,
      "step": 4673
    },
    {
      "epoch": 6.2907133243607,
      "grad_norm": 1.105306625366211,
      "learning_rate": 6.028135288835678e-05,
      "loss": 0.169,
      "step": 4674
    },
    {
      "epoch": 6.292059219380889,
      "grad_norm": 0.8779970407485962,
      "learning_rate": 6.025142173002095e-05,
      "loss": 0.1444,
      "step": 4675
    },
    {
      "epoch": 6.293405114401077,
      "grad_norm": 0.6288024187088013,
      "learning_rate": 6.022149057168513e-05,
      "loss": 0.1326,
      "step": 4676
    },
    {
      "epoch": 6.294751009421265,
      "grad_norm": 0.778536856174469,
      "learning_rate": 6.0191559413349296e-05,
      "loss": 0.128,
      "step": 4677
    },
    {
      "epoch": 6.296096904441454,
      "grad_norm": 0.6848782896995544,
      "learning_rate": 6.016162825501347e-05,
      "loss": 0.127,
      "step": 4678
    },
    {
      "epoch": 6.297442799461642,
      "grad_norm": 0.8046603202819824,
      "learning_rate": 6.013169709667764e-05,
      "loss": 0.1552,
      "step": 4679
    },
    {
      "epoch": 6.29878869448183,
      "grad_norm": 0.5523344278335571,
      "learning_rate": 6.010176593834182e-05,
      "loss": 0.1351,
      "step": 4680
    },
    {
      "epoch": 6.300134589502019,
      "grad_norm": 0.844694197177887,
      "learning_rate": 6.0071834780005986e-05,
      "loss": 0.1316,
      "step": 4681
    },
    {
      "epoch": 6.301480484522207,
      "grad_norm": 1.1288955211639404,
      "learning_rate": 6.004190362167016e-05,
      "loss": 0.1624,
      "step": 4682
    },
    {
      "epoch": 6.302826379542395,
      "grad_norm": 0.7696584463119507,
      "learning_rate": 6.001197246333433e-05,
      "loss": 0.1311,
      "step": 4683
    },
    {
      "epoch": 6.304172274562584,
      "grad_norm": 1.032209873199463,
      "learning_rate": 5.998204130499851e-05,
      "loss": 0.1859,
      "step": 4684
    },
    {
      "epoch": 6.305518169582773,
      "grad_norm": 0.6208010315895081,
      "learning_rate": 5.9952110146662676e-05,
      "loss": 0.1174,
      "step": 4685
    },
    {
      "epoch": 6.306864064602961,
      "grad_norm": 0.6493851542472839,
      "learning_rate": 5.992217898832685e-05,
      "loss": 0.096,
      "step": 4686
    },
    {
      "epoch": 6.30820995962315,
      "grad_norm": 0.8777436017990112,
      "learning_rate": 5.989224782999102e-05,
      "loss": 0.15,
      "step": 4687
    },
    {
      "epoch": 6.309555854643338,
      "grad_norm": 0.7286166548728943,
      "learning_rate": 5.98623166716552e-05,
      "loss": 0.1139,
      "step": 4688
    },
    {
      "epoch": 6.310901749663526,
      "grad_norm": 1.14163076877594,
      "learning_rate": 5.9832385513319366e-05,
      "loss": 0.1716,
      "step": 4689
    },
    {
      "epoch": 6.312247644683715,
      "grad_norm": 0.6059476733207703,
      "learning_rate": 5.980245435498354e-05,
      "loss": 0.1049,
      "step": 4690
    },
    {
      "epoch": 6.313593539703903,
      "grad_norm": 0.9723162055015564,
      "learning_rate": 5.977252319664771e-05,
      "loss": 0.1182,
      "step": 4691
    },
    {
      "epoch": 6.314939434724091,
      "grad_norm": 0.5506004691123962,
      "learning_rate": 5.974259203831189e-05,
      "loss": 0.1154,
      "step": 4692
    },
    {
      "epoch": 6.31628532974428,
      "grad_norm": 0.8905937671661377,
      "learning_rate": 5.9712660879976057e-05,
      "loss": 0.1715,
      "step": 4693
    },
    {
      "epoch": 6.317631224764469,
      "grad_norm": 0.5707568526268005,
      "learning_rate": 5.968272972164023e-05,
      "loss": 0.1141,
      "step": 4694
    },
    {
      "epoch": 6.318977119784657,
      "grad_norm": 0.9941897392272949,
      "learning_rate": 5.96527985633044e-05,
      "loss": 0.1244,
      "step": 4695
    },
    {
      "epoch": 6.3203230148048455,
      "grad_norm": 1.2865835428237915,
      "learning_rate": 5.962286740496858e-05,
      "loss": 0.1865,
      "step": 4696
    },
    {
      "epoch": 6.321668909825034,
      "grad_norm": 0.9202827215194702,
      "learning_rate": 5.959293624663275e-05,
      "loss": 0.1377,
      "step": 4697
    },
    {
      "epoch": 6.323014804845222,
      "grad_norm": 0.7488109469413757,
      "learning_rate": 5.956300508829692e-05,
      "loss": 0.1201,
      "step": 4698
    },
    {
      "epoch": 6.3243606998654105,
      "grad_norm": 0.8043507933616638,
      "learning_rate": 5.953307392996109e-05,
      "loss": 0.1436,
      "step": 4699
    },
    {
      "epoch": 6.325706594885599,
      "grad_norm": 2.1544148921966553,
      "learning_rate": 5.950314277162527e-05,
      "loss": 0.2307,
      "step": 4700
    },
    {
      "epoch": 6.327052489905787,
      "grad_norm": 0.5535166263580322,
      "learning_rate": 5.947321161328944e-05,
      "loss": 0.1181,
      "step": 4701
    },
    {
      "epoch": 6.3283983849259755,
      "grad_norm": 0.5310461521148682,
      "learning_rate": 5.9443280454953606e-05,
      "loss": 0.1269,
      "step": 4702
    },
    {
      "epoch": 6.329744279946164,
      "grad_norm": 0.821438193321228,
      "learning_rate": 5.941334929661778e-05,
      "loss": 0.1306,
      "step": 4703
    },
    {
      "epoch": 6.331090174966352,
      "grad_norm": 0.7970553040504456,
      "learning_rate": 5.938341813828195e-05,
      "loss": 0.1526,
      "step": 4704
    },
    {
      "epoch": 6.332436069986541,
      "grad_norm": 0.5910783410072327,
      "learning_rate": 5.935348697994613e-05,
      "loss": 0.1281,
      "step": 4705
    },
    {
      "epoch": 6.33378196500673,
      "grad_norm": 0.8542709350585938,
      "learning_rate": 5.9323555821610296e-05,
      "loss": 0.1552,
      "step": 4706
    },
    {
      "epoch": 6.335127860026918,
      "grad_norm": 0.6868414282798767,
      "learning_rate": 5.929362466327447e-05,
      "loss": 0.1222,
      "step": 4707
    },
    {
      "epoch": 6.336473755047106,
      "grad_norm": 0.6568849086761475,
      "learning_rate": 5.926369350493864e-05,
      "loss": 0.1024,
      "step": 4708
    },
    {
      "epoch": 6.337819650067295,
      "grad_norm": 1.1711276769638062,
      "learning_rate": 5.923376234660282e-05,
      "loss": 0.1484,
      "step": 4709
    },
    {
      "epoch": 6.339165545087483,
      "grad_norm": 0.6283777356147766,
      "learning_rate": 5.9203831188266986e-05,
      "loss": 0.1229,
      "step": 4710
    },
    {
      "epoch": 6.340511440107671,
      "grad_norm": 1.2720048427581787,
      "learning_rate": 5.917390002993116e-05,
      "loss": 0.1456,
      "step": 4711
    },
    {
      "epoch": 6.34185733512786,
      "grad_norm": 0.9873647093772888,
      "learning_rate": 5.914396887159533e-05,
      "loss": 0.1831,
      "step": 4712
    },
    {
      "epoch": 6.343203230148048,
      "grad_norm": 0.7763381600379944,
      "learning_rate": 5.911403771325951e-05,
      "loss": 0.1344,
      "step": 4713
    },
    {
      "epoch": 6.344549125168237,
      "grad_norm": 0.9691369533538818,
      "learning_rate": 5.9084106554923676e-05,
      "loss": 0.1459,
      "step": 4714
    },
    {
      "epoch": 6.345895020188426,
      "grad_norm": 0.9250697493553162,
      "learning_rate": 5.905417539658785e-05,
      "loss": 0.1182,
      "step": 4715
    },
    {
      "epoch": 6.347240915208614,
      "grad_norm": 1.2489666938781738,
      "learning_rate": 5.902424423825202e-05,
      "loss": 0.1997,
      "step": 4716
    },
    {
      "epoch": 6.348586810228802,
      "grad_norm": 0.7716193199157715,
      "learning_rate": 5.89943130799162e-05,
      "loss": 0.1364,
      "step": 4717
    },
    {
      "epoch": 6.349932705248991,
      "grad_norm": 0.561290979385376,
      "learning_rate": 5.8964381921580367e-05,
      "loss": 0.1281,
      "step": 4718
    },
    {
      "epoch": 6.351278600269179,
      "grad_norm": 0.645694375038147,
      "learning_rate": 5.893445076324454e-05,
      "loss": 0.1525,
      "step": 4719
    },
    {
      "epoch": 6.352624495289367,
      "grad_norm": 0.6271504759788513,
      "learning_rate": 5.890451960490871e-05,
      "loss": 0.1182,
      "step": 4720
    },
    {
      "epoch": 6.353970390309556,
      "grad_norm": 1.0448365211486816,
      "learning_rate": 5.887458844657289e-05,
      "loss": 0.1302,
      "step": 4721
    },
    {
      "epoch": 6.355316285329744,
      "grad_norm": 0.6921966075897217,
      "learning_rate": 5.884465728823706e-05,
      "loss": 0.1384,
      "step": 4722
    },
    {
      "epoch": 6.356662180349932,
      "grad_norm": 0.7778928875923157,
      "learning_rate": 5.881472612990123e-05,
      "loss": 0.1367,
      "step": 4723
    },
    {
      "epoch": 6.358008075370121,
      "grad_norm": 0.7581859230995178,
      "learning_rate": 5.87847949715654e-05,
      "loss": 0.1393,
      "step": 4724
    },
    {
      "epoch": 6.35935397039031,
      "grad_norm": 1.391185998916626,
      "learning_rate": 5.875486381322958e-05,
      "loss": 0.1879,
      "step": 4725
    },
    {
      "epoch": 6.360699865410498,
      "grad_norm": 1.0845736265182495,
      "learning_rate": 5.872493265489375e-05,
      "loss": 0.1427,
      "step": 4726
    },
    {
      "epoch": 6.362045760430687,
      "grad_norm": 1.0494431257247925,
      "learning_rate": 5.869500149655792e-05,
      "loss": 0.1799,
      "step": 4727
    },
    {
      "epoch": 6.363391655450875,
      "grad_norm": 0.7462733387947083,
      "learning_rate": 5.866507033822209e-05,
      "loss": 0.1369,
      "step": 4728
    },
    {
      "epoch": 6.364737550471063,
      "grad_norm": 0.8972724676132202,
      "learning_rate": 5.863513917988627e-05,
      "loss": 0.1641,
      "step": 4729
    },
    {
      "epoch": 6.366083445491252,
      "grad_norm": 0.7234984636306763,
      "learning_rate": 5.860520802155044e-05,
      "loss": 0.1241,
      "step": 4730
    },
    {
      "epoch": 6.36742934051144,
      "grad_norm": 1.4608813524246216,
      "learning_rate": 5.857527686321461e-05,
      "loss": 0.1372,
      "step": 4731
    },
    {
      "epoch": 6.368775235531628,
      "grad_norm": 0.7840924859046936,
      "learning_rate": 5.854534570487878e-05,
      "loss": 0.1175,
      "step": 4732
    },
    {
      "epoch": 6.370121130551817,
      "grad_norm": 0.8576868176460266,
      "learning_rate": 5.851541454654296e-05,
      "loss": 0.1521,
      "step": 4733
    },
    {
      "epoch": 6.371467025572006,
      "grad_norm": 1.2432974576950073,
      "learning_rate": 5.848548338820713e-05,
      "loss": 0.208,
      "step": 4734
    },
    {
      "epoch": 6.372812920592194,
      "grad_norm": 0.8412240147590637,
      "learning_rate": 5.84555522298713e-05,
      "loss": 0.1365,
      "step": 4735
    },
    {
      "epoch": 6.3741588156123825,
      "grad_norm": 1.2291826009750366,
      "learning_rate": 5.842562107153547e-05,
      "loss": 0.2079,
      "step": 4736
    },
    {
      "epoch": 6.375504710632571,
      "grad_norm": 0.5048710703849792,
      "learning_rate": 5.839568991319965e-05,
      "loss": 0.1091,
      "step": 4737
    },
    {
      "epoch": 6.376850605652759,
      "grad_norm": 0.7556487321853638,
      "learning_rate": 5.836575875486382e-05,
      "loss": 0.1372,
      "step": 4738
    },
    {
      "epoch": 6.3781965006729475,
      "grad_norm": 0.6552051901817322,
      "learning_rate": 5.833582759652799e-05,
      "loss": 0.1167,
      "step": 4739
    },
    {
      "epoch": 6.379542395693136,
      "grad_norm": 0.6942342519760132,
      "learning_rate": 5.830589643819216e-05,
      "loss": 0.1286,
      "step": 4740
    },
    {
      "epoch": 6.380888290713324,
      "grad_norm": 1.1390002965927124,
      "learning_rate": 5.827596527985634e-05,
      "loss": 0.1743,
      "step": 4741
    },
    {
      "epoch": 6.3822341857335125,
      "grad_norm": 0.9121243357658386,
      "learning_rate": 5.824603412152051e-05,
      "loss": 0.1446,
      "step": 4742
    },
    {
      "epoch": 6.383580080753701,
      "grad_norm": 1.3447370529174805,
      "learning_rate": 5.821610296318468e-05,
      "loss": 0.1579,
      "step": 4743
    },
    {
      "epoch": 6.384925975773889,
      "grad_norm": 0.8374789953231812,
      "learning_rate": 5.818617180484885e-05,
      "loss": 0.1394,
      "step": 4744
    },
    {
      "epoch": 6.386271870794078,
      "grad_norm": 0.49648404121398926,
      "learning_rate": 5.815624064651303e-05,
      "loss": 0.093,
      "step": 4745
    },
    {
      "epoch": 6.387617765814267,
      "grad_norm": 0.5842772126197815,
      "learning_rate": 5.81263094881772e-05,
      "loss": 0.1068,
      "step": 4746
    },
    {
      "epoch": 6.388963660834455,
      "grad_norm": 0.7942135334014893,
      "learning_rate": 5.8096378329841374e-05,
      "loss": 0.1617,
      "step": 4747
    },
    {
      "epoch": 6.390309555854643,
      "grad_norm": 1.0880805253982544,
      "learning_rate": 5.806644717150554e-05,
      "loss": 0.1982,
      "step": 4748
    },
    {
      "epoch": 6.391655450874832,
      "grad_norm": 0.9840410351753235,
      "learning_rate": 5.803651601316972e-05,
      "loss": 0.1505,
      "step": 4749
    },
    {
      "epoch": 6.39300134589502,
      "grad_norm": 0.9068461656570435,
      "learning_rate": 5.800658485483389e-05,
      "loss": 0.1569,
      "step": 4750
    },
    {
      "epoch": 6.3943472409152085,
      "grad_norm": 0.6060565114021301,
      "learning_rate": 5.7976653696498064e-05,
      "loss": 0.1067,
      "step": 4751
    },
    {
      "epoch": 6.395693135935397,
      "grad_norm": 1.0640052556991577,
      "learning_rate": 5.794672253816223e-05,
      "loss": 0.1818,
      "step": 4752
    },
    {
      "epoch": 6.397039030955585,
      "grad_norm": 0.6390401721000671,
      "learning_rate": 5.791679137982641e-05,
      "loss": 0.1333,
      "step": 4753
    },
    {
      "epoch": 6.398384925975774,
      "grad_norm": 0.6859819293022156,
      "learning_rate": 5.788686022149058e-05,
      "loss": 0.1192,
      "step": 4754
    },
    {
      "epoch": 6.399730820995963,
      "grad_norm": 0.8817535638809204,
      "learning_rate": 5.7856929063154754e-05,
      "loss": 0.1414,
      "step": 4755
    },
    {
      "epoch": 6.401076716016151,
      "grad_norm": 0.5851075053215027,
      "learning_rate": 5.782699790481892e-05,
      "loss": 0.1539,
      "step": 4756
    },
    {
      "epoch": 6.402422611036339,
      "grad_norm": 0.730076253414154,
      "learning_rate": 5.7797066746483085e-05,
      "loss": 0.1386,
      "step": 4757
    },
    {
      "epoch": 6.403768506056528,
      "grad_norm": 0.7672093510627747,
      "learning_rate": 5.776713558814726e-05,
      "loss": 0.1573,
      "step": 4758
    },
    {
      "epoch": 6.405114401076716,
      "grad_norm": 1.0356634855270386,
      "learning_rate": 5.773720442981143e-05,
      "loss": 0.2083,
      "step": 4759
    },
    {
      "epoch": 6.406460296096904,
      "grad_norm": 0.7035526633262634,
      "learning_rate": 5.7707273271475606e-05,
      "loss": 0.1561,
      "step": 4760
    },
    {
      "epoch": 6.407806191117093,
      "grad_norm": 0.5838502645492554,
      "learning_rate": 5.7677342113139775e-05,
      "loss": 0.1163,
      "step": 4761
    },
    {
      "epoch": 6.409152086137281,
      "grad_norm": 0.974826455116272,
      "learning_rate": 5.764741095480395e-05,
      "loss": 0.1414,
      "step": 4762
    },
    {
      "epoch": 6.410497981157469,
      "grad_norm": 0.666853129863739,
      "learning_rate": 5.761747979646812e-05,
      "loss": 0.1455,
      "step": 4763
    },
    {
      "epoch": 6.411843876177658,
      "grad_norm": 1.004955768585205,
      "learning_rate": 5.7587548638132296e-05,
      "loss": 0.16,
      "step": 4764
    },
    {
      "epoch": 6.413189771197847,
      "grad_norm": 0.7740733027458191,
      "learning_rate": 5.7557617479796466e-05,
      "loss": 0.1527,
      "step": 4765
    },
    {
      "epoch": 6.414535666218035,
      "grad_norm": 0.7179692983627319,
      "learning_rate": 5.752768632146064e-05,
      "loss": 0.1527,
      "step": 4766
    },
    {
      "epoch": 6.415881561238224,
      "grad_norm": 1.049228310585022,
      "learning_rate": 5.749775516312481e-05,
      "loss": 0.1534,
      "step": 4767
    },
    {
      "epoch": 6.417227456258412,
      "grad_norm": 0.9644225835800171,
      "learning_rate": 5.746782400478899e-05,
      "loss": 0.1517,
      "step": 4768
    },
    {
      "epoch": 6.4185733512786,
      "grad_norm": 1.1193665266036987,
      "learning_rate": 5.7437892846453156e-05,
      "loss": 0.1769,
      "step": 4769
    },
    {
      "epoch": 6.419919246298789,
      "grad_norm": 0.819644570350647,
      "learning_rate": 5.740796168811733e-05,
      "loss": 0.134,
      "step": 4770
    },
    {
      "epoch": 6.421265141318977,
      "grad_norm": 0.8832932710647583,
      "learning_rate": 5.73780305297815e-05,
      "loss": 0.1392,
      "step": 4771
    },
    {
      "epoch": 6.422611036339165,
      "grad_norm": 0.8314943313598633,
      "learning_rate": 5.734809937144568e-05,
      "loss": 0.1383,
      "step": 4772
    },
    {
      "epoch": 6.423956931359354,
      "grad_norm": 0.7736491560935974,
      "learning_rate": 5.7318168213109846e-05,
      "loss": 0.1348,
      "step": 4773
    },
    {
      "epoch": 6.425302826379543,
      "grad_norm": 0.6111149787902832,
      "learning_rate": 5.728823705477402e-05,
      "loss": 0.1225,
      "step": 4774
    },
    {
      "epoch": 6.426648721399731,
      "grad_norm": 0.8495790958404541,
      "learning_rate": 5.725830589643819e-05,
      "loss": 0.1494,
      "step": 4775
    },
    {
      "epoch": 6.4279946164199195,
      "grad_norm": 0.9040073752403259,
      "learning_rate": 5.722837473810237e-05,
      "loss": 0.1346,
      "step": 4776
    },
    {
      "epoch": 6.429340511440108,
      "grad_norm": 0.5832589864730835,
      "learning_rate": 5.7198443579766536e-05,
      "loss": 0.1129,
      "step": 4777
    },
    {
      "epoch": 6.430686406460296,
      "grad_norm": 1.0344252586364746,
      "learning_rate": 5.716851242143071e-05,
      "loss": 0.22,
      "step": 4778
    },
    {
      "epoch": 6.4320323014804845,
      "grad_norm": 0.5016440749168396,
      "learning_rate": 5.713858126309488e-05,
      "loss": 0.0869,
      "step": 4779
    },
    {
      "epoch": 6.433378196500673,
      "grad_norm": 0.8986403942108154,
      "learning_rate": 5.710865010475906e-05,
      "loss": 0.1277,
      "step": 4780
    },
    {
      "epoch": 6.434724091520861,
      "grad_norm": 0.6110129356384277,
      "learning_rate": 5.7078718946423226e-05,
      "loss": 0.1412,
      "step": 4781
    },
    {
      "epoch": 6.43606998654105,
      "grad_norm": 0.6976105570793152,
      "learning_rate": 5.70487877880874e-05,
      "loss": 0.141,
      "step": 4782
    },
    {
      "epoch": 6.437415881561238,
      "grad_norm": 0.8243070244789124,
      "learning_rate": 5.701885662975157e-05,
      "loss": 0.1033,
      "step": 4783
    },
    {
      "epoch": 6.438761776581426,
      "grad_norm": 0.969728410243988,
      "learning_rate": 5.698892547141575e-05,
      "loss": 0.1662,
      "step": 4784
    },
    {
      "epoch": 6.4401076716016155,
      "grad_norm": 1.0326460599899292,
      "learning_rate": 5.6958994313079916e-05,
      "loss": 0.1643,
      "step": 4785
    },
    {
      "epoch": 6.441453566621804,
      "grad_norm": 0.9845210909843445,
      "learning_rate": 5.692906315474409e-05,
      "loss": 0.1424,
      "step": 4786
    },
    {
      "epoch": 6.442799461641992,
      "grad_norm": 1.1330828666687012,
      "learning_rate": 5.689913199640826e-05,
      "loss": 0.2187,
      "step": 4787
    },
    {
      "epoch": 6.4441453566621805,
      "grad_norm": 0.7456565499305725,
      "learning_rate": 5.686920083807243e-05,
      "loss": 0.1685,
      "step": 4788
    },
    {
      "epoch": 6.445491251682369,
      "grad_norm": 0.4346097409725189,
      "learning_rate": 5.6839269679736607e-05,
      "loss": 0.0935,
      "step": 4789
    },
    {
      "epoch": 6.446837146702557,
      "grad_norm": 1.2196568250656128,
      "learning_rate": 5.6809338521400776e-05,
      "loss": 0.1953,
      "step": 4790
    },
    {
      "epoch": 6.4481830417227455,
      "grad_norm": 0.9979850053787231,
      "learning_rate": 5.677940736306495e-05,
      "loss": 0.1563,
      "step": 4791
    },
    {
      "epoch": 6.449528936742934,
      "grad_norm": 0.9633206725120544,
      "learning_rate": 5.674947620472912e-05,
      "loss": 0.1405,
      "step": 4792
    },
    {
      "epoch": 6.450874831763122,
      "grad_norm": 0.6286495327949524,
      "learning_rate": 5.67195450463933e-05,
      "loss": 0.1161,
      "step": 4793
    },
    {
      "epoch": 6.4522207267833105,
      "grad_norm": 0.5766083598136902,
      "learning_rate": 5.6689613888057466e-05,
      "loss": 0.107,
      "step": 4794
    },
    {
      "epoch": 6.4535666218035,
      "grad_norm": 0.6886235475540161,
      "learning_rate": 5.665968272972164e-05,
      "loss": 0.1198,
      "step": 4795
    },
    {
      "epoch": 6.454912516823688,
      "grad_norm": 1.1108887195587158,
      "learning_rate": 5.662975157138581e-05,
      "loss": 0.1261,
      "step": 4796
    },
    {
      "epoch": 6.456258411843876,
      "grad_norm": 0.8497651219367981,
      "learning_rate": 5.659982041304999e-05,
      "loss": 0.1814,
      "step": 4797
    },
    {
      "epoch": 6.457604306864065,
      "grad_norm": 0.5823066830635071,
      "learning_rate": 5.6569889254714156e-05,
      "loss": 0.1023,
      "step": 4798
    },
    {
      "epoch": 6.458950201884253,
      "grad_norm": 0.7500326037406921,
      "learning_rate": 5.653995809637833e-05,
      "loss": 0.1386,
      "step": 4799
    },
    {
      "epoch": 6.460296096904441,
      "grad_norm": 0.7907665371894836,
      "learning_rate": 5.65100269380425e-05,
      "loss": 0.1315,
      "step": 4800
    },
    {
      "epoch": 6.46164199192463,
      "grad_norm": 0.7757927179336548,
      "learning_rate": 5.648009577970668e-05,
      "loss": 0.1178,
      "step": 4801
    },
    {
      "epoch": 6.462987886944818,
      "grad_norm": 0.8873041272163391,
      "learning_rate": 5.6450164621370846e-05,
      "loss": 0.1574,
      "step": 4802
    },
    {
      "epoch": 6.464333781965006,
      "grad_norm": 0.9609248638153076,
      "learning_rate": 5.642023346303502e-05,
      "loss": 0.1514,
      "step": 4803
    },
    {
      "epoch": 6.465679676985195,
      "grad_norm": 0.6825739145278931,
      "learning_rate": 5.639030230469919e-05,
      "loss": 0.1245,
      "step": 4804
    },
    {
      "epoch": 6.467025572005384,
      "grad_norm": 0.6938872933387756,
      "learning_rate": 5.636037114636337e-05,
      "loss": 0.1298,
      "step": 4805
    },
    {
      "epoch": 6.468371467025572,
      "grad_norm": 1.0407822132110596,
      "learning_rate": 5.6330439988027536e-05,
      "loss": 0.1536,
      "step": 4806
    },
    {
      "epoch": 6.469717362045761,
      "grad_norm": 0.8674576282501221,
      "learning_rate": 5.630050882969171e-05,
      "loss": 0.136,
      "step": 4807
    },
    {
      "epoch": 6.471063257065949,
      "grad_norm": 0.6496952772140503,
      "learning_rate": 5.627057767135588e-05,
      "loss": 0.1419,
      "step": 4808
    },
    {
      "epoch": 6.472409152086137,
      "grad_norm": 0.9024211168289185,
      "learning_rate": 5.624064651302006e-05,
      "loss": 0.1503,
      "step": 4809
    },
    {
      "epoch": 6.473755047106326,
      "grad_norm": 0.5721644163131714,
      "learning_rate": 5.6210715354684226e-05,
      "loss": 0.1177,
      "step": 4810
    },
    {
      "epoch": 6.475100942126514,
      "grad_norm": 0.7109239101409912,
      "learning_rate": 5.61807841963484e-05,
      "loss": 0.1239,
      "step": 4811
    },
    {
      "epoch": 6.476446837146702,
      "grad_norm": 0.7083527445793152,
      "learning_rate": 5.615085303801257e-05,
      "loss": 0.1063,
      "step": 4812
    },
    {
      "epoch": 6.477792732166891,
      "grad_norm": 0.9645434021949768,
      "learning_rate": 5.612092187967675e-05,
      "loss": 0.146,
      "step": 4813
    },
    {
      "epoch": 6.479138627187079,
      "grad_norm": 0.663784384727478,
      "learning_rate": 5.6090990721340917e-05,
      "loss": 0.1271,
      "step": 4814
    },
    {
      "epoch": 6.480484522207268,
      "grad_norm": 1.0648903846740723,
      "learning_rate": 5.606105956300509e-05,
      "loss": 0.1468,
      "step": 4815
    },
    {
      "epoch": 6.481830417227457,
      "grad_norm": 0.7481698393821716,
      "learning_rate": 5.603112840466926e-05,
      "loss": 0.1344,
      "step": 4816
    },
    {
      "epoch": 6.483176312247645,
      "grad_norm": 0.9052632451057434,
      "learning_rate": 5.600119724633344e-05,
      "loss": 0.1158,
      "step": 4817
    },
    {
      "epoch": 6.484522207267833,
      "grad_norm": 1.0571409463882446,
      "learning_rate": 5.597126608799761e-05,
      "loss": 0.1694,
      "step": 4818
    },
    {
      "epoch": 6.485868102288022,
      "grad_norm": 0.7060638070106506,
      "learning_rate": 5.594133492966178e-05,
      "loss": 0.1232,
      "step": 4819
    },
    {
      "epoch": 6.48721399730821,
      "grad_norm": 1.538055419921875,
      "learning_rate": 5.591140377132595e-05,
      "loss": 0.2038,
      "step": 4820
    },
    {
      "epoch": 6.488559892328398,
      "grad_norm": 0.9070025086402893,
      "learning_rate": 5.588147261299013e-05,
      "loss": 0.1337,
      "step": 4821
    },
    {
      "epoch": 6.489905787348587,
      "grad_norm": 0.8277075290679932,
      "learning_rate": 5.58515414546543e-05,
      "loss": 0.1446,
      "step": 4822
    },
    {
      "epoch": 6.491251682368775,
      "grad_norm": 1.2651475667953491,
      "learning_rate": 5.582161029631847e-05,
      "loss": 0.1382,
      "step": 4823
    },
    {
      "epoch": 6.492597577388963,
      "grad_norm": 0.7633070349693298,
      "learning_rate": 5.579167913798264e-05,
      "loss": 0.1161,
      "step": 4824
    },
    {
      "epoch": 6.4939434724091525,
      "grad_norm": 0.7951033711433411,
      "learning_rate": 5.576174797964682e-05,
      "loss": 0.1384,
      "step": 4825
    },
    {
      "epoch": 6.495289367429341,
      "grad_norm": 0.7057631611824036,
      "learning_rate": 5.573181682131099e-05,
      "loss": 0.1228,
      "step": 4826
    },
    {
      "epoch": 6.496635262449529,
      "grad_norm": 0.6452338695526123,
      "learning_rate": 5.570188566297516e-05,
      "loss": 0.1076,
      "step": 4827
    },
    {
      "epoch": 6.4979811574697175,
      "grad_norm": 0.7507243752479553,
      "learning_rate": 5.567195450463933e-05,
      "loss": 0.1304,
      "step": 4828
    },
    {
      "epoch": 6.499327052489906,
      "grad_norm": 1.134068489074707,
      "learning_rate": 5.564202334630351e-05,
      "loss": 0.2813,
      "step": 4829
    },
    {
      "epoch": 6.500672947510094,
      "grad_norm": 0.7240986824035645,
      "learning_rate": 5.561209218796768e-05,
      "loss": 0.1323,
      "step": 4830
    },
    {
      "epoch": 6.5020188425302825,
      "grad_norm": 0.7996537089347839,
      "learning_rate": 5.558216102963185e-05,
      "loss": 0.128,
      "step": 4831
    },
    {
      "epoch": 6.503364737550471,
      "grad_norm": 0.9781081080436707,
      "learning_rate": 5.555222987129602e-05,
      "loss": 0.1409,
      "step": 4832
    },
    {
      "epoch": 6.504710632570659,
      "grad_norm": 0.9039422869682312,
      "learning_rate": 5.55222987129602e-05,
      "loss": 0.131,
      "step": 4833
    },
    {
      "epoch": 6.506056527590848,
      "grad_norm": 0.8492863774299622,
      "learning_rate": 5.549236755462437e-05,
      "loss": 0.1502,
      "step": 4834
    },
    {
      "epoch": 6.507402422611037,
      "grad_norm": 0.9429156184196472,
      "learning_rate": 5.546243639628854e-05,
      "loss": 0.1571,
      "step": 4835
    },
    {
      "epoch": 6.508748317631225,
      "grad_norm": 0.7085637450218201,
      "learning_rate": 5.543250523795271e-05,
      "loss": 0.1381,
      "step": 4836
    },
    {
      "epoch": 6.510094212651413,
      "grad_norm": 0.8120570778846741,
      "learning_rate": 5.540257407961689e-05,
      "loss": 0.1348,
      "step": 4837
    },
    {
      "epoch": 6.511440107671602,
      "grad_norm": 0.6111083030700684,
      "learning_rate": 5.537264292128106e-05,
      "loss": 0.1194,
      "step": 4838
    },
    {
      "epoch": 6.51278600269179,
      "grad_norm": 1.2504256963729858,
      "learning_rate": 5.534271176294523e-05,
      "loss": 0.2013,
      "step": 4839
    },
    {
      "epoch": 6.514131897711978,
      "grad_norm": 0.8685964941978455,
      "learning_rate": 5.53127806046094e-05,
      "loss": 0.1455,
      "step": 4840
    },
    {
      "epoch": 6.515477792732167,
      "grad_norm": 0.8073166608810425,
      "learning_rate": 5.528284944627358e-05,
      "loss": 0.1285,
      "step": 4841
    },
    {
      "epoch": 6.516823687752355,
      "grad_norm": 0.7043840885162354,
      "learning_rate": 5.525291828793775e-05,
      "loss": 0.1301,
      "step": 4842
    },
    {
      "epoch": 6.518169582772543,
      "grad_norm": 0.9474307298660278,
      "learning_rate": 5.5222987129601924e-05,
      "loss": 0.1383,
      "step": 4843
    },
    {
      "epoch": 6.519515477792732,
      "grad_norm": 0.8859505653381348,
      "learning_rate": 5.519305597126609e-05,
      "loss": 0.1298,
      "step": 4844
    },
    {
      "epoch": 6.52086137281292,
      "grad_norm": 1.2175325155258179,
      "learning_rate": 5.516312481293027e-05,
      "loss": 0.1445,
      "step": 4845
    },
    {
      "epoch": 6.522207267833109,
      "grad_norm": 0.7562782764434814,
      "learning_rate": 5.513319365459444e-05,
      "loss": 0.129,
      "step": 4846
    },
    {
      "epoch": 6.523553162853298,
      "grad_norm": 0.6917371153831482,
      "learning_rate": 5.5103262496258614e-05,
      "loss": 0.092,
      "step": 4847
    },
    {
      "epoch": 6.524899057873486,
      "grad_norm": 0.9506335258483887,
      "learning_rate": 5.507333133792278e-05,
      "loss": 0.1549,
      "step": 4848
    },
    {
      "epoch": 6.526244952893674,
      "grad_norm": 1.217268943786621,
      "learning_rate": 5.504340017958696e-05,
      "loss": 0.3134,
      "step": 4849
    },
    {
      "epoch": 6.527590847913863,
      "grad_norm": 0.8781557679176331,
      "learning_rate": 5.501346902125113e-05,
      "loss": 0.1271,
      "step": 4850
    },
    {
      "epoch": 6.528936742934051,
      "grad_norm": 0.7505413889884949,
      "learning_rate": 5.4983537862915304e-05,
      "loss": 0.1282,
      "step": 4851
    },
    {
      "epoch": 6.530282637954239,
      "grad_norm": 0.9998317360877991,
      "learning_rate": 5.495360670457947e-05,
      "loss": 0.1633,
      "step": 4852
    },
    {
      "epoch": 6.531628532974428,
      "grad_norm": 0.9381672143936157,
      "learning_rate": 5.492367554624365e-05,
      "loss": 0.2058,
      "step": 4853
    },
    {
      "epoch": 6.532974427994617,
      "grad_norm": 0.7547744512557983,
      "learning_rate": 5.489374438790782e-05,
      "loss": 0.1555,
      "step": 4854
    },
    {
      "epoch": 6.534320323014805,
      "grad_norm": 0.9340147376060486,
      "learning_rate": 5.4863813229571994e-05,
      "loss": 0.1804,
      "step": 4855
    },
    {
      "epoch": 6.535666218034994,
      "grad_norm": 0.5837389230728149,
      "learning_rate": 5.483388207123616e-05,
      "loss": 0.1396,
      "step": 4856
    },
    {
      "epoch": 6.537012113055182,
      "grad_norm": 0.7259634137153625,
      "learning_rate": 5.480395091290034e-05,
      "loss": 0.1356,
      "step": 4857
    },
    {
      "epoch": 6.53835800807537,
      "grad_norm": 0.6199390292167664,
      "learning_rate": 5.477401975456451e-05,
      "loss": 0.1114,
      "step": 4858
    },
    {
      "epoch": 6.539703903095559,
      "grad_norm": 0.8059371709823608,
      "learning_rate": 5.4744088596228684e-05,
      "loss": 0.1577,
      "step": 4859
    },
    {
      "epoch": 6.541049798115747,
      "grad_norm": 0.6614275574684143,
      "learning_rate": 5.471415743789285e-05,
      "loss": 0.1316,
      "step": 4860
    },
    {
      "epoch": 6.542395693135935,
      "grad_norm": 1.0923231840133667,
      "learning_rate": 5.4684226279557016e-05,
      "loss": 0.1524,
      "step": 4861
    },
    {
      "epoch": 6.543741588156124,
      "grad_norm": 0.9881851077079773,
      "learning_rate": 5.465429512122119e-05,
      "loss": 0.159,
      "step": 4862
    },
    {
      "epoch": 6.545087483176312,
      "grad_norm": 0.8490166068077087,
      "learning_rate": 5.462436396288536e-05,
      "loss": 0.1305,
      "step": 4863
    },
    {
      "epoch": 6.5464333781965,
      "grad_norm": 0.6202622652053833,
      "learning_rate": 5.459443280454954e-05,
      "loss": 0.1341,
      "step": 4864
    },
    {
      "epoch": 6.547779273216689,
      "grad_norm": 0.7483165264129639,
      "learning_rate": 5.4564501646213706e-05,
      "loss": 0.1553,
      "step": 4865
    },
    {
      "epoch": 6.549125168236878,
      "grad_norm": 0.7677620053291321,
      "learning_rate": 5.453457048787788e-05,
      "loss": 0.1387,
      "step": 4866
    },
    {
      "epoch": 6.550471063257066,
      "grad_norm": 0.7551449537277222,
      "learning_rate": 5.450463932954205e-05,
      "loss": 0.1257,
      "step": 4867
    },
    {
      "epoch": 6.5518169582772545,
      "grad_norm": 2.047579765319824,
      "learning_rate": 5.447470817120623e-05,
      "loss": 0.1463,
      "step": 4868
    },
    {
      "epoch": 6.553162853297443,
      "grad_norm": 1.1849361658096313,
      "learning_rate": 5.4444777012870396e-05,
      "loss": 0.1711,
      "step": 4869
    },
    {
      "epoch": 6.554508748317631,
      "grad_norm": 0.6854761242866516,
      "learning_rate": 5.441484585453457e-05,
      "loss": 0.1554,
      "step": 4870
    },
    {
      "epoch": 6.5558546433378195,
      "grad_norm": 0.9883361458778381,
      "learning_rate": 5.438491469619874e-05,
      "loss": 0.1429,
      "step": 4871
    },
    {
      "epoch": 6.557200538358008,
      "grad_norm": 0.9909922480583191,
      "learning_rate": 5.435498353786292e-05,
      "loss": 0.1775,
      "step": 4872
    },
    {
      "epoch": 6.558546433378196,
      "grad_norm": 0.7948686480522156,
      "learning_rate": 5.4325052379527086e-05,
      "loss": 0.1401,
      "step": 4873
    },
    {
      "epoch": 6.5598923283983845,
      "grad_norm": 1.075819730758667,
      "learning_rate": 5.4295121221191255e-05,
      "loss": 0.1662,
      "step": 4874
    },
    {
      "epoch": 6.561238223418574,
      "grad_norm": 0.5121909379959106,
      "learning_rate": 5.426519006285543e-05,
      "loss": 0.1086,
      "step": 4875
    },
    {
      "epoch": 6.562584118438762,
      "grad_norm": 0.7003293633460999,
      "learning_rate": 5.42352589045196e-05,
      "loss": 0.1196,
      "step": 4876
    },
    {
      "epoch": 6.56393001345895,
      "grad_norm": 0.47928479313850403,
      "learning_rate": 5.4205327746183776e-05,
      "loss": 0.1001,
      "step": 4877
    },
    {
      "epoch": 6.565275908479139,
      "grad_norm": 0.8860291242599487,
      "learning_rate": 5.4175396587847945e-05,
      "loss": 0.1726,
      "step": 4878
    },
    {
      "epoch": 6.566621803499327,
      "grad_norm": 0.8207228183746338,
      "learning_rate": 5.414546542951212e-05,
      "loss": 0.1521,
      "step": 4879
    },
    {
      "epoch": 6.5679676985195155,
      "grad_norm": 1.4396177530288696,
      "learning_rate": 5.411553427117629e-05,
      "loss": 0.1332,
      "step": 4880
    },
    {
      "epoch": 6.569313593539704,
      "grad_norm": 0.7409277558326721,
      "learning_rate": 5.4085603112840466e-05,
      "loss": 0.1535,
      "step": 4881
    },
    {
      "epoch": 6.570659488559892,
      "grad_norm": 0.8341748714447021,
      "learning_rate": 5.4055671954504636e-05,
      "loss": 0.1594,
      "step": 4882
    },
    {
      "epoch": 6.5720053835800805,
      "grad_norm": 1.1258578300476074,
      "learning_rate": 5.402574079616881e-05,
      "loss": 0.2207,
      "step": 4883
    },
    {
      "epoch": 6.573351278600269,
      "grad_norm": 0.6902353763580322,
      "learning_rate": 5.399580963783298e-05,
      "loss": 0.1513,
      "step": 4884
    },
    {
      "epoch": 6.574697173620457,
      "grad_norm": 1.525800108909607,
      "learning_rate": 5.3965878479497157e-05,
      "loss": 0.1816,
      "step": 4885
    },
    {
      "epoch": 6.576043068640646,
      "grad_norm": 0.9404891729354858,
      "learning_rate": 5.3935947321161326e-05,
      "loss": 0.1396,
      "step": 4886
    },
    {
      "epoch": 6.577388963660835,
      "grad_norm": 0.9890981316566467,
      "learning_rate": 5.39060161628255e-05,
      "loss": 0.1635,
      "step": 4887
    },
    {
      "epoch": 6.578734858681023,
      "grad_norm": 0.7160295248031616,
      "learning_rate": 5.387608500448967e-05,
      "loss": 0.1434,
      "step": 4888
    },
    {
      "epoch": 6.580080753701211,
      "grad_norm": 0.8445749878883362,
      "learning_rate": 5.384615384615385e-05,
      "loss": 0.1674,
      "step": 4889
    },
    {
      "epoch": 6.5814266487214,
      "grad_norm": 0.9033368825912476,
      "learning_rate": 5.3816222687818016e-05,
      "loss": 0.1498,
      "step": 4890
    },
    {
      "epoch": 6.582772543741588,
      "grad_norm": 0.8522064089775085,
      "learning_rate": 5.378629152948219e-05,
      "loss": 0.166,
      "step": 4891
    },
    {
      "epoch": 6.584118438761776,
      "grad_norm": 0.8671964406967163,
      "learning_rate": 5.375636037114636e-05,
      "loss": 0.1262,
      "step": 4892
    },
    {
      "epoch": 6.585464333781965,
      "grad_norm": 1.0546042919158936,
      "learning_rate": 5.372642921281054e-05,
      "loss": 0.1355,
      "step": 4893
    },
    {
      "epoch": 6.586810228802153,
      "grad_norm": 0.6872749924659729,
      "learning_rate": 5.3696498054474706e-05,
      "loss": 0.1154,
      "step": 4894
    },
    {
      "epoch": 6.588156123822342,
      "grad_norm": 0.6047090888023376,
      "learning_rate": 5.366656689613888e-05,
      "loss": 0.116,
      "step": 4895
    },
    {
      "epoch": 6.589502018842531,
      "grad_norm": 0.8048008680343628,
      "learning_rate": 5.363663573780305e-05,
      "loss": 0.1655,
      "step": 4896
    },
    {
      "epoch": 6.590847913862719,
      "grad_norm": 0.6853356957435608,
      "learning_rate": 5.360670457946723e-05,
      "loss": 0.1254,
      "step": 4897
    },
    {
      "epoch": 6.592193808882907,
      "grad_norm": 0.5872986316680908,
      "learning_rate": 5.3576773421131396e-05,
      "loss": 0.208,
      "step": 4898
    },
    {
      "epoch": 6.593539703903096,
      "grad_norm": 0.8137908577919006,
      "learning_rate": 5.354684226279557e-05,
      "loss": 0.154,
      "step": 4899
    },
    {
      "epoch": 6.594885598923284,
      "grad_norm": 0.8089746832847595,
      "learning_rate": 5.351691110445974e-05,
      "loss": 0.1206,
      "step": 4900
    },
    {
      "epoch": 6.596231493943472,
      "grad_norm": 0.987352728843689,
      "learning_rate": 5.348697994612392e-05,
      "loss": 0.1548,
      "step": 4901
    },
    {
      "epoch": 6.597577388963661,
      "grad_norm": 0.911464273929596,
      "learning_rate": 5.3457048787788086e-05,
      "loss": 0.1954,
      "step": 4902
    },
    {
      "epoch": 6.598923283983849,
      "grad_norm": 1.15366792678833,
      "learning_rate": 5.342711762945226e-05,
      "loss": 0.1382,
      "step": 4903
    },
    {
      "epoch": 6.600269179004037,
      "grad_norm": 0.966915488243103,
      "learning_rate": 5.339718647111643e-05,
      "loss": 0.1356,
      "step": 4904
    },
    {
      "epoch": 6.601615074024226,
      "grad_norm": 1.0575610399246216,
      "learning_rate": 5.336725531278061e-05,
      "loss": 0.148,
      "step": 4905
    },
    {
      "epoch": 6.602960969044415,
      "grad_norm": 0.926020085811615,
      "learning_rate": 5.3337324154444776e-05,
      "loss": 0.1433,
      "step": 4906
    },
    {
      "epoch": 6.604306864064603,
      "grad_norm": 0.8253908753395081,
      "learning_rate": 5.330739299610895e-05,
      "loss": 0.1332,
      "step": 4907
    },
    {
      "epoch": 6.6056527590847915,
      "grad_norm": 0.6330381035804749,
      "learning_rate": 5.327746183777312e-05,
      "loss": 0.1247,
      "step": 4908
    },
    {
      "epoch": 6.60699865410498,
      "grad_norm": 1.005540370941162,
      "learning_rate": 5.32475306794373e-05,
      "loss": 0.1987,
      "step": 4909
    },
    {
      "epoch": 6.608344549125168,
      "grad_norm": 1.09758722782135,
      "learning_rate": 5.3217599521101467e-05,
      "loss": 0.191,
      "step": 4910
    },
    {
      "epoch": 6.609690444145357,
      "grad_norm": 0.9827168583869934,
      "learning_rate": 5.318766836276564e-05,
      "loss": 0.1705,
      "step": 4911
    },
    {
      "epoch": 6.611036339165545,
      "grad_norm": 0.9432363510131836,
      "learning_rate": 5.315773720442981e-05,
      "loss": 0.1419,
      "step": 4912
    },
    {
      "epoch": 6.612382234185733,
      "grad_norm": 0.8747667074203491,
      "learning_rate": 5.312780604609399e-05,
      "loss": 0.124,
      "step": 4913
    },
    {
      "epoch": 6.613728129205922,
      "grad_norm": 0.7323288321495056,
      "learning_rate": 5.309787488775816e-05,
      "loss": 0.1197,
      "step": 4914
    },
    {
      "epoch": 6.615074024226111,
      "grad_norm": 0.752846896648407,
      "learning_rate": 5.306794372942233e-05,
      "loss": 0.1171,
      "step": 4915
    },
    {
      "epoch": 6.616419919246299,
      "grad_norm": 0.8960864543914795,
      "learning_rate": 5.30380125710865e-05,
      "loss": 0.1552,
      "step": 4916
    },
    {
      "epoch": 6.6177658142664875,
      "grad_norm": 0.7249034643173218,
      "learning_rate": 5.300808141275068e-05,
      "loss": 0.1448,
      "step": 4917
    },
    {
      "epoch": 6.619111709286676,
      "grad_norm": 0.6493715047836304,
      "learning_rate": 5.297815025441485e-05,
      "loss": 0.1,
      "step": 4918
    },
    {
      "epoch": 6.620457604306864,
      "grad_norm": 0.6103866696357727,
      "learning_rate": 5.294821909607902e-05,
      "loss": 0.1312,
      "step": 4919
    },
    {
      "epoch": 6.6218034993270525,
      "grad_norm": 0.561945378780365,
      "learning_rate": 5.291828793774319e-05,
      "loss": 0.1244,
      "step": 4920
    },
    {
      "epoch": 6.623149394347241,
      "grad_norm": 0.7714282870292664,
      "learning_rate": 5.288835677940737e-05,
      "loss": 0.1612,
      "step": 4921
    },
    {
      "epoch": 6.624495289367429,
      "grad_norm": 0.9494754076004028,
      "learning_rate": 5.285842562107154e-05,
      "loss": 0.1882,
      "step": 4922
    },
    {
      "epoch": 6.6258411843876175,
      "grad_norm": 0.7374385595321655,
      "learning_rate": 5.282849446273571e-05,
      "loss": 0.1227,
      "step": 4923
    },
    {
      "epoch": 6.627187079407806,
      "grad_norm": 0.678381085395813,
      "learning_rate": 5.279856330439988e-05,
      "loss": 0.1192,
      "step": 4924
    },
    {
      "epoch": 6.628532974427994,
      "grad_norm": 0.7396881580352783,
      "learning_rate": 5.276863214606406e-05,
      "loss": 0.1233,
      "step": 4925
    },
    {
      "epoch": 6.629878869448183,
      "grad_norm": 1.0686370134353638,
      "learning_rate": 5.273870098772823e-05,
      "loss": 0.1775,
      "step": 4926
    },
    {
      "epoch": 6.631224764468372,
      "grad_norm": 0.740798830986023,
      "learning_rate": 5.27087698293924e-05,
      "loss": 0.1233,
      "step": 4927
    },
    {
      "epoch": 6.63257065948856,
      "grad_norm": 0.9305097460746765,
      "learning_rate": 5.267883867105657e-05,
      "loss": 0.1315,
      "step": 4928
    },
    {
      "epoch": 6.633916554508748,
      "grad_norm": 0.7736250758171082,
      "learning_rate": 5.264890751272075e-05,
      "loss": 0.1658,
      "step": 4929
    },
    {
      "epoch": 6.635262449528937,
      "grad_norm": 1.3183619976043701,
      "learning_rate": 5.261897635438492e-05,
      "loss": 0.17,
      "step": 4930
    },
    {
      "epoch": 6.636608344549125,
      "grad_norm": 0.8559912443161011,
      "learning_rate": 5.258904519604909e-05,
      "loss": 0.1669,
      "step": 4931
    },
    {
      "epoch": 6.637954239569313,
      "grad_norm": 0.9569888114929199,
      "learning_rate": 5.255911403771326e-05,
      "loss": 0.1616,
      "step": 4932
    },
    {
      "epoch": 6.639300134589502,
      "grad_norm": 0.728704035282135,
      "learning_rate": 5.252918287937744e-05,
      "loss": 0.131,
      "step": 4933
    },
    {
      "epoch": 6.64064602960969,
      "grad_norm": 0.9645611643791199,
      "learning_rate": 5.249925172104161e-05,
      "loss": 0.1465,
      "step": 4934
    },
    {
      "epoch": 6.641991924629879,
      "grad_norm": 0.99766606092453,
      "learning_rate": 5.246932056270578e-05,
      "loss": 0.1605,
      "step": 4935
    },
    {
      "epoch": 6.643337819650068,
      "grad_norm": 0.7198231220245361,
      "learning_rate": 5.243938940436995e-05,
      "loss": 0.131,
      "step": 4936
    },
    {
      "epoch": 6.644683714670256,
      "grad_norm": 0.5418539643287659,
      "learning_rate": 5.240945824603413e-05,
      "loss": 0.1168,
      "step": 4937
    },
    {
      "epoch": 6.646029609690444,
      "grad_norm": 0.6728618144989014,
      "learning_rate": 5.23795270876983e-05,
      "loss": 0.11,
      "step": 4938
    },
    {
      "epoch": 6.647375504710633,
      "grad_norm": 0.7334451079368591,
      "learning_rate": 5.2349595929362473e-05,
      "loss": 0.139,
      "step": 4939
    },
    {
      "epoch": 6.648721399730821,
      "grad_norm": 0.9152845740318298,
      "learning_rate": 5.231966477102664e-05,
      "loss": 0.1871,
      "step": 4940
    },
    {
      "epoch": 6.650067294751009,
      "grad_norm": 0.9099321961402893,
      "learning_rate": 5.228973361269082e-05,
      "loss": 0.1588,
      "step": 4941
    },
    {
      "epoch": 6.651413189771198,
      "grad_norm": 1.0289922952651978,
      "learning_rate": 5.225980245435499e-05,
      "loss": 0.1445,
      "step": 4942
    },
    {
      "epoch": 6.652759084791386,
      "grad_norm": 0.7591966986656189,
      "learning_rate": 5.2229871296019164e-05,
      "loss": 0.1242,
      "step": 4943
    },
    {
      "epoch": 6.654104979811574,
      "grad_norm": 0.8340063691139221,
      "learning_rate": 5.219994013768333e-05,
      "loss": 0.1718,
      "step": 4944
    },
    {
      "epoch": 6.655450874831763,
      "grad_norm": 0.8553676605224609,
      "learning_rate": 5.217000897934751e-05,
      "loss": 0.1321,
      "step": 4945
    },
    {
      "epoch": 6.656796769851952,
      "grad_norm": 0.5662961602210999,
      "learning_rate": 5.214007782101168e-05,
      "loss": 0.1327,
      "step": 4946
    },
    {
      "epoch": 6.65814266487214,
      "grad_norm": 0.7238078117370605,
      "learning_rate": 5.2110146662675854e-05,
      "loss": 0.1189,
      "step": 4947
    },
    {
      "epoch": 6.659488559892329,
      "grad_norm": 0.566646158695221,
      "learning_rate": 5.208021550434002e-05,
      "loss": 0.1157,
      "step": 4948
    },
    {
      "epoch": 6.660834454912517,
      "grad_norm": 0.9731996655464172,
      "learning_rate": 5.20502843460042e-05,
      "loss": 0.1446,
      "step": 4949
    },
    {
      "epoch": 6.662180349932705,
      "grad_norm": 0.9827585220336914,
      "learning_rate": 5.202035318766837e-05,
      "loss": 0.1445,
      "step": 4950
    },
    {
      "epoch": 6.663526244952894,
      "grad_norm": 0.7562956213951111,
      "learning_rate": 5.1990422029332544e-05,
      "loss": 0.1487,
      "step": 4951
    },
    {
      "epoch": 6.664872139973082,
      "grad_norm": 1.0903598070144653,
      "learning_rate": 5.196049087099671e-05,
      "loss": 0.1554,
      "step": 4952
    },
    {
      "epoch": 6.66621803499327,
      "grad_norm": 1.2297954559326172,
      "learning_rate": 5.193055971266089e-05,
      "loss": 0.1467,
      "step": 4953
    },
    {
      "epoch": 6.667563930013459,
      "grad_norm": 0.7336735725402832,
      "learning_rate": 5.190062855432506e-05,
      "loss": 0.1398,
      "step": 4954
    },
    {
      "epoch": 6.668909825033648,
      "grad_norm": 0.6354379653930664,
      "learning_rate": 5.1870697395989234e-05,
      "loss": 0.1251,
      "step": 4955
    },
    {
      "epoch": 6.670255720053836,
      "grad_norm": 0.9264532327651978,
      "learning_rate": 5.18407662376534e-05,
      "loss": 0.1244,
      "step": 4956
    },
    {
      "epoch": 6.6716016150740245,
      "grad_norm": 1.0679982900619507,
      "learning_rate": 5.181083507931758e-05,
      "loss": 0.1573,
      "step": 4957
    },
    {
      "epoch": 6.672947510094213,
      "grad_norm": 1.113189697265625,
      "learning_rate": 5.178090392098175e-05,
      "loss": 0.194,
      "step": 4958
    },
    {
      "epoch": 6.674293405114401,
      "grad_norm": 0.8599793314933777,
      "learning_rate": 5.1750972762645924e-05,
      "loss": 0.158,
      "step": 4959
    },
    {
      "epoch": 6.6756393001345895,
      "grad_norm": 0.8334876298904419,
      "learning_rate": 5.172104160431009e-05,
      "loss": 0.1273,
      "step": 4960
    },
    {
      "epoch": 6.676985195154778,
      "grad_norm": 0.6828373074531555,
      "learning_rate": 5.169111044597427e-05,
      "loss": 0.1274,
      "step": 4961
    },
    {
      "epoch": 6.678331090174966,
      "grad_norm": 1.2635074853897095,
      "learning_rate": 5.166117928763844e-05,
      "loss": 0.1758,
      "step": 4962
    },
    {
      "epoch": 6.6796769851951545,
      "grad_norm": 0.9255368709564209,
      "learning_rate": 5.1631248129302614e-05,
      "loss": 0.1429,
      "step": 4963
    },
    {
      "epoch": 6.681022880215343,
      "grad_norm": 0.9867300987243652,
      "learning_rate": 5.1601316970966784e-05,
      "loss": 0.2143,
      "step": 4964
    },
    {
      "epoch": 6.682368775235531,
      "grad_norm": 0.6873618364334106,
      "learning_rate": 5.157138581263096e-05,
      "loss": 0.133,
      "step": 4965
    },
    {
      "epoch": 6.68371467025572,
      "grad_norm": 0.764272153377533,
      "learning_rate": 5.1541454654295115e-05,
      "loss": 0.1497,
      "step": 4966
    },
    {
      "epoch": 6.685060565275909,
      "grad_norm": 0.9454498887062073,
      "learning_rate": 5.151152349595929e-05,
      "loss": 0.1689,
      "step": 4967
    },
    {
      "epoch": 6.686406460296097,
      "grad_norm": 1.4131025075912476,
      "learning_rate": 5.148159233762346e-05,
      "loss": 0.2153,
      "step": 4968
    },
    {
      "epoch": 6.687752355316285,
      "grad_norm": 1.0832891464233398,
      "learning_rate": 5.1451661179287636e-05,
      "loss": 0.1908,
      "step": 4969
    },
    {
      "epoch": 6.689098250336474,
      "grad_norm": 0.577847957611084,
      "learning_rate": 5.1421730020951805e-05,
      "loss": 0.133,
      "step": 4970
    },
    {
      "epoch": 6.690444145356662,
      "grad_norm": 0.7721669673919678,
      "learning_rate": 5.139179886261598e-05,
      "loss": 0.1516,
      "step": 4971
    },
    {
      "epoch": 6.69179004037685,
      "grad_norm": 0.5933099389076233,
      "learning_rate": 5.136186770428015e-05,
      "loss": 0.1306,
      "step": 4972
    },
    {
      "epoch": 6.693135935397039,
      "grad_norm": 0.6787578463554382,
      "learning_rate": 5.1331936545944326e-05,
      "loss": 0.1351,
      "step": 4973
    },
    {
      "epoch": 6.694481830417227,
      "grad_norm": 1.041110634803772,
      "learning_rate": 5.1302005387608495e-05,
      "loss": 0.1516,
      "step": 4974
    },
    {
      "epoch": 6.695827725437416,
      "grad_norm": 0.7376534938812256,
      "learning_rate": 5.127207422927267e-05,
      "loss": 0.1273,
      "step": 4975
    },
    {
      "epoch": 6.697173620457605,
      "grad_norm": 0.6887644529342651,
      "learning_rate": 5.124214307093684e-05,
      "loss": 0.1214,
      "step": 4976
    },
    {
      "epoch": 6.698519515477793,
      "grad_norm": 0.6382591724395752,
      "learning_rate": 5.1212211912601016e-05,
      "loss": 0.129,
      "step": 4977
    },
    {
      "epoch": 6.699865410497981,
      "grad_norm": 0.6718162894248962,
      "learning_rate": 5.1182280754265185e-05,
      "loss": 0.1395,
      "step": 4978
    },
    {
      "epoch": 6.70121130551817,
      "grad_norm": 0.654697597026825,
      "learning_rate": 5.115234959592936e-05,
      "loss": 0.1307,
      "step": 4979
    },
    {
      "epoch": 6.702557200538358,
      "grad_norm": 0.8425620794296265,
      "learning_rate": 5.112241843759353e-05,
      "loss": 0.1456,
      "step": 4980
    },
    {
      "epoch": 6.703903095558546,
      "grad_norm": 0.828620195388794,
      "learning_rate": 5.1092487279257706e-05,
      "loss": 0.1382,
      "step": 4981
    },
    {
      "epoch": 6.705248990578735,
      "grad_norm": 0.7767436504364014,
      "learning_rate": 5.1062556120921876e-05,
      "loss": 0.1207,
      "step": 4982
    },
    {
      "epoch": 6.706594885598923,
      "grad_norm": 0.9101703763008118,
      "learning_rate": 5.103262496258605e-05,
      "loss": 0.1219,
      "step": 4983
    },
    {
      "epoch": 6.707940780619111,
      "grad_norm": 0.526080846786499,
      "learning_rate": 5.100269380425022e-05,
      "loss": 0.1185,
      "step": 4984
    },
    {
      "epoch": 6.7092866756393,
      "grad_norm": 0.8964555859565735,
      "learning_rate": 5.09727626459144e-05,
      "loss": 0.1701,
      "step": 4985
    },
    {
      "epoch": 6.710632570659489,
      "grad_norm": 0.7609519958496094,
      "learning_rate": 5.0942831487578566e-05,
      "loss": 0.1228,
      "step": 4986
    },
    {
      "epoch": 6.711978465679677,
      "grad_norm": 0.6436033844947815,
      "learning_rate": 5.091290032924274e-05,
      "loss": 0.1183,
      "step": 4987
    },
    {
      "epoch": 6.713324360699866,
      "grad_norm": 0.874975323677063,
      "learning_rate": 5.088296917090691e-05,
      "loss": 0.193,
      "step": 4988
    },
    {
      "epoch": 6.714670255720054,
      "grad_norm": 0.7686699628829956,
      "learning_rate": 5.085303801257109e-05,
      "loss": 0.1477,
      "step": 4989
    },
    {
      "epoch": 6.716016150740242,
      "grad_norm": 0.8319742679595947,
      "learning_rate": 5.0823106854235256e-05,
      "loss": 0.2139,
      "step": 4990
    },
    {
      "epoch": 6.717362045760431,
      "grad_norm": 0.8647512793540955,
      "learning_rate": 5.079317569589943e-05,
      "loss": 0.1717,
      "step": 4991
    },
    {
      "epoch": 6.718707940780619,
      "grad_norm": 0.9404080510139465,
      "learning_rate": 5.07632445375636e-05,
      "loss": 0.1375,
      "step": 4992
    },
    {
      "epoch": 6.720053835800807,
      "grad_norm": 0.7115312814712524,
      "learning_rate": 5.073331337922778e-05,
      "loss": 0.1422,
      "step": 4993
    },
    {
      "epoch": 6.721399730820996,
      "grad_norm": 0.6507083773612976,
      "learning_rate": 5.0703382220891946e-05,
      "loss": 0.1234,
      "step": 4994
    },
    {
      "epoch": 6.722745625841185,
      "grad_norm": 0.7840871214866638,
      "learning_rate": 5.067345106255612e-05,
      "loss": 0.1331,
      "step": 4995
    },
    {
      "epoch": 6.724091520861373,
      "grad_norm": 0.7281883955001831,
      "learning_rate": 5.064351990422029e-05,
      "loss": 0.1333,
      "step": 4996
    },
    {
      "epoch": 6.7254374158815615,
      "grad_norm": 0.9071818590164185,
      "learning_rate": 5.061358874588447e-05,
      "loss": 0.1647,
      "step": 4997
    },
    {
      "epoch": 6.72678331090175,
      "grad_norm": 0.8005740642547607,
      "learning_rate": 5.0583657587548636e-05,
      "loss": 0.1454,
      "step": 4998
    },
    {
      "epoch": 6.728129205921938,
      "grad_norm": 1.4831459522247314,
      "learning_rate": 5.055372642921281e-05,
      "loss": 0.2301,
      "step": 4999
    },
    {
      "epoch": 6.7294751009421265,
      "grad_norm": 0.7225229144096375,
      "learning_rate": 5.052379527087698e-05,
      "loss": 0.1332,
      "step": 5000
    },
    {
      "epoch": 6.730820995962315,
      "grad_norm": 0.8811348080635071,
      "learning_rate": 5.049386411254116e-05,
      "loss": 0.1984,
      "step": 5001
    },
    {
      "epoch": 6.732166890982503,
      "grad_norm": 0.6399688720703125,
      "learning_rate": 5.0463932954205326e-05,
      "loss": 0.1525,
      "step": 5002
    },
    {
      "epoch": 6.7335127860026915,
      "grad_norm": 0.6135942339897156,
      "learning_rate": 5.04340017958695e-05,
      "loss": 0.1406,
      "step": 5003
    },
    {
      "epoch": 6.73485868102288,
      "grad_norm": 0.9070454835891724,
      "learning_rate": 5.040407063753367e-05,
      "loss": 0.1508,
      "step": 5004
    },
    {
      "epoch": 6.736204576043068,
      "grad_norm": 1.2230662107467651,
      "learning_rate": 5.037413947919785e-05,
      "loss": 0.1517,
      "step": 5005
    },
    {
      "epoch": 6.737550471063257,
      "grad_norm": 1.24309241771698,
      "learning_rate": 5.0344208320862017e-05,
      "loss": 0.1503,
      "step": 5006
    },
    {
      "epoch": 6.738896366083446,
      "grad_norm": 0.8252668380737305,
      "learning_rate": 5.031427716252619e-05,
      "loss": 0.1282,
      "step": 5007
    },
    {
      "epoch": 6.740242261103634,
      "grad_norm": 0.9069490432739258,
      "learning_rate": 5.028434600419036e-05,
      "loss": 0.128,
      "step": 5008
    },
    {
      "epoch": 6.7415881561238225,
      "grad_norm": 0.8317643404006958,
      "learning_rate": 5.025441484585454e-05,
      "loss": 0.1516,
      "step": 5009
    },
    {
      "epoch": 6.742934051144011,
      "grad_norm": 1.1856942176818848,
      "learning_rate": 5.022448368751871e-05,
      "loss": 0.2078,
      "step": 5010
    },
    {
      "epoch": 6.744279946164199,
      "grad_norm": 0.6850715279579163,
      "learning_rate": 5.019455252918288e-05,
      "loss": 0.1317,
      "step": 5011
    },
    {
      "epoch": 6.7456258411843875,
      "grad_norm": 0.8446711897850037,
      "learning_rate": 5.016462137084705e-05,
      "loss": 0.1557,
      "step": 5012
    },
    {
      "epoch": 6.746971736204576,
      "grad_norm": 0.882466733455658,
      "learning_rate": 5.013469021251123e-05,
      "loss": 0.1478,
      "step": 5013
    },
    {
      "epoch": 6.748317631224764,
      "grad_norm": 0.5685859322547913,
      "learning_rate": 5.01047590541754e-05,
      "loss": 0.1156,
      "step": 5014
    },
    {
      "epoch": 6.749663526244953,
      "grad_norm": 0.793394923210144,
      "learning_rate": 5.007482789583957e-05,
      "loss": 0.1223,
      "step": 5015
    },
    {
      "epoch": 6.751009421265142,
      "grad_norm": 0.6146506071090698,
      "learning_rate": 5.004489673750374e-05,
      "loss": 0.1147,
      "step": 5016
    },
    {
      "epoch": 6.75235531628533,
      "grad_norm": 1.2132943868637085,
      "learning_rate": 5.001496557916792e-05,
      "loss": 0.1921,
      "step": 5017
    },
    {
      "epoch": 6.753701211305518,
      "grad_norm": 0.8046795129776001,
      "learning_rate": 4.998503442083209e-05,
      "loss": 0.115,
      "step": 5018
    },
    {
      "epoch": 6.755047106325707,
      "grad_norm": 1.2516894340515137,
      "learning_rate": 4.995510326249626e-05,
      "loss": 0.1881,
      "step": 5019
    },
    {
      "epoch": 6.756393001345895,
      "grad_norm": 1.1188279390335083,
      "learning_rate": 4.992517210416043e-05,
      "loss": 0.2002,
      "step": 5020
    },
    {
      "epoch": 6.757738896366083,
      "grad_norm": 0.7820137143135071,
      "learning_rate": 4.989524094582461e-05,
      "loss": 0.1554,
      "step": 5021
    },
    {
      "epoch": 6.759084791386272,
      "grad_norm": 0.8752161264419556,
      "learning_rate": 4.986530978748878e-05,
      "loss": 0.1635,
      "step": 5022
    },
    {
      "epoch": 6.76043068640646,
      "grad_norm": 0.49069055914878845,
      "learning_rate": 4.983537862915295e-05,
      "loss": 0.1023,
      "step": 5023
    },
    {
      "epoch": 6.761776581426648,
      "grad_norm": 1.0166170597076416,
      "learning_rate": 4.980544747081712e-05,
      "loss": 0.1555,
      "step": 5024
    },
    {
      "epoch": 6.763122476446837,
      "grad_norm": 0.7354609370231628,
      "learning_rate": 4.97755163124813e-05,
      "loss": 0.1262,
      "step": 5025
    },
    {
      "epoch": 6.764468371467026,
      "grad_norm": 0.9930567741394043,
      "learning_rate": 4.974558515414547e-05,
      "loss": 0.1847,
      "step": 5026
    },
    {
      "epoch": 6.765814266487214,
      "grad_norm": 0.8025059700012207,
      "learning_rate": 4.971565399580964e-05,
      "loss": 0.1248,
      "step": 5027
    },
    {
      "epoch": 6.767160161507403,
      "grad_norm": 0.9203365445137024,
      "learning_rate": 4.968572283747381e-05,
      "loss": 0.1296,
      "step": 5028
    },
    {
      "epoch": 6.768506056527591,
      "grad_norm": 0.9023696184158325,
      "learning_rate": 4.965579167913799e-05,
      "loss": 0.1621,
      "step": 5029
    },
    {
      "epoch": 6.769851951547779,
      "grad_norm": 0.8769673705101013,
      "learning_rate": 4.962586052080216e-05,
      "loss": 0.1536,
      "step": 5030
    },
    {
      "epoch": 6.771197846567968,
      "grad_norm": 0.6769135594367981,
      "learning_rate": 4.959592936246633e-05,
      "loss": 0.132,
      "step": 5031
    },
    {
      "epoch": 6.772543741588156,
      "grad_norm": 1.0038297176361084,
      "learning_rate": 4.95659982041305e-05,
      "loss": 0.2089,
      "step": 5032
    },
    {
      "epoch": 6.773889636608344,
      "grad_norm": 0.7518383860588074,
      "learning_rate": 4.953606704579468e-05,
      "loss": 0.1683,
      "step": 5033
    },
    {
      "epoch": 6.775235531628533,
      "grad_norm": 1.0713438987731934,
      "learning_rate": 4.950613588745885e-05,
      "loss": 0.1659,
      "step": 5034
    },
    {
      "epoch": 6.776581426648722,
      "grad_norm": 0.6343798041343689,
      "learning_rate": 4.9476204729123023e-05,
      "loss": 0.1224,
      "step": 5035
    },
    {
      "epoch": 6.77792732166891,
      "grad_norm": 0.9158733487129211,
      "learning_rate": 4.944627357078719e-05,
      "loss": 0.1392,
      "step": 5036
    },
    {
      "epoch": 6.7792732166890985,
      "grad_norm": 0.9962179660797119,
      "learning_rate": 4.941634241245137e-05,
      "loss": 0.1728,
      "step": 5037
    },
    {
      "epoch": 6.780619111709287,
      "grad_norm": 0.7073933482170105,
      "learning_rate": 4.938641125411554e-05,
      "loss": 0.1221,
      "step": 5038
    },
    {
      "epoch": 6.781965006729475,
      "grad_norm": 0.8219898343086243,
      "learning_rate": 4.9356480095779714e-05,
      "loss": 0.1425,
      "step": 5039
    },
    {
      "epoch": 6.783310901749664,
      "grad_norm": 1.055517554283142,
      "learning_rate": 4.932654893744388e-05,
      "loss": 0.1722,
      "step": 5040
    },
    {
      "epoch": 6.784656796769852,
      "grad_norm": 0.6709393858909607,
      "learning_rate": 4.929661777910806e-05,
      "loss": 0.1095,
      "step": 5041
    },
    {
      "epoch": 6.78600269179004,
      "grad_norm": 0.8619018197059631,
      "learning_rate": 4.926668662077223e-05,
      "loss": 0.1635,
      "step": 5042
    },
    {
      "epoch": 6.787348586810229,
      "grad_norm": 0.790757954120636,
      "learning_rate": 4.9236755462436404e-05,
      "loss": 0.119,
      "step": 5043
    },
    {
      "epoch": 6.788694481830417,
      "grad_norm": 0.5472744107246399,
      "learning_rate": 4.9206824304100566e-05,
      "loss": 0.0963,
      "step": 5044
    },
    {
      "epoch": 6.790040376850605,
      "grad_norm": 0.8219900727272034,
      "learning_rate": 4.917689314576474e-05,
      "loss": 0.1275,
      "step": 5045
    },
    {
      "epoch": 6.7913862718707945,
      "grad_norm": 0.6525952219963074,
      "learning_rate": 4.914696198742891e-05,
      "loss": 0.1358,
      "step": 5046
    },
    {
      "epoch": 6.792732166890983,
      "grad_norm": 0.9875062704086304,
      "learning_rate": 4.911703082909309e-05,
      "loss": 0.1534,
      "step": 5047
    },
    {
      "epoch": 6.794078061911171,
      "grad_norm": 1.0503498315811157,
      "learning_rate": 4.9087099670757256e-05,
      "loss": 0.1635,
      "step": 5048
    },
    {
      "epoch": 6.7954239569313595,
      "grad_norm": 0.7615059018135071,
      "learning_rate": 4.905716851242143e-05,
      "loss": 0.1454,
      "step": 5049
    },
    {
      "epoch": 6.796769851951548,
      "grad_norm": 1.068457007408142,
      "learning_rate": 4.90272373540856e-05,
      "loss": 0.2133,
      "step": 5050
    },
    {
      "epoch": 6.798115746971736,
      "grad_norm": 0.7433648705482483,
      "learning_rate": 4.899730619574978e-05,
      "loss": 0.1291,
      "step": 5051
    },
    {
      "epoch": 6.7994616419919245,
      "grad_norm": 0.6388506889343262,
      "learning_rate": 4.8967375037413946e-05,
      "loss": 0.1221,
      "step": 5052
    },
    {
      "epoch": 6.800807537012113,
      "grad_norm": 0.625818133354187,
      "learning_rate": 4.893744387907812e-05,
      "loss": 0.1321,
      "step": 5053
    },
    {
      "epoch": 6.802153432032301,
      "grad_norm": 0.8255726099014282,
      "learning_rate": 4.890751272074229e-05,
      "loss": 0.135,
      "step": 5054
    },
    {
      "epoch": 6.80349932705249,
      "grad_norm": 0.8083246350288391,
      "learning_rate": 4.887758156240647e-05,
      "loss": 0.1237,
      "step": 5055
    },
    {
      "epoch": 6.804845222072679,
      "grad_norm": 0.713180422782898,
      "learning_rate": 4.8847650404070637e-05,
      "loss": 0.115,
      "step": 5056
    },
    {
      "epoch": 6.806191117092867,
      "grad_norm": 0.8893124461174011,
      "learning_rate": 4.881771924573481e-05,
      "loss": 0.1333,
      "step": 5057
    },
    {
      "epoch": 6.807537012113055,
      "grad_norm": 0.8411466479301453,
      "learning_rate": 4.878778808739898e-05,
      "loss": 0.1683,
      "step": 5058
    },
    {
      "epoch": 6.808882907133244,
      "grad_norm": 0.9546594023704529,
      "learning_rate": 4.875785692906316e-05,
      "loss": 0.1864,
      "step": 5059
    },
    {
      "epoch": 6.810228802153432,
      "grad_norm": 0.5412986278533936,
      "learning_rate": 4.872792577072733e-05,
      "loss": 0.1053,
      "step": 5060
    },
    {
      "epoch": 6.81157469717362,
      "grad_norm": 0.6158880591392517,
      "learning_rate": 4.86979946123915e-05,
      "loss": 0.1126,
      "step": 5061
    },
    {
      "epoch": 6.812920592193809,
      "grad_norm": 0.8500446081161499,
      "learning_rate": 4.866806345405567e-05,
      "loss": 0.1597,
      "step": 5062
    },
    {
      "epoch": 6.814266487213997,
      "grad_norm": 0.5709138512611389,
      "learning_rate": 4.863813229571985e-05,
      "loss": 0.1353,
      "step": 5063
    },
    {
      "epoch": 6.815612382234185,
      "grad_norm": 0.7659060955047607,
      "learning_rate": 4.860820113738402e-05,
      "loss": 0.1452,
      "step": 5064
    },
    {
      "epoch": 6.816958277254374,
      "grad_norm": 0.8203771114349365,
      "learning_rate": 4.857826997904819e-05,
      "loss": 0.1241,
      "step": 5065
    },
    {
      "epoch": 6.818304172274562,
      "grad_norm": 0.6225804686546326,
      "learning_rate": 4.854833882071236e-05,
      "loss": 0.1113,
      "step": 5066
    },
    {
      "epoch": 6.819650067294751,
      "grad_norm": 0.6511821150779724,
      "learning_rate": 4.851840766237654e-05,
      "loss": 0.1391,
      "step": 5067
    },
    {
      "epoch": 6.82099596231494,
      "grad_norm": 0.8040839433670044,
      "learning_rate": 4.848847650404071e-05,
      "loss": 0.1428,
      "step": 5068
    },
    {
      "epoch": 6.822341857335128,
      "grad_norm": 0.9270761013031006,
      "learning_rate": 4.845854534570488e-05,
      "loss": 0.1439,
      "step": 5069
    },
    {
      "epoch": 6.823687752355316,
      "grad_norm": 0.9334228038787842,
      "learning_rate": 4.842861418736905e-05,
      "loss": 0.2945,
      "step": 5070
    },
    {
      "epoch": 6.825033647375505,
      "grad_norm": 1.2411302328109741,
      "learning_rate": 4.839868302903323e-05,
      "loss": 0.1649,
      "step": 5071
    },
    {
      "epoch": 6.826379542395693,
      "grad_norm": 0.7980921268463135,
      "learning_rate": 4.83687518706974e-05,
      "loss": 0.1283,
      "step": 5072
    },
    {
      "epoch": 6.827725437415881,
      "grad_norm": 0.768547773361206,
      "learning_rate": 4.833882071236157e-05,
      "loss": 0.1576,
      "step": 5073
    },
    {
      "epoch": 6.82907133243607,
      "grad_norm": 0.7740652561187744,
      "learning_rate": 4.830888955402574e-05,
      "loss": 0.1241,
      "step": 5074
    },
    {
      "epoch": 6.830417227456259,
      "grad_norm": 1.048891305923462,
      "learning_rate": 4.827895839568992e-05,
      "loss": 0.182,
      "step": 5075
    },
    {
      "epoch": 6.831763122476447,
      "grad_norm": 0.8190065622329712,
      "learning_rate": 4.824902723735409e-05,
      "loss": 0.1627,
      "step": 5076
    },
    {
      "epoch": 6.833109017496636,
      "grad_norm": 0.9085182547569275,
      "learning_rate": 4.821909607901826e-05,
      "loss": 0.1157,
      "step": 5077
    },
    {
      "epoch": 6.834454912516824,
      "grad_norm": 0.8473381400108337,
      "learning_rate": 4.818916492068243e-05,
      "loss": 0.1641,
      "step": 5078
    },
    {
      "epoch": 6.835800807537012,
      "grad_norm": 0.9156942367553711,
      "learning_rate": 4.815923376234661e-05,
      "loss": 0.2199,
      "step": 5079
    },
    {
      "epoch": 6.837146702557201,
      "grad_norm": 1.0551855564117432,
      "learning_rate": 4.812930260401078e-05,
      "loss": 0.1761,
      "step": 5080
    },
    {
      "epoch": 6.838492597577389,
      "grad_norm": 0.6549797058105469,
      "learning_rate": 4.8099371445674953e-05,
      "loss": 0.1114,
      "step": 5081
    },
    {
      "epoch": 6.839838492597577,
      "grad_norm": 0.5721012949943542,
      "learning_rate": 4.806944028733912e-05,
      "loss": 0.0993,
      "step": 5082
    },
    {
      "epoch": 6.841184387617766,
      "grad_norm": 0.49349069595336914,
      "learning_rate": 4.80395091290033e-05,
      "loss": 0.1109,
      "step": 5083
    },
    {
      "epoch": 6.842530282637954,
      "grad_norm": 0.9117171168327332,
      "learning_rate": 4.800957797066747e-05,
      "loss": 0.1143,
      "step": 5084
    },
    {
      "epoch": 6.843876177658142,
      "grad_norm": 0.5912522673606873,
      "learning_rate": 4.7979646812331644e-05,
      "loss": 0.1216,
      "step": 5085
    },
    {
      "epoch": 6.845222072678331,
      "grad_norm": 0.6435560584068298,
      "learning_rate": 4.794971565399581e-05,
      "loss": 0.1679,
      "step": 5086
    },
    {
      "epoch": 6.84656796769852,
      "grad_norm": 0.900235652923584,
      "learning_rate": 4.791978449565999e-05,
      "loss": 0.1367,
      "step": 5087
    },
    {
      "epoch": 6.847913862718708,
      "grad_norm": 0.6794875264167786,
      "learning_rate": 4.788985333732416e-05,
      "loss": 0.1316,
      "step": 5088
    },
    {
      "epoch": 6.8492597577388965,
      "grad_norm": 0.8335312604904175,
      "learning_rate": 4.7859922178988334e-05,
      "loss": 0.1153,
      "step": 5089
    },
    {
      "epoch": 6.850605652759085,
      "grad_norm": 0.7620874047279358,
      "learning_rate": 4.78299910206525e-05,
      "loss": 0.1415,
      "step": 5090
    },
    {
      "epoch": 6.851951547779273,
      "grad_norm": 0.8004450798034668,
      "learning_rate": 4.780005986231668e-05,
      "loss": 0.1309,
      "step": 5091
    },
    {
      "epoch": 6.8532974427994615,
      "grad_norm": 0.7573407888412476,
      "learning_rate": 4.777012870398085e-05,
      "loss": 0.1293,
      "step": 5092
    },
    {
      "epoch": 6.85464333781965,
      "grad_norm": 0.8307089805603027,
      "learning_rate": 4.7740197545645024e-05,
      "loss": 0.1215,
      "step": 5093
    },
    {
      "epoch": 6.855989232839838,
      "grad_norm": 1.0910224914550781,
      "learning_rate": 4.771026638730919e-05,
      "loss": 0.1572,
      "step": 5094
    },
    {
      "epoch": 6.857335127860027,
      "grad_norm": 0.9051745533943176,
      "learning_rate": 4.768033522897337e-05,
      "loss": 0.1812,
      "step": 5095
    },
    {
      "epoch": 6.858681022880216,
      "grad_norm": 0.6883916854858398,
      "learning_rate": 4.765040407063753e-05,
      "loss": 0.1534,
      "step": 5096
    },
    {
      "epoch": 6.860026917900404,
      "grad_norm": 0.9853479266166687,
      "learning_rate": 4.762047291230171e-05,
      "loss": 0.1334,
      "step": 5097
    },
    {
      "epoch": 6.861372812920592,
      "grad_norm": 0.8972616195678711,
      "learning_rate": 4.7590541753965876e-05,
      "loss": 0.1437,
      "step": 5098
    },
    {
      "epoch": 6.862718707940781,
      "grad_norm": 1.1150648593902588,
      "learning_rate": 4.756061059563005e-05,
      "loss": 0.1328,
      "step": 5099
    },
    {
      "epoch": 6.864064602960969,
      "grad_norm": 0.9345860481262207,
      "learning_rate": 4.753067943729422e-05,
      "loss": 0.1545,
      "step": 5100
    },
    {
      "epoch": 6.865410497981157,
      "grad_norm": 0.6432127356529236,
      "learning_rate": 4.75007482789584e-05,
      "loss": 0.117,
      "step": 5101
    },
    {
      "epoch": 6.866756393001346,
      "grad_norm": 1.4281868934631348,
      "learning_rate": 4.7470817120622567e-05,
      "loss": 0.1977,
      "step": 5102
    },
    {
      "epoch": 6.868102288021534,
      "grad_norm": 0.7853490710258484,
      "learning_rate": 4.744088596228674e-05,
      "loss": 0.1206,
      "step": 5103
    },
    {
      "epoch": 6.8694481830417224,
      "grad_norm": 0.7431837320327759,
      "learning_rate": 4.741095480395091e-05,
      "loss": 0.1051,
      "step": 5104
    },
    {
      "epoch": 6.870794078061911,
      "grad_norm": 0.6512047648429871,
      "learning_rate": 4.738102364561509e-05,
      "loss": 0.1249,
      "step": 5105
    },
    {
      "epoch": 6.872139973082099,
      "grad_norm": 1.0404008626937866,
      "learning_rate": 4.735109248727926e-05,
      "loss": 0.191,
      "step": 5106
    },
    {
      "epoch": 6.873485868102288,
      "grad_norm": 1.6605966091156006,
      "learning_rate": 4.732116132894343e-05,
      "loss": 0.2532,
      "step": 5107
    },
    {
      "epoch": 6.874831763122477,
      "grad_norm": 0.7534964680671692,
      "learning_rate": 4.72912301706076e-05,
      "loss": 0.1232,
      "step": 5108
    },
    {
      "epoch": 6.876177658142665,
      "grad_norm": 0.6385449767112732,
      "learning_rate": 4.726129901227178e-05,
      "loss": 0.1353,
      "step": 5109
    },
    {
      "epoch": 6.877523553162853,
      "grad_norm": 0.8662054538726807,
      "learning_rate": 4.723136785393595e-05,
      "loss": 0.1372,
      "step": 5110
    },
    {
      "epoch": 6.878869448183042,
      "grad_norm": 0.7507004141807556,
      "learning_rate": 4.720143669560012e-05,
      "loss": 0.1516,
      "step": 5111
    },
    {
      "epoch": 6.88021534320323,
      "grad_norm": 0.8397424221038818,
      "learning_rate": 4.717150553726429e-05,
      "loss": 0.1485,
      "step": 5112
    },
    {
      "epoch": 6.881561238223418,
      "grad_norm": 0.7539165019989014,
      "learning_rate": 4.714157437892847e-05,
      "loss": 0.1491,
      "step": 5113
    },
    {
      "epoch": 6.882907133243607,
      "grad_norm": 0.9823771715164185,
      "learning_rate": 4.711164322059264e-05,
      "loss": 0.1522,
      "step": 5114
    },
    {
      "epoch": 6.884253028263795,
      "grad_norm": 0.8381479382514954,
      "learning_rate": 4.708171206225681e-05,
      "loss": 0.1218,
      "step": 5115
    },
    {
      "epoch": 6.885598923283984,
      "grad_norm": 0.7792623043060303,
      "learning_rate": 4.705178090392098e-05,
      "loss": 0.1383,
      "step": 5116
    },
    {
      "epoch": 6.886944818304173,
      "grad_norm": 1.0219584703445435,
      "learning_rate": 4.702184974558516e-05,
      "loss": 0.1522,
      "step": 5117
    },
    {
      "epoch": 6.888290713324361,
      "grad_norm": 1.2659046649932861,
      "learning_rate": 4.699191858724933e-05,
      "loss": 0.2657,
      "step": 5118
    },
    {
      "epoch": 6.889636608344549,
      "grad_norm": 1.1175178289413452,
      "learning_rate": 4.69619874289135e-05,
      "loss": 0.1743,
      "step": 5119
    },
    {
      "epoch": 6.890982503364738,
      "grad_norm": 0.900413990020752,
      "learning_rate": 4.693205627057767e-05,
      "loss": 0.1505,
      "step": 5120
    },
    {
      "epoch": 6.892328398384926,
      "grad_norm": 0.9934285283088684,
      "learning_rate": 4.690212511224185e-05,
      "loss": 0.1499,
      "step": 5121
    },
    {
      "epoch": 6.893674293405114,
      "grad_norm": 0.8726945519447327,
      "learning_rate": 4.687219395390602e-05,
      "loss": 0.1279,
      "step": 5122
    },
    {
      "epoch": 6.895020188425303,
      "grad_norm": 1.2147364616394043,
      "learning_rate": 4.684226279557019e-05,
      "loss": 0.1804,
      "step": 5123
    },
    {
      "epoch": 6.896366083445491,
      "grad_norm": 0.6550343632698059,
      "learning_rate": 4.681233163723436e-05,
      "loss": 0.0977,
      "step": 5124
    },
    {
      "epoch": 6.897711978465679,
      "grad_norm": 0.7964258790016174,
      "learning_rate": 4.678240047889854e-05,
      "loss": 0.128,
      "step": 5125
    },
    {
      "epoch": 6.899057873485868,
      "grad_norm": 0.6897597312927246,
      "learning_rate": 4.675246932056271e-05,
      "loss": 0.1412,
      "step": 5126
    },
    {
      "epoch": 6.900403768506057,
      "grad_norm": 0.6051140427589417,
      "learning_rate": 4.672253816222688e-05,
      "loss": 0.168,
      "step": 5127
    },
    {
      "epoch": 6.901749663526245,
      "grad_norm": 0.760479211807251,
      "learning_rate": 4.669260700389105e-05,
      "loss": 0.1385,
      "step": 5128
    },
    {
      "epoch": 6.9030955585464335,
      "grad_norm": 0.7484156489372253,
      "learning_rate": 4.666267584555523e-05,
      "loss": 0.1193,
      "step": 5129
    },
    {
      "epoch": 6.904441453566622,
      "grad_norm": 0.7032835483551025,
      "learning_rate": 4.66327446872194e-05,
      "loss": 0.1252,
      "step": 5130
    },
    {
      "epoch": 6.90578734858681,
      "grad_norm": 0.6242789626121521,
      "learning_rate": 4.6602813528883573e-05,
      "loss": 0.132,
      "step": 5131
    },
    {
      "epoch": 6.9071332436069985,
      "grad_norm": 0.6980637311935425,
      "learning_rate": 4.657288237054774e-05,
      "loss": 0.1283,
      "step": 5132
    },
    {
      "epoch": 6.908479138627187,
      "grad_norm": 0.7920987606048584,
      "learning_rate": 4.654295121221192e-05,
      "loss": 0.1406,
      "step": 5133
    },
    {
      "epoch": 6.909825033647375,
      "grad_norm": 1.1174671649932861,
      "learning_rate": 4.651302005387609e-05,
      "loss": 0.183,
      "step": 5134
    },
    {
      "epoch": 6.9111709286675636,
      "grad_norm": 0.6565341353416443,
      "learning_rate": 4.6483088895540264e-05,
      "loss": 0.1298,
      "step": 5135
    },
    {
      "epoch": 6.912516823687753,
      "grad_norm": 0.9902509450912476,
      "learning_rate": 4.645315773720443e-05,
      "loss": 0.1119,
      "step": 5136
    },
    {
      "epoch": 6.913862718707941,
      "grad_norm": 0.667068600654602,
      "learning_rate": 4.642322657886861e-05,
      "loss": 0.1223,
      "step": 5137
    },
    {
      "epoch": 6.9152086137281294,
      "grad_norm": 0.7803992629051208,
      "learning_rate": 4.639329542053278e-05,
      "loss": 0.1281,
      "step": 5138
    },
    {
      "epoch": 6.916554508748318,
      "grad_norm": 0.7846812009811401,
      "learning_rate": 4.6363364262196954e-05,
      "loss": 0.1225,
      "step": 5139
    },
    {
      "epoch": 6.917900403768506,
      "grad_norm": 0.7141607403755188,
      "learning_rate": 4.633343310386112e-05,
      "loss": 0.1073,
      "step": 5140
    },
    {
      "epoch": 6.9192462987886945,
      "grad_norm": 1.1604790687561035,
      "learning_rate": 4.63035019455253e-05,
      "loss": 0.1798,
      "step": 5141
    },
    {
      "epoch": 6.920592193808883,
      "grad_norm": 0.9744147062301636,
      "learning_rate": 4.627357078718947e-05,
      "loss": 0.1268,
      "step": 5142
    },
    {
      "epoch": 6.921938088829071,
      "grad_norm": 0.7277880907058716,
      "learning_rate": 4.6243639628853644e-05,
      "loss": 0.1316,
      "step": 5143
    },
    {
      "epoch": 6.9232839838492595,
      "grad_norm": 0.5823826193809509,
      "learning_rate": 4.621370847051781e-05,
      "loss": 0.1073,
      "step": 5144
    },
    {
      "epoch": 6.924629878869448,
      "grad_norm": 0.6015048623085022,
      "learning_rate": 4.618377731218199e-05,
      "loss": 0.1137,
      "step": 5145
    },
    {
      "epoch": 6.925975773889636,
      "grad_norm": 0.8136399388313293,
      "learning_rate": 4.615384615384616e-05,
      "loss": 0.1423,
      "step": 5146
    },
    {
      "epoch": 6.927321668909825,
      "grad_norm": 0.9473341703414917,
      "learning_rate": 4.6123914995510334e-05,
      "loss": 0.1338,
      "step": 5147
    },
    {
      "epoch": 6.928667563930014,
      "grad_norm": 0.9637566804885864,
      "learning_rate": 4.60939838371745e-05,
      "loss": 0.1796,
      "step": 5148
    },
    {
      "epoch": 6.930013458950202,
      "grad_norm": 0.7332287430763245,
      "learning_rate": 4.606405267883867e-05,
      "loss": 0.1326,
      "step": 5149
    },
    {
      "epoch": 6.93135935397039,
      "grad_norm": 0.8885645270347595,
      "learning_rate": 4.603412152050284e-05,
      "loss": 0.1507,
      "step": 5150
    },
    {
      "epoch": 6.932705248990579,
      "grad_norm": 0.6673038601875305,
      "learning_rate": 4.600419036216702e-05,
      "loss": 0.1099,
      "step": 5151
    },
    {
      "epoch": 6.934051144010767,
      "grad_norm": 1.0209091901779175,
      "learning_rate": 4.5974259203831187e-05,
      "loss": 0.1625,
      "step": 5152
    },
    {
      "epoch": 6.935397039030955,
      "grad_norm": 0.898644208908081,
      "learning_rate": 4.594432804549536e-05,
      "loss": 0.1649,
      "step": 5153
    },
    {
      "epoch": 6.936742934051144,
      "grad_norm": 0.7362647652626038,
      "learning_rate": 4.591439688715953e-05,
      "loss": 0.1297,
      "step": 5154
    },
    {
      "epoch": 6.938088829071332,
      "grad_norm": 0.9921444654464722,
      "learning_rate": 4.588446572882371e-05,
      "loss": 0.1475,
      "step": 5155
    },
    {
      "epoch": 6.939434724091521,
      "grad_norm": 0.8222169876098633,
      "learning_rate": 4.585453457048788e-05,
      "loss": 0.1087,
      "step": 5156
    },
    {
      "epoch": 6.94078061911171,
      "grad_norm": 1.0387318134307861,
      "learning_rate": 4.582460341215205e-05,
      "loss": 0.1814,
      "step": 5157
    },
    {
      "epoch": 6.942126514131898,
      "grad_norm": 0.7881025671958923,
      "learning_rate": 4.579467225381622e-05,
      "loss": 0.1288,
      "step": 5158
    },
    {
      "epoch": 6.943472409152086,
      "grad_norm": 0.9760428071022034,
      "learning_rate": 4.57647410954804e-05,
      "loss": 0.1623,
      "step": 5159
    },
    {
      "epoch": 6.944818304172275,
      "grad_norm": 0.7638880014419556,
      "learning_rate": 4.573480993714457e-05,
      "loss": 0.1698,
      "step": 5160
    },
    {
      "epoch": 6.946164199192463,
      "grad_norm": 0.9699234962463379,
      "learning_rate": 4.570487877880874e-05,
      "loss": 0.1398,
      "step": 5161
    },
    {
      "epoch": 6.947510094212651,
      "grad_norm": 0.8969114422798157,
      "learning_rate": 4.567494762047291e-05,
      "loss": 0.1433,
      "step": 5162
    },
    {
      "epoch": 6.94885598923284,
      "grad_norm": 0.738680362701416,
      "learning_rate": 4.564501646213709e-05,
      "loss": 0.1355,
      "step": 5163
    },
    {
      "epoch": 6.950201884253028,
      "grad_norm": 0.818088948726654,
      "learning_rate": 4.561508530380126e-05,
      "loss": 0.1236,
      "step": 5164
    },
    {
      "epoch": 6.951547779273216,
      "grad_norm": 0.8592800498008728,
      "learning_rate": 4.558515414546543e-05,
      "loss": 0.1227,
      "step": 5165
    },
    {
      "epoch": 6.952893674293405,
      "grad_norm": 0.8488660454750061,
      "learning_rate": 4.55552229871296e-05,
      "loss": 0.1168,
      "step": 5166
    },
    {
      "epoch": 6.954239569313594,
      "grad_norm": 0.5478752255439758,
      "learning_rate": 4.552529182879378e-05,
      "loss": 0.1156,
      "step": 5167
    },
    {
      "epoch": 6.955585464333782,
      "grad_norm": 1.0349788665771484,
      "learning_rate": 4.549536067045795e-05,
      "loss": 0.1672,
      "step": 5168
    },
    {
      "epoch": 6.956931359353971,
      "grad_norm": 0.6995744705200195,
      "learning_rate": 4.546542951212212e-05,
      "loss": 0.1617,
      "step": 5169
    },
    {
      "epoch": 6.958277254374159,
      "grad_norm": 0.8188542723655701,
      "learning_rate": 4.543549835378629e-05,
      "loss": 0.1319,
      "step": 5170
    },
    {
      "epoch": 6.959623149394347,
      "grad_norm": 0.9391673803329468,
      "learning_rate": 4.540556719545047e-05,
      "loss": 0.1501,
      "step": 5171
    },
    {
      "epoch": 6.960969044414536,
      "grad_norm": 0.8587226271629333,
      "learning_rate": 4.537563603711464e-05,
      "loss": 0.1395,
      "step": 5172
    },
    {
      "epoch": 6.962314939434724,
      "grad_norm": 0.7459571957588196,
      "learning_rate": 4.534570487877881e-05,
      "loss": 0.1353,
      "step": 5173
    },
    {
      "epoch": 6.963660834454912,
      "grad_norm": 0.9049404263496399,
      "learning_rate": 4.531577372044298e-05,
      "loss": 0.1759,
      "step": 5174
    },
    {
      "epoch": 6.965006729475101,
      "grad_norm": 0.7403085827827454,
      "learning_rate": 4.528584256210716e-05,
      "loss": 0.1455,
      "step": 5175
    },
    {
      "epoch": 6.96635262449529,
      "grad_norm": 0.8043424487113953,
      "learning_rate": 4.525591140377133e-05,
      "loss": 0.1765,
      "step": 5176
    },
    {
      "epoch": 6.967698519515478,
      "grad_norm": 0.5399779081344604,
      "learning_rate": 4.5225980245435503e-05,
      "loss": 0.1036,
      "step": 5177
    },
    {
      "epoch": 6.9690444145356665,
      "grad_norm": 0.6993245482444763,
      "learning_rate": 4.519604908709967e-05,
      "loss": 0.1267,
      "step": 5178
    },
    {
      "epoch": 6.970390309555855,
      "grad_norm": 0.6044167280197144,
      "learning_rate": 4.516611792876385e-05,
      "loss": 0.1085,
      "step": 5179
    },
    {
      "epoch": 6.971736204576043,
      "grad_norm": 1.0671716928482056,
      "learning_rate": 4.513618677042802e-05,
      "loss": 0.1688,
      "step": 5180
    },
    {
      "epoch": 6.9730820995962315,
      "grad_norm": 0.7991764545440674,
      "learning_rate": 4.5106255612092194e-05,
      "loss": 0.1403,
      "step": 5181
    },
    {
      "epoch": 6.97442799461642,
      "grad_norm": 1.087336778640747,
      "learning_rate": 4.507632445375636e-05,
      "loss": 0.1623,
      "step": 5182
    },
    {
      "epoch": 6.975773889636608,
      "grad_norm": 0.6684970855712891,
      "learning_rate": 4.504639329542054e-05,
      "loss": 0.122,
      "step": 5183
    },
    {
      "epoch": 6.9771197846567965,
      "grad_norm": 0.8280861973762512,
      "learning_rate": 4.501646213708471e-05,
      "loss": 0.1365,
      "step": 5184
    },
    {
      "epoch": 6.978465679676985,
      "grad_norm": 0.8816851377487183,
      "learning_rate": 4.4986530978748884e-05,
      "loss": 0.1292,
      "step": 5185
    },
    {
      "epoch": 6.979811574697173,
      "grad_norm": 0.6982057094573975,
      "learning_rate": 4.495659982041305e-05,
      "loss": 0.1383,
      "step": 5186
    },
    {
      "epoch": 6.981157469717362,
      "grad_norm": 0.5697147846221924,
      "learning_rate": 4.492666866207723e-05,
      "loss": 0.0949,
      "step": 5187
    },
    {
      "epoch": 6.982503364737551,
      "grad_norm": 1.0465033054351807,
      "learning_rate": 4.48967375037414e-05,
      "loss": 0.1943,
      "step": 5188
    },
    {
      "epoch": 6.983849259757739,
      "grad_norm": 0.9073016047477722,
      "learning_rate": 4.4866806345405574e-05,
      "loss": 0.1344,
      "step": 5189
    },
    {
      "epoch": 6.985195154777927,
      "grad_norm": 1.089377522468567,
      "learning_rate": 4.483687518706974e-05,
      "loss": 0.1783,
      "step": 5190
    },
    {
      "epoch": 6.986541049798116,
      "grad_norm": 0.8814459443092346,
      "learning_rate": 4.480694402873392e-05,
      "loss": 0.1399,
      "step": 5191
    },
    {
      "epoch": 6.987886944818304,
      "grad_norm": 0.7917923927307129,
      "learning_rate": 4.477701287039809e-05,
      "loss": 0.1446,
      "step": 5192
    },
    {
      "epoch": 6.989232839838492,
      "grad_norm": 0.47111180424690247,
      "learning_rate": 4.4747081712062264e-05,
      "loss": 0.1005,
      "step": 5193
    },
    {
      "epoch": 6.990578734858681,
      "grad_norm": 0.6652751564979553,
      "learning_rate": 4.471715055372643e-05,
      "loss": 0.1431,
      "step": 5194
    },
    {
      "epoch": 6.991924629878869,
      "grad_norm": 0.7922879457473755,
      "learning_rate": 4.468721939539061e-05,
      "loss": 0.1439,
      "step": 5195
    },
    {
      "epoch": 6.993270524899058,
      "grad_norm": 0.7801540493965149,
      "learning_rate": 4.465728823705478e-05,
      "loss": 0.143,
      "step": 5196
    },
    {
      "epoch": 6.994616419919247,
      "grad_norm": 1.0695605278015137,
      "learning_rate": 4.4627357078718954e-05,
      "loss": 0.1336,
      "step": 5197
    },
    {
      "epoch": 6.995962314939435,
      "grad_norm": 0.3406941592693329,
      "learning_rate": 4.459742592038312e-05,
      "loss": 0.1038,
      "step": 5198
    },
    {
      "epoch": 6.997308209959623,
      "grad_norm": 0.7983923554420471,
      "learning_rate": 4.45674947620473e-05,
      "loss": 0.1634,
      "step": 5199
    },
    {
      "epoch": 6.998654104979812,
      "grad_norm": 0.8750874996185303,
      "learning_rate": 4.453756360371147e-05,
      "loss": 0.1527,
      "step": 5200
    },
    {
      "epoch": 7.0,
      "grad_norm": 1.6963858604431152,
      "learning_rate": 4.450763244537564e-05,
      "loss": 0.2107,
      "step": 5201
    },
    {
      "epoch": 7.001345895020188,
      "grad_norm": 0.4333656132221222,
      "learning_rate": 4.447770128703981e-05,
      "loss": 0.0828,
      "step": 5202
    },
    {
      "epoch": 7.002691790040377,
      "grad_norm": 0.39868098497390747,
      "learning_rate": 4.444777012870398e-05,
      "loss": 0.111,
      "step": 5203
    },
    {
      "epoch": 7.004037685060565,
      "grad_norm": 0.4033975899219513,
      "learning_rate": 4.441783897036815e-05,
      "loss": 0.0934,
      "step": 5204
    },
    {
      "epoch": 7.005383580080753,
      "grad_norm": 0.644642174243927,
      "learning_rate": 4.438790781203233e-05,
      "loss": 0.1083,
      "step": 5205
    },
    {
      "epoch": 7.006729475100942,
      "grad_norm": 0.3761195242404938,
      "learning_rate": 4.43579766536965e-05,
      "loss": 0.0997,
      "step": 5206
    },
    {
      "epoch": 7.008075370121131,
      "grad_norm": 0.6595135927200317,
      "learning_rate": 4.432804549536067e-05,
      "loss": 0.1491,
      "step": 5207
    },
    {
      "epoch": 7.009421265141319,
      "grad_norm": 0.5177046656608582,
      "learning_rate": 4.429811433702484e-05,
      "loss": 0.0885,
      "step": 5208
    },
    {
      "epoch": 7.010767160161508,
      "grad_norm": 0.5066378712654114,
      "learning_rate": 4.426818317868902e-05,
      "loss": 0.1123,
      "step": 5209
    },
    {
      "epoch": 7.012113055181696,
      "grad_norm": 0.5027602314949036,
      "learning_rate": 4.423825202035319e-05,
      "loss": 0.1041,
      "step": 5210
    },
    {
      "epoch": 7.013458950201884,
      "grad_norm": 0.2928062677383423,
      "learning_rate": 4.420832086201736e-05,
      "loss": 0.08,
      "step": 5211
    },
    {
      "epoch": 7.014804845222073,
      "grad_norm": 0.4763999581336975,
      "learning_rate": 4.417838970368153e-05,
      "loss": 0.1177,
      "step": 5212
    },
    {
      "epoch": 7.016150740242261,
      "grad_norm": 0.45436832308769226,
      "learning_rate": 4.414845854534571e-05,
      "loss": 0.0961,
      "step": 5213
    },
    {
      "epoch": 7.017496635262449,
      "grad_norm": 0.45763152837753296,
      "learning_rate": 4.411852738700988e-05,
      "loss": 0.1073,
      "step": 5214
    },
    {
      "epoch": 7.018842530282638,
      "grad_norm": 0.6663364171981812,
      "learning_rate": 4.408859622867405e-05,
      "loss": 0.1478,
      "step": 5215
    },
    {
      "epoch": 7.020188425302826,
      "grad_norm": 0.47149407863616943,
      "learning_rate": 4.405866507033822e-05,
      "loss": 0.0977,
      "step": 5216
    },
    {
      "epoch": 7.021534320323015,
      "grad_norm": 0.4328003525733948,
      "learning_rate": 4.40287339120024e-05,
      "loss": 0.1027,
      "step": 5217
    },
    {
      "epoch": 7.0228802153432035,
      "grad_norm": 0.6209664940834045,
      "learning_rate": 4.399880275366657e-05,
      "loss": 0.0996,
      "step": 5218
    },
    {
      "epoch": 7.024226110363392,
      "grad_norm": 0.4262644350528717,
      "learning_rate": 4.396887159533074e-05,
      "loss": 0.0816,
      "step": 5219
    },
    {
      "epoch": 7.02557200538358,
      "grad_norm": 0.6173681616783142,
      "learning_rate": 4.393894043699491e-05,
      "loss": 0.1176,
      "step": 5220
    },
    {
      "epoch": 7.0269179004037685,
      "grad_norm": 0.5873759984970093,
      "learning_rate": 4.390900927865909e-05,
      "loss": 0.1277,
      "step": 5221
    },
    {
      "epoch": 7.028263795423957,
      "grad_norm": 0.40128564834594727,
      "learning_rate": 4.387907812032326e-05,
      "loss": 0.0873,
      "step": 5222
    },
    {
      "epoch": 7.029609690444145,
      "grad_norm": 0.3885830342769623,
      "learning_rate": 4.384914696198743e-05,
      "loss": 0.0943,
      "step": 5223
    },
    {
      "epoch": 7.0309555854643335,
      "grad_norm": 0.7398029565811157,
      "learning_rate": 4.38192158036516e-05,
      "loss": 0.138,
      "step": 5224
    },
    {
      "epoch": 7.032301480484522,
      "grad_norm": 0.4522861838340759,
      "learning_rate": 4.378928464531578e-05,
      "loss": 0.0832,
      "step": 5225
    },
    {
      "epoch": 7.03364737550471,
      "grad_norm": 1.0215567350387573,
      "learning_rate": 4.375935348697995e-05,
      "loss": 0.108,
      "step": 5226
    },
    {
      "epoch": 7.034993270524899,
      "grad_norm": 0.5941375494003296,
      "learning_rate": 4.3729422328644123e-05,
      "loss": 0.1048,
      "step": 5227
    },
    {
      "epoch": 7.036339165545088,
      "grad_norm": 1.261771559715271,
      "learning_rate": 4.369949117030829e-05,
      "loss": 0.1088,
      "step": 5228
    },
    {
      "epoch": 7.037685060565276,
      "grad_norm": 0.6154075860977173,
      "learning_rate": 4.366956001197247e-05,
      "loss": 0.1029,
      "step": 5229
    },
    {
      "epoch": 7.039030955585464,
      "grad_norm": 0.9423056244850159,
      "learning_rate": 4.363962885363664e-05,
      "loss": 0.1056,
      "step": 5230
    },
    {
      "epoch": 7.040376850605653,
      "grad_norm": 0.584601640701294,
      "learning_rate": 4.3609697695300814e-05,
      "loss": 0.0861,
      "step": 5231
    },
    {
      "epoch": 7.041722745625841,
      "grad_norm": 0.5776723623275757,
      "learning_rate": 4.357976653696498e-05,
      "loss": 0.1031,
      "step": 5232
    },
    {
      "epoch": 7.043068640646029,
      "grad_norm": 0.4710892140865326,
      "learning_rate": 4.354983537862916e-05,
      "loss": 0.0852,
      "step": 5233
    },
    {
      "epoch": 7.044414535666218,
      "grad_norm": 0.41470351815223694,
      "learning_rate": 4.351990422029333e-05,
      "loss": 0.0956,
      "step": 5234
    },
    {
      "epoch": 7.045760430686406,
      "grad_norm": 0.41304489970207214,
      "learning_rate": 4.3489973061957504e-05,
      "loss": 0.0857,
      "step": 5235
    },
    {
      "epoch": 7.0471063257065945,
      "grad_norm": 0.42306143045425415,
      "learning_rate": 4.346004190362167e-05,
      "loss": 0.0916,
      "step": 5236
    },
    {
      "epoch": 7.048452220726784,
      "grad_norm": 0.5886825919151306,
      "learning_rate": 4.343011074528585e-05,
      "loss": 0.1424,
      "step": 5237
    },
    {
      "epoch": 7.049798115746972,
      "grad_norm": 1.0541985034942627,
      "learning_rate": 4.340017958695002e-05,
      "loss": 0.1137,
      "step": 5238
    },
    {
      "epoch": 7.05114401076716,
      "grad_norm": 0.4462851881980896,
      "learning_rate": 4.3370248428614194e-05,
      "loss": 0.0929,
      "step": 5239
    },
    {
      "epoch": 7.052489905787349,
      "grad_norm": 1.0697511434555054,
      "learning_rate": 4.334031727027836e-05,
      "loss": 0.1432,
      "step": 5240
    },
    {
      "epoch": 7.053835800807537,
      "grad_norm": 0.6055755019187927,
      "learning_rate": 4.331038611194254e-05,
      "loss": 0.0913,
      "step": 5241
    },
    {
      "epoch": 7.055181695827725,
      "grad_norm": 0.7000539302825928,
      "learning_rate": 4.328045495360671e-05,
      "loss": 0.1221,
      "step": 5242
    },
    {
      "epoch": 7.056527590847914,
      "grad_norm": 0.9788700938224792,
      "learning_rate": 4.3250523795270884e-05,
      "loss": 0.1125,
      "step": 5243
    },
    {
      "epoch": 7.057873485868102,
      "grad_norm": 0.5814850926399231,
      "learning_rate": 4.322059263693505e-05,
      "loss": 0.151,
      "step": 5244
    },
    {
      "epoch": 7.05921938088829,
      "grad_norm": 0.5255818367004395,
      "learning_rate": 4.319066147859923e-05,
      "loss": 0.088,
      "step": 5245
    },
    {
      "epoch": 7.060565275908479,
      "grad_norm": 0.79588383436203,
      "learning_rate": 4.31607303202634e-05,
      "loss": 0.1284,
      "step": 5246
    },
    {
      "epoch": 7.061911170928668,
      "grad_norm": 0.5588502287864685,
      "learning_rate": 4.3130799161927574e-05,
      "loss": 0.1232,
      "step": 5247
    },
    {
      "epoch": 7.063257065948856,
      "grad_norm": 0.4040955901145935,
      "learning_rate": 4.310086800359174e-05,
      "loss": 0.0852,
      "step": 5248
    },
    {
      "epoch": 7.064602960969045,
      "grad_norm": 0.5985196828842163,
      "learning_rate": 4.307093684525591e-05,
      "loss": 0.1001,
      "step": 5249
    },
    {
      "epoch": 7.065948855989233,
      "grad_norm": 0.7158616781234741,
      "learning_rate": 4.304100568692009e-05,
      "loss": 0.1081,
      "step": 5250
    },
    {
      "epoch": 7.067294751009421,
      "grad_norm": 0.6520981788635254,
      "learning_rate": 4.301107452858426e-05,
      "loss": 0.1147,
      "step": 5251
    },
    {
      "epoch": 7.06864064602961,
      "grad_norm": 0.49474841356277466,
      "learning_rate": 4.2981143370248434e-05,
      "loss": 0.0932,
      "step": 5252
    },
    {
      "epoch": 7.069986541049798,
      "grad_norm": 0.535699725151062,
      "learning_rate": 4.29512122119126e-05,
      "loss": 0.1281,
      "step": 5253
    },
    {
      "epoch": 7.071332436069986,
      "grad_norm": 0.32378384470939636,
      "learning_rate": 4.292128105357677e-05,
      "loss": 0.0851,
      "step": 5254
    },
    {
      "epoch": 7.072678331090175,
      "grad_norm": 0.4970085024833679,
      "learning_rate": 4.289134989524095e-05,
      "loss": 0.0846,
      "step": 5255
    },
    {
      "epoch": 7.074024226110363,
      "grad_norm": 0.546034574508667,
      "learning_rate": 4.286141873690512e-05,
      "loss": 0.1274,
      "step": 5256
    },
    {
      "epoch": 7.075370121130552,
      "grad_norm": 0.4178631603717804,
      "learning_rate": 4.283148757856929e-05,
      "loss": 0.0983,
      "step": 5257
    },
    {
      "epoch": 7.0767160161507405,
      "grad_norm": 0.579606831073761,
      "learning_rate": 4.280155642023346e-05,
      "loss": 0.1152,
      "step": 5258
    },
    {
      "epoch": 7.078061911170929,
      "grad_norm": 0.5842707753181458,
      "learning_rate": 4.277162526189764e-05,
      "loss": 0.1219,
      "step": 5259
    },
    {
      "epoch": 7.079407806191117,
      "grad_norm": 0.45155397057533264,
      "learning_rate": 4.274169410356181e-05,
      "loss": 0.1146,
      "step": 5260
    },
    {
      "epoch": 7.0807537012113055,
      "grad_norm": 0.5215469598770142,
      "learning_rate": 4.271176294522598e-05,
      "loss": 0.1289,
      "step": 5261
    },
    {
      "epoch": 7.082099596231494,
      "grad_norm": 1.0967135429382324,
      "learning_rate": 4.268183178689015e-05,
      "loss": 0.1189,
      "step": 5262
    },
    {
      "epoch": 7.083445491251682,
      "grad_norm": 0.46581125259399414,
      "learning_rate": 4.265190062855433e-05,
      "loss": 0.0799,
      "step": 5263
    },
    {
      "epoch": 7.0847913862718706,
      "grad_norm": 0.3528509736061096,
      "learning_rate": 4.26219694702185e-05,
      "loss": 0.079,
      "step": 5264
    },
    {
      "epoch": 7.086137281292059,
      "grad_norm": 0.574362576007843,
      "learning_rate": 4.259203831188267e-05,
      "loss": 0.0956,
      "step": 5265
    },
    {
      "epoch": 7.087483176312247,
      "grad_norm": 0.3973371684551239,
      "learning_rate": 4.256210715354684e-05,
      "loss": 0.1041,
      "step": 5266
    },
    {
      "epoch": 7.0888290713324364,
      "grad_norm": 0.6734114289283752,
      "learning_rate": 4.253217599521102e-05,
      "loss": 0.1177,
      "step": 5267
    },
    {
      "epoch": 7.090174966352625,
      "grad_norm": 0.9161816239356995,
      "learning_rate": 4.250224483687519e-05,
      "loss": 0.1447,
      "step": 5268
    },
    {
      "epoch": 7.091520861372813,
      "grad_norm": 0.6081399321556091,
      "learning_rate": 4.247231367853936e-05,
      "loss": 0.1045,
      "step": 5269
    },
    {
      "epoch": 7.0928667563930015,
      "grad_norm": 0.6183412075042725,
      "learning_rate": 4.244238252020353e-05,
      "loss": 0.0839,
      "step": 5270
    },
    {
      "epoch": 7.09421265141319,
      "grad_norm": 0.39422744512557983,
      "learning_rate": 4.241245136186771e-05,
      "loss": 0.0893,
      "step": 5271
    },
    {
      "epoch": 7.095558546433378,
      "grad_norm": 0.7016512155532837,
      "learning_rate": 4.238252020353188e-05,
      "loss": 0.1164,
      "step": 5272
    },
    {
      "epoch": 7.0969044414535665,
      "grad_norm": 0.4774259626865387,
      "learning_rate": 4.2352589045196053e-05,
      "loss": 0.1111,
      "step": 5273
    },
    {
      "epoch": 7.098250336473755,
      "grad_norm": 0.423248291015625,
      "learning_rate": 4.232265788686022e-05,
      "loss": 0.0969,
      "step": 5274
    },
    {
      "epoch": 7.099596231493943,
      "grad_norm": 0.49731937050819397,
      "learning_rate": 4.22927267285244e-05,
      "loss": 0.1177,
      "step": 5275
    },
    {
      "epoch": 7.1009421265141315,
      "grad_norm": 0.6321211457252502,
      "learning_rate": 4.226279557018857e-05,
      "loss": 0.1349,
      "step": 5276
    },
    {
      "epoch": 7.102288021534321,
      "grad_norm": 0.35401469469070435,
      "learning_rate": 4.2232864411852744e-05,
      "loss": 0.079,
      "step": 5277
    },
    {
      "epoch": 7.103633916554509,
      "grad_norm": 0.6039149165153503,
      "learning_rate": 4.220293325351691e-05,
      "loss": 0.1016,
      "step": 5278
    },
    {
      "epoch": 7.104979811574697,
      "grad_norm": 0.5542803406715393,
      "learning_rate": 4.217300209518109e-05,
      "loss": 0.104,
      "step": 5279
    },
    {
      "epoch": 7.106325706594886,
      "grad_norm": 0.4402729868888855,
      "learning_rate": 4.214307093684526e-05,
      "loss": 0.1023,
      "step": 5280
    },
    {
      "epoch": 7.107671601615074,
      "grad_norm": 0.7327107787132263,
      "learning_rate": 4.2113139778509434e-05,
      "loss": 0.1023,
      "step": 5281
    },
    {
      "epoch": 7.109017496635262,
      "grad_norm": 0.575093686580658,
      "learning_rate": 4.20832086201736e-05,
      "loss": 0.0909,
      "step": 5282
    },
    {
      "epoch": 7.110363391655451,
      "grad_norm": 0.6793907880783081,
      "learning_rate": 4.205327746183778e-05,
      "loss": 0.1388,
      "step": 5283
    },
    {
      "epoch": 7.111709286675639,
      "grad_norm": 0.8237841725349426,
      "learning_rate": 4.202334630350195e-05,
      "loss": 0.1781,
      "step": 5284
    },
    {
      "epoch": 7.113055181695827,
      "grad_norm": 0.615291953086853,
      "learning_rate": 4.1993415145166124e-05,
      "loss": 0.0854,
      "step": 5285
    },
    {
      "epoch": 7.114401076716016,
      "grad_norm": 0.3915052115917206,
      "learning_rate": 4.196348398683029e-05,
      "loss": 0.0867,
      "step": 5286
    },
    {
      "epoch": 7.115746971736205,
      "grad_norm": 0.31753256916999817,
      "learning_rate": 4.193355282849447e-05,
      "loss": 0.0721,
      "step": 5287
    },
    {
      "epoch": 7.117092866756393,
      "grad_norm": 0.6445313692092896,
      "learning_rate": 4.190362167015864e-05,
      "loss": 0.1091,
      "step": 5288
    },
    {
      "epoch": 7.118438761776582,
      "grad_norm": 0.5475718975067139,
      "learning_rate": 4.1873690511822814e-05,
      "loss": 0.1125,
      "step": 5289
    },
    {
      "epoch": 7.11978465679677,
      "grad_norm": 0.5693735480308533,
      "learning_rate": 4.184375935348698e-05,
      "loss": 0.1062,
      "step": 5290
    },
    {
      "epoch": 7.121130551816958,
      "grad_norm": 0.5631760954856873,
      "learning_rate": 4.181382819515116e-05,
      "loss": 0.1286,
      "step": 5291
    },
    {
      "epoch": 7.122476446837147,
      "grad_norm": 0.4996736943721771,
      "learning_rate": 4.178389703681533e-05,
      "loss": 0.1007,
      "step": 5292
    },
    {
      "epoch": 7.123822341857335,
      "grad_norm": 0.7704231142997742,
      "learning_rate": 4.17539658784795e-05,
      "loss": 0.1539,
      "step": 5293
    },
    {
      "epoch": 7.125168236877523,
      "grad_norm": 0.3490818738937378,
      "learning_rate": 4.172403472014367e-05,
      "loss": 0.0978,
      "step": 5294
    },
    {
      "epoch": 7.126514131897712,
      "grad_norm": 0.3520379364490509,
      "learning_rate": 4.169410356180784e-05,
      "loss": 0.0853,
      "step": 5295
    },
    {
      "epoch": 7.1278600269179,
      "grad_norm": 0.41221174597740173,
      "learning_rate": 4.166417240347202e-05,
      "loss": 0.0817,
      "step": 5296
    },
    {
      "epoch": 7.129205921938089,
      "grad_norm": 0.5927211046218872,
      "learning_rate": 4.163424124513619e-05,
      "loss": 0.0997,
      "step": 5297
    },
    {
      "epoch": 7.1305518169582776,
      "grad_norm": 0.525092601776123,
      "learning_rate": 4.1604310086800363e-05,
      "loss": 0.1166,
      "step": 5298
    },
    {
      "epoch": 7.131897711978466,
      "grad_norm": 0.40503525733947754,
      "learning_rate": 4.157437892846453e-05,
      "loss": 0.085,
      "step": 5299
    },
    {
      "epoch": 7.133243606998654,
      "grad_norm": 0.45809778571128845,
      "learning_rate": 4.154444777012871e-05,
      "loss": 0.1029,
      "step": 5300
    },
    {
      "epoch": 7.134589502018843,
      "grad_norm": 0.6187840104103088,
      "learning_rate": 4.151451661179288e-05,
      "loss": 0.102,
      "step": 5301
    },
    {
      "epoch": 7.135935397039031,
      "grad_norm": 0.44503429532051086,
      "learning_rate": 4.1484585453457054e-05,
      "loss": 0.1017,
      "step": 5302
    },
    {
      "epoch": 7.137281292059219,
      "grad_norm": 0.48235729336738586,
      "learning_rate": 4.145465429512122e-05,
      "loss": 0.0926,
      "step": 5303
    },
    {
      "epoch": 7.138627187079408,
      "grad_norm": 0.6905999779701233,
      "learning_rate": 4.14247231367854e-05,
      "loss": 0.135,
      "step": 5304
    },
    {
      "epoch": 7.139973082099596,
      "grad_norm": 0.6520718932151794,
      "learning_rate": 4.139479197844957e-05,
      "loss": 0.0944,
      "step": 5305
    },
    {
      "epoch": 7.141318977119784,
      "grad_norm": 0.5799647569656372,
      "learning_rate": 4.136486082011374e-05,
      "loss": 0.1239,
      "step": 5306
    },
    {
      "epoch": 7.1426648721399735,
      "grad_norm": 0.48187685012817383,
      "learning_rate": 4.133492966177791e-05,
      "loss": 0.0833,
      "step": 5307
    },
    {
      "epoch": 7.144010767160162,
      "grad_norm": 0.5285073518753052,
      "learning_rate": 4.130499850344208e-05,
      "loss": 0.1083,
      "step": 5308
    },
    {
      "epoch": 7.14535666218035,
      "grad_norm": 0.5481546521186829,
      "learning_rate": 4.127506734510626e-05,
      "loss": 0.1372,
      "step": 5309
    },
    {
      "epoch": 7.1467025572005385,
      "grad_norm": 0.5058860182762146,
      "learning_rate": 4.124513618677043e-05,
      "loss": 0.1166,
      "step": 5310
    },
    {
      "epoch": 7.148048452220727,
      "grad_norm": 0.5709555149078369,
      "learning_rate": 4.12152050284346e-05,
      "loss": 0.1011,
      "step": 5311
    },
    {
      "epoch": 7.149394347240915,
      "grad_norm": 0.6386258006095886,
      "learning_rate": 4.118527387009877e-05,
      "loss": 0.1043,
      "step": 5312
    },
    {
      "epoch": 7.1507402422611035,
      "grad_norm": 0.5351727604866028,
      "learning_rate": 4.115534271176295e-05,
      "loss": 0.109,
      "step": 5313
    },
    {
      "epoch": 7.152086137281292,
      "grad_norm": 1.053004264831543,
      "learning_rate": 4.112541155342712e-05,
      "loss": 0.1453,
      "step": 5314
    },
    {
      "epoch": 7.15343203230148,
      "grad_norm": 0.4628974199295044,
      "learning_rate": 4.109548039509129e-05,
      "loss": 0.0928,
      "step": 5315
    },
    {
      "epoch": 7.1547779273216685,
      "grad_norm": 0.8880185484886169,
      "learning_rate": 4.106554923675546e-05,
      "loss": 0.1303,
      "step": 5316
    },
    {
      "epoch": 7.156123822341858,
      "grad_norm": 0.9667189717292786,
      "learning_rate": 4.103561807841964e-05,
      "loss": 0.1091,
      "step": 5317
    },
    {
      "epoch": 7.157469717362046,
      "grad_norm": 0.5938847064971924,
      "learning_rate": 4.100568692008381e-05,
      "loss": 0.1042,
      "step": 5318
    },
    {
      "epoch": 7.158815612382234,
      "grad_norm": 0.4277404844760895,
      "learning_rate": 4.097575576174798e-05,
      "loss": 0.0894,
      "step": 5319
    },
    {
      "epoch": 7.160161507402423,
      "grad_norm": 0.5758271813392639,
      "learning_rate": 4.094582460341215e-05,
      "loss": 0.1002,
      "step": 5320
    },
    {
      "epoch": 7.161507402422611,
      "grad_norm": 0.7135682106018066,
      "learning_rate": 4.091589344507633e-05,
      "loss": 0.1233,
      "step": 5321
    },
    {
      "epoch": 7.162853297442799,
      "grad_norm": 0.3471245765686035,
      "learning_rate": 4.08859622867405e-05,
      "loss": 0.1019,
      "step": 5322
    },
    {
      "epoch": 7.164199192462988,
      "grad_norm": 0.6257430911064148,
      "learning_rate": 4.0856031128404673e-05,
      "loss": 0.0975,
      "step": 5323
    },
    {
      "epoch": 7.165545087483176,
      "grad_norm": 0.41476184129714966,
      "learning_rate": 4.082609997006884e-05,
      "loss": 0.0923,
      "step": 5324
    },
    {
      "epoch": 7.166890982503364,
      "grad_norm": 0.5780094861984253,
      "learning_rate": 4.079616881173302e-05,
      "loss": 0.134,
      "step": 5325
    },
    {
      "epoch": 7.168236877523553,
      "grad_norm": 0.5013047456741333,
      "learning_rate": 4.076623765339719e-05,
      "loss": 0.1035,
      "step": 5326
    },
    {
      "epoch": 7.169582772543742,
      "grad_norm": 0.6990243196487427,
      "learning_rate": 4.0736306495061364e-05,
      "loss": 0.1003,
      "step": 5327
    },
    {
      "epoch": 7.17092866756393,
      "grad_norm": 0.45907464623451233,
      "learning_rate": 4.070637533672553e-05,
      "loss": 0.1102,
      "step": 5328
    },
    {
      "epoch": 7.172274562584119,
      "grad_norm": 0.39570704102516174,
      "learning_rate": 4.067644417838971e-05,
      "loss": 0.0713,
      "step": 5329
    },
    {
      "epoch": 7.173620457604307,
      "grad_norm": 0.675459623336792,
      "learning_rate": 4.064651302005388e-05,
      "loss": 0.0998,
      "step": 5330
    },
    {
      "epoch": 7.174966352624495,
      "grad_norm": 0.39394503831863403,
      "learning_rate": 4.0616581861718054e-05,
      "loss": 0.084,
      "step": 5331
    },
    {
      "epoch": 7.176312247644684,
      "grad_norm": 0.4871062636375427,
      "learning_rate": 4.058665070338222e-05,
      "loss": 0.0887,
      "step": 5332
    },
    {
      "epoch": 7.177658142664872,
      "grad_norm": 0.9929616451263428,
      "learning_rate": 4.05567195450464e-05,
      "loss": 0.1089,
      "step": 5333
    },
    {
      "epoch": 7.17900403768506,
      "grad_norm": 0.40187859535217285,
      "learning_rate": 4.052678838671057e-05,
      "loss": 0.0863,
      "step": 5334
    },
    {
      "epoch": 7.180349932705249,
      "grad_norm": 0.7031670212745667,
      "learning_rate": 4.049685722837474e-05,
      "loss": 0.1475,
      "step": 5335
    },
    {
      "epoch": 7.181695827725437,
      "grad_norm": 0.8188710808753967,
      "learning_rate": 4.046692607003891e-05,
      "loss": 0.1135,
      "step": 5336
    },
    {
      "epoch": 7.183041722745626,
      "grad_norm": 0.6570622324943542,
      "learning_rate": 4.043699491170308e-05,
      "loss": 0.083,
      "step": 5337
    },
    {
      "epoch": 7.184387617765815,
      "grad_norm": 0.36841049790382385,
      "learning_rate": 4.040706375336726e-05,
      "loss": 0.097,
      "step": 5338
    },
    {
      "epoch": 7.185733512786003,
      "grad_norm": 0.7849308848381042,
      "learning_rate": 4.037713259503143e-05,
      "loss": 0.1289,
      "step": 5339
    },
    {
      "epoch": 7.187079407806191,
      "grad_norm": 0.502903938293457,
      "learning_rate": 4.03472014366956e-05,
      "loss": 0.0879,
      "step": 5340
    },
    {
      "epoch": 7.18842530282638,
      "grad_norm": 0.36278241872787476,
      "learning_rate": 4.031727027835977e-05,
      "loss": 0.0872,
      "step": 5341
    },
    {
      "epoch": 7.189771197846568,
      "grad_norm": 0.7139447331428528,
      "learning_rate": 4.028733912002395e-05,
      "loss": 0.1473,
      "step": 5342
    },
    {
      "epoch": 7.191117092866756,
      "grad_norm": 0.5038448572158813,
      "learning_rate": 4.025740796168812e-05,
      "loss": 0.0853,
      "step": 5343
    },
    {
      "epoch": 7.192462987886945,
      "grad_norm": 0.9293859004974365,
      "learning_rate": 4.022747680335229e-05,
      "loss": 0.1342,
      "step": 5344
    },
    {
      "epoch": 7.193808882907133,
      "grad_norm": 0.4484531283378601,
      "learning_rate": 4.019754564501646e-05,
      "loss": 0.0904,
      "step": 5345
    },
    {
      "epoch": 7.195154777927321,
      "grad_norm": 0.6598268747329712,
      "learning_rate": 4.016761448668064e-05,
      "loss": 0.1024,
      "step": 5346
    },
    {
      "epoch": 7.1965006729475105,
      "grad_norm": 0.4264848530292511,
      "learning_rate": 4.013768332834481e-05,
      "loss": 0.0879,
      "step": 5347
    },
    {
      "epoch": 7.197846567967699,
      "grad_norm": 0.40640929341316223,
      "learning_rate": 4.0107752170008983e-05,
      "loss": 0.1099,
      "step": 5348
    },
    {
      "epoch": 7.199192462987887,
      "grad_norm": 0.6636741757392883,
      "learning_rate": 4.007782101167315e-05,
      "loss": 0.123,
      "step": 5349
    },
    {
      "epoch": 7.2005383580080755,
      "grad_norm": 0.5090377330780029,
      "learning_rate": 4.004788985333733e-05,
      "loss": 0.1114,
      "step": 5350
    },
    {
      "epoch": 7.201884253028264,
      "grad_norm": 0.3542972505092621,
      "learning_rate": 4.00179586950015e-05,
      "loss": 0.0849,
      "step": 5351
    },
    {
      "epoch": 7.203230148048452,
      "grad_norm": 0.5465635657310486,
      "learning_rate": 3.9988027536665674e-05,
      "loss": 0.0999,
      "step": 5352
    },
    {
      "epoch": 7.2045760430686405,
      "grad_norm": 0.8864239454269409,
      "learning_rate": 3.995809637832984e-05,
      "loss": 0.1267,
      "step": 5353
    },
    {
      "epoch": 7.205921938088829,
      "grad_norm": 1.2651556730270386,
      "learning_rate": 3.992816521999402e-05,
      "loss": 0.0928,
      "step": 5354
    },
    {
      "epoch": 7.207267833109017,
      "grad_norm": 1.014229655265808,
      "learning_rate": 3.989823406165819e-05,
      "loss": 0.2213,
      "step": 5355
    },
    {
      "epoch": 7.2086137281292055,
      "grad_norm": 0.4456195533275604,
      "learning_rate": 3.9868302903322364e-05,
      "loss": 0.0995,
      "step": 5356
    },
    {
      "epoch": 7.209959623149395,
      "grad_norm": 0.5114061236381531,
      "learning_rate": 3.983837174498653e-05,
      "loss": 0.1104,
      "step": 5357
    },
    {
      "epoch": 7.211305518169583,
      "grad_norm": 0.5867816805839539,
      "learning_rate": 3.98084405866507e-05,
      "loss": 0.1025,
      "step": 5358
    },
    {
      "epoch": 7.212651413189771,
      "grad_norm": 0.44464197754859924,
      "learning_rate": 3.977850942831488e-05,
      "loss": 0.0929,
      "step": 5359
    },
    {
      "epoch": 7.21399730820996,
      "grad_norm": 0.41968461871147156,
      "learning_rate": 3.974857826997905e-05,
      "loss": 0.0816,
      "step": 5360
    },
    {
      "epoch": 7.215343203230148,
      "grad_norm": 0.6400025486946106,
      "learning_rate": 3.971864711164322e-05,
      "loss": 0.1029,
      "step": 5361
    },
    {
      "epoch": 7.216689098250336,
      "grad_norm": 0.5488262176513672,
      "learning_rate": 3.968871595330739e-05,
      "loss": 0.0986,
      "step": 5362
    },
    {
      "epoch": 7.218034993270525,
      "grad_norm": 0.47512340545654297,
      "learning_rate": 3.965878479497157e-05,
      "loss": 0.0975,
      "step": 5363
    },
    {
      "epoch": 7.219380888290713,
      "grad_norm": 1.0557293891906738,
      "learning_rate": 3.962885363663574e-05,
      "loss": 0.1442,
      "step": 5364
    },
    {
      "epoch": 7.2207267833109015,
      "grad_norm": 0.5365698337554932,
      "learning_rate": 3.959892247829991e-05,
      "loss": 0.0965,
      "step": 5365
    },
    {
      "epoch": 7.22207267833109,
      "grad_norm": 0.4177844226360321,
      "learning_rate": 3.956899131996408e-05,
      "loss": 0.0935,
      "step": 5366
    },
    {
      "epoch": 7.223418573351279,
      "grad_norm": 0.5235618352890015,
      "learning_rate": 3.953906016162826e-05,
      "loss": 0.1015,
      "step": 5367
    },
    {
      "epoch": 7.224764468371467,
      "grad_norm": 0.8080611824989319,
      "learning_rate": 3.950912900329243e-05,
      "loss": 0.1605,
      "step": 5368
    },
    {
      "epoch": 7.226110363391656,
      "grad_norm": 0.674031674861908,
      "learning_rate": 3.94791978449566e-05,
      "loss": 0.0934,
      "step": 5369
    },
    {
      "epoch": 7.227456258411844,
      "grad_norm": 0.4211328625679016,
      "learning_rate": 3.944926668662077e-05,
      "loss": 0.0876,
      "step": 5370
    },
    {
      "epoch": 7.228802153432032,
      "grad_norm": 0.6726739406585693,
      "learning_rate": 3.941933552828495e-05,
      "loss": 0.115,
      "step": 5371
    },
    {
      "epoch": 7.230148048452221,
      "grad_norm": 0.5851719975471497,
      "learning_rate": 3.938940436994912e-05,
      "loss": 0.0988,
      "step": 5372
    },
    {
      "epoch": 7.231493943472409,
      "grad_norm": 0.6916833519935608,
      "learning_rate": 3.9359473211613294e-05,
      "loss": 0.1449,
      "step": 5373
    },
    {
      "epoch": 7.232839838492597,
      "grad_norm": 0.7849874496459961,
      "learning_rate": 3.932954205327746e-05,
      "loss": 0.1099,
      "step": 5374
    },
    {
      "epoch": 7.234185733512786,
      "grad_norm": 0.3328913152217865,
      "learning_rate": 3.929961089494164e-05,
      "loss": 0.0945,
      "step": 5375
    },
    {
      "epoch": 7.235531628532974,
      "grad_norm": 0.3575689196586609,
      "learning_rate": 3.926967973660581e-05,
      "loss": 0.0941,
      "step": 5376
    },
    {
      "epoch": 7.236877523553163,
      "grad_norm": 1.0516040325164795,
      "learning_rate": 3.9239748578269984e-05,
      "loss": 0.1654,
      "step": 5377
    },
    {
      "epoch": 7.238223418573352,
      "grad_norm": 0.41510462760925293,
      "learning_rate": 3.920981741993415e-05,
      "loss": 0.1103,
      "step": 5378
    },
    {
      "epoch": 7.23956931359354,
      "grad_norm": 1.2816165685653687,
      "learning_rate": 3.917988626159832e-05,
      "loss": 0.1372,
      "step": 5379
    },
    {
      "epoch": 7.240915208613728,
      "grad_norm": 0.31389859318733215,
      "learning_rate": 3.91499551032625e-05,
      "loss": 0.0707,
      "step": 5380
    },
    {
      "epoch": 7.242261103633917,
      "grad_norm": 0.4866223931312561,
      "learning_rate": 3.912002394492667e-05,
      "loss": 0.1273,
      "step": 5381
    },
    {
      "epoch": 7.243606998654105,
      "grad_norm": 0.5717775225639343,
      "learning_rate": 3.909009278659084e-05,
      "loss": 0.1118,
      "step": 5382
    },
    {
      "epoch": 7.244952893674293,
      "grad_norm": 0.3931911289691925,
      "learning_rate": 3.906016162825501e-05,
      "loss": 0.0784,
      "step": 5383
    },
    {
      "epoch": 7.246298788694482,
      "grad_norm": 0.46279409527778625,
      "learning_rate": 3.903023046991919e-05,
      "loss": 0.1023,
      "step": 5384
    },
    {
      "epoch": 7.24764468371467,
      "grad_norm": 0.6279249787330627,
      "learning_rate": 3.900029931158336e-05,
      "loss": 0.0991,
      "step": 5385
    },
    {
      "epoch": 7.248990578734858,
      "grad_norm": 0.5019586086273193,
      "learning_rate": 3.897036815324753e-05,
      "loss": 0.1181,
      "step": 5386
    },
    {
      "epoch": 7.250336473755047,
      "grad_norm": 0.5253816246986389,
      "learning_rate": 3.89404369949117e-05,
      "loss": 0.1413,
      "step": 5387
    },
    {
      "epoch": 7.251682368775236,
      "grad_norm": 0.5405439138412476,
      "learning_rate": 3.891050583657588e-05,
      "loss": 0.1027,
      "step": 5388
    },
    {
      "epoch": 7.253028263795424,
      "grad_norm": 0.367127388715744,
      "learning_rate": 3.888057467824005e-05,
      "loss": 0.0972,
      "step": 5389
    },
    {
      "epoch": 7.2543741588156125,
      "grad_norm": 0.6592867970466614,
      "learning_rate": 3.885064351990422e-05,
      "loss": 0.1372,
      "step": 5390
    },
    {
      "epoch": 7.255720053835801,
      "grad_norm": 0.443771094083786,
      "learning_rate": 3.882071236156839e-05,
      "loss": 0.0886,
      "step": 5391
    },
    {
      "epoch": 7.257065948855989,
      "grad_norm": 0.46708840131759644,
      "learning_rate": 3.879078120323257e-05,
      "loss": 0.1315,
      "step": 5392
    },
    {
      "epoch": 7.2584118438761775,
      "grad_norm": 0.5560361742973328,
      "learning_rate": 3.876085004489674e-05,
      "loss": 0.1063,
      "step": 5393
    },
    {
      "epoch": 7.259757738896366,
      "grad_norm": 0.5017976760864258,
      "learning_rate": 3.8730918886560913e-05,
      "loss": 0.1073,
      "step": 5394
    },
    {
      "epoch": 7.261103633916554,
      "grad_norm": 0.6719421148300171,
      "learning_rate": 3.870098772822508e-05,
      "loss": 0.1296,
      "step": 5395
    },
    {
      "epoch": 7.262449528936743,
      "grad_norm": 0.6579431891441345,
      "learning_rate": 3.867105656988926e-05,
      "loss": 0.0972,
      "step": 5396
    },
    {
      "epoch": 7.263795423956932,
      "grad_norm": 0.6105261445045471,
      "learning_rate": 3.864112541155343e-05,
      "loss": 0.105,
      "step": 5397
    },
    {
      "epoch": 7.26514131897712,
      "grad_norm": 0.31921514868736267,
      "learning_rate": 3.8611194253217604e-05,
      "loss": 0.0702,
      "step": 5398
    },
    {
      "epoch": 7.2664872139973085,
      "grad_norm": 0.49274706840515137,
      "learning_rate": 3.858126309488177e-05,
      "loss": 0.1288,
      "step": 5399
    },
    {
      "epoch": 7.267833109017497,
      "grad_norm": 0.5322997570037842,
      "learning_rate": 3.855133193654595e-05,
      "loss": 0.1142,
      "step": 5400
    },
    {
      "epoch": 7.269179004037685,
      "grad_norm": 0.37486499547958374,
      "learning_rate": 3.852140077821012e-05,
      "loss": 0.0902,
      "step": 5401
    },
    {
      "epoch": 7.2705248990578735,
      "grad_norm": 0.4761049449443817,
      "learning_rate": 3.8491469619874294e-05,
      "loss": 0.1121,
      "step": 5402
    },
    {
      "epoch": 7.271870794078062,
      "grad_norm": 0.37975576519966125,
      "learning_rate": 3.846153846153846e-05,
      "loss": 0.0959,
      "step": 5403
    },
    {
      "epoch": 7.27321668909825,
      "grad_norm": 0.6943584084510803,
      "learning_rate": 3.843160730320264e-05,
      "loss": 0.1371,
      "step": 5404
    },
    {
      "epoch": 7.2745625841184385,
      "grad_norm": 0.31628862023353577,
      "learning_rate": 3.840167614486681e-05,
      "loss": 0.0763,
      "step": 5405
    },
    {
      "epoch": 7.275908479138627,
      "grad_norm": 0.6171517372131348,
      "learning_rate": 3.8371744986530984e-05,
      "loss": 0.1371,
      "step": 5406
    },
    {
      "epoch": 7.277254374158815,
      "grad_norm": 0.4344361126422882,
      "learning_rate": 3.834181382819515e-05,
      "loss": 0.1122,
      "step": 5407
    },
    {
      "epoch": 7.278600269179004,
      "grad_norm": 0.40725353360176086,
      "learning_rate": 3.831188266985933e-05,
      "loss": 0.0868,
      "step": 5408
    },
    {
      "epoch": 7.279946164199193,
      "grad_norm": 0.5659511685371399,
      "learning_rate": 3.82819515115235e-05,
      "loss": 0.1218,
      "step": 5409
    },
    {
      "epoch": 7.281292059219381,
      "grad_norm": 0.42592501640319824,
      "learning_rate": 3.825202035318767e-05,
      "loss": 0.0826,
      "step": 5410
    },
    {
      "epoch": 7.282637954239569,
      "grad_norm": 0.5141270756721497,
      "learning_rate": 3.822208919485184e-05,
      "loss": 0.0969,
      "step": 5411
    },
    {
      "epoch": 7.283983849259758,
      "grad_norm": 0.5760367512702942,
      "learning_rate": 3.819215803651601e-05,
      "loss": 0.103,
      "step": 5412
    },
    {
      "epoch": 7.285329744279946,
      "grad_norm": 0.539993405342102,
      "learning_rate": 3.816222687818019e-05,
      "loss": 0.0943,
      "step": 5413
    },
    {
      "epoch": 7.286675639300134,
      "grad_norm": 0.9855464100837708,
      "learning_rate": 3.813229571984436e-05,
      "loss": 0.1317,
      "step": 5414
    },
    {
      "epoch": 7.288021534320323,
      "grad_norm": 0.6739727258682251,
      "learning_rate": 3.810236456150853e-05,
      "loss": 0.1048,
      "step": 5415
    },
    {
      "epoch": 7.289367429340511,
      "grad_norm": 0.32897424697875977,
      "learning_rate": 3.80724334031727e-05,
      "loss": 0.0839,
      "step": 5416
    },
    {
      "epoch": 7.2907133243607,
      "grad_norm": 0.7268689274787903,
      "learning_rate": 3.804250224483688e-05,
      "loss": 0.1034,
      "step": 5417
    },
    {
      "epoch": 7.292059219380889,
      "grad_norm": 0.39548465609550476,
      "learning_rate": 3.801257108650105e-05,
      "loss": 0.1105,
      "step": 5418
    },
    {
      "epoch": 7.293405114401077,
      "grad_norm": 0.36838462948799133,
      "learning_rate": 3.7982639928165223e-05,
      "loss": 0.0887,
      "step": 5419
    },
    {
      "epoch": 7.294751009421265,
      "grad_norm": 0.8560608625411987,
      "learning_rate": 3.795270876982939e-05,
      "loss": 0.1108,
      "step": 5420
    },
    {
      "epoch": 7.296096904441454,
      "grad_norm": 0.9718467593193054,
      "learning_rate": 3.792277761149356e-05,
      "loss": 0.1395,
      "step": 5421
    },
    {
      "epoch": 7.297442799461642,
      "grad_norm": 0.6347852945327759,
      "learning_rate": 3.789284645315774e-05,
      "loss": 0.1087,
      "step": 5422
    },
    {
      "epoch": 7.29878869448183,
      "grad_norm": 0.6293415427207947,
      "learning_rate": 3.786291529482191e-05,
      "loss": 0.1121,
      "step": 5423
    },
    {
      "epoch": 7.300134589502019,
      "grad_norm": 0.6994421482086182,
      "learning_rate": 3.783298413648608e-05,
      "loss": 0.1277,
      "step": 5424
    },
    {
      "epoch": 7.301480484522207,
      "grad_norm": 0.9260048270225525,
      "learning_rate": 3.780305297815025e-05,
      "loss": 0.1182,
      "step": 5425
    },
    {
      "epoch": 7.302826379542395,
      "grad_norm": 0.478092223405838,
      "learning_rate": 3.777312181981443e-05,
      "loss": 0.1243,
      "step": 5426
    },
    {
      "epoch": 7.304172274562584,
      "grad_norm": 0.4959579110145569,
      "learning_rate": 3.77431906614786e-05,
      "loss": 0.1094,
      "step": 5427
    },
    {
      "epoch": 7.305518169582773,
      "grad_norm": 0.47548186779022217,
      "learning_rate": 3.771325950314277e-05,
      "loss": 0.1003,
      "step": 5428
    },
    {
      "epoch": 7.306864064602961,
      "grad_norm": 0.39368149638175964,
      "learning_rate": 3.768332834480694e-05,
      "loss": 0.0736,
      "step": 5429
    },
    {
      "epoch": 7.30820995962315,
      "grad_norm": 0.5291030406951904,
      "learning_rate": 3.765339718647112e-05,
      "loss": 0.0988,
      "step": 5430
    },
    {
      "epoch": 7.309555854643338,
      "grad_norm": 0.4403159022331238,
      "learning_rate": 3.762346602813529e-05,
      "loss": 0.098,
      "step": 5431
    },
    {
      "epoch": 7.310901749663526,
      "grad_norm": 0.5398394465446472,
      "learning_rate": 3.759353486979946e-05,
      "loss": 0.1511,
      "step": 5432
    },
    {
      "epoch": 7.312247644683715,
      "grad_norm": 0.664901614189148,
      "learning_rate": 3.756360371146363e-05,
      "loss": 0.1326,
      "step": 5433
    },
    {
      "epoch": 7.313593539703903,
      "grad_norm": 0.533126711845398,
      "learning_rate": 3.753367255312781e-05,
      "loss": 0.1016,
      "step": 5434
    },
    {
      "epoch": 7.314939434724091,
      "grad_norm": 0.9582322835922241,
      "learning_rate": 3.750374139479198e-05,
      "loss": 0.1841,
      "step": 5435
    },
    {
      "epoch": 7.31628532974428,
      "grad_norm": 0.4315539598464966,
      "learning_rate": 3.747381023645615e-05,
      "loss": 0.098,
      "step": 5436
    },
    {
      "epoch": 7.317631224764469,
      "grad_norm": 0.499973863363266,
      "learning_rate": 3.744387907812032e-05,
      "loss": 0.0976,
      "step": 5437
    },
    {
      "epoch": 7.318977119784657,
      "grad_norm": 0.454036682844162,
      "learning_rate": 3.74139479197845e-05,
      "loss": 0.1089,
      "step": 5438
    },
    {
      "epoch": 7.3203230148048455,
      "grad_norm": 0.769903838634491,
      "learning_rate": 3.738401676144867e-05,
      "loss": 0.1058,
      "step": 5439
    },
    {
      "epoch": 7.321668909825034,
      "grad_norm": 0.4386531412601471,
      "learning_rate": 3.735408560311284e-05,
      "loss": 0.104,
      "step": 5440
    },
    {
      "epoch": 7.323014804845222,
      "grad_norm": 0.7717793583869934,
      "learning_rate": 3.732415444477701e-05,
      "loss": 0.116,
      "step": 5441
    },
    {
      "epoch": 7.3243606998654105,
      "grad_norm": 0.6413722038269043,
      "learning_rate": 3.729422328644119e-05,
      "loss": 0.1292,
      "step": 5442
    },
    {
      "epoch": 7.325706594885599,
      "grad_norm": 0.44247087836265564,
      "learning_rate": 3.726429212810536e-05,
      "loss": 0.0998,
      "step": 5443
    },
    {
      "epoch": 7.327052489905787,
      "grad_norm": 0.39318448305130005,
      "learning_rate": 3.7234360969769533e-05,
      "loss": 0.1119,
      "step": 5444
    },
    {
      "epoch": 7.3283983849259755,
      "grad_norm": 0.4565681517124176,
      "learning_rate": 3.72044298114337e-05,
      "loss": 0.1112,
      "step": 5445
    },
    {
      "epoch": 7.329744279946164,
      "grad_norm": 1.0646374225616455,
      "learning_rate": 3.717449865309788e-05,
      "loss": 0.1785,
      "step": 5446
    },
    {
      "epoch": 7.331090174966352,
      "grad_norm": 0.758609414100647,
      "learning_rate": 3.714456749476205e-05,
      "loss": 0.1188,
      "step": 5447
    },
    {
      "epoch": 7.332436069986541,
      "grad_norm": 0.3906548321247101,
      "learning_rate": 3.7114636336426224e-05,
      "loss": 0.1011,
      "step": 5448
    },
    {
      "epoch": 7.33378196500673,
      "grad_norm": 0.37548545002937317,
      "learning_rate": 3.708470517809039e-05,
      "loss": 0.091,
      "step": 5449
    },
    {
      "epoch": 7.335127860026918,
      "grad_norm": 0.5481333136558533,
      "learning_rate": 3.705477401975457e-05,
      "loss": 0.1168,
      "step": 5450
    },
    {
      "epoch": 7.336473755047106,
      "grad_norm": 0.4042716920375824,
      "learning_rate": 3.702484286141874e-05,
      "loss": 0.0868,
      "step": 5451
    },
    {
      "epoch": 7.337819650067295,
      "grad_norm": 0.5186471343040466,
      "learning_rate": 3.6994911703082914e-05,
      "loss": 0.103,
      "step": 5452
    },
    {
      "epoch": 7.339165545087483,
      "grad_norm": 0.6028823256492615,
      "learning_rate": 3.696498054474708e-05,
      "loss": 0.1221,
      "step": 5453
    },
    {
      "epoch": 7.340511440107671,
      "grad_norm": 0.5655741095542908,
      "learning_rate": 3.693504938641126e-05,
      "loss": 0.0957,
      "step": 5454
    },
    {
      "epoch": 7.34185733512786,
      "grad_norm": 0.9872926473617554,
      "learning_rate": 3.690511822807543e-05,
      "loss": 0.1315,
      "step": 5455
    },
    {
      "epoch": 7.343203230148048,
      "grad_norm": 0.736969530582428,
      "learning_rate": 3.6875187069739604e-05,
      "loss": 0.1232,
      "step": 5456
    },
    {
      "epoch": 7.344549125168237,
      "grad_norm": 1.1943734884262085,
      "learning_rate": 3.684525591140377e-05,
      "loss": 0.1009,
      "step": 5457
    },
    {
      "epoch": 7.345895020188426,
      "grad_norm": 0.8079413175582886,
      "learning_rate": 3.681532475306795e-05,
      "loss": 0.1192,
      "step": 5458
    },
    {
      "epoch": 7.347240915208614,
      "grad_norm": 0.4981935918331146,
      "learning_rate": 3.678539359473212e-05,
      "loss": 0.1153,
      "step": 5459
    },
    {
      "epoch": 7.348586810228802,
      "grad_norm": 0.5481457710266113,
      "learning_rate": 3.6755462436396294e-05,
      "loss": 0.0933,
      "step": 5460
    },
    {
      "epoch": 7.349932705248991,
      "grad_norm": 0.6738449931144714,
      "learning_rate": 3.672553127806046e-05,
      "loss": 0.1285,
      "step": 5461
    },
    {
      "epoch": 7.351278600269179,
      "grad_norm": 0.6058605313301086,
      "learning_rate": 3.669560011972463e-05,
      "loss": 0.0962,
      "step": 5462
    },
    {
      "epoch": 7.352624495289367,
      "grad_norm": 0.43290409445762634,
      "learning_rate": 3.666566896138881e-05,
      "loss": 0.1147,
      "step": 5463
    },
    {
      "epoch": 7.353970390309556,
      "grad_norm": 0.377821683883667,
      "learning_rate": 3.663573780305298e-05,
      "loss": 0.1013,
      "step": 5464
    },
    {
      "epoch": 7.355316285329744,
      "grad_norm": 0.39372527599334717,
      "learning_rate": 3.6605806644717147e-05,
      "loss": 0.0736,
      "step": 5465
    },
    {
      "epoch": 7.356662180349932,
      "grad_norm": 1.0375816822052002,
      "learning_rate": 3.657587548638132e-05,
      "loss": 0.1126,
      "step": 5466
    },
    {
      "epoch": 7.358008075370121,
      "grad_norm": 0.5265425443649292,
      "learning_rate": 3.654594432804549e-05,
      "loss": 0.0851,
      "step": 5467
    },
    {
      "epoch": 7.35935397039031,
      "grad_norm": 0.6523418426513672,
      "learning_rate": 3.651601316970967e-05,
      "loss": 0.0855,
      "step": 5468
    },
    {
      "epoch": 7.360699865410498,
      "grad_norm": 0.5802758932113647,
      "learning_rate": 3.648608201137384e-05,
      "loss": 0.1312,
      "step": 5469
    },
    {
      "epoch": 7.362045760430687,
      "grad_norm": 0.3522513210773468,
      "learning_rate": 3.645615085303801e-05,
      "loss": 0.0861,
      "step": 5470
    },
    {
      "epoch": 7.363391655450875,
      "grad_norm": 0.45464810729026794,
      "learning_rate": 3.642621969470218e-05,
      "loss": 0.0915,
      "step": 5471
    },
    {
      "epoch": 7.364737550471063,
      "grad_norm": 1.1112271547317505,
      "learning_rate": 3.639628853636636e-05,
      "loss": 0.2393,
      "step": 5472
    },
    {
      "epoch": 7.366083445491252,
      "grad_norm": 0.5560862421989441,
      "learning_rate": 3.636635737803053e-05,
      "loss": 0.1061,
      "step": 5473
    },
    {
      "epoch": 7.36742934051144,
      "grad_norm": 0.6586828827857971,
      "learning_rate": 3.63364262196947e-05,
      "loss": 0.1395,
      "step": 5474
    },
    {
      "epoch": 7.368775235531628,
      "grad_norm": 0.36452943086624146,
      "learning_rate": 3.630649506135887e-05,
      "loss": 0.0875,
      "step": 5475
    },
    {
      "epoch": 7.370121130551817,
      "grad_norm": 0.28763672709465027,
      "learning_rate": 3.627656390302305e-05,
      "loss": 0.07,
      "step": 5476
    },
    {
      "epoch": 7.371467025572006,
      "grad_norm": 1.042804479598999,
      "learning_rate": 3.624663274468722e-05,
      "loss": 0.2086,
      "step": 5477
    },
    {
      "epoch": 7.372812920592194,
      "grad_norm": 0.3322891891002655,
      "learning_rate": 3.621670158635139e-05,
      "loss": 0.1018,
      "step": 5478
    },
    {
      "epoch": 7.3741588156123825,
      "grad_norm": 0.46698427200317383,
      "learning_rate": 3.618677042801556e-05,
      "loss": 0.107,
      "step": 5479
    },
    {
      "epoch": 7.375504710632571,
      "grad_norm": 0.3669217526912689,
      "learning_rate": 3.615683926967974e-05,
      "loss": 0.077,
      "step": 5480
    },
    {
      "epoch": 7.376850605652759,
      "grad_norm": 0.9577977061271667,
      "learning_rate": 3.612690811134391e-05,
      "loss": 0.1352,
      "step": 5481
    },
    {
      "epoch": 7.3781965006729475,
      "grad_norm": 0.5514007806777954,
      "learning_rate": 3.609697695300808e-05,
      "loss": 0.1167,
      "step": 5482
    },
    {
      "epoch": 7.379542395693136,
      "grad_norm": 0.4405003786087036,
      "learning_rate": 3.606704579467225e-05,
      "loss": 0.1,
      "step": 5483
    },
    {
      "epoch": 7.380888290713324,
      "grad_norm": 0.3411722779273987,
      "learning_rate": 3.603711463633643e-05,
      "loss": 0.0773,
      "step": 5484
    },
    {
      "epoch": 7.3822341857335125,
      "grad_norm": 0.5201855301856995,
      "learning_rate": 3.60071834780006e-05,
      "loss": 0.0903,
      "step": 5485
    },
    {
      "epoch": 7.383580080753701,
      "grad_norm": 0.8419420719146729,
      "learning_rate": 3.597725231966477e-05,
      "loss": 0.1342,
      "step": 5486
    },
    {
      "epoch": 7.384925975773889,
      "grad_norm": 0.5351986289024353,
      "learning_rate": 3.594732116132894e-05,
      "loss": 0.0965,
      "step": 5487
    },
    {
      "epoch": 7.386271870794078,
      "grad_norm": 0.4259755313396454,
      "learning_rate": 3.591739000299312e-05,
      "loss": 0.0978,
      "step": 5488
    },
    {
      "epoch": 7.387617765814267,
      "grad_norm": 0.33300888538360596,
      "learning_rate": 3.588745884465729e-05,
      "loss": 0.0815,
      "step": 5489
    },
    {
      "epoch": 7.388963660834455,
      "grad_norm": 0.5927473902702332,
      "learning_rate": 3.5857527686321463e-05,
      "loss": 0.1107,
      "step": 5490
    },
    {
      "epoch": 7.390309555854643,
      "grad_norm": 0.40024861693382263,
      "learning_rate": 3.582759652798563e-05,
      "loss": 0.0786,
      "step": 5491
    },
    {
      "epoch": 7.391655450874832,
      "grad_norm": 1.1486210823059082,
      "learning_rate": 3.579766536964981e-05,
      "loss": 0.151,
      "step": 5492
    },
    {
      "epoch": 7.39300134589502,
      "grad_norm": 0.6805046200752258,
      "learning_rate": 3.576773421131398e-05,
      "loss": 0.0999,
      "step": 5493
    },
    {
      "epoch": 7.3943472409152085,
      "grad_norm": 0.49274885654449463,
      "learning_rate": 3.5737803052978154e-05,
      "loss": 0.1029,
      "step": 5494
    },
    {
      "epoch": 7.395693135935397,
      "grad_norm": 0.6279993653297424,
      "learning_rate": 3.570787189464232e-05,
      "loss": 0.1368,
      "step": 5495
    },
    {
      "epoch": 7.397039030955585,
      "grad_norm": 0.5183766484260559,
      "learning_rate": 3.56779407363065e-05,
      "loss": 0.1108,
      "step": 5496
    },
    {
      "epoch": 7.398384925975774,
      "grad_norm": 0.43583643436431885,
      "learning_rate": 3.564800957797067e-05,
      "loss": 0.0803,
      "step": 5497
    },
    {
      "epoch": 7.399730820995963,
      "grad_norm": 0.5939487218856812,
      "learning_rate": 3.5618078419634844e-05,
      "loss": 0.0969,
      "step": 5498
    },
    {
      "epoch": 7.401076716016151,
      "grad_norm": 0.5106578469276428,
      "learning_rate": 3.558814726129901e-05,
      "loss": 0.0961,
      "step": 5499
    },
    {
      "epoch": 7.402422611036339,
      "grad_norm": 0.8715376257896423,
      "learning_rate": 3.555821610296319e-05,
      "loss": 0.1417,
      "step": 5500
    },
    {
      "epoch": 7.403768506056528,
      "grad_norm": 0.8984994888305664,
      "learning_rate": 3.552828494462736e-05,
      "loss": 0.1395,
      "step": 5501
    },
    {
      "epoch": 7.405114401076716,
      "grad_norm": 0.5290030241012573,
      "learning_rate": 3.5498353786291534e-05,
      "loss": 0.0846,
      "step": 5502
    },
    {
      "epoch": 7.406460296096904,
      "grad_norm": 0.5866173505783081,
      "learning_rate": 3.54684226279557e-05,
      "loss": 0.1088,
      "step": 5503
    },
    {
      "epoch": 7.407806191117093,
      "grad_norm": 0.9853978753089905,
      "learning_rate": 3.543849146961988e-05,
      "loss": 0.1376,
      "step": 5504
    },
    {
      "epoch": 7.409152086137281,
      "grad_norm": 0.5139390826225281,
      "learning_rate": 3.540856031128405e-05,
      "loss": 0.1288,
      "step": 5505
    },
    {
      "epoch": 7.410497981157469,
      "grad_norm": 0.38074415922164917,
      "learning_rate": 3.5378629152948224e-05,
      "loss": 0.0789,
      "step": 5506
    },
    {
      "epoch": 7.411843876177658,
      "grad_norm": 0.6667924523353577,
      "learning_rate": 3.534869799461239e-05,
      "loss": 0.1599,
      "step": 5507
    },
    {
      "epoch": 7.413189771197847,
      "grad_norm": 0.5283180475234985,
      "learning_rate": 3.531876683627657e-05,
      "loss": 0.1145,
      "step": 5508
    },
    {
      "epoch": 7.414535666218035,
      "grad_norm": 0.835772693157196,
      "learning_rate": 3.528883567794074e-05,
      "loss": 0.1577,
      "step": 5509
    },
    {
      "epoch": 7.415881561238224,
      "grad_norm": 0.35695764422416687,
      "learning_rate": 3.5258904519604914e-05,
      "loss": 0.085,
      "step": 5510
    },
    {
      "epoch": 7.417227456258412,
      "grad_norm": 0.394092321395874,
      "learning_rate": 3.522897336126908e-05,
      "loss": 0.1013,
      "step": 5511
    },
    {
      "epoch": 7.4185733512786,
      "grad_norm": 0.39425185322761536,
      "learning_rate": 3.519904220293326e-05,
      "loss": 0.0856,
      "step": 5512
    },
    {
      "epoch": 7.419919246298789,
      "grad_norm": 0.5102159380912781,
      "learning_rate": 3.516911104459743e-05,
      "loss": 0.1105,
      "step": 5513
    },
    {
      "epoch": 7.421265141318977,
      "grad_norm": 0.41709208488464355,
      "learning_rate": 3.51391798862616e-05,
      "loss": 0.095,
      "step": 5514
    },
    {
      "epoch": 7.422611036339165,
      "grad_norm": 0.49256372451782227,
      "learning_rate": 3.510924872792577e-05,
      "loss": 0.1069,
      "step": 5515
    },
    {
      "epoch": 7.423956931359354,
      "grad_norm": 0.3320908546447754,
      "learning_rate": 3.507931756958994e-05,
      "loss": 0.0815,
      "step": 5516
    },
    {
      "epoch": 7.425302826379543,
      "grad_norm": 0.5888515114784241,
      "learning_rate": 3.504938641125411e-05,
      "loss": 0.1194,
      "step": 5517
    },
    {
      "epoch": 7.426648721399731,
      "grad_norm": 0.5338671207427979,
      "learning_rate": 3.501945525291829e-05,
      "loss": 0.0806,
      "step": 5518
    },
    {
      "epoch": 7.4279946164199195,
      "grad_norm": 0.4485015571117401,
      "learning_rate": 3.498952409458246e-05,
      "loss": 0.1166,
      "step": 5519
    },
    {
      "epoch": 7.429340511440108,
      "grad_norm": 0.39777427911758423,
      "learning_rate": 3.495959293624663e-05,
      "loss": 0.0933,
      "step": 5520
    },
    {
      "epoch": 7.430686406460296,
      "grad_norm": 0.37181970477104187,
      "learning_rate": 3.49296617779108e-05,
      "loss": 0.1023,
      "step": 5521
    },
    {
      "epoch": 7.4320323014804845,
      "grad_norm": 0.7557536363601685,
      "learning_rate": 3.489973061957498e-05,
      "loss": 0.1299,
      "step": 5522
    },
    {
      "epoch": 7.433378196500673,
      "grad_norm": 0.569239616394043,
      "learning_rate": 3.486979946123915e-05,
      "loss": 0.1032,
      "step": 5523
    },
    {
      "epoch": 7.434724091520861,
      "grad_norm": 0.7514578104019165,
      "learning_rate": 3.483986830290332e-05,
      "loss": 0.0967,
      "step": 5524
    },
    {
      "epoch": 7.43606998654105,
      "grad_norm": 1.1591099500656128,
      "learning_rate": 3.480993714456749e-05,
      "loss": 0.1661,
      "step": 5525
    },
    {
      "epoch": 7.437415881561238,
      "grad_norm": 0.5214411020278931,
      "learning_rate": 3.478000598623167e-05,
      "loss": 0.0963,
      "step": 5526
    },
    {
      "epoch": 7.438761776581426,
      "grad_norm": 0.3785581588745117,
      "learning_rate": 3.475007482789584e-05,
      "loss": 0.0847,
      "step": 5527
    },
    {
      "epoch": 7.4401076716016155,
      "grad_norm": 0.4439954161643982,
      "learning_rate": 3.472014366956001e-05,
      "loss": 0.1014,
      "step": 5528
    },
    {
      "epoch": 7.441453566621804,
      "grad_norm": 0.7033604979515076,
      "learning_rate": 3.469021251122418e-05,
      "loss": 0.1111,
      "step": 5529
    },
    {
      "epoch": 7.442799461641992,
      "grad_norm": 0.41206687688827515,
      "learning_rate": 3.466028135288836e-05,
      "loss": 0.0942,
      "step": 5530
    },
    {
      "epoch": 7.4441453566621805,
      "grad_norm": 0.8640713691711426,
      "learning_rate": 3.463035019455253e-05,
      "loss": 0.1688,
      "step": 5531
    },
    {
      "epoch": 7.445491251682369,
      "grad_norm": 0.5779554843902588,
      "learning_rate": 3.46004190362167e-05,
      "loss": 0.1072,
      "step": 5532
    },
    {
      "epoch": 7.446837146702557,
      "grad_norm": 0.3741513788700104,
      "learning_rate": 3.457048787788087e-05,
      "loss": 0.1034,
      "step": 5533
    },
    {
      "epoch": 7.4481830417227455,
      "grad_norm": 0.4993402361869812,
      "learning_rate": 3.454055671954505e-05,
      "loss": 0.08,
      "step": 5534
    },
    {
      "epoch": 7.449528936742934,
      "grad_norm": 0.5333206057548523,
      "learning_rate": 3.451062556120922e-05,
      "loss": 0.1211,
      "step": 5535
    },
    {
      "epoch": 7.450874831763122,
      "grad_norm": 0.4294253885746002,
      "learning_rate": 3.448069440287339e-05,
      "loss": 0.0999,
      "step": 5536
    },
    {
      "epoch": 7.4522207267833105,
      "grad_norm": 0.45682451128959656,
      "learning_rate": 3.445076324453756e-05,
      "loss": 0.1052,
      "step": 5537
    },
    {
      "epoch": 7.4535666218035,
      "grad_norm": 0.5826787352561951,
      "learning_rate": 3.442083208620174e-05,
      "loss": 0.1108,
      "step": 5538
    },
    {
      "epoch": 7.454912516823688,
      "grad_norm": 0.9270480275154114,
      "learning_rate": 3.439090092786591e-05,
      "loss": 0.1628,
      "step": 5539
    },
    {
      "epoch": 7.456258411843876,
      "grad_norm": 0.49507391452789307,
      "learning_rate": 3.4360969769530083e-05,
      "loss": 0.0968,
      "step": 5540
    },
    {
      "epoch": 7.457604306864065,
      "grad_norm": 0.9287358522415161,
      "learning_rate": 3.433103861119425e-05,
      "loss": 0.1504,
      "step": 5541
    },
    {
      "epoch": 7.458950201884253,
      "grad_norm": 0.9669667482376099,
      "learning_rate": 3.430110745285843e-05,
      "loss": 0.1106,
      "step": 5542
    },
    {
      "epoch": 7.460296096904441,
      "grad_norm": 1.2080342769622803,
      "learning_rate": 3.42711762945226e-05,
      "loss": 0.1596,
      "step": 5543
    },
    {
      "epoch": 7.46164199192463,
      "grad_norm": 0.5390872359275818,
      "learning_rate": 3.4241245136186774e-05,
      "loss": 0.0947,
      "step": 5544
    },
    {
      "epoch": 7.462987886944818,
      "grad_norm": 0.6452310085296631,
      "learning_rate": 3.421131397785094e-05,
      "loss": 0.1217,
      "step": 5545
    },
    {
      "epoch": 7.464333781965006,
      "grad_norm": 0.45254120230674744,
      "learning_rate": 3.418138281951512e-05,
      "loss": 0.0974,
      "step": 5546
    },
    {
      "epoch": 7.465679676985195,
      "grad_norm": 0.8223339319229126,
      "learning_rate": 3.415145166117929e-05,
      "loss": 0.1202,
      "step": 5547
    },
    {
      "epoch": 7.467025572005384,
      "grad_norm": 0.49705561995506287,
      "learning_rate": 3.4121520502843464e-05,
      "loss": 0.1069,
      "step": 5548
    },
    {
      "epoch": 7.468371467025572,
      "grad_norm": 0.5208194255828857,
      "learning_rate": 3.409158934450763e-05,
      "loss": 0.1194,
      "step": 5549
    },
    {
      "epoch": 7.469717362045761,
      "grad_norm": 0.3239710032939911,
      "learning_rate": 3.406165818617181e-05,
      "loss": 0.0657,
      "step": 5550
    },
    {
      "epoch": 7.471063257065949,
      "grad_norm": 0.6223863959312439,
      "learning_rate": 3.403172702783598e-05,
      "loss": 0.1402,
      "step": 5551
    },
    {
      "epoch": 7.472409152086137,
      "grad_norm": 0.6346257328987122,
      "learning_rate": 3.4001795869500154e-05,
      "loss": 0.1038,
      "step": 5552
    },
    {
      "epoch": 7.473755047106326,
      "grad_norm": 0.5539040565490723,
      "learning_rate": 3.397186471116432e-05,
      "loss": 0.1001,
      "step": 5553
    },
    {
      "epoch": 7.475100942126514,
      "grad_norm": 0.5934041142463684,
      "learning_rate": 3.39419335528285e-05,
      "loss": 0.0988,
      "step": 5554
    },
    {
      "epoch": 7.476446837146702,
      "grad_norm": 0.5437764525413513,
      "learning_rate": 3.391200239449267e-05,
      "loss": 0.0901,
      "step": 5555
    },
    {
      "epoch": 7.477792732166891,
      "grad_norm": 0.46868425607681274,
      "learning_rate": 3.3882071236156844e-05,
      "loss": 0.1157,
      "step": 5556
    },
    {
      "epoch": 7.479138627187079,
      "grad_norm": 0.41789621114730835,
      "learning_rate": 3.385214007782101e-05,
      "loss": 0.0879,
      "step": 5557
    },
    {
      "epoch": 7.480484522207268,
      "grad_norm": 0.9377905130386353,
      "learning_rate": 3.382220891948519e-05,
      "loss": 0.0984,
      "step": 5558
    },
    {
      "epoch": 7.481830417227457,
      "grad_norm": 0.47598227858543396,
      "learning_rate": 3.379227776114936e-05,
      "loss": 0.0964,
      "step": 5559
    },
    {
      "epoch": 7.483176312247645,
      "grad_norm": 0.6458187699317932,
      "learning_rate": 3.3762346602813534e-05,
      "loss": 0.1153,
      "step": 5560
    },
    {
      "epoch": 7.484522207267833,
      "grad_norm": 1.010472059249878,
      "learning_rate": 3.37324154444777e-05,
      "loss": 0.1432,
      "step": 5561
    },
    {
      "epoch": 7.485868102288022,
      "grad_norm": 0.4876784086227417,
      "learning_rate": 3.370248428614188e-05,
      "loss": 0.0995,
      "step": 5562
    },
    {
      "epoch": 7.48721399730821,
      "grad_norm": 0.42687034606933594,
      "learning_rate": 3.367255312780605e-05,
      "loss": 0.1248,
      "step": 5563
    },
    {
      "epoch": 7.488559892328398,
      "grad_norm": 1.6833220720291138,
      "learning_rate": 3.3642621969470224e-05,
      "loss": 0.1629,
      "step": 5564
    },
    {
      "epoch": 7.489905787348587,
      "grad_norm": 0.49183720350265503,
      "learning_rate": 3.3612690811134394e-05,
      "loss": 0.1201,
      "step": 5565
    },
    {
      "epoch": 7.491251682368775,
      "grad_norm": 0.8220638632774353,
      "learning_rate": 3.358275965279856e-05,
      "loss": 0.1426,
      "step": 5566
    },
    {
      "epoch": 7.492597577388963,
      "grad_norm": 0.4905939996242523,
      "learning_rate": 3.355282849446273e-05,
      "loss": 0.0891,
      "step": 5567
    },
    {
      "epoch": 7.4939434724091525,
      "grad_norm": 0.45288535952568054,
      "learning_rate": 3.352289733612691e-05,
      "loss": 0.1028,
      "step": 5568
    },
    {
      "epoch": 7.495289367429341,
      "grad_norm": 0.7814345955848694,
      "learning_rate": 3.349296617779108e-05,
      "loss": 0.1453,
      "step": 5569
    },
    {
      "epoch": 7.496635262449529,
      "grad_norm": 0.8912461996078491,
      "learning_rate": 3.346303501945525e-05,
      "loss": 0.1551,
      "step": 5570
    },
    {
      "epoch": 7.4979811574697175,
      "grad_norm": 0.3268997073173523,
      "learning_rate": 3.343310386111942e-05,
      "loss": 0.0814,
      "step": 5571
    },
    {
      "epoch": 7.499327052489906,
      "grad_norm": 0.5744509100914001,
      "learning_rate": 3.34031727027836e-05,
      "loss": 0.1152,
      "step": 5572
    },
    {
      "epoch": 7.500672947510094,
      "grad_norm": 1.0688889026641846,
      "learning_rate": 3.337324154444777e-05,
      "loss": 0.1343,
      "step": 5573
    },
    {
      "epoch": 7.5020188425302825,
      "grad_norm": 0.4250040650367737,
      "learning_rate": 3.334331038611194e-05,
      "loss": 0.0916,
      "step": 5574
    },
    {
      "epoch": 7.503364737550471,
      "grad_norm": 0.5699291229248047,
      "learning_rate": 3.331337922777611e-05,
      "loss": 0.1104,
      "step": 5575
    },
    {
      "epoch": 7.504710632570659,
      "grad_norm": 0.6078813672065735,
      "learning_rate": 3.328344806944029e-05,
      "loss": 0.1171,
      "step": 5576
    },
    {
      "epoch": 7.506056527590848,
      "grad_norm": 0.7407456636428833,
      "learning_rate": 3.325351691110446e-05,
      "loss": 0.1063,
      "step": 5577
    },
    {
      "epoch": 7.507402422611037,
      "grad_norm": 0.4515751004219055,
      "learning_rate": 3.322358575276863e-05,
      "loss": 0.1012,
      "step": 5578
    },
    {
      "epoch": 7.508748317631225,
      "grad_norm": 0.5359148383140564,
      "learning_rate": 3.31936545944328e-05,
      "loss": 0.11,
      "step": 5579
    },
    {
      "epoch": 7.510094212651413,
      "grad_norm": 0.5375447273254395,
      "learning_rate": 3.316372343609698e-05,
      "loss": 0.092,
      "step": 5580
    },
    {
      "epoch": 7.511440107671602,
      "grad_norm": 0.8390529751777649,
      "learning_rate": 3.313379227776115e-05,
      "loss": 0.103,
      "step": 5581
    },
    {
      "epoch": 7.51278600269179,
      "grad_norm": 0.5536816120147705,
      "learning_rate": 3.310386111942532e-05,
      "loss": 0.1109,
      "step": 5582
    },
    {
      "epoch": 7.514131897711978,
      "grad_norm": 0.5824776887893677,
      "learning_rate": 3.307392996108949e-05,
      "loss": 0.0968,
      "step": 5583
    },
    {
      "epoch": 7.515477792732167,
      "grad_norm": 0.5969691872596741,
      "learning_rate": 3.304399880275367e-05,
      "loss": 0.0881,
      "step": 5584
    },
    {
      "epoch": 7.516823687752355,
      "grad_norm": 0.5985352396965027,
      "learning_rate": 3.301406764441784e-05,
      "loss": 0.1045,
      "step": 5585
    },
    {
      "epoch": 7.518169582772543,
      "grad_norm": 0.6021749377250671,
      "learning_rate": 3.2984136486082013e-05,
      "loss": 0.1324,
      "step": 5586
    },
    {
      "epoch": 7.519515477792732,
      "grad_norm": 0.3981040120124817,
      "learning_rate": 3.295420532774618e-05,
      "loss": 0.0894,
      "step": 5587
    },
    {
      "epoch": 7.52086137281292,
      "grad_norm": 0.728896975517273,
      "learning_rate": 3.292427416941036e-05,
      "loss": 0.1286,
      "step": 5588
    },
    {
      "epoch": 7.522207267833109,
      "grad_norm": 0.5589407086372375,
      "learning_rate": 3.289434301107453e-05,
      "loss": 0.1182,
      "step": 5589
    },
    {
      "epoch": 7.523553162853298,
      "grad_norm": 1.6136517524719238,
      "learning_rate": 3.2864411852738704e-05,
      "loss": 0.1141,
      "step": 5590
    },
    {
      "epoch": 7.524899057873486,
      "grad_norm": 0.31712058186531067,
      "learning_rate": 3.283448069440287e-05,
      "loss": 0.0802,
      "step": 5591
    },
    {
      "epoch": 7.526244952893674,
      "grad_norm": 0.3450738787651062,
      "learning_rate": 3.280454953606705e-05,
      "loss": 0.09,
      "step": 5592
    },
    {
      "epoch": 7.527590847913863,
      "grad_norm": 0.570662796497345,
      "learning_rate": 3.277461837773122e-05,
      "loss": 0.1096,
      "step": 5593
    },
    {
      "epoch": 7.528936742934051,
      "grad_norm": 0.34964239597320557,
      "learning_rate": 3.2744687219395394e-05,
      "loss": 0.0939,
      "step": 5594
    },
    {
      "epoch": 7.530282637954239,
      "grad_norm": 0.4601711332798004,
      "learning_rate": 3.271475606105956e-05,
      "loss": 0.121,
      "step": 5595
    },
    {
      "epoch": 7.531628532974428,
      "grad_norm": 0.5845367908477783,
      "learning_rate": 3.268482490272374e-05,
      "loss": 0.1097,
      "step": 5596
    },
    {
      "epoch": 7.532974427994617,
      "grad_norm": 0.6448238492012024,
      "learning_rate": 3.265489374438791e-05,
      "loss": 0.1053,
      "step": 5597
    },
    {
      "epoch": 7.534320323014805,
      "grad_norm": 1.3858040571212769,
      "learning_rate": 3.2624962586052084e-05,
      "loss": 0.146,
      "step": 5598
    },
    {
      "epoch": 7.535666218034994,
      "grad_norm": 0.4234446585178375,
      "learning_rate": 3.259503142771625e-05,
      "loss": 0.1233,
      "step": 5599
    },
    {
      "epoch": 7.537012113055182,
      "grad_norm": 0.7184252738952637,
      "learning_rate": 3.256510026938043e-05,
      "loss": 0.131,
      "step": 5600
    },
    {
      "epoch": 7.53835800807537,
      "grad_norm": 0.6979736089706421,
      "learning_rate": 3.25351691110446e-05,
      "loss": 0.0989,
      "step": 5601
    },
    {
      "epoch": 7.539703903095559,
      "grad_norm": 0.7367843985557556,
      "learning_rate": 3.2505237952708774e-05,
      "loss": 0.1096,
      "step": 5602
    },
    {
      "epoch": 7.541049798115747,
      "grad_norm": 0.7621906995773315,
      "learning_rate": 3.247530679437294e-05,
      "loss": 0.1289,
      "step": 5603
    },
    {
      "epoch": 7.542395693135935,
      "grad_norm": 0.46237969398498535,
      "learning_rate": 3.244537563603712e-05,
      "loss": 0.0763,
      "step": 5604
    },
    {
      "epoch": 7.543741588156124,
      "grad_norm": 0.5101777911186218,
      "learning_rate": 3.241544447770129e-05,
      "loss": 0.1029,
      "step": 5605
    },
    {
      "epoch": 7.545087483176312,
      "grad_norm": 0.5216280817985535,
      "learning_rate": 3.2385513319365464e-05,
      "loss": 0.1083,
      "step": 5606
    },
    {
      "epoch": 7.5464333781965,
      "grad_norm": 0.4716157019138336,
      "learning_rate": 3.235558216102963e-05,
      "loss": 0.1022,
      "step": 5607
    },
    {
      "epoch": 7.547779273216689,
      "grad_norm": 0.7506363391876221,
      "learning_rate": 3.232565100269381e-05,
      "loss": 0.1185,
      "step": 5608
    },
    {
      "epoch": 7.549125168236878,
      "grad_norm": 0.9696333408355713,
      "learning_rate": 3.229571984435798e-05,
      "loss": 0.1012,
      "step": 5609
    },
    {
      "epoch": 7.550471063257066,
      "grad_norm": 0.5571080446243286,
      "learning_rate": 3.2265788686022154e-05,
      "loss": 0.1419,
      "step": 5610
    },
    {
      "epoch": 7.5518169582772545,
      "grad_norm": 0.5339188575744629,
      "learning_rate": 3.2235857527686323e-05,
      "loss": 0.1014,
      "step": 5611
    },
    {
      "epoch": 7.553162853297443,
      "grad_norm": 0.7278589606285095,
      "learning_rate": 3.22059263693505e-05,
      "loss": 0.1095,
      "step": 5612
    },
    {
      "epoch": 7.554508748317631,
      "grad_norm": 0.8348422646522522,
      "learning_rate": 3.217599521101467e-05,
      "loss": 0.149,
      "step": 5613
    },
    {
      "epoch": 7.5558546433378195,
      "grad_norm": 0.5417314767837524,
      "learning_rate": 3.2146064052678844e-05,
      "loss": 0.1194,
      "step": 5614
    },
    {
      "epoch": 7.557200538358008,
      "grad_norm": 0.7662434577941895,
      "learning_rate": 3.2116132894343014e-05,
      "loss": 0.0933,
      "step": 5615
    },
    {
      "epoch": 7.558546433378196,
      "grad_norm": 0.505057692527771,
      "learning_rate": 3.208620173600719e-05,
      "loss": 0.1184,
      "step": 5616
    },
    {
      "epoch": 7.5598923283983845,
      "grad_norm": 0.7168338894844055,
      "learning_rate": 3.205627057767136e-05,
      "loss": 0.1258,
      "step": 5617
    },
    {
      "epoch": 7.561238223418574,
      "grad_norm": 1.0448004007339478,
      "learning_rate": 3.202633941933553e-05,
      "loss": 0.1399,
      "step": 5618
    },
    {
      "epoch": 7.562584118438762,
      "grad_norm": 0.9872552156448364,
      "learning_rate": 3.19964082609997e-05,
      "loss": 0.1529,
      "step": 5619
    },
    {
      "epoch": 7.56393001345895,
      "grad_norm": 0.9303889870643616,
      "learning_rate": 3.196647710266387e-05,
      "loss": 0.1979,
      "step": 5620
    },
    {
      "epoch": 7.565275908479139,
      "grad_norm": 0.3717092275619507,
      "learning_rate": 3.193654594432804e-05,
      "loss": 0.0852,
      "step": 5621
    },
    {
      "epoch": 7.566621803499327,
      "grad_norm": 0.4827651083469391,
      "learning_rate": 3.190661478599222e-05,
      "loss": 0.0925,
      "step": 5622
    },
    {
      "epoch": 7.5679676985195155,
      "grad_norm": 0.7016109228134155,
      "learning_rate": 3.187668362765639e-05,
      "loss": 0.1157,
      "step": 5623
    },
    {
      "epoch": 7.569313593539704,
      "grad_norm": 0.9100197553634644,
      "learning_rate": 3.184675246932056e-05,
      "loss": 0.1118,
      "step": 5624
    },
    {
      "epoch": 7.570659488559892,
      "grad_norm": 0.7584497928619385,
      "learning_rate": 3.181682131098473e-05,
      "loss": 0.135,
      "step": 5625
    },
    {
      "epoch": 7.5720053835800805,
      "grad_norm": 0.9146572947502136,
      "learning_rate": 3.178689015264891e-05,
      "loss": 0.1179,
      "step": 5626
    },
    {
      "epoch": 7.573351278600269,
      "grad_norm": 0.7847893238067627,
      "learning_rate": 3.175695899431308e-05,
      "loss": 0.1589,
      "step": 5627
    },
    {
      "epoch": 7.574697173620457,
      "grad_norm": 0.9128921031951904,
      "learning_rate": 3.172702783597725e-05,
      "loss": 0.129,
      "step": 5628
    },
    {
      "epoch": 7.576043068640646,
      "grad_norm": 0.422492653131485,
      "learning_rate": 3.169709667764142e-05,
      "loss": 0.0985,
      "step": 5629
    },
    {
      "epoch": 7.577388963660835,
      "grad_norm": 1.2380316257476807,
      "learning_rate": 3.16671655193056e-05,
      "loss": 0.1251,
      "step": 5630
    },
    {
      "epoch": 7.578734858681023,
      "grad_norm": 0.6067301630973816,
      "learning_rate": 3.163723436096977e-05,
      "loss": 0.0982,
      "step": 5631
    },
    {
      "epoch": 7.580080753701211,
      "grad_norm": 0.40919631719589233,
      "learning_rate": 3.160730320263394e-05,
      "loss": 0.0948,
      "step": 5632
    },
    {
      "epoch": 7.5814266487214,
      "grad_norm": 0.6176419854164124,
      "learning_rate": 3.157737204429811e-05,
      "loss": 0.1277,
      "step": 5633
    },
    {
      "epoch": 7.582772543741588,
      "grad_norm": 0.5525017976760864,
      "learning_rate": 3.154744088596229e-05,
      "loss": 0.1044,
      "step": 5634
    },
    {
      "epoch": 7.584118438761776,
      "grad_norm": 0.3835274577140808,
      "learning_rate": 3.151750972762646e-05,
      "loss": 0.0813,
      "step": 5635
    },
    {
      "epoch": 7.585464333781965,
      "grad_norm": 0.6736547946929932,
      "learning_rate": 3.1487578569290633e-05,
      "loss": 0.1035,
      "step": 5636
    },
    {
      "epoch": 7.586810228802153,
      "grad_norm": 0.5540714263916016,
      "learning_rate": 3.14576474109548e-05,
      "loss": 0.1191,
      "step": 5637
    },
    {
      "epoch": 7.588156123822342,
      "grad_norm": 0.7325525879859924,
      "learning_rate": 3.142771625261898e-05,
      "loss": 0.1094,
      "step": 5638
    },
    {
      "epoch": 7.589502018842531,
      "grad_norm": 0.789638340473175,
      "learning_rate": 3.139778509428315e-05,
      "loss": 0.1355,
      "step": 5639
    },
    {
      "epoch": 7.590847913862719,
      "grad_norm": 0.4634368121623993,
      "learning_rate": 3.1367853935947324e-05,
      "loss": 0.0794,
      "step": 5640
    },
    {
      "epoch": 7.592193808882907,
      "grad_norm": 0.6117505431175232,
      "learning_rate": 3.133792277761149e-05,
      "loss": 0.1133,
      "step": 5641
    },
    {
      "epoch": 7.593539703903096,
      "grad_norm": 0.7080652713775635,
      "learning_rate": 3.130799161927567e-05,
      "loss": 0.1137,
      "step": 5642
    },
    {
      "epoch": 7.594885598923284,
      "grad_norm": 0.6777584552764893,
      "learning_rate": 3.127806046093984e-05,
      "loss": 0.1447,
      "step": 5643
    },
    {
      "epoch": 7.596231493943472,
      "grad_norm": 0.49551451206207275,
      "learning_rate": 3.1248129302604014e-05,
      "loss": 0.0997,
      "step": 5644
    },
    {
      "epoch": 7.597577388963661,
      "grad_norm": 0.3528899848461151,
      "learning_rate": 3.121819814426818e-05,
      "loss": 0.0782,
      "step": 5645
    },
    {
      "epoch": 7.598923283983849,
      "grad_norm": 1.0542770624160767,
      "learning_rate": 3.118826698593236e-05,
      "loss": 0.1143,
      "step": 5646
    },
    {
      "epoch": 7.600269179004037,
      "grad_norm": 1.5814789533615112,
      "learning_rate": 3.115833582759653e-05,
      "loss": 0.1411,
      "step": 5647
    },
    {
      "epoch": 7.601615074024226,
      "grad_norm": 0.46374133229255676,
      "learning_rate": 3.1128404669260704e-05,
      "loss": 0.0966,
      "step": 5648
    },
    {
      "epoch": 7.602960969044415,
      "grad_norm": 0.42066723108291626,
      "learning_rate": 3.109847351092487e-05,
      "loss": 0.1081,
      "step": 5649
    },
    {
      "epoch": 7.604306864064603,
      "grad_norm": 0.8854082822799683,
      "learning_rate": 3.106854235258905e-05,
      "loss": 0.1086,
      "step": 5650
    },
    {
      "epoch": 7.6056527590847915,
      "grad_norm": 0.7275092601776123,
      "learning_rate": 3.103861119425322e-05,
      "loss": 0.1034,
      "step": 5651
    },
    {
      "epoch": 7.60699865410498,
      "grad_norm": 0.6011845469474792,
      "learning_rate": 3.1008680035917394e-05,
      "loss": 0.1357,
      "step": 5652
    },
    {
      "epoch": 7.608344549125168,
      "grad_norm": 0.8749225735664368,
      "learning_rate": 3.097874887758156e-05,
      "loss": 0.1453,
      "step": 5653
    },
    {
      "epoch": 7.609690444145357,
      "grad_norm": 0.7301173210144043,
      "learning_rate": 3.094881771924574e-05,
      "loss": 0.119,
      "step": 5654
    },
    {
      "epoch": 7.611036339165545,
      "grad_norm": 0.4414309561252594,
      "learning_rate": 3.091888656090991e-05,
      "loss": 0.1089,
      "step": 5655
    },
    {
      "epoch": 7.612382234185733,
      "grad_norm": 0.7409760355949402,
      "learning_rate": 3.0888955402574084e-05,
      "loss": 0.1286,
      "step": 5656
    },
    {
      "epoch": 7.613728129205922,
      "grad_norm": 0.4573446810245514,
      "learning_rate": 3.085902424423825e-05,
      "loss": 0.1106,
      "step": 5657
    },
    {
      "epoch": 7.615074024226111,
      "grad_norm": 0.6292306780815125,
      "learning_rate": 3.082909308590243e-05,
      "loss": 0.1323,
      "step": 5658
    },
    {
      "epoch": 7.616419919246299,
      "grad_norm": 0.644822359085083,
      "learning_rate": 3.07991619275666e-05,
      "loss": 0.1008,
      "step": 5659
    },
    {
      "epoch": 7.6177658142664875,
      "grad_norm": 0.8290281295776367,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 0.1659,
      "step": 5660
    },
    {
      "epoch": 7.619111709286676,
      "grad_norm": 1.0463449954986572,
      "learning_rate": 3.0739299610894944e-05,
      "loss": 0.153,
      "step": 5661
    },
    {
      "epoch": 7.620457604306864,
      "grad_norm": 0.7329936027526855,
      "learning_rate": 3.070936845255912e-05,
      "loss": 0.1082,
      "step": 5662
    },
    {
      "epoch": 7.6218034993270525,
      "grad_norm": 0.9809557199478149,
      "learning_rate": 3.067943729422329e-05,
      "loss": 0.1003,
      "step": 5663
    },
    {
      "epoch": 7.623149394347241,
      "grad_norm": 0.48252055048942566,
      "learning_rate": 3.0649506135887465e-05,
      "loss": 0.0826,
      "step": 5664
    },
    {
      "epoch": 7.624495289367429,
      "grad_norm": 0.5546777844429016,
      "learning_rate": 3.0619574977551634e-05,
      "loss": 0.1007,
      "step": 5665
    },
    {
      "epoch": 7.6258411843876175,
      "grad_norm": 0.6460707783699036,
      "learning_rate": 3.058964381921581e-05,
      "loss": 0.1113,
      "step": 5666
    },
    {
      "epoch": 7.627187079407806,
      "grad_norm": 0.6810941696166992,
      "learning_rate": 3.055971266087998e-05,
      "loss": 0.1118,
      "step": 5667
    },
    {
      "epoch": 7.628532974427994,
      "grad_norm": 0.47078391909599304,
      "learning_rate": 3.0529781502544155e-05,
      "loss": 0.0904,
      "step": 5668
    },
    {
      "epoch": 7.629878869448183,
      "grad_norm": 0.5827846527099609,
      "learning_rate": 3.0499850344208324e-05,
      "loss": 0.0892,
      "step": 5669
    },
    {
      "epoch": 7.631224764468372,
      "grad_norm": 0.5014626383781433,
      "learning_rate": 3.0469919185872496e-05,
      "loss": 0.1006,
      "step": 5670
    },
    {
      "epoch": 7.63257065948856,
      "grad_norm": 0.6888775825500488,
      "learning_rate": 3.0439988027536665e-05,
      "loss": 0.1195,
      "step": 5671
    },
    {
      "epoch": 7.633916554508748,
      "grad_norm": 0.6483575105667114,
      "learning_rate": 3.0410056869200838e-05,
      "loss": 0.1154,
      "step": 5672
    },
    {
      "epoch": 7.635262449528937,
      "grad_norm": 0.6236859560012817,
      "learning_rate": 3.038012571086501e-05,
      "loss": 0.0981,
      "step": 5673
    },
    {
      "epoch": 7.636608344549125,
      "grad_norm": 0.5660936236381531,
      "learning_rate": 3.0350194552529183e-05,
      "loss": 0.0851,
      "step": 5674
    },
    {
      "epoch": 7.637954239569313,
      "grad_norm": 0.3971276581287384,
      "learning_rate": 3.0320263394193356e-05,
      "loss": 0.1177,
      "step": 5675
    },
    {
      "epoch": 7.639300134589502,
      "grad_norm": 0.6749401092529297,
      "learning_rate": 3.0290332235857528e-05,
      "loss": 0.1168,
      "step": 5676
    },
    {
      "epoch": 7.64064602960969,
      "grad_norm": 0.6328307390213013,
      "learning_rate": 3.02604010775217e-05,
      "loss": 0.1287,
      "step": 5677
    },
    {
      "epoch": 7.641991924629879,
      "grad_norm": 0.4358687400817871,
      "learning_rate": 3.0230469919185873e-05,
      "loss": 0.0965,
      "step": 5678
    },
    {
      "epoch": 7.643337819650068,
      "grad_norm": 0.5430183410644531,
      "learning_rate": 3.0200538760850046e-05,
      "loss": 0.1331,
      "step": 5679
    },
    {
      "epoch": 7.644683714670256,
      "grad_norm": 0.5730209946632385,
      "learning_rate": 3.017060760251422e-05,
      "loss": 0.0942,
      "step": 5680
    },
    {
      "epoch": 7.646029609690444,
      "grad_norm": 0.6773437261581421,
      "learning_rate": 3.014067644417839e-05,
      "loss": 0.1091,
      "step": 5681
    },
    {
      "epoch": 7.647375504710633,
      "grad_norm": 0.3829602599143982,
      "learning_rate": 3.0110745285842563e-05,
      "loss": 0.0927,
      "step": 5682
    },
    {
      "epoch": 7.648721399730821,
      "grad_norm": 1.0226948261260986,
      "learning_rate": 3.0080814127506736e-05,
      "loss": 0.1013,
      "step": 5683
    },
    {
      "epoch": 7.650067294751009,
      "grad_norm": 0.8717846870422363,
      "learning_rate": 3.005088296917091e-05,
      "loss": 0.1334,
      "step": 5684
    },
    {
      "epoch": 7.651413189771198,
      "grad_norm": 0.3533180356025696,
      "learning_rate": 3.002095181083508e-05,
      "loss": 0.0967,
      "step": 5685
    },
    {
      "epoch": 7.652759084791386,
      "grad_norm": 0.8278287649154663,
      "learning_rate": 2.9991020652499254e-05,
      "loss": 0.1204,
      "step": 5686
    },
    {
      "epoch": 7.654104979811574,
      "grad_norm": 0.46863746643066406,
      "learning_rate": 2.9961089494163426e-05,
      "loss": 0.1136,
      "step": 5687
    },
    {
      "epoch": 7.655450874831763,
      "grad_norm": 0.5766865611076355,
      "learning_rate": 2.99311583358276e-05,
      "loss": 0.1303,
      "step": 5688
    },
    {
      "epoch": 7.656796769851952,
      "grad_norm": 0.550739049911499,
      "learning_rate": 2.990122717749177e-05,
      "loss": 0.1256,
      "step": 5689
    },
    {
      "epoch": 7.65814266487214,
      "grad_norm": 0.4536500573158264,
      "learning_rate": 2.9871296019155944e-05,
      "loss": 0.0856,
      "step": 5690
    },
    {
      "epoch": 7.659488559892329,
      "grad_norm": 0.6177504658699036,
      "learning_rate": 2.9841364860820116e-05,
      "loss": 0.1168,
      "step": 5691
    },
    {
      "epoch": 7.660834454912517,
      "grad_norm": 0.34197667241096497,
      "learning_rate": 2.981143370248429e-05,
      "loss": 0.0878,
      "step": 5692
    },
    {
      "epoch": 7.662180349932705,
      "grad_norm": 0.4009997546672821,
      "learning_rate": 2.978150254414846e-05,
      "loss": 0.0921,
      "step": 5693
    },
    {
      "epoch": 7.663526244952894,
      "grad_norm": 0.42765992879867554,
      "learning_rate": 2.9751571385812634e-05,
      "loss": 0.0954,
      "step": 5694
    },
    {
      "epoch": 7.664872139973082,
      "grad_norm": 0.9641844034194946,
      "learning_rate": 2.9721640227476803e-05,
      "loss": 0.155,
      "step": 5695
    },
    {
      "epoch": 7.66621803499327,
      "grad_norm": 0.475678950548172,
      "learning_rate": 2.9691709069140976e-05,
      "loss": 0.0854,
      "step": 5696
    },
    {
      "epoch": 7.667563930013459,
      "grad_norm": 0.4729965329170227,
      "learning_rate": 2.9661777910805148e-05,
      "loss": 0.094,
      "step": 5697
    },
    {
      "epoch": 7.668909825033648,
      "grad_norm": 0.49336689710617065,
      "learning_rate": 2.963184675246932e-05,
      "loss": 0.0985,
      "step": 5698
    },
    {
      "epoch": 7.670255720053836,
      "grad_norm": 0.32494157552719116,
      "learning_rate": 2.9601915594133493e-05,
      "loss": 0.092,
      "step": 5699
    },
    {
      "epoch": 7.6716016150740245,
      "grad_norm": 0.6529451608657837,
      "learning_rate": 2.9571984435797666e-05,
      "loss": 0.1236,
      "step": 5700
    },
    {
      "epoch": 7.672947510094213,
      "grad_norm": 0.4029250144958496,
      "learning_rate": 2.9542053277461838e-05,
      "loss": 0.0818,
      "step": 5701
    },
    {
      "epoch": 7.674293405114401,
      "grad_norm": 0.7511747479438782,
      "learning_rate": 2.951212211912601e-05,
      "loss": 0.1203,
      "step": 5702
    },
    {
      "epoch": 7.6756393001345895,
      "grad_norm": 0.9575023651123047,
      "learning_rate": 2.9482190960790183e-05,
      "loss": 0.1072,
      "step": 5703
    },
    {
      "epoch": 7.676985195154778,
      "grad_norm": 0.685330331325531,
      "learning_rate": 2.9452259802454356e-05,
      "loss": 0.1233,
      "step": 5704
    },
    {
      "epoch": 7.678331090174966,
      "grad_norm": 0.7592896223068237,
      "learning_rate": 2.942232864411853e-05,
      "loss": 0.1249,
      "step": 5705
    },
    {
      "epoch": 7.6796769851951545,
      "grad_norm": 0.37838244438171387,
      "learning_rate": 2.93923974857827e-05,
      "loss": 0.0773,
      "step": 5706
    },
    {
      "epoch": 7.681022880215343,
      "grad_norm": 0.6812286972999573,
      "learning_rate": 2.9362466327446873e-05,
      "loss": 0.104,
      "step": 5707
    },
    {
      "epoch": 7.682368775235531,
      "grad_norm": 0.36752989888191223,
      "learning_rate": 2.9332535169111046e-05,
      "loss": 0.0904,
      "step": 5708
    },
    {
      "epoch": 7.68371467025572,
      "grad_norm": 0.3277930021286011,
      "learning_rate": 2.930260401077522e-05,
      "loss": 0.0838,
      "step": 5709
    },
    {
      "epoch": 7.685060565275909,
      "grad_norm": 0.3686140477657318,
      "learning_rate": 2.927267285243939e-05,
      "loss": 0.0804,
      "step": 5710
    },
    {
      "epoch": 7.686406460296097,
      "grad_norm": 0.3877939283847809,
      "learning_rate": 2.9242741694103564e-05,
      "loss": 0.0997,
      "step": 5711
    },
    {
      "epoch": 7.687752355316285,
      "grad_norm": 0.47314706444740295,
      "learning_rate": 2.9212810535767736e-05,
      "loss": 0.1162,
      "step": 5712
    },
    {
      "epoch": 7.689098250336474,
      "grad_norm": 0.45062845945358276,
      "learning_rate": 2.918287937743191e-05,
      "loss": 0.0951,
      "step": 5713
    },
    {
      "epoch": 7.690444145356662,
      "grad_norm": 0.5099713206291199,
      "learning_rate": 2.915294821909608e-05,
      "loss": 0.1188,
      "step": 5714
    },
    {
      "epoch": 7.69179004037685,
      "grad_norm": 0.48080167174339294,
      "learning_rate": 2.9123017060760254e-05,
      "loss": 0.0952,
      "step": 5715
    },
    {
      "epoch": 7.693135935397039,
      "grad_norm": 0.5468433499336243,
      "learning_rate": 2.9093085902424426e-05,
      "loss": 0.0866,
      "step": 5716
    },
    {
      "epoch": 7.694481830417227,
      "grad_norm": 0.8383750915527344,
      "learning_rate": 2.90631547440886e-05,
      "loss": 0.1269,
      "step": 5717
    },
    {
      "epoch": 7.695827725437416,
      "grad_norm": 0.5239286422729492,
      "learning_rate": 2.903322358575277e-05,
      "loss": 0.0799,
      "step": 5718
    },
    {
      "epoch": 7.697173620457605,
      "grad_norm": 0.42236676812171936,
      "learning_rate": 2.9003292427416944e-05,
      "loss": 0.0867,
      "step": 5719
    },
    {
      "epoch": 7.698519515477793,
      "grad_norm": 0.45055344700813293,
      "learning_rate": 2.8973361269081116e-05,
      "loss": 0.0934,
      "step": 5720
    },
    {
      "epoch": 7.699865410497981,
      "grad_norm": 0.40426599979400635,
      "learning_rate": 2.894343011074529e-05,
      "loss": 0.1096,
      "step": 5721
    },
    {
      "epoch": 7.70121130551817,
      "grad_norm": 0.5094035863876343,
      "learning_rate": 2.891349895240946e-05,
      "loss": 0.0948,
      "step": 5722
    },
    {
      "epoch": 7.702557200538358,
      "grad_norm": 0.6301919221878052,
      "learning_rate": 2.888356779407363e-05,
      "loss": 0.1149,
      "step": 5723
    },
    {
      "epoch": 7.703903095558546,
      "grad_norm": 0.5535814166069031,
      "learning_rate": 2.8853636635737803e-05,
      "loss": 0.0924,
      "step": 5724
    },
    {
      "epoch": 7.705248990578735,
      "grad_norm": 0.5139070153236389,
      "learning_rate": 2.8823705477401976e-05,
      "loss": 0.093,
      "step": 5725
    },
    {
      "epoch": 7.706594885598923,
      "grad_norm": 0.6251028776168823,
      "learning_rate": 2.8793774319066148e-05,
      "loss": 0.1184,
      "step": 5726
    },
    {
      "epoch": 7.707940780619111,
      "grad_norm": 0.5036976933479309,
      "learning_rate": 2.876384316073032e-05,
      "loss": 0.0948,
      "step": 5727
    },
    {
      "epoch": 7.7092866756393,
      "grad_norm": 0.5023714900016785,
      "learning_rate": 2.8733912002394493e-05,
      "loss": 0.0807,
      "step": 5728
    },
    {
      "epoch": 7.710632570659489,
      "grad_norm": 0.9073609113693237,
      "learning_rate": 2.8703980844058666e-05,
      "loss": 0.1192,
      "step": 5729
    },
    {
      "epoch": 7.711978465679677,
      "grad_norm": 0.5657939314842224,
      "learning_rate": 2.867404968572284e-05,
      "loss": 0.1284,
      "step": 5730
    },
    {
      "epoch": 7.713324360699866,
      "grad_norm": 1.0525248050689697,
      "learning_rate": 2.864411852738701e-05,
      "loss": 0.1509,
      "step": 5731
    },
    {
      "epoch": 7.714670255720054,
      "grad_norm": 0.5340527296066284,
      "learning_rate": 2.8614187369051183e-05,
      "loss": 0.1026,
      "step": 5732
    },
    {
      "epoch": 7.716016150740242,
      "grad_norm": 0.6413524746894836,
      "learning_rate": 2.8584256210715356e-05,
      "loss": 0.1219,
      "step": 5733
    },
    {
      "epoch": 7.717362045760431,
      "grad_norm": 0.5933973789215088,
      "learning_rate": 2.855432505237953e-05,
      "loss": 0.104,
      "step": 5734
    },
    {
      "epoch": 7.718707940780619,
      "grad_norm": 0.5047112703323364,
      "learning_rate": 2.85243938940437e-05,
      "loss": 0.0917,
      "step": 5735
    },
    {
      "epoch": 7.720053835800807,
      "grad_norm": 1.1078225374221802,
      "learning_rate": 2.8494462735707874e-05,
      "loss": 0.1705,
      "step": 5736
    },
    {
      "epoch": 7.721399730820996,
      "grad_norm": 0.5975448489189148,
      "learning_rate": 2.8464531577372046e-05,
      "loss": 0.1074,
      "step": 5737
    },
    {
      "epoch": 7.722745625841185,
      "grad_norm": 0.46072113513946533,
      "learning_rate": 2.8434600419036215e-05,
      "loss": 0.0974,
      "step": 5738
    },
    {
      "epoch": 7.724091520861373,
      "grad_norm": 0.3837079703807831,
      "learning_rate": 2.8404669260700388e-05,
      "loss": 0.0848,
      "step": 5739
    },
    {
      "epoch": 7.7254374158815615,
      "grad_norm": 0.8595892786979675,
      "learning_rate": 2.837473810236456e-05,
      "loss": 0.1127,
      "step": 5740
    },
    {
      "epoch": 7.72678331090175,
      "grad_norm": 0.5789591073989868,
      "learning_rate": 2.8344806944028733e-05,
      "loss": 0.1147,
      "step": 5741
    },
    {
      "epoch": 7.728129205921938,
      "grad_norm": 0.8924887180328369,
      "learning_rate": 2.8314875785692905e-05,
      "loss": 0.1113,
      "step": 5742
    },
    {
      "epoch": 7.7294751009421265,
      "grad_norm": 0.6970000267028809,
      "learning_rate": 2.8284944627357078e-05,
      "loss": 0.1147,
      "step": 5743
    },
    {
      "epoch": 7.730820995962315,
      "grad_norm": 0.8694815635681152,
      "learning_rate": 2.825501346902125e-05,
      "loss": 0.1509,
      "step": 5744
    },
    {
      "epoch": 7.732166890982503,
      "grad_norm": 0.7630379796028137,
      "learning_rate": 2.8225082310685423e-05,
      "loss": 0.1583,
      "step": 5745
    },
    {
      "epoch": 7.7335127860026915,
      "grad_norm": 0.533041775226593,
      "learning_rate": 2.8195151152349596e-05,
      "loss": 0.1384,
      "step": 5746
    },
    {
      "epoch": 7.73485868102288,
      "grad_norm": 0.3012135922908783,
      "learning_rate": 2.8165219994013768e-05,
      "loss": 0.0733,
      "step": 5747
    },
    {
      "epoch": 7.736204576043068,
      "grad_norm": 0.5078191757202148,
      "learning_rate": 2.813528883567794e-05,
      "loss": 0.1063,
      "step": 5748
    },
    {
      "epoch": 7.737550471063257,
      "grad_norm": 0.6999015808105469,
      "learning_rate": 2.8105357677342113e-05,
      "loss": 0.1386,
      "step": 5749
    },
    {
      "epoch": 7.738896366083446,
      "grad_norm": 0.5115563273429871,
      "learning_rate": 2.8075426519006286e-05,
      "loss": 0.1151,
      "step": 5750
    },
    {
      "epoch": 7.740242261103634,
      "grad_norm": 0.437195748090744,
      "learning_rate": 2.8045495360670458e-05,
      "loss": 0.1175,
      "step": 5751
    },
    {
      "epoch": 7.7415881561238225,
      "grad_norm": 0.4101896584033966,
      "learning_rate": 2.801556420233463e-05,
      "loss": 0.0901,
      "step": 5752
    },
    {
      "epoch": 7.742934051144011,
      "grad_norm": 0.4662429392337799,
      "learning_rate": 2.7985633043998803e-05,
      "loss": 0.0987,
      "step": 5753
    },
    {
      "epoch": 7.744279946164199,
      "grad_norm": 0.5554024577140808,
      "learning_rate": 2.7955701885662976e-05,
      "loss": 0.1133,
      "step": 5754
    },
    {
      "epoch": 7.7456258411843875,
      "grad_norm": 0.5300502777099609,
      "learning_rate": 2.792577072732715e-05,
      "loss": 0.1056,
      "step": 5755
    },
    {
      "epoch": 7.746971736204576,
      "grad_norm": 0.49869629740715027,
      "learning_rate": 2.789583956899132e-05,
      "loss": 0.1034,
      "step": 5756
    },
    {
      "epoch": 7.748317631224764,
      "grad_norm": 0.698806643486023,
      "learning_rate": 2.7865908410655493e-05,
      "loss": 0.0934,
      "step": 5757
    },
    {
      "epoch": 7.749663526244953,
      "grad_norm": 0.5791306495666504,
      "learning_rate": 2.7835977252319666e-05,
      "loss": 0.0933,
      "step": 5758
    },
    {
      "epoch": 7.751009421265142,
      "grad_norm": 0.3316461145877838,
      "learning_rate": 2.780604609398384e-05,
      "loss": 0.0683,
      "step": 5759
    },
    {
      "epoch": 7.75235531628533,
      "grad_norm": 0.9190552830696106,
      "learning_rate": 2.777611493564801e-05,
      "loss": 0.1634,
      "step": 5760
    },
    {
      "epoch": 7.753701211305518,
      "grad_norm": 0.6850911378860474,
      "learning_rate": 2.7746183777312184e-05,
      "loss": 0.1232,
      "step": 5761
    },
    {
      "epoch": 7.755047106325707,
      "grad_norm": 0.5571617484092712,
      "learning_rate": 2.7716252618976356e-05,
      "loss": 0.1386,
      "step": 5762
    },
    {
      "epoch": 7.756393001345895,
      "grad_norm": 0.6703639030456543,
      "learning_rate": 2.768632146064053e-05,
      "loss": 0.1459,
      "step": 5763
    },
    {
      "epoch": 7.757738896366083,
      "grad_norm": 1.5293809175491333,
      "learning_rate": 2.76563903023047e-05,
      "loss": 0.1363,
      "step": 5764
    },
    {
      "epoch": 7.759084791386272,
      "grad_norm": 0.7032864093780518,
      "learning_rate": 2.7626459143968874e-05,
      "loss": 0.089,
      "step": 5765
    },
    {
      "epoch": 7.76043068640646,
      "grad_norm": 0.4645538926124573,
      "learning_rate": 2.7596527985633046e-05,
      "loss": 0.0988,
      "step": 5766
    },
    {
      "epoch": 7.761776581426648,
      "grad_norm": 0.7573336362838745,
      "learning_rate": 2.756659682729722e-05,
      "loss": 0.1485,
      "step": 5767
    },
    {
      "epoch": 7.763122476446837,
      "grad_norm": 0.6226209998130798,
      "learning_rate": 2.753666566896139e-05,
      "loss": 0.1033,
      "step": 5768
    },
    {
      "epoch": 7.764468371467026,
      "grad_norm": 0.41943615674972534,
      "learning_rate": 2.7506734510625564e-05,
      "loss": 0.1064,
      "step": 5769
    },
    {
      "epoch": 7.765814266487214,
      "grad_norm": 0.38396456837654114,
      "learning_rate": 2.7476803352289736e-05,
      "loss": 0.0913,
      "step": 5770
    },
    {
      "epoch": 7.767160161507403,
      "grad_norm": 0.3499728739261627,
      "learning_rate": 2.744687219395391e-05,
      "loss": 0.0831,
      "step": 5771
    },
    {
      "epoch": 7.768506056527591,
      "grad_norm": 1.2127636671066284,
      "learning_rate": 2.741694103561808e-05,
      "loss": 0.1522,
      "step": 5772
    },
    {
      "epoch": 7.769851951547779,
      "grad_norm": 0.6411063075065613,
      "learning_rate": 2.7387009877282254e-05,
      "loss": 0.1024,
      "step": 5773
    },
    {
      "epoch": 7.771197846567968,
      "grad_norm": 0.8239448666572571,
      "learning_rate": 2.7357078718946427e-05,
      "loss": 0.1148,
      "step": 5774
    },
    {
      "epoch": 7.772543741588156,
      "grad_norm": 0.6725661754608154,
      "learning_rate": 2.7327147560610596e-05,
      "loss": 0.1161,
      "step": 5775
    },
    {
      "epoch": 7.773889636608344,
      "grad_norm": 0.7988502383232117,
      "learning_rate": 2.729721640227477e-05,
      "loss": 0.1264,
      "step": 5776
    },
    {
      "epoch": 7.775235531628533,
      "grad_norm": 0.7268118262290955,
      "learning_rate": 2.726728524393894e-05,
      "loss": 0.1046,
      "step": 5777
    },
    {
      "epoch": 7.776581426648722,
      "grad_norm": 1.762985348701477,
      "learning_rate": 2.7237354085603113e-05,
      "loss": 0.1314,
      "step": 5778
    },
    {
      "epoch": 7.77792732166891,
      "grad_norm": 0.42442333698272705,
      "learning_rate": 2.7207422927267286e-05,
      "loss": 0.1005,
      "step": 5779
    },
    {
      "epoch": 7.7792732166890985,
      "grad_norm": 0.6722462177276611,
      "learning_rate": 2.717749176893146e-05,
      "loss": 0.1107,
      "step": 5780
    },
    {
      "epoch": 7.780619111709287,
      "grad_norm": 1.571532964706421,
      "learning_rate": 2.7147560610595628e-05,
      "loss": 0.2326,
      "step": 5781
    },
    {
      "epoch": 7.781965006729475,
      "grad_norm": 1.0716501474380493,
      "learning_rate": 2.71176294522598e-05,
      "loss": 0.1456,
      "step": 5782
    },
    {
      "epoch": 7.783310901749664,
      "grad_norm": 0.6630212068557739,
      "learning_rate": 2.7087698293923973e-05,
      "loss": 0.1104,
      "step": 5783
    },
    {
      "epoch": 7.784656796769852,
      "grad_norm": 0.5620594024658203,
      "learning_rate": 2.7057767135588145e-05,
      "loss": 0.1115,
      "step": 5784
    },
    {
      "epoch": 7.78600269179004,
      "grad_norm": 0.2718345522880554,
      "learning_rate": 2.7027835977252318e-05,
      "loss": 0.0621,
      "step": 5785
    },
    {
      "epoch": 7.787348586810229,
      "grad_norm": 0.5092280507087708,
      "learning_rate": 2.699790481891649e-05,
      "loss": 0.0809,
      "step": 5786
    },
    {
      "epoch": 7.788694481830417,
      "grad_norm": 0.8307287096977234,
      "learning_rate": 2.6967973660580663e-05,
      "loss": 0.1077,
      "step": 5787
    },
    {
      "epoch": 7.790040376850605,
      "grad_norm": 0.7911398410797119,
      "learning_rate": 2.6938042502244835e-05,
      "loss": 0.1294,
      "step": 5788
    },
    {
      "epoch": 7.7913862718707945,
      "grad_norm": 0.363892525434494,
      "learning_rate": 2.6908111343909008e-05,
      "loss": 0.0808,
      "step": 5789
    },
    {
      "epoch": 7.792732166890983,
      "grad_norm": 2.1862974166870117,
      "learning_rate": 2.687818018557318e-05,
      "loss": 0.11,
      "step": 5790
    },
    {
      "epoch": 7.794078061911171,
      "grad_norm": 0.9442411661148071,
      "learning_rate": 2.6848249027237353e-05,
      "loss": 0.1378,
      "step": 5791
    },
    {
      "epoch": 7.7954239569313595,
      "grad_norm": 0.8206028938293457,
      "learning_rate": 2.6818317868901526e-05,
      "loss": 0.1543,
      "step": 5792
    },
    {
      "epoch": 7.796769851951548,
      "grad_norm": 0.5311790108680725,
      "learning_rate": 2.6788386710565698e-05,
      "loss": 0.0923,
      "step": 5793
    },
    {
      "epoch": 7.798115746971736,
      "grad_norm": 0.45370757579803467,
      "learning_rate": 2.675845555222987e-05,
      "loss": 0.0941,
      "step": 5794
    },
    {
      "epoch": 7.7994616419919245,
      "grad_norm": 0.6169579029083252,
      "learning_rate": 2.6728524393894043e-05,
      "loss": 0.1084,
      "step": 5795
    },
    {
      "epoch": 7.800807537012113,
      "grad_norm": 0.7240616679191589,
      "learning_rate": 2.6698593235558216e-05,
      "loss": 0.1272,
      "step": 5796
    },
    {
      "epoch": 7.802153432032301,
      "grad_norm": 0.3236328661441803,
      "learning_rate": 2.6668662077222388e-05,
      "loss": 0.0762,
      "step": 5797
    },
    {
      "epoch": 7.80349932705249,
      "grad_norm": 0.5134086012840271,
      "learning_rate": 2.663873091888656e-05,
      "loss": 0.1164,
      "step": 5798
    },
    {
      "epoch": 7.804845222072679,
      "grad_norm": 0.7758695483207703,
      "learning_rate": 2.6608799760550733e-05,
      "loss": 0.1089,
      "step": 5799
    },
    {
      "epoch": 7.806191117092867,
      "grad_norm": 0.6182924509048462,
      "learning_rate": 2.6578868602214906e-05,
      "loss": 0.1217,
      "step": 5800
    },
    {
      "epoch": 7.807537012113055,
      "grad_norm": 0.8725599646568298,
      "learning_rate": 2.654893744387908e-05,
      "loss": 0.0981,
      "step": 5801
    },
    {
      "epoch": 7.808882907133244,
      "grad_norm": 0.386116623878479,
      "learning_rate": 2.651900628554325e-05,
      "loss": 0.0858,
      "step": 5802
    },
    {
      "epoch": 7.810228802153432,
      "grad_norm": 0.6291871666908264,
      "learning_rate": 2.6489075127207423e-05,
      "loss": 0.0876,
      "step": 5803
    },
    {
      "epoch": 7.81157469717362,
      "grad_norm": 0.632171094417572,
      "learning_rate": 2.6459143968871596e-05,
      "loss": 0.1141,
      "step": 5804
    },
    {
      "epoch": 7.812920592193809,
      "grad_norm": 0.6593850255012512,
      "learning_rate": 2.642921281053577e-05,
      "loss": 0.106,
      "step": 5805
    },
    {
      "epoch": 7.814266487213997,
      "grad_norm": 0.47502192854881287,
      "learning_rate": 2.639928165219994e-05,
      "loss": 0.1155,
      "step": 5806
    },
    {
      "epoch": 7.815612382234185,
      "grad_norm": 0.4411885142326355,
      "learning_rate": 2.6369350493864114e-05,
      "loss": 0.1018,
      "step": 5807
    },
    {
      "epoch": 7.816958277254374,
      "grad_norm": 0.45475777983665466,
      "learning_rate": 2.6339419335528286e-05,
      "loss": 0.0945,
      "step": 5808
    },
    {
      "epoch": 7.818304172274562,
      "grad_norm": 0.38393864035606384,
      "learning_rate": 2.630948817719246e-05,
      "loss": 0.1023,
      "step": 5809
    },
    {
      "epoch": 7.819650067294751,
      "grad_norm": 0.5143385529518127,
      "learning_rate": 2.627955701885663e-05,
      "loss": 0.0969,
      "step": 5810
    },
    {
      "epoch": 7.82099596231494,
      "grad_norm": 0.6638394594192505,
      "learning_rate": 2.6249625860520804e-05,
      "loss": 0.1349,
      "step": 5811
    },
    {
      "epoch": 7.822341857335128,
      "grad_norm": 0.4710942804813385,
      "learning_rate": 2.6219694702184976e-05,
      "loss": 0.1018,
      "step": 5812
    },
    {
      "epoch": 7.823687752355316,
      "grad_norm": 0.6457349061965942,
      "learning_rate": 2.618976354384915e-05,
      "loss": 0.1023,
      "step": 5813
    },
    {
      "epoch": 7.825033647375505,
      "grad_norm": 0.48648494482040405,
      "learning_rate": 2.615983238551332e-05,
      "loss": 0.095,
      "step": 5814
    },
    {
      "epoch": 7.826379542395693,
      "grad_norm": 0.5665823817253113,
      "learning_rate": 2.6129901227177494e-05,
      "loss": 0.1177,
      "step": 5815
    },
    {
      "epoch": 7.827725437415881,
      "grad_norm": 0.565089225769043,
      "learning_rate": 2.6099970068841666e-05,
      "loss": 0.1178,
      "step": 5816
    },
    {
      "epoch": 7.82907133243607,
      "grad_norm": 0.43034636974334717,
      "learning_rate": 2.607003891050584e-05,
      "loss": 0.1139,
      "step": 5817
    },
    {
      "epoch": 7.830417227456259,
      "grad_norm": 0.5308020710945129,
      "learning_rate": 2.604010775217001e-05,
      "loss": 0.1123,
      "step": 5818
    },
    {
      "epoch": 7.831763122476447,
      "grad_norm": 0.509839653968811,
      "learning_rate": 2.6010176593834184e-05,
      "loss": 0.0982,
      "step": 5819
    },
    {
      "epoch": 7.833109017496636,
      "grad_norm": 0.431318461894989,
      "learning_rate": 2.5980245435498357e-05,
      "loss": 0.0723,
      "step": 5820
    },
    {
      "epoch": 7.834454912516824,
      "grad_norm": 0.5857537984848022,
      "learning_rate": 2.595031427716253e-05,
      "loss": 0.1023,
      "step": 5821
    },
    {
      "epoch": 7.835800807537012,
      "grad_norm": 0.9535529613494873,
      "learning_rate": 2.59203831188267e-05,
      "loss": 0.1863,
      "step": 5822
    },
    {
      "epoch": 7.837146702557201,
      "grad_norm": 0.617926836013794,
      "learning_rate": 2.5890451960490874e-05,
      "loss": 0.1285,
      "step": 5823
    },
    {
      "epoch": 7.838492597577389,
      "grad_norm": 0.5428311824798584,
      "learning_rate": 2.5860520802155047e-05,
      "loss": 0.1078,
      "step": 5824
    },
    {
      "epoch": 7.839838492597577,
      "grad_norm": 0.4883008599281311,
      "learning_rate": 2.583058964381922e-05,
      "loss": 0.1142,
      "step": 5825
    },
    {
      "epoch": 7.841184387617766,
      "grad_norm": 1.0924513339996338,
      "learning_rate": 2.5800658485483392e-05,
      "loss": 0.1381,
      "step": 5826
    },
    {
      "epoch": 7.842530282637954,
      "grad_norm": 0.37447649240493774,
      "learning_rate": 2.5770727327147558e-05,
      "loss": 0.0804,
      "step": 5827
    },
    {
      "epoch": 7.843876177658142,
      "grad_norm": 0.6507850885391235,
      "learning_rate": 2.574079616881173e-05,
      "loss": 0.1346,
      "step": 5828
    },
    {
      "epoch": 7.845222072678331,
      "grad_norm": 0.5087389945983887,
      "learning_rate": 2.5710865010475903e-05,
      "loss": 0.117,
      "step": 5829
    },
    {
      "epoch": 7.84656796769852,
      "grad_norm": 0.68745356798172,
      "learning_rate": 2.5680933852140075e-05,
      "loss": 0.1514,
      "step": 5830
    },
    {
      "epoch": 7.847913862718708,
      "grad_norm": 0.44269534945487976,
      "learning_rate": 2.5651002693804248e-05,
      "loss": 0.1019,
      "step": 5831
    },
    {
      "epoch": 7.8492597577388965,
      "grad_norm": 0.7025030255317688,
      "learning_rate": 2.562107153546842e-05,
      "loss": 0.1779,
      "step": 5832
    },
    {
      "epoch": 7.850605652759085,
      "grad_norm": 0.9391591548919678,
      "learning_rate": 2.5591140377132593e-05,
      "loss": 0.1189,
      "step": 5833
    },
    {
      "epoch": 7.851951547779273,
      "grad_norm": 0.820697009563446,
      "learning_rate": 2.5561209218796765e-05,
      "loss": 0.141,
      "step": 5834
    },
    {
      "epoch": 7.8532974427994615,
      "grad_norm": 0.5025902390480042,
      "learning_rate": 2.5531278060460938e-05,
      "loss": 0.0815,
      "step": 5835
    },
    {
      "epoch": 7.85464333781965,
      "grad_norm": 0.7067062854766846,
      "learning_rate": 2.550134690212511e-05,
      "loss": 0.127,
      "step": 5836
    },
    {
      "epoch": 7.855989232839838,
      "grad_norm": 0.450374573469162,
      "learning_rate": 2.5471415743789283e-05,
      "loss": 0.0895,
      "step": 5837
    },
    {
      "epoch": 7.857335127860027,
      "grad_norm": 0.439625084400177,
      "learning_rate": 2.5441484585453455e-05,
      "loss": 0.0976,
      "step": 5838
    },
    {
      "epoch": 7.858681022880216,
      "grad_norm": 0.4066103398799896,
      "learning_rate": 2.5411553427117628e-05,
      "loss": 0.1018,
      "step": 5839
    },
    {
      "epoch": 7.860026917900404,
      "grad_norm": 0.6001182794570923,
      "learning_rate": 2.53816222687818e-05,
      "loss": 0.1383,
      "step": 5840
    },
    {
      "epoch": 7.861372812920592,
      "grad_norm": 0.5261787176132202,
      "learning_rate": 2.5351691110445973e-05,
      "loss": 0.1098,
      "step": 5841
    },
    {
      "epoch": 7.862718707940781,
      "grad_norm": 0.5985881686210632,
      "learning_rate": 2.5321759952110146e-05,
      "loss": 0.0992,
      "step": 5842
    },
    {
      "epoch": 7.864064602960969,
      "grad_norm": 0.4576001465320587,
      "learning_rate": 2.5291828793774318e-05,
      "loss": 0.101,
      "step": 5843
    },
    {
      "epoch": 7.865410497981157,
      "grad_norm": 0.6709400415420532,
      "learning_rate": 2.526189763543849e-05,
      "loss": 0.1767,
      "step": 5844
    },
    {
      "epoch": 7.866756393001346,
      "grad_norm": 0.5153415203094482,
      "learning_rate": 2.5231966477102663e-05,
      "loss": 0.0913,
      "step": 5845
    },
    {
      "epoch": 7.868102288021534,
      "grad_norm": 0.44357675313949585,
      "learning_rate": 2.5202035318766836e-05,
      "loss": 0.0994,
      "step": 5846
    },
    {
      "epoch": 7.8694481830417224,
      "grad_norm": 0.6857523322105408,
      "learning_rate": 2.5172104160431008e-05,
      "loss": 0.1313,
      "step": 5847
    },
    {
      "epoch": 7.870794078061911,
      "grad_norm": 0.8209977149963379,
      "learning_rate": 2.514217300209518e-05,
      "loss": 0.1017,
      "step": 5848
    },
    {
      "epoch": 7.872139973082099,
      "grad_norm": 0.5281233787536621,
      "learning_rate": 2.5112241843759353e-05,
      "loss": 0.1071,
      "step": 5849
    },
    {
      "epoch": 7.873485868102288,
      "grad_norm": 0.8472031354904175,
      "learning_rate": 2.5082310685423526e-05,
      "loss": 0.0921,
      "step": 5850
    },
    {
      "epoch": 7.874831763122477,
      "grad_norm": 1.1520754098892212,
      "learning_rate": 2.50523795270877e-05,
      "loss": 0.1608,
      "step": 5851
    },
    {
      "epoch": 7.876177658142665,
      "grad_norm": 0.49872148036956787,
      "learning_rate": 2.502244836875187e-05,
      "loss": 0.1005,
      "step": 5852
    },
    {
      "epoch": 7.877523553162853,
      "grad_norm": 0.5222457647323608,
      "learning_rate": 2.4992517210416043e-05,
      "loss": 0.0993,
      "step": 5853
    },
    {
      "epoch": 7.878869448183042,
      "grad_norm": 0.42109009623527527,
      "learning_rate": 2.4962586052080216e-05,
      "loss": 0.1043,
      "step": 5854
    },
    {
      "epoch": 7.88021534320323,
      "grad_norm": 0.7170899510383606,
      "learning_rate": 2.493265489374439e-05,
      "loss": 0.1209,
      "step": 5855
    },
    {
      "epoch": 7.881561238223418,
      "grad_norm": 0.9158852696418762,
      "learning_rate": 2.490272373540856e-05,
      "loss": 0.1158,
      "step": 5856
    },
    {
      "epoch": 7.882907133243607,
      "grad_norm": 0.688066303730011,
      "learning_rate": 2.4872792577072734e-05,
      "loss": 0.123,
      "step": 5857
    },
    {
      "epoch": 7.884253028263795,
      "grad_norm": 0.7403021454811096,
      "learning_rate": 2.4842861418736906e-05,
      "loss": 0.1187,
      "step": 5858
    },
    {
      "epoch": 7.885598923283984,
      "grad_norm": 1.1607762575149536,
      "learning_rate": 2.481293026040108e-05,
      "loss": 0.1332,
      "step": 5859
    },
    {
      "epoch": 7.886944818304173,
      "grad_norm": 0.648638129234314,
      "learning_rate": 2.478299910206525e-05,
      "loss": 0.0972,
      "step": 5860
    },
    {
      "epoch": 7.888290713324361,
      "grad_norm": 0.33386296033859253,
      "learning_rate": 2.4753067943729424e-05,
      "loss": 0.0876,
      "step": 5861
    },
    {
      "epoch": 7.889636608344549,
      "grad_norm": 0.42119187116622925,
      "learning_rate": 2.4723136785393596e-05,
      "loss": 0.0966,
      "step": 5862
    },
    {
      "epoch": 7.890982503364738,
      "grad_norm": 1.1369253396987915,
      "learning_rate": 2.469320562705777e-05,
      "loss": 0.1099,
      "step": 5863
    },
    {
      "epoch": 7.892328398384926,
      "grad_norm": 0.5609558820724487,
      "learning_rate": 2.466327446872194e-05,
      "loss": 0.1067,
      "step": 5864
    },
    {
      "epoch": 7.893674293405114,
      "grad_norm": 0.5693145394325256,
      "learning_rate": 2.4633343310386114e-05,
      "loss": 0.124,
      "step": 5865
    },
    {
      "epoch": 7.895020188425303,
      "grad_norm": 0.6908345818519592,
      "learning_rate": 2.4603412152050283e-05,
      "loss": 0.1164,
      "step": 5866
    },
    {
      "epoch": 7.896366083445491,
      "grad_norm": 0.40249568223953247,
      "learning_rate": 2.4573480993714456e-05,
      "loss": 0.1032,
      "step": 5867
    },
    {
      "epoch": 7.897711978465679,
      "grad_norm": 0.9126347899436951,
      "learning_rate": 2.4543549835378628e-05,
      "loss": 0.1344,
      "step": 5868
    },
    {
      "epoch": 7.899057873485868,
      "grad_norm": 0.5912065505981445,
      "learning_rate": 2.45136186770428e-05,
      "loss": 0.115,
      "step": 5869
    },
    {
      "epoch": 7.900403768506057,
      "grad_norm": 0.7315599322319031,
      "learning_rate": 2.4483687518706973e-05,
      "loss": 0.1159,
      "step": 5870
    },
    {
      "epoch": 7.901749663526245,
      "grad_norm": 0.390755832195282,
      "learning_rate": 2.4453756360371146e-05,
      "loss": 0.0882,
      "step": 5871
    },
    {
      "epoch": 7.9030955585464335,
      "grad_norm": 0.37056732177734375,
      "learning_rate": 2.4423825202035318e-05,
      "loss": 0.0824,
      "step": 5872
    },
    {
      "epoch": 7.904441453566622,
      "grad_norm": 0.3529256582260132,
      "learning_rate": 2.439389404369949e-05,
      "loss": 0.0756,
      "step": 5873
    },
    {
      "epoch": 7.90578734858681,
      "grad_norm": 0.5568663477897644,
      "learning_rate": 2.4363962885363663e-05,
      "loss": 0.0926,
      "step": 5874
    },
    {
      "epoch": 7.9071332436069985,
      "grad_norm": 0.48150432109832764,
      "learning_rate": 2.4334031727027836e-05,
      "loss": 0.0947,
      "step": 5875
    },
    {
      "epoch": 7.908479138627187,
      "grad_norm": 0.7860534191131592,
      "learning_rate": 2.430410056869201e-05,
      "loss": 0.0995,
      "step": 5876
    },
    {
      "epoch": 7.909825033647375,
      "grad_norm": 0.38576483726501465,
      "learning_rate": 2.427416941035618e-05,
      "loss": 0.0954,
      "step": 5877
    },
    {
      "epoch": 7.9111709286675636,
      "grad_norm": 0.5090817809104919,
      "learning_rate": 2.4244238252020354e-05,
      "loss": 0.1216,
      "step": 5878
    },
    {
      "epoch": 7.912516823687753,
      "grad_norm": 0.8374388813972473,
      "learning_rate": 2.4214307093684526e-05,
      "loss": 0.1016,
      "step": 5879
    },
    {
      "epoch": 7.913862718707941,
      "grad_norm": 0.7541847229003906,
      "learning_rate": 2.41843759353487e-05,
      "loss": 0.1211,
      "step": 5880
    },
    {
      "epoch": 7.9152086137281294,
      "grad_norm": 0.7757217884063721,
      "learning_rate": 2.415444477701287e-05,
      "loss": 0.1284,
      "step": 5881
    },
    {
      "epoch": 7.916554508748318,
      "grad_norm": 0.46673843264579773,
      "learning_rate": 2.4124513618677044e-05,
      "loss": 0.1188,
      "step": 5882
    },
    {
      "epoch": 7.917900403768506,
      "grad_norm": 1.086381196975708,
      "learning_rate": 2.4094582460341216e-05,
      "loss": 0.0947,
      "step": 5883
    },
    {
      "epoch": 7.9192462987886945,
      "grad_norm": 0.26796722412109375,
      "learning_rate": 2.406465130200539e-05,
      "loss": 0.0869,
      "step": 5884
    },
    {
      "epoch": 7.920592193808883,
      "grad_norm": 0.7796570658683777,
      "learning_rate": 2.403472014366956e-05,
      "loss": 0.1549,
      "step": 5885
    },
    {
      "epoch": 7.921938088829071,
      "grad_norm": 0.3644442558288574,
      "learning_rate": 2.4004788985333734e-05,
      "loss": 0.0914,
      "step": 5886
    },
    {
      "epoch": 7.9232839838492595,
      "grad_norm": 0.5229971408843994,
      "learning_rate": 2.3974857826997906e-05,
      "loss": 0.0807,
      "step": 5887
    },
    {
      "epoch": 7.924629878869448,
      "grad_norm": 0.4965039789676666,
      "learning_rate": 2.394492666866208e-05,
      "loss": 0.1062,
      "step": 5888
    },
    {
      "epoch": 7.925975773889636,
      "grad_norm": 0.636685311794281,
      "learning_rate": 2.391499551032625e-05,
      "loss": 0.1286,
      "step": 5889
    },
    {
      "epoch": 7.927321668909825,
      "grad_norm": 0.813267171382904,
      "learning_rate": 2.3885064351990424e-05,
      "loss": 0.1516,
      "step": 5890
    },
    {
      "epoch": 7.928667563930014,
      "grad_norm": 0.640508234500885,
      "learning_rate": 2.3855133193654596e-05,
      "loss": 0.1245,
      "step": 5891
    },
    {
      "epoch": 7.930013458950202,
      "grad_norm": 0.587878942489624,
      "learning_rate": 2.3825202035318766e-05,
      "loss": 0.0896,
      "step": 5892
    },
    {
      "epoch": 7.93135935397039,
      "grad_norm": 0.627133846282959,
      "learning_rate": 2.3795270876982938e-05,
      "loss": 0.1092,
      "step": 5893
    },
    {
      "epoch": 7.932705248990579,
      "grad_norm": 0.5261929631233215,
      "learning_rate": 2.376533971864711e-05,
      "loss": 0.1213,
      "step": 5894
    },
    {
      "epoch": 7.934051144010767,
      "grad_norm": 0.421573668718338,
      "learning_rate": 2.3735408560311283e-05,
      "loss": 0.0987,
      "step": 5895
    },
    {
      "epoch": 7.935397039030955,
      "grad_norm": 0.4751652777194977,
      "learning_rate": 2.3705477401975456e-05,
      "loss": 0.1125,
      "step": 5896
    },
    {
      "epoch": 7.936742934051144,
      "grad_norm": 0.6418229937553406,
      "learning_rate": 2.367554624363963e-05,
      "loss": 0.095,
      "step": 5897
    },
    {
      "epoch": 7.938088829071332,
      "grad_norm": 0.650728166103363,
      "learning_rate": 2.36456150853038e-05,
      "loss": 0.1337,
      "step": 5898
    },
    {
      "epoch": 7.939434724091521,
      "grad_norm": 0.4394778907299042,
      "learning_rate": 2.3615683926967973e-05,
      "loss": 0.1032,
      "step": 5899
    },
    {
      "epoch": 7.94078061911171,
      "grad_norm": 0.35954493284225464,
      "learning_rate": 2.3585752768632146e-05,
      "loss": 0.0808,
      "step": 5900
    },
    {
      "epoch": 7.942126514131898,
      "grad_norm": 0.4070870876312256,
      "learning_rate": 2.355582161029632e-05,
      "loss": 0.097,
      "step": 5901
    },
    {
      "epoch": 7.943472409152086,
      "grad_norm": 0.487774133682251,
      "learning_rate": 2.352589045196049e-05,
      "loss": 0.12,
      "step": 5902
    },
    {
      "epoch": 7.944818304172275,
      "grad_norm": 0.5035156607627869,
      "learning_rate": 2.3495959293624664e-05,
      "loss": 0.1028,
      "step": 5903
    },
    {
      "epoch": 7.946164199192463,
      "grad_norm": 1.0359313488006592,
      "learning_rate": 2.3466028135288836e-05,
      "loss": 0.1131,
      "step": 5904
    },
    {
      "epoch": 7.947510094212651,
      "grad_norm": 0.5950589776039124,
      "learning_rate": 2.343609697695301e-05,
      "loss": 0.0848,
      "step": 5905
    },
    {
      "epoch": 7.94885598923284,
      "grad_norm": 0.5162825584411621,
      "learning_rate": 2.340616581861718e-05,
      "loss": 0.1038,
      "step": 5906
    },
    {
      "epoch": 7.950201884253028,
      "grad_norm": 0.9495925307273865,
      "learning_rate": 2.3376234660281354e-05,
      "loss": 0.1526,
      "step": 5907
    },
    {
      "epoch": 7.951547779273216,
      "grad_norm": 1.3934953212738037,
      "learning_rate": 2.3346303501945526e-05,
      "loss": 0.1642,
      "step": 5908
    },
    {
      "epoch": 7.952893674293405,
      "grad_norm": 0.5725675821304321,
      "learning_rate": 2.33163723436097e-05,
      "loss": 0.127,
      "step": 5909
    },
    {
      "epoch": 7.954239569313594,
      "grad_norm": 0.8477947115898132,
      "learning_rate": 2.328644118527387e-05,
      "loss": 0.1211,
      "step": 5910
    },
    {
      "epoch": 7.955585464333782,
      "grad_norm": 0.41722404956817627,
      "learning_rate": 2.3256510026938044e-05,
      "loss": 0.0825,
      "step": 5911
    },
    {
      "epoch": 7.956931359353971,
      "grad_norm": 0.6816229224205017,
      "learning_rate": 2.3226578868602216e-05,
      "loss": 0.0891,
      "step": 5912
    },
    {
      "epoch": 7.958277254374159,
      "grad_norm": 0.6898479461669922,
      "learning_rate": 2.319664771026639e-05,
      "loss": 0.1033,
      "step": 5913
    },
    {
      "epoch": 7.959623149394347,
      "grad_norm": 0.3816474676132202,
      "learning_rate": 2.316671655193056e-05,
      "loss": 0.0965,
      "step": 5914
    },
    {
      "epoch": 7.960969044414536,
      "grad_norm": 0.586138129234314,
      "learning_rate": 2.3136785393594734e-05,
      "loss": 0.1122,
      "step": 5915
    },
    {
      "epoch": 7.962314939434724,
      "grad_norm": 1.4535847902297974,
      "learning_rate": 2.3106854235258907e-05,
      "loss": 0.1545,
      "step": 5916
    },
    {
      "epoch": 7.963660834454912,
      "grad_norm": 0.4863716959953308,
      "learning_rate": 2.307692307692308e-05,
      "loss": 0.0845,
      "step": 5917
    },
    {
      "epoch": 7.965006729475101,
      "grad_norm": 0.6647167801856995,
      "learning_rate": 2.304699191858725e-05,
      "loss": 0.1117,
      "step": 5918
    },
    {
      "epoch": 7.96635262449529,
      "grad_norm": 0.46984949707984924,
      "learning_rate": 2.301706076025142e-05,
      "loss": 0.1066,
      "step": 5919
    },
    {
      "epoch": 7.967698519515478,
      "grad_norm": 1.0001709461212158,
      "learning_rate": 2.2987129601915593e-05,
      "loss": 0.1526,
      "step": 5920
    },
    {
      "epoch": 7.9690444145356665,
      "grad_norm": 0.4238795042037964,
      "learning_rate": 2.2957198443579766e-05,
      "loss": 0.0868,
      "step": 5921
    },
    {
      "epoch": 7.970390309555855,
      "grad_norm": 0.4542076289653778,
      "learning_rate": 2.292726728524394e-05,
      "loss": 0.1011,
      "step": 5922
    },
    {
      "epoch": 7.971736204576043,
      "grad_norm": 0.8373411893844604,
      "learning_rate": 2.289733612690811e-05,
      "loss": 0.1164,
      "step": 5923
    },
    {
      "epoch": 7.9730820995962315,
      "grad_norm": 0.3983004093170166,
      "learning_rate": 2.2867404968572283e-05,
      "loss": 0.0863,
      "step": 5924
    },
    {
      "epoch": 7.97442799461642,
      "grad_norm": 0.49253666400909424,
      "learning_rate": 2.2837473810236456e-05,
      "loss": 0.1138,
      "step": 5925
    },
    {
      "epoch": 7.975773889636608,
      "grad_norm": 0.965928852558136,
      "learning_rate": 2.280754265190063e-05,
      "loss": 0.1864,
      "step": 5926
    },
    {
      "epoch": 7.9771197846567965,
      "grad_norm": 0.4919833242893219,
      "learning_rate": 2.27776114935648e-05,
      "loss": 0.0905,
      "step": 5927
    },
    {
      "epoch": 7.978465679676985,
      "grad_norm": 0.8537764549255371,
      "learning_rate": 2.2747680335228974e-05,
      "loss": 0.12,
      "step": 5928
    },
    {
      "epoch": 7.979811574697173,
      "grad_norm": 1.0311412811279297,
      "learning_rate": 2.2717749176893146e-05,
      "loss": 0.1727,
      "step": 5929
    },
    {
      "epoch": 7.981157469717362,
      "grad_norm": 0.4491475522518158,
      "learning_rate": 2.268781801855732e-05,
      "loss": 0.1064,
      "step": 5930
    },
    {
      "epoch": 7.982503364737551,
      "grad_norm": 1.0899137258529663,
      "learning_rate": 2.265788686022149e-05,
      "loss": 0.1341,
      "step": 5931
    },
    {
      "epoch": 7.983849259757739,
      "grad_norm": 0.4324992895126343,
      "learning_rate": 2.2627955701885664e-05,
      "loss": 0.0781,
      "step": 5932
    },
    {
      "epoch": 7.985195154777927,
      "grad_norm": 0.335737407207489,
      "learning_rate": 2.2598024543549836e-05,
      "loss": 0.0892,
      "step": 5933
    },
    {
      "epoch": 7.986541049798116,
      "grad_norm": 0.5374876260757446,
      "learning_rate": 2.256809338521401e-05,
      "loss": 0.1094,
      "step": 5934
    },
    {
      "epoch": 7.987886944818304,
      "grad_norm": 0.5232747793197632,
      "learning_rate": 2.253816222687818e-05,
      "loss": 0.1225,
      "step": 5935
    },
    {
      "epoch": 7.989232839838492,
      "grad_norm": 0.5099799633026123,
      "learning_rate": 2.2508231068542354e-05,
      "loss": 0.0922,
      "step": 5936
    },
    {
      "epoch": 7.990578734858681,
      "grad_norm": 0.6123384833335876,
      "learning_rate": 2.2478299910206526e-05,
      "loss": 0.0946,
      "step": 5937
    },
    {
      "epoch": 7.991924629878869,
      "grad_norm": 0.9842960238456726,
      "learning_rate": 2.24483687518707e-05,
      "loss": 0.1174,
      "step": 5938
    },
    {
      "epoch": 7.993270524899058,
      "grad_norm": 0.6943508386611938,
      "learning_rate": 2.241843759353487e-05,
      "loss": 0.1308,
      "step": 5939
    },
    {
      "epoch": 7.994616419919247,
      "grad_norm": 1.1428155899047852,
      "learning_rate": 2.2388506435199044e-05,
      "loss": 0.1924,
      "step": 5940
    },
    {
      "epoch": 7.995962314939435,
      "grad_norm": 0.8597797751426697,
      "learning_rate": 2.2358575276863217e-05,
      "loss": 0.1539,
      "step": 5941
    },
    {
      "epoch": 7.997308209959623,
      "grad_norm": 0.42495521903038025,
      "learning_rate": 2.232864411852739e-05,
      "loss": 0.0934,
      "step": 5942
    },
    {
      "epoch": 7.998654104979812,
      "grad_norm": 0.6293737888336182,
      "learning_rate": 2.229871296019156e-05,
      "loss": 0.1267,
      "step": 5943
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.689386248588562,
      "learning_rate": 2.2268781801855734e-05,
      "loss": 0.1433,
      "step": 5944
    },
    {
      "epoch": 8.001345895020188,
      "grad_norm": 0.4224320650100708,
      "learning_rate": 2.2238850643519903e-05,
      "loss": 0.1074,
      "step": 5945
    },
    {
      "epoch": 8.002691790040377,
      "grad_norm": 0.3795849680900574,
      "learning_rate": 2.2208919485184076e-05,
      "loss": 0.0925,
      "step": 5946
    },
    {
      "epoch": 8.004037685060565,
      "grad_norm": 0.4217328727245331,
      "learning_rate": 2.217898832684825e-05,
      "loss": 0.0796,
      "step": 5947
    },
    {
      "epoch": 8.005383580080753,
      "grad_norm": 0.33949872851371765,
      "learning_rate": 2.214905716851242e-05,
      "loss": 0.0934,
      "step": 5948
    },
    {
      "epoch": 8.006729475100942,
      "grad_norm": 0.414903461933136,
      "learning_rate": 2.2119126010176593e-05,
      "loss": 0.0791,
      "step": 5949
    },
    {
      "epoch": 8.00807537012113,
      "grad_norm": 0.3123369812965393,
      "learning_rate": 2.2089194851840766e-05,
      "loss": 0.0876,
      "step": 5950
    },
    {
      "epoch": 8.009421265141318,
      "grad_norm": 0.33468374609947205,
      "learning_rate": 2.205926369350494e-05,
      "loss": 0.0802,
      "step": 5951
    },
    {
      "epoch": 8.010767160161507,
      "grad_norm": 0.25591567158699036,
      "learning_rate": 2.202933253516911e-05,
      "loss": 0.0711,
      "step": 5952
    },
    {
      "epoch": 8.012113055181695,
      "grad_norm": 0.419687956571579,
      "learning_rate": 2.1999401376833284e-05,
      "loss": 0.1081,
      "step": 5953
    },
    {
      "epoch": 8.013458950201883,
      "grad_norm": 0.2773793339729309,
      "learning_rate": 2.1969470218497456e-05,
      "loss": 0.0823,
      "step": 5954
    },
    {
      "epoch": 8.014804845222073,
      "grad_norm": 0.3483402132987976,
      "learning_rate": 2.193953906016163e-05,
      "loss": 0.091,
      "step": 5955
    },
    {
      "epoch": 8.016150740242262,
      "grad_norm": 0.6160963773727417,
      "learning_rate": 2.19096079018258e-05,
      "loss": 0.1065,
      "step": 5956
    },
    {
      "epoch": 8.01749663526245,
      "grad_norm": 0.8777322769165039,
      "learning_rate": 2.1879676743489974e-05,
      "loss": 0.1036,
      "step": 5957
    },
    {
      "epoch": 8.018842530282638,
      "grad_norm": 0.2706625163555145,
      "learning_rate": 2.1849745585154146e-05,
      "loss": 0.0742,
      "step": 5958
    },
    {
      "epoch": 8.020188425302827,
      "grad_norm": 0.7626864314079285,
      "learning_rate": 2.181981442681832e-05,
      "loss": 0.0933,
      "step": 5959
    },
    {
      "epoch": 8.021534320323015,
      "grad_norm": 0.3235725462436676,
      "learning_rate": 2.178988326848249e-05,
      "loss": 0.0813,
      "step": 5960
    },
    {
      "epoch": 8.022880215343204,
      "grad_norm": 0.352161169052124,
      "learning_rate": 2.1759952110146664e-05,
      "loss": 0.0986,
      "step": 5961
    },
    {
      "epoch": 8.024226110363392,
      "grad_norm": 0.360089510679245,
      "learning_rate": 2.1730020951810836e-05,
      "loss": 0.0879,
      "step": 5962
    },
    {
      "epoch": 8.02557200538358,
      "grad_norm": 0.31677955389022827,
      "learning_rate": 2.170008979347501e-05,
      "loss": 0.0811,
      "step": 5963
    },
    {
      "epoch": 8.026917900403769,
      "grad_norm": 0.2877243161201477,
      "learning_rate": 2.167015863513918e-05,
      "loss": 0.0826,
      "step": 5964
    },
    {
      "epoch": 8.028263795423957,
      "grad_norm": 0.5095882415771484,
      "learning_rate": 2.1640227476803354e-05,
      "loss": 0.0923,
      "step": 5965
    },
    {
      "epoch": 8.029609690444145,
      "grad_norm": 0.29874080419540405,
      "learning_rate": 2.1610296318467527e-05,
      "loss": 0.0883,
      "step": 5966
    },
    {
      "epoch": 8.030955585464334,
      "grad_norm": 1.0323758125305176,
      "learning_rate": 2.15803651601317e-05,
      "loss": 0.1454,
      "step": 5967
    },
    {
      "epoch": 8.032301480484522,
      "grad_norm": 0.2697489261627197,
      "learning_rate": 2.155043400179587e-05,
      "loss": 0.0781,
      "step": 5968
    },
    {
      "epoch": 8.03364737550471,
      "grad_norm": 0.5904021263122559,
      "learning_rate": 2.1520502843460044e-05,
      "loss": 0.1019,
      "step": 5969
    },
    {
      "epoch": 8.034993270524899,
      "grad_norm": 0.4745975732803345,
      "learning_rate": 2.1490571685124217e-05,
      "loss": 0.098,
      "step": 5970
    },
    {
      "epoch": 8.036339165545087,
      "grad_norm": 0.3563344180583954,
      "learning_rate": 2.1460640526788386e-05,
      "loss": 0.1047,
      "step": 5971
    },
    {
      "epoch": 8.037685060565275,
      "grad_norm": 0.5051081776618958,
      "learning_rate": 2.143070936845256e-05,
      "loss": 0.103,
      "step": 5972
    },
    {
      "epoch": 8.039030955585464,
      "grad_norm": 0.3702779710292816,
      "learning_rate": 2.140077821011673e-05,
      "loss": 0.0907,
      "step": 5973
    },
    {
      "epoch": 8.040376850605652,
      "grad_norm": 0.35747382044792175,
      "learning_rate": 2.1370847051780904e-05,
      "loss": 0.1011,
      "step": 5974
    },
    {
      "epoch": 8.041722745625842,
      "grad_norm": 0.6086372137069702,
      "learning_rate": 2.1340915893445076e-05,
      "loss": 0.1004,
      "step": 5975
    },
    {
      "epoch": 8.04306864064603,
      "grad_norm": 0.2556420564651489,
      "learning_rate": 2.131098473510925e-05,
      "loss": 0.0746,
      "step": 5976
    },
    {
      "epoch": 8.044414535666219,
      "grad_norm": 0.40319958329200745,
      "learning_rate": 2.128105357677342e-05,
      "loss": 0.1103,
      "step": 5977
    },
    {
      "epoch": 8.045760430686407,
      "grad_norm": 0.34432148933410645,
      "learning_rate": 2.1251122418437594e-05,
      "loss": 0.0781,
      "step": 5978
    },
    {
      "epoch": 8.047106325706595,
      "grad_norm": 0.33654865622520447,
      "learning_rate": 2.1221191260101766e-05,
      "loss": 0.0922,
      "step": 5979
    },
    {
      "epoch": 8.048452220726784,
      "grad_norm": 0.47274747490882874,
      "learning_rate": 2.119126010176594e-05,
      "loss": 0.087,
      "step": 5980
    },
    {
      "epoch": 8.049798115746972,
      "grad_norm": 0.4070166051387787,
      "learning_rate": 2.116132894343011e-05,
      "loss": 0.0852,
      "step": 5981
    },
    {
      "epoch": 8.05114401076716,
      "grad_norm": 0.6540122032165527,
      "learning_rate": 2.1131397785094284e-05,
      "loss": 0.1548,
      "step": 5982
    },
    {
      "epoch": 8.052489905787349,
      "grad_norm": 0.36428359150886536,
      "learning_rate": 2.1101466626758456e-05,
      "loss": 0.0829,
      "step": 5983
    },
    {
      "epoch": 8.053835800807537,
      "grad_norm": 0.29402172565460205,
      "learning_rate": 2.107153546842263e-05,
      "loss": 0.0779,
      "step": 5984
    },
    {
      "epoch": 8.055181695827725,
      "grad_norm": 0.295088529586792,
      "learning_rate": 2.10416043100868e-05,
      "loss": 0.0837,
      "step": 5985
    },
    {
      "epoch": 8.056527590847914,
      "grad_norm": 0.3611849844455719,
      "learning_rate": 2.1011673151750974e-05,
      "loss": 0.0819,
      "step": 5986
    },
    {
      "epoch": 8.057873485868102,
      "grad_norm": 0.43409404158592224,
      "learning_rate": 2.0981741993415146e-05,
      "loss": 0.0896,
      "step": 5987
    },
    {
      "epoch": 8.05921938088829,
      "grad_norm": 0.28274476528167725,
      "learning_rate": 2.095181083507932e-05,
      "loss": 0.0678,
      "step": 5988
    },
    {
      "epoch": 8.060565275908479,
      "grad_norm": 0.3234388530254364,
      "learning_rate": 2.092187967674349e-05,
      "loss": 0.0889,
      "step": 5989
    },
    {
      "epoch": 8.061911170928667,
      "grad_norm": 0.4826141595840454,
      "learning_rate": 2.0891948518407664e-05,
      "loss": 0.11,
      "step": 5990
    },
    {
      "epoch": 8.063257065948855,
      "grad_norm": 0.27441275119781494,
      "learning_rate": 2.0862017360071837e-05,
      "loss": 0.0677,
      "step": 5991
    },
    {
      "epoch": 8.064602960969044,
      "grad_norm": 0.39125117659568787,
      "learning_rate": 2.083208620173601e-05,
      "loss": 0.1309,
      "step": 5992
    },
    {
      "epoch": 8.065948855989232,
      "grad_norm": 0.3885371685028076,
      "learning_rate": 2.0802155043400182e-05,
      "loss": 0.1044,
      "step": 5993
    },
    {
      "epoch": 8.06729475100942,
      "grad_norm": 0.6698672771453857,
      "learning_rate": 2.0772223885064354e-05,
      "loss": 0.0953,
      "step": 5994
    },
    {
      "epoch": 8.06864064602961,
      "grad_norm": 1.0525972843170166,
      "learning_rate": 2.0742292726728527e-05,
      "loss": 0.1068,
      "step": 5995
    },
    {
      "epoch": 8.069986541049799,
      "grad_norm": 0.4381771385669708,
      "learning_rate": 2.07123615683927e-05,
      "loss": 0.1051,
      "step": 5996
    },
    {
      "epoch": 8.071332436069987,
      "grad_norm": 0.364441454410553,
      "learning_rate": 2.068243041005687e-05,
      "loss": 0.0819,
      "step": 5997
    },
    {
      "epoch": 8.072678331090176,
      "grad_norm": 0.3037736415863037,
      "learning_rate": 2.065249925172104e-05,
      "loss": 0.0735,
      "step": 5998
    },
    {
      "epoch": 8.074024226110364,
      "grad_norm": 0.3661328852176666,
      "learning_rate": 2.0622568093385214e-05,
      "loss": 0.0815,
      "step": 5999
    },
    {
      "epoch": 8.075370121130552,
      "grad_norm": 0.2750324010848999,
      "learning_rate": 2.0592636935049386e-05,
      "loss": 0.0724,
      "step": 6000
    },
    {
      "epoch": 8.07671601615074,
      "grad_norm": 0.28274479508399963,
      "learning_rate": 2.056270577671356e-05,
      "loss": 0.08,
      "step": 6001
    },
    {
      "epoch": 8.078061911170929,
      "grad_norm": 0.3467499613761902,
      "learning_rate": 2.053277461837773e-05,
      "loss": 0.0818,
      "step": 6002
    },
    {
      "epoch": 8.079407806191117,
      "grad_norm": 0.40191635489463806,
      "learning_rate": 2.0502843460041904e-05,
      "loss": 0.0814,
      "step": 6003
    },
    {
      "epoch": 8.080753701211306,
      "grad_norm": 0.5907441973686218,
      "learning_rate": 2.0472912301706076e-05,
      "loss": 0.0972,
      "step": 6004
    },
    {
      "epoch": 8.082099596231494,
      "grad_norm": 0.2740143835544586,
      "learning_rate": 2.044298114337025e-05,
      "loss": 0.0688,
      "step": 6005
    },
    {
      "epoch": 8.083445491251682,
      "grad_norm": 0.2795967161655426,
      "learning_rate": 2.041304998503442e-05,
      "loss": 0.0744,
      "step": 6006
    },
    {
      "epoch": 8.08479138627187,
      "grad_norm": 0.3398914337158203,
      "learning_rate": 2.0383118826698594e-05,
      "loss": 0.0783,
      "step": 6007
    },
    {
      "epoch": 8.086137281292059,
      "grad_norm": 0.28261417150497437,
      "learning_rate": 2.0353187668362766e-05,
      "loss": 0.0792,
      "step": 6008
    },
    {
      "epoch": 8.087483176312247,
      "grad_norm": 0.30727335810661316,
      "learning_rate": 2.032325651002694e-05,
      "loss": 0.0793,
      "step": 6009
    },
    {
      "epoch": 8.088829071332436,
      "grad_norm": 0.5981548428535461,
      "learning_rate": 2.029332535169111e-05,
      "loss": 0.1117,
      "step": 6010
    },
    {
      "epoch": 8.090174966352624,
      "grad_norm": 0.3326394259929657,
      "learning_rate": 2.0263394193355284e-05,
      "loss": 0.0875,
      "step": 6011
    },
    {
      "epoch": 8.091520861372812,
      "grad_norm": 0.4240748882293701,
      "learning_rate": 2.0233463035019457e-05,
      "loss": 0.1053,
      "step": 6012
    },
    {
      "epoch": 8.092866756393,
      "grad_norm": 0.48086661100387573,
      "learning_rate": 2.020353187668363e-05,
      "loss": 0.1062,
      "step": 6013
    },
    {
      "epoch": 8.094212651413189,
      "grad_norm": 0.40179428458213806,
      "learning_rate": 2.01736007183478e-05,
      "loss": 0.0971,
      "step": 6014
    },
    {
      "epoch": 8.095558546433379,
      "grad_norm": 0.8723105192184448,
      "learning_rate": 2.0143669560011974e-05,
      "loss": 0.1107,
      "step": 6015
    },
    {
      "epoch": 8.096904441453567,
      "grad_norm": 0.37595340609550476,
      "learning_rate": 2.0113738401676147e-05,
      "loss": 0.0741,
      "step": 6016
    },
    {
      "epoch": 8.098250336473756,
      "grad_norm": 0.6217743158340454,
      "learning_rate": 2.008380724334032e-05,
      "loss": 0.1088,
      "step": 6017
    },
    {
      "epoch": 8.099596231493944,
      "grad_norm": 0.42511388659477234,
      "learning_rate": 2.0053876085004492e-05,
      "loss": 0.0976,
      "step": 6018
    },
    {
      "epoch": 8.100942126514132,
      "grad_norm": 0.3055683672428131,
      "learning_rate": 2.0023944926668664e-05,
      "loss": 0.0792,
      "step": 6019
    },
    {
      "epoch": 8.10228802153432,
      "grad_norm": 0.5772174596786499,
      "learning_rate": 1.9994013768332837e-05,
      "loss": 0.0939,
      "step": 6020
    },
    {
      "epoch": 8.103633916554509,
      "grad_norm": 0.4749605357646942,
      "learning_rate": 1.996408260999701e-05,
      "loss": 0.0984,
      "step": 6021
    },
    {
      "epoch": 8.104979811574697,
      "grad_norm": 0.4227842390537262,
      "learning_rate": 1.9934151451661182e-05,
      "loss": 0.099,
      "step": 6022
    },
    {
      "epoch": 8.106325706594886,
      "grad_norm": 0.45644447207450867,
      "learning_rate": 1.990422029332535e-05,
      "loss": 0.1176,
      "step": 6023
    },
    {
      "epoch": 8.107671601615074,
      "grad_norm": 0.4595705568790436,
      "learning_rate": 1.9874289134989524e-05,
      "loss": 0.12,
      "step": 6024
    },
    {
      "epoch": 8.109017496635262,
      "grad_norm": 0.3600306808948517,
      "learning_rate": 1.9844357976653696e-05,
      "loss": 0.0734,
      "step": 6025
    },
    {
      "epoch": 8.11036339165545,
      "grad_norm": 0.4347843825817108,
      "learning_rate": 1.981442681831787e-05,
      "loss": 0.0934,
      "step": 6026
    },
    {
      "epoch": 8.111709286675639,
      "grad_norm": 0.5390971899032593,
      "learning_rate": 1.978449565998204e-05,
      "loss": 0.0965,
      "step": 6027
    },
    {
      "epoch": 8.113055181695827,
      "grad_norm": 0.44551512598991394,
      "learning_rate": 1.9754564501646214e-05,
      "loss": 0.1094,
      "step": 6028
    },
    {
      "epoch": 8.114401076716016,
      "grad_norm": 0.28585609793663025,
      "learning_rate": 1.9724633343310386e-05,
      "loss": 0.0625,
      "step": 6029
    },
    {
      "epoch": 8.115746971736204,
      "grad_norm": 0.2971654236316681,
      "learning_rate": 1.969470218497456e-05,
      "loss": 0.0787,
      "step": 6030
    },
    {
      "epoch": 8.117092866756392,
      "grad_norm": 0.3277720510959625,
      "learning_rate": 1.966477102663873e-05,
      "loss": 0.0625,
      "step": 6031
    },
    {
      "epoch": 8.11843876177658,
      "grad_norm": 0.33770614862442017,
      "learning_rate": 1.9634839868302904e-05,
      "loss": 0.0724,
      "step": 6032
    },
    {
      "epoch": 8.119784656796769,
      "grad_norm": 0.368337482213974,
      "learning_rate": 1.9604908709967076e-05,
      "loss": 0.0956,
      "step": 6033
    },
    {
      "epoch": 8.121130551816957,
      "grad_norm": 0.5782835483551025,
      "learning_rate": 1.957497755163125e-05,
      "loss": 0.1128,
      "step": 6034
    },
    {
      "epoch": 8.122476446837148,
      "grad_norm": 0.2577088475227356,
      "learning_rate": 1.954504639329542e-05,
      "loss": 0.0765,
      "step": 6035
    },
    {
      "epoch": 8.123822341857336,
      "grad_norm": 0.34571412205696106,
      "learning_rate": 1.9515115234959594e-05,
      "loss": 0.0702,
      "step": 6036
    },
    {
      "epoch": 8.125168236877524,
      "grad_norm": 0.3222897946834564,
      "learning_rate": 1.9485184076623767e-05,
      "loss": 0.088,
      "step": 6037
    },
    {
      "epoch": 8.126514131897713,
      "grad_norm": 0.34465551376342773,
      "learning_rate": 1.945525291828794e-05,
      "loss": 0.0885,
      "step": 6038
    },
    {
      "epoch": 8.1278600269179,
      "grad_norm": 0.31614166498184204,
      "learning_rate": 1.942532175995211e-05,
      "loss": 0.0879,
      "step": 6039
    },
    {
      "epoch": 8.12920592193809,
      "grad_norm": 0.4218389093875885,
      "learning_rate": 1.9395390601616284e-05,
      "loss": 0.0788,
      "step": 6040
    },
    {
      "epoch": 8.130551816958278,
      "grad_norm": 0.3657234013080597,
      "learning_rate": 1.9365459443280457e-05,
      "loss": 0.0697,
      "step": 6041
    },
    {
      "epoch": 8.131897711978466,
      "grad_norm": 0.3254162073135376,
      "learning_rate": 1.933552828494463e-05,
      "loss": 0.0701,
      "step": 6042
    },
    {
      "epoch": 8.133243606998654,
      "grad_norm": 0.4590442180633545,
      "learning_rate": 1.9305597126608802e-05,
      "loss": 0.1106,
      "step": 6043
    },
    {
      "epoch": 8.134589502018843,
      "grad_norm": 0.4617130756378174,
      "learning_rate": 1.9275665968272974e-05,
      "loss": 0.091,
      "step": 6044
    },
    {
      "epoch": 8.135935397039031,
      "grad_norm": 1.0352110862731934,
      "learning_rate": 1.9245734809937147e-05,
      "loss": 0.1499,
      "step": 6045
    },
    {
      "epoch": 8.13728129205922,
      "grad_norm": 0.5424364805221558,
      "learning_rate": 1.921580365160132e-05,
      "loss": 0.1139,
      "step": 6046
    },
    {
      "epoch": 8.138627187079408,
      "grad_norm": 0.6624072790145874,
      "learning_rate": 1.9185872493265492e-05,
      "loss": 0.0992,
      "step": 6047
    },
    {
      "epoch": 8.139973082099596,
      "grad_norm": 0.41999953985214233,
      "learning_rate": 1.9155941334929664e-05,
      "loss": 0.0814,
      "step": 6048
    },
    {
      "epoch": 8.141318977119784,
      "grad_norm": 0.43922051787376404,
      "learning_rate": 1.9126010176593834e-05,
      "loss": 0.0945,
      "step": 6049
    },
    {
      "epoch": 8.142664872139973,
      "grad_norm": 0.5168256163597107,
      "learning_rate": 1.9096079018258006e-05,
      "loss": 0.0892,
      "step": 6050
    },
    {
      "epoch": 8.144010767160161,
      "grad_norm": 0.3522162437438965,
      "learning_rate": 1.906614785992218e-05,
      "loss": 0.0976,
      "step": 6051
    },
    {
      "epoch": 8.14535666218035,
      "grad_norm": 0.8098278045654297,
      "learning_rate": 1.903621670158635e-05,
      "loss": 0.1196,
      "step": 6052
    },
    {
      "epoch": 8.146702557200538,
      "grad_norm": 0.2431633621454239,
      "learning_rate": 1.9006285543250524e-05,
      "loss": 0.0622,
      "step": 6053
    },
    {
      "epoch": 8.148048452220726,
      "grad_norm": 0.3560878336429596,
      "learning_rate": 1.8976354384914696e-05,
      "loss": 0.0809,
      "step": 6054
    },
    {
      "epoch": 8.149394347240916,
      "grad_norm": 0.4411293864250183,
      "learning_rate": 1.894642322657887e-05,
      "loss": 0.098,
      "step": 6055
    },
    {
      "epoch": 8.150740242261104,
      "grad_norm": 0.3166111409664154,
      "learning_rate": 1.891649206824304e-05,
      "loss": 0.083,
      "step": 6056
    },
    {
      "epoch": 8.152086137281293,
      "grad_norm": 0.3582967221736908,
      "learning_rate": 1.8886560909907214e-05,
      "loss": 0.0841,
      "step": 6057
    },
    {
      "epoch": 8.153432032301481,
      "grad_norm": 0.48840948939323425,
      "learning_rate": 1.8856629751571386e-05,
      "loss": 0.1152,
      "step": 6058
    },
    {
      "epoch": 8.15477792732167,
      "grad_norm": 0.3968437612056732,
      "learning_rate": 1.882669859323556e-05,
      "loss": 0.0824,
      "step": 6059
    },
    {
      "epoch": 8.156123822341858,
      "grad_norm": 0.35009703040122986,
      "learning_rate": 1.879676743489973e-05,
      "loss": 0.0778,
      "step": 6060
    },
    {
      "epoch": 8.157469717362046,
      "grad_norm": 0.45136934518814087,
      "learning_rate": 1.8766836276563904e-05,
      "loss": 0.1017,
      "step": 6061
    },
    {
      "epoch": 8.158815612382234,
      "grad_norm": 0.2864525616168976,
      "learning_rate": 1.8736905118228077e-05,
      "loss": 0.0618,
      "step": 6062
    },
    {
      "epoch": 8.160161507402423,
      "grad_norm": 0.37536683678627014,
      "learning_rate": 1.870697395989225e-05,
      "loss": 0.0809,
      "step": 6063
    },
    {
      "epoch": 8.161507402422611,
      "grad_norm": 0.34861454367637634,
      "learning_rate": 1.867704280155642e-05,
      "loss": 0.0684,
      "step": 6064
    },
    {
      "epoch": 8.1628532974428,
      "grad_norm": 0.4300631582736969,
      "learning_rate": 1.8647111643220594e-05,
      "loss": 0.0853,
      "step": 6065
    },
    {
      "epoch": 8.164199192462988,
      "grad_norm": 0.4592370390892029,
      "learning_rate": 1.8617180484884767e-05,
      "loss": 0.0966,
      "step": 6066
    },
    {
      "epoch": 8.165545087483176,
      "grad_norm": 0.32935237884521484,
      "learning_rate": 1.858724932654894e-05,
      "loss": 0.0717,
      "step": 6067
    },
    {
      "epoch": 8.166890982503364,
      "grad_norm": 0.37540486454963684,
      "learning_rate": 1.8557318168213112e-05,
      "loss": 0.0738,
      "step": 6068
    },
    {
      "epoch": 8.168236877523553,
      "grad_norm": 0.43899357318878174,
      "learning_rate": 1.8527387009877284e-05,
      "loss": 0.0855,
      "step": 6069
    },
    {
      "epoch": 8.169582772543741,
      "grad_norm": 0.48689591884613037,
      "learning_rate": 1.8497455851541457e-05,
      "loss": 0.1103,
      "step": 6070
    },
    {
      "epoch": 8.17092866756393,
      "grad_norm": 0.38728073239326477,
      "learning_rate": 1.846752469320563e-05,
      "loss": 0.0856,
      "step": 6071
    },
    {
      "epoch": 8.172274562584118,
      "grad_norm": 0.3617619276046753,
      "learning_rate": 1.8437593534869802e-05,
      "loss": 0.0816,
      "step": 6072
    },
    {
      "epoch": 8.173620457604306,
      "grad_norm": 0.615336537361145,
      "learning_rate": 1.8407662376533975e-05,
      "loss": 0.1356,
      "step": 6073
    },
    {
      "epoch": 8.174966352624494,
      "grad_norm": 0.8748589754104614,
      "learning_rate": 1.8377731218198147e-05,
      "loss": 0.1367,
      "step": 6074
    },
    {
      "epoch": 8.176312247644685,
      "grad_norm": 0.24380242824554443,
      "learning_rate": 1.8347800059862316e-05,
      "loss": 0.0655,
      "step": 6075
    },
    {
      "epoch": 8.177658142664873,
      "grad_norm": 0.3726380169391632,
      "learning_rate": 1.831786890152649e-05,
      "loss": 0.086,
      "step": 6076
    },
    {
      "epoch": 8.179004037685061,
      "grad_norm": 0.33453673124313354,
      "learning_rate": 1.828793774319066e-05,
      "loss": 0.0916,
      "step": 6077
    },
    {
      "epoch": 8.18034993270525,
      "grad_norm": 0.5222949385643005,
      "learning_rate": 1.8258006584854834e-05,
      "loss": 0.1296,
      "step": 6078
    },
    {
      "epoch": 8.181695827725438,
      "grad_norm": 0.5301405191421509,
      "learning_rate": 1.8228075426519006e-05,
      "loss": 0.1133,
      "step": 6079
    },
    {
      "epoch": 8.183041722745626,
      "grad_norm": 0.4354124963283539,
      "learning_rate": 1.819814426818318e-05,
      "loss": 0.112,
      "step": 6080
    },
    {
      "epoch": 8.184387617765815,
      "grad_norm": 0.4908435046672821,
      "learning_rate": 1.816821310984735e-05,
      "loss": 0.0932,
      "step": 6081
    },
    {
      "epoch": 8.185733512786003,
      "grad_norm": 0.40835073590278625,
      "learning_rate": 1.8138281951511524e-05,
      "loss": 0.0921,
      "step": 6082
    },
    {
      "epoch": 8.187079407806191,
      "grad_norm": 0.32467183470726013,
      "learning_rate": 1.8108350793175696e-05,
      "loss": 0.0704,
      "step": 6083
    },
    {
      "epoch": 8.18842530282638,
      "grad_norm": 0.24100898206233978,
      "learning_rate": 1.807841963483987e-05,
      "loss": 0.0618,
      "step": 6084
    },
    {
      "epoch": 8.189771197846568,
      "grad_norm": 0.4293907880783081,
      "learning_rate": 1.804848847650404e-05,
      "loss": 0.0796,
      "step": 6085
    },
    {
      "epoch": 8.191117092866756,
      "grad_norm": 0.5450527667999268,
      "learning_rate": 1.8018557318168214e-05,
      "loss": 0.0928,
      "step": 6086
    },
    {
      "epoch": 8.192462987886945,
      "grad_norm": 0.3883993327617645,
      "learning_rate": 1.7988626159832387e-05,
      "loss": 0.0912,
      "step": 6087
    },
    {
      "epoch": 8.193808882907133,
      "grad_norm": 0.6498045325279236,
      "learning_rate": 1.795869500149656e-05,
      "loss": 0.117,
      "step": 6088
    },
    {
      "epoch": 8.195154777927321,
      "grad_norm": 0.404345840215683,
      "learning_rate": 1.7928763843160732e-05,
      "loss": 0.078,
      "step": 6089
    },
    {
      "epoch": 8.19650067294751,
      "grad_norm": 0.44657790660858154,
      "learning_rate": 1.7898832684824904e-05,
      "loss": 0.0808,
      "step": 6090
    },
    {
      "epoch": 8.197846567967698,
      "grad_norm": 0.3303156793117523,
      "learning_rate": 1.7868901526489077e-05,
      "loss": 0.0747,
      "step": 6091
    },
    {
      "epoch": 8.199192462987886,
      "grad_norm": 0.8162952661514282,
      "learning_rate": 1.783897036815325e-05,
      "loss": 0.1106,
      "step": 6092
    },
    {
      "epoch": 8.200538358008075,
      "grad_norm": 0.4163707196712494,
      "learning_rate": 1.7809039209817422e-05,
      "loss": 0.114,
      "step": 6093
    },
    {
      "epoch": 8.201884253028263,
      "grad_norm": 0.4674728214740753,
      "learning_rate": 1.7779108051481594e-05,
      "loss": 0.1053,
      "step": 6094
    },
    {
      "epoch": 8.203230148048453,
      "grad_norm": 0.41536542773246765,
      "learning_rate": 1.7749176893145767e-05,
      "loss": 0.0954,
      "step": 6095
    },
    {
      "epoch": 8.204576043068641,
      "grad_norm": 0.3496467173099518,
      "learning_rate": 1.771924573480994e-05,
      "loss": 0.0908,
      "step": 6096
    },
    {
      "epoch": 8.20592193808883,
      "grad_norm": 0.5208042860031128,
      "learning_rate": 1.7689314576474112e-05,
      "loss": 0.1076,
      "step": 6097
    },
    {
      "epoch": 8.207267833109018,
      "grad_norm": 0.37126967310905457,
      "learning_rate": 1.7659383418138285e-05,
      "loss": 0.0922,
      "step": 6098
    },
    {
      "epoch": 8.208613728129206,
      "grad_norm": 0.40157902240753174,
      "learning_rate": 1.7629452259802457e-05,
      "loss": 0.0851,
      "step": 6099
    },
    {
      "epoch": 8.209959623149395,
      "grad_norm": 0.30023059248924255,
      "learning_rate": 1.759952110146663e-05,
      "loss": 0.0743,
      "step": 6100
    },
    {
      "epoch": 8.211305518169583,
      "grad_norm": 0.3520866632461548,
      "learning_rate": 1.75695899431308e-05,
      "loss": 0.0791,
      "step": 6101
    },
    {
      "epoch": 8.212651413189771,
      "grad_norm": 0.3582180142402649,
      "learning_rate": 1.753965878479497e-05,
      "loss": 0.077,
      "step": 6102
    },
    {
      "epoch": 8.21399730820996,
      "grad_norm": 0.31467244029045105,
      "learning_rate": 1.7509727626459144e-05,
      "loss": 0.0881,
      "step": 6103
    },
    {
      "epoch": 8.215343203230148,
      "grad_norm": 0.3814401924610138,
      "learning_rate": 1.7479796468123316e-05,
      "loss": 0.0854,
      "step": 6104
    },
    {
      "epoch": 8.216689098250336,
      "grad_norm": 0.6453099250793457,
      "learning_rate": 1.744986530978749e-05,
      "loss": 0.108,
      "step": 6105
    },
    {
      "epoch": 8.218034993270525,
      "grad_norm": 0.41958749294281006,
      "learning_rate": 1.741993415145166e-05,
      "loss": 0.095,
      "step": 6106
    },
    {
      "epoch": 8.219380888290713,
      "grad_norm": 0.5869056582450867,
      "learning_rate": 1.7390002993115834e-05,
      "loss": 0.0986,
      "step": 6107
    },
    {
      "epoch": 8.220726783310901,
      "grad_norm": 0.28109660744667053,
      "learning_rate": 1.7360071834780007e-05,
      "loss": 0.0741,
      "step": 6108
    },
    {
      "epoch": 8.22207267833109,
      "grad_norm": 0.4075985550880432,
      "learning_rate": 1.733014067644418e-05,
      "loss": 0.0788,
      "step": 6109
    },
    {
      "epoch": 8.223418573351278,
      "grad_norm": 0.2990174889564514,
      "learning_rate": 1.730020951810835e-05,
      "loss": 0.08,
      "step": 6110
    },
    {
      "epoch": 8.224764468371466,
      "grad_norm": 0.3539864718914032,
      "learning_rate": 1.7270278359772524e-05,
      "loss": 0.0885,
      "step": 6111
    },
    {
      "epoch": 8.226110363391655,
      "grad_norm": 0.30211833119392395,
      "learning_rate": 1.7240347201436697e-05,
      "loss": 0.0903,
      "step": 6112
    },
    {
      "epoch": 8.227456258411843,
      "grad_norm": 0.4785079061985016,
      "learning_rate": 1.721041604310087e-05,
      "loss": 0.0897,
      "step": 6113
    },
    {
      "epoch": 8.228802153432031,
      "grad_norm": 0.3557986617088318,
      "learning_rate": 1.7180484884765042e-05,
      "loss": 0.0843,
      "step": 6114
    },
    {
      "epoch": 8.230148048452222,
      "grad_norm": 0.5280547142028809,
      "learning_rate": 1.7150553726429214e-05,
      "loss": 0.1201,
      "step": 6115
    },
    {
      "epoch": 8.23149394347241,
      "grad_norm": 0.4622989892959595,
      "learning_rate": 1.7120622568093387e-05,
      "loss": 0.114,
      "step": 6116
    },
    {
      "epoch": 8.232839838492598,
      "grad_norm": 0.5480119585990906,
      "learning_rate": 1.709069140975756e-05,
      "loss": 0.101,
      "step": 6117
    },
    {
      "epoch": 8.234185733512787,
      "grad_norm": 0.7318770885467529,
      "learning_rate": 1.7060760251421732e-05,
      "loss": 0.1171,
      "step": 6118
    },
    {
      "epoch": 8.235531628532975,
      "grad_norm": 0.7563386559486389,
      "learning_rate": 1.7030829093085904e-05,
      "loss": 0.1253,
      "step": 6119
    },
    {
      "epoch": 8.236877523553163,
      "grad_norm": 0.430801123380661,
      "learning_rate": 1.7000897934750077e-05,
      "loss": 0.0801,
      "step": 6120
    },
    {
      "epoch": 8.238223418573352,
      "grad_norm": 0.6589198708534241,
      "learning_rate": 1.697096677641425e-05,
      "loss": 0.1207,
      "step": 6121
    },
    {
      "epoch": 8.23956931359354,
      "grad_norm": 0.5254690647125244,
      "learning_rate": 1.6941035618078422e-05,
      "loss": 0.0903,
      "step": 6122
    },
    {
      "epoch": 8.240915208613728,
      "grad_norm": 0.3241923153400421,
      "learning_rate": 1.6911104459742595e-05,
      "loss": 0.0881,
      "step": 6123
    },
    {
      "epoch": 8.242261103633917,
      "grad_norm": 0.29983025789260864,
      "learning_rate": 1.6881173301406767e-05,
      "loss": 0.0745,
      "step": 6124
    },
    {
      "epoch": 8.243606998654105,
      "grad_norm": 0.4417456388473511,
      "learning_rate": 1.685124214307094e-05,
      "loss": 0.1078,
      "step": 6125
    },
    {
      "epoch": 8.244952893674293,
      "grad_norm": 0.33943411707878113,
      "learning_rate": 1.6821310984735112e-05,
      "loss": 0.0875,
      "step": 6126
    },
    {
      "epoch": 8.246298788694482,
      "grad_norm": 0.38827964663505554,
      "learning_rate": 1.679137982639928e-05,
      "loss": 0.0912,
      "step": 6127
    },
    {
      "epoch": 8.24764468371467,
      "grad_norm": 0.31593087315559387,
      "learning_rate": 1.6761448668063454e-05,
      "loss": 0.065,
      "step": 6128
    },
    {
      "epoch": 8.248990578734858,
      "grad_norm": 0.29046088457107544,
      "learning_rate": 1.6731517509727626e-05,
      "loss": 0.0594,
      "step": 6129
    },
    {
      "epoch": 8.250336473755047,
      "grad_norm": 0.2887195944786072,
      "learning_rate": 1.67015863513918e-05,
      "loss": 0.0743,
      "step": 6130
    },
    {
      "epoch": 8.251682368775235,
      "grad_norm": 0.37704530358314514,
      "learning_rate": 1.667165519305597e-05,
      "loss": 0.0872,
      "step": 6131
    },
    {
      "epoch": 8.253028263795423,
      "grad_norm": 0.4555785357952118,
      "learning_rate": 1.6641724034720144e-05,
      "loss": 0.1072,
      "step": 6132
    },
    {
      "epoch": 8.254374158815612,
      "grad_norm": 0.5094744563102722,
      "learning_rate": 1.6611792876384317e-05,
      "loss": 0.0913,
      "step": 6133
    },
    {
      "epoch": 8.2557200538358,
      "grad_norm": 0.39174556732177734,
      "learning_rate": 1.658186171804849e-05,
      "loss": 0.0816,
      "step": 6134
    },
    {
      "epoch": 8.257065948855988,
      "grad_norm": 0.33111441135406494,
      "learning_rate": 1.655193055971266e-05,
      "loss": 0.076,
      "step": 6135
    },
    {
      "epoch": 8.258411843876178,
      "grad_norm": 1.2581629753112793,
      "learning_rate": 1.6521999401376834e-05,
      "loss": 0.1035,
      "step": 6136
    },
    {
      "epoch": 8.259757738896367,
      "grad_norm": 0.36813998222351074,
      "learning_rate": 1.6492068243041007e-05,
      "loss": 0.0924,
      "step": 6137
    },
    {
      "epoch": 8.261103633916555,
      "grad_norm": 0.3815809488296509,
      "learning_rate": 1.646213708470518e-05,
      "loss": 0.0807,
      "step": 6138
    },
    {
      "epoch": 8.262449528936743,
      "grad_norm": 0.40646111965179443,
      "learning_rate": 1.6432205926369352e-05,
      "loss": 0.0994,
      "step": 6139
    },
    {
      "epoch": 8.263795423956932,
      "grad_norm": 0.3179445266723633,
      "learning_rate": 1.6402274768033524e-05,
      "loss": 0.0817,
      "step": 6140
    },
    {
      "epoch": 8.26514131897712,
      "grad_norm": 0.7624956965446472,
      "learning_rate": 1.6372343609697697e-05,
      "loss": 0.074,
      "step": 6141
    },
    {
      "epoch": 8.266487213997308,
      "grad_norm": 0.3119643032550812,
      "learning_rate": 1.634241245136187e-05,
      "loss": 0.0777,
      "step": 6142
    },
    {
      "epoch": 8.267833109017497,
      "grad_norm": 0.37766095995903015,
      "learning_rate": 1.6312481293026042e-05,
      "loss": 0.0902,
      "step": 6143
    },
    {
      "epoch": 8.269179004037685,
      "grad_norm": 0.7077319025993347,
      "learning_rate": 1.6282550134690214e-05,
      "loss": 0.1049,
      "step": 6144
    },
    {
      "epoch": 8.270524899057873,
      "grad_norm": 0.5684922337532043,
      "learning_rate": 1.6252618976354387e-05,
      "loss": 0.1201,
      "step": 6145
    },
    {
      "epoch": 8.271870794078062,
      "grad_norm": 0.6031558513641357,
      "learning_rate": 1.622268781801856e-05,
      "loss": 0.111,
      "step": 6146
    },
    {
      "epoch": 8.27321668909825,
      "grad_norm": 0.6301554441452026,
      "learning_rate": 1.6192756659682732e-05,
      "loss": 0.108,
      "step": 6147
    },
    {
      "epoch": 8.274562584118438,
      "grad_norm": 0.3640053868293762,
      "learning_rate": 1.6162825501346905e-05,
      "loss": 0.091,
      "step": 6148
    },
    {
      "epoch": 8.275908479138627,
      "grad_norm": 0.6773055791854858,
      "learning_rate": 1.6132894343011077e-05,
      "loss": 0.1241,
      "step": 6149
    },
    {
      "epoch": 8.277254374158815,
      "grad_norm": 0.3450818657875061,
      "learning_rate": 1.610296318467525e-05,
      "loss": 0.0873,
      "step": 6150
    },
    {
      "epoch": 8.278600269179003,
      "grad_norm": 0.40096184611320496,
      "learning_rate": 1.6073032026339422e-05,
      "loss": 0.0966,
      "step": 6151
    },
    {
      "epoch": 8.279946164199192,
      "grad_norm": 0.9015893340110779,
      "learning_rate": 1.6043100868003595e-05,
      "loss": 0.1234,
      "step": 6152
    },
    {
      "epoch": 8.28129205921938,
      "grad_norm": 0.3128736615180969,
      "learning_rate": 1.6013169709667764e-05,
      "loss": 0.0728,
      "step": 6153
    },
    {
      "epoch": 8.282637954239569,
      "grad_norm": 0.5031026005744934,
      "learning_rate": 1.5983238551331936e-05,
      "loss": 0.1285,
      "step": 6154
    },
    {
      "epoch": 8.283983849259759,
      "grad_norm": 0.35852137207984924,
      "learning_rate": 1.595330739299611e-05,
      "loss": 0.0828,
      "step": 6155
    },
    {
      "epoch": 8.285329744279947,
      "grad_norm": 0.23168525099754333,
      "learning_rate": 1.592337623466028e-05,
      "loss": 0.0572,
      "step": 6156
    },
    {
      "epoch": 8.286675639300135,
      "grad_norm": 1.0045499801635742,
      "learning_rate": 1.5893445076324454e-05,
      "loss": 0.1336,
      "step": 6157
    },
    {
      "epoch": 8.288021534320324,
      "grad_norm": 0.48871105909347534,
      "learning_rate": 1.5863513917988627e-05,
      "loss": 0.0818,
      "step": 6158
    },
    {
      "epoch": 8.289367429340512,
      "grad_norm": 0.8538946509361267,
      "learning_rate": 1.58335827596528e-05,
      "loss": 0.105,
      "step": 6159
    },
    {
      "epoch": 8.2907133243607,
      "grad_norm": 0.9609708189964294,
      "learning_rate": 1.580365160131697e-05,
      "loss": 0.0941,
      "step": 6160
    },
    {
      "epoch": 8.292059219380889,
      "grad_norm": 0.28279364109039307,
      "learning_rate": 1.5773720442981144e-05,
      "loss": 0.0675,
      "step": 6161
    },
    {
      "epoch": 8.293405114401077,
      "grad_norm": 0.5582718849182129,
      "learning_rate": 1.5743789284645317e-05,
      "loss": 0.1017,
      "step": 6162
    },
    {
      "epoch": 8.294751009421265,
      "grad_norm": 0.37279045581817627,
      "learning_rate": 1.571385812630949e-05,
      "loss": 0.0678,
      "step": 6163
    },
    {
      "epoch": 8.296096904441454,
      "grad_norm": 0.533901572227478,
      "learning_rate": 1.5683926967973662e-05,
      "loss": 0.1277,
      "step": 6164
    },
    {
      "epoch": 8.297442799461642,
      "grad_norm": 0.36389967799186707,
      "learning_rate": 1.5653995809637834e-05,
      "loss": 0.0811,
      "step": 6165
    },
    {
      "epoch": 8.29878869448183,
      "grad_norm": 0.46011123061180115,
      "learning_rate": 1.5624064651302007e-05,
      "loss": 0.0977,
      "step": 6166
    },
    {
      "epoch": 8.300134589502019,
      "grad_norm": 0.31712767481803894,
      "learning_rate": 1.559413349296618e-05,
      "loss": 0.1035,
      "step": 6167
    },
    {
      "epoch": 8.301480484522207,
      "grad_norm": 0.5947055220603943,
      "learning_rate": 1.5564202334630352e-05,
      "loss": 0.1313,
      "step": 6168
    },
    {
      "epoch": 8.302826379542395,
      "grad_norm": 0.4878474771976471,
      "learning_rate": 1.5534271176294524e-05,
      "loss": 0.0967,
      "step": 6169
    },
    {
      "epoch": 8.304172274562584,
      "grad_norm": 0.2772602438926697,
      "learning_rate": 1.5504340017958697e-05,
      "loss": 0.0744,
      "step": 6170
    },
    {
      "epoch": 8.305518169582772,
      "grad_norm": 0.3273827135562897,
      "learning_rate": 1.547440885962287e-05,
      "loss": 0.0764,
      "step": 6171
    },
    {
      "epoch": 8.30686406460296,
      "grad_norm": 0.26318079233169556,
      "learning_rate": 1.5444477701287042e-05,
      "loss": 0.0672,
      "step": 6172
    },
    {
      "epoch": 8.308209959623149,
      "grad_norm": 0.5532728433609009,
      "learning_rate": 1.5414546542951215e-05,
      "loss": 0.1125,
      "step": 6173
    },
    {
      "epoch": 8.309555854643337,
      "grad_norm": 0.431326687335968,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 0.1158,
      "step": 6174
    },
    {
      "epoch": 8.310901749663525,
      "grad_norm": 1.5142269134521484,
      "learning_rate": 1.535468422627956e-05,
      "loss": 0.1095,
      "step": 6175
    },
    {
      "epoch": 8.312247644683715,
      "grad_norm": 0.4288486838340759,
      "learning_rate": 1.5324753067943732e-05,
      "loss": 0.0959,
      "step": 6176
    },
    {
      "epoch": 8.313593539703904,
      "grad_norm": 0.313434362411499,
      "learning_rate": 1.5294821909607905e-05,
      "loss": 0.071,
      "step": 6177
    },
    {
      "epoch": 8.314939434724092,
      "grad_norm": 0.4262843430042267,
      "learning_rate": 1.5264890751272077e-05,
      "loss": 0.0924,
      "step": 6178
    },
    {
      "epoch": 8.31628532974428,
      "grad_norm": 0.36977553367614746,
      "learning_rate": 1.5234959592936248e-05,
      "loss": 0.0792,
      "step": 6179
    },
    {
      "epoch": 8.317631224764469,
      "grad_norm": 1.195637822151184,
      "learning_rate": 1.5205028434600419e-05,
      "loss": 0.1444,
      "step": 6180
    },
    {
      "epoch": 8.318977119784657,
      "grad_norm": 0.2743544578552246,
      "learning_rate": 1.5175097276264592e-05,
      "loss": 0.0735,
      "step": 6181
    },
    {
      "epoch": 8.320323014804845,
      "grad_norm": 0.9562361836433411,
      "learning_rate": 1.5145166117928764e-05,
      "loss": 0.1013,
      "step": 6182
    },
    {
      "epoch": 8.321668909825034,
      "grad_norm": 0.3477862775325775,
      "learning_rate": 1.5115234959592937e-05,
      "loss": 0.0759,
      "step": 6183
    },
    {
      "epoch": 8.323014804845222,
      "grad_norm": 0.4970057010650635,
      "learning_rate": 1.508530380125711e-05,
      "loss": 0.1208,
      "step": 6184
    },
    {
      "epoch": 8.32436069986541,
      "grad_norm": 0.8560307025909424,
      "learning_rate": 1.5055372642921282e-05,
      "loss": 0.1091,
      "step": 6185
    },
    {
      "epoch": 8.325706594885599,
      "grad_norm": 0.9872437119483948,
      "learning_rate": 1.5025441484585454e-05,
      "loss": 0.1198,
      "step": 6186
    },
    {
      "epoch": 8.327052489905787,
      "grad_norm": 0.2971535921096802,
      "learning_rate": 1.4995510326249627e-05,
      "loss": 0.0779,
      "step": 6187
    },
    {
      "epoch": 8.328398384925976,
      "grad_norm": 0.3357890546321869,
      "learning_rate": 1.49655791679138e-05,
      "loss": 0.0656,
      "step": 6188
    },
    {
      "epoch": 8.329744279946164,
      "grad_norm": 1.599735975265503,
      "learning_rate": 1.4935648009577972e-05,
      "loss": 0.1483,
      "step": 6189
    },
    {
      "epoch": 8.331090174966352,
      "grad_norm": 0.6957239508628845,
      "learning_rate": 1.4905716851242144e-05,
      "loss": 0.1479,
      "step": 6190
    },
    {
      "epoch": 8.33243606998654,
      "grad_norm": 0.6139087677001953,
      "learning_rate": 1.4875785692906317e-05,
      "loss": 0.0811,
      "step": 6191
    },
    {
      "epoch": 8.333781965006729,
      "grad_norm": 0.4353117346763611,
      "learning_rate": 1.4845854534570488e-05,
      "loss": 0.0799,
      "step": 6192
    },
    {
      "epoch": 8.335127860026917,
      "grad_norm": 0.5295983552932739,
      "learning_rate": 1.481592337623466e-05,
      "loss": 0.1261,
      "step": 6193
    },
    {
      "epoch": 8.336473755047106,
      "grad_norm": 0.3981146812438965,
      "learning_rate": 1.4785992217898833e-05,
      "loss": 0.091,
      "step": 6194
    },
    {
      "epoch": 8.337819650067296,
      "grad_norm": 0.49571242928504944,
      "learning_rate": 1.4756061059563005e-05,
      "loss": 0.097,
      "step": 6195
    },
    {
      "epoch": 8.339165545087484,
      "grad_norm": 0.4087570607662201,
      "learning_rate": 1.4726129901227178e-05,
      "loss": 0.0988,
      "step": 6196
    },
    {
      "epoch": 8.340511440107672,
      "grad_norm": 0.5824164748191833,
      "learning_rate": 1.469619874289135e-05,
      "loss": 0.125,
      "step": 6197
    },
    {
      "epoch": 8.34185733512786,
      "grad_norm": 0.40127044916152954,
      "learning_rate": 1.4666267584555523e-05,
      "loss": 0.0923,
      "step": 6198
    },
    {
      "epoch": 8.343203230148049,
      "grad_norm": 0.4176931381225586,
      "learning_rate": 1.4636336426219696e-05,
      "loss": 0.0836,
      "step": 6199
    },
    {
      "epoch": 8.344549125168237,
      "grad_norm": 0.780669629573822,
      "learning_rate": 1.4606405267883868e-05,
      "loss": 0.1337,
      "step": 6200
    },
    {
      "epoch": 8.345895020188426,
      "grad_norm": 0.2739112675189972,
      "learning_rate": 1.457647410954804e-05,
      "loss": 0.0806,
      "step": 6201
    },
    {
      "epoch": 8.347240915208614,
      "grad_norm": 0.34362536668777466,
      "learning_rate": 1.4546542951212213e-05,
      "loss": 0.0813,
      "step": 6202
    },
    {
      "epoch": 8.348586810228802,
      "grad_norm": 0.27146777510643005,
      "learning_rate": 1.4516611792876386e-05,
      "loss": 0.072,
      "step": 6203
    },
    {
      "epoch": 8.34993270524899,
      "grad_norm": 0.5997567772865295,
      "learning_rate": 1.4486680634540558e-05,
      "loss": 0.1299,
      "step": 6204
    },
    {
      "epoch": 8.351278600269179,
      "grad_norm": 0.29942670464515686,
      "learning_rate": 1.445674947620473e-05,
      "loss": 0.077,
      "step": 6205
    },
    {
      "epoch": 8.352624495289367,
      "grad_norm": 0.48333287239074707,
      "learning_rate": 1.4426818317868902e-05,
      "loss": 0.0874,
      "step": 6206
    },
    {
      "epoch": 8.353970390309556,
      "grad_norm": 0.355752170085907,
      "learning_rate": 1.4396887159533074e-05,
      "loss": 0.0786,
      "step": 6207
    },
    {
      "epoch": 8.355316285329744,
      "grad_norm": 0.5501332879066467,
      "learning_rate": 1.4366956001197247e-05,
      "loss": 0.1045,
      "step": 6208
    },
    {
      "epoch": 8.356662180349932,
      "grad_norm": 0.389938622713089,
      "learning_rate": 1.433702484286142e-05,
      "loss": 0.0863,
      "step": 6209
    },
    {
      "epoch": 8.35800807537012,
      "grad_norm": 0.40731847286224365,
      "learning_rate": 1.4307093684525592e-05,
      "loss": 0.0746,
      "step": 6210
    },
    {
      "epoch": 8.359353970390309,
      "grad_norm": 0.4870748519897461,
      "learning_rate": 1.4277162526189764e-05,
      "loss": 0.1154,
      "step": 6211
    },
    {
      "epoch": 8.360699865410497,
      "grad_norm": 0.48124974966049194,
      "learning_rate": 1.4247231367853937e-05,
      "loss": 0.0889,
      "step": 6212
    },
    {
      "epoch": 8.362045760430686,
      "grad_norm": 0.6138256788253784,
      "learning_rate": 1.4217300209518108e-05,
      "loss": 0.0702,
      "step": 6213
    },
    {
      "epoch": 8.363391655450874,
      "grad_norm": 0.5007814764976501,
      "learning_rate": 1.418736905118228e-05,
      "loss": 0.1084,
      "step": 6214
    },
    {
      "epoch": 8.364737550471062,
      "grad_norm": 0.24779920279979706,
      "learning_rate": 1.4157437892846453e-05,
      "loss": 0.0889,
      "step": 6215
    },
    {
      "epoch": 8.366083445491252,
      "grad_norm": 0.36820754408836365,
      "learning_rate": 1.4127506734510625e-05,
      "loss": 0.0839,
      "step": 6216
    },
    {
      "epoch": 8.36742934051144,
      "grad_norm": 0.6292504072189331,
      "learning_rate": 1.4097575576174798e-05,
      "loss": 0.0921,
      "step": 6217
    },
    {
      "epoch": 8.36877523553163,
      "grad_norm": 0.3710763156414032,
      "learning_rate": 1.406764441783897e-05,
      "loss": 0.105,
      "step": 6218
    },
    {
      "epoch": 8.370121130551818,
      "grad_norm": 0.3086654245853424,
      "learning_rate": 1.4037713259503143e-05,
      "loss": 0.0711,
      "step": 6219
    },
    {
      "epoch": 8.371467025572006,
      "grad_norm": 0.3097742795944214,
      "learning_rate": 1.4007782101167315e-05,
      "loss": 0.0778,
      "step": 6220
    },
    {
      "epoch": 8.372812920592194,
      "grad_norm": 0.5739796757698059,
      "learning_rate": 1.3977850942831488e-05,
      "loss": 0.1074,
      "step": 6221
    },
    {
      "epoch": 8.374158815612383,
      "grad_norm": 0.2652641832828522,
      "learning_rate": 1.394791978449566e-05,
      "loss": 0.0691,
      "step": 6222
    },
    {
      "epoch": 8.37550471063257,
      "grad_norm": 0.46321365237236023,
      "learning_rate": 1.3917988626159833e-05,
      "loss": 0.0728,
      "step": 6223
    },
    {
      "epoch": 8.37685060565276,
      "grad_norm": 0.4061315357685089,
      "learning_rate": 1.3888057467824006e-05,
      "loss": 0.0879,
      "step": 6224
    },
    {
      "epoch": 8.378196500672948,
      "grad_norm": 0.3036929965019226,
      "learning_rate": 1.3858126309488178e-05,
      "loss": 0.0693,
      "step": 6225
    },
    {
      "epoch": 8.379542395693136,
      "grad_norm": 0.33159080147743225,
      "learning_rate": 1.382819515115235e-05,
      "loss": 0.0832,
      "step": 6226
    },
    {
      "epoch": 8.380888290713324,
      "grad_norm": 0.33379682898521423,
      "learning_rate": 1.3798263992816523e-05,
      "loss": 0.0766,
      "step": 6227
    },
    {
      "epoch": 8.382234185733513,
      "grad_norm": 0.39466923475265503,
      "learning_rate": 1.3768332834480696e-05,
      "loss": 0.0867,
      "step": 6228
    },
    {
      "epoch": 8.3835800807537,
      "grad_norm": 0.40628477931022644,
      "learning_rate": 1.3738401676144868e-05,
      "loss": 0.0841,
      "step": 6229
    },
    {
      "epoch": 8.38492597577389,
      "grad_norm": 0.38723546266555786,
      "learning_rate": 1.370847051780904e-05,
      "loss": 0.0929,
      "step": 6230
    },
    {
      "epoch": 8.386271870794078,
      "grad_norm": 1.1895372867584229,
      "learning_rate": 1.3678539359473213e-05,
      "loss": 0.111,
      "step": 6231
    },
    {
      "epoch": 8.387617765814266,
      "grad_norm": 0.38855230808258057,
      "learning_rate": 1.3648608201137384e-05,
      "loss": 0.0975,
      "step": 6232
    },
    {
      "epoch": 8.388963660834454,
      "grad_norm": 0.34890007972717285,
      "learning_rate": 1.3618677042801557e-05,
      "loss": 0.0725,
      "step": 6233
    },
    {
      "epoch": 8.390309555854643,
      "grad_norm": 0.2734978199005127,
      "learning_rate": 1.358874588446573e-05,
      "loss": 0.0605,
      "step": 6234
    },
    {
      "epoch": 8.391655450874833,
      "grad_norm": 0.5477520227432251,
      "learning_rate": 1.35588147261299e-05,
      "loss": 0.1011,
      "step": 6235
    },
    {
      "epoch": 8.393001345895021,
      "grad_norm": 0.4103712737560272,
      "learning_rate": 1.3528883567794073e-05,
      "loss": 0.0866,
      "step": 6236
    },
    {
      "epoch": 8.39434724091521,
      "grad_norm": 0.38310059905052185,
      "learning_rate": 1.3498952409458245e-05,
      "loss": 0.0862,
      "step": 6237
    },
    {
      "epoch": 8.395693135935398,
      "grad_norm": 0.38872647285461426,
      "learning_rate": 1.3469021251122418e-05,
      "loss": 0.0889,
      "step": 6238
    },
    {
      "epoch": 8.397039030955586,
      "grad_norm": 0.892894983291626,
      "learning_rate": 1.343909009278659e-05,
      "loss": 0.1065,
      "step": 6239
    },
    {
      "epoch": 8.398384925975774,
      "grad_norm": 0.39452600479125977,
      "learning_rate": 1.3409158934450763e-05,
      "loss": 0.0983,
      "step": 6240
    },
    {
      "epoch": 8.399730820995963,
      "grad_norm": 0.36734822392463684,
      "learning_rate": 1.3379227776114935e-05,
      "loss": 0.0907,
      "step": 6241
    },
    {
      "epoch": 8.401076716016151,
      "grad_norm": 0.8047773241996765,
      "learning_rate": 1.3349296617779108e-05,
      "loss": 0.1264,
      "step": 6242
    },
    {
      "epoch": 8.40242261103634,
      "grad_norm": 0.4306955337524414,
      "learning_rate": 1.331936545944328e-05,
      "loss": 0.0791,
      "step": 6243
    },
    {
      "epoch": 8.403768506056528,
      "grad_norm": 0.3038167357444763,
      "learning_rate": 1.3289434301107453e-05,
      "loss": 0.069,
      "step": 6244
    },
    {
      "epoch": 8.405114401076716,
      "grad_norm": 0.3882775902748108,
      "learning_rate": 1.3259503142771625e-05,
      "loss": 0.1086,
      "step": 6245
    },
    {
      "epoch": 8.406460296096904,
      "grad_norm": 0.43212026357650757,
      "learning_rate": 1.3229571984435798e-05,
      "loss": 0.0779,
      "step": 6246
    },
    {
      "epoch": 8.407806191117093,
      "grad_norm": 0.27639076113700867,
      "learning_rate": 1.319964082609997e-05,
      "loss": 0.0714,
      "step": 6247
    },
    {
      "epoch": 8.409152086137281,
      "grad_norm": 0.7948271036148071,
      "learning_rate": 1.3169709667764143e-05,
      "loss": 0.0782,
      "step": 6248
    },
    {
      "epoch": 8.41049798115747,
      "grad_norm": 0.4493277966976166,
      "learning_rate": 1.3139778509428316e-05,
      "loss": 0.1006,
      "step": 6249
    },
    {
      "epoch": 8.411843876177658,
      "grad_norm": 0.4320667088031769,
      "learning_rate": 1.3109847351092488e-05,
      "loss": 0.1153,
      "step": 6250
    },
    {
      "epoch": 8.413189771197846,
      "grad_norm": 0.31325414776802063,
      "learning_rate": 1.307991619275666e-05,
      "loss": 0.0915,
      "step": 6251
    },
    {
      "epoch": 8.414535666218034,
      "grad_norm": 0.2305803894996643,
      "learning_rate": 1.3049985034420833e-05,
      "loss": 0.0661,
      "step": 6252
    },
    {
      "epoch": 8.415881561238223,
      "grad_norm": 0.3026341497898102,
      "learning_rate": 1.3020053876085006e-05,
      "loss": 0.0766,
      "step": 6253
    },
    {
      "epoch": 8.417227456258411,
      "grad_norm": 0.3722366690635681,
      "learning_rate": 1.2990122717749178e-05,
      "loss": 0.0797,
      "step": 6254
    },
    {
      "epoch": 8.4185733512786,
      "grad_norm": 0.4632672667503357,
      "learning_rate": 1.296019155941335e-05,
      "loss": 0.0942,
      "step": 6255
    },
    {
      "epoch": 8.41991924629879,
      "grad_norm": 0.4538939595222473,
      "learning_rate": 1.2930260401077523e-05,
      "loss": 0.0927,
      "step": 6256
    },
    {
      "epoch": 8.421265141318978,
      "grad_norm": 0.5232705473899841,
      "learning_rate": 1.2900329242741696e-05,
      "loss": 0.1164,
      "step": 6257
    },
    {
      "epoch": 8.422611036339166,
      "grad_norm": 0.2941001057624817,
      "learning_rate": 1.2870398084405865e-05,
      "loss": 0.0789,
      "step": 6258
    },
    {
      "epoch": 8.423956931359355,
      "grad_norm": 0.48642590641975403,
      "learning_rate": 1.2840466926070038e-05,
      "loss": 0.1052,
      "step": 6259
    },
    {
      "epoch": 8.425302826379543,
      "grad_norm": 0.3391626477241516,
      "learning_rate": 1.281053576773421e-05,
      "loss": 0.07,
      "step": 6260
    },
    {
      "epoch": 8.426648721399731,
      "grad_norm": 0.764260470867157,
      "learning_rate": 1.2780604609398383e-05,
      "loss": 0.1087,
      "step": 6261
    },
    {
      "epoch": 8.42799461641992,
      "grad_norm": 0.6626319289207458,
      "learning_rate": 1.2750673451062555e-05,
      "loss": 0.1288,
      "step": 6262
    },
    {
      "epoch": 8.429340511440108,
      "grad_norm": 0.4229840636253357,
      "learning_rate": 1.2720742292726728e-05,
      "loss": 0.0928,
      "step": 6263
    },
    {
      "epoch": 8.430686406460296,
      "grad_norm": 0.3671691119670868,
      "learning_rate": 1.26908111343909e-05,
      "loss": 0.0945,
      "step": 6264
    },
    {
      "epoch": 8.432032301480485,
      "grad_norm": 0.4281097650527954,
      "learning_rate": 1.2660879976055073e-05,
      "loss": 0.0993,
      "step": 6265
    },
    {
      "epoch": 8.433378196500673,
      "grad_norm": 0.2883622348308563,
      "learning_rate": 1.2630948817719245e-05,
      "loss": 0.0694,
      "step": 6266
    },
    {
      "epoch": 8.434724091520861,
      "grad_norm": 0.49908652901649475,
      "learning_rate": 1.2601017659383418e-05,
      "loss": 0.0916,
      "step": 6267
    },
    {
      "epoch": 8.43606998654105,
      "grad_norm": 0.5554499626159668,
      "learning_rate": 1.257108650104759e-05,
      "loss": 0.0722,
      "step": 6268
    },
    {
      "epoch": 8.437415881561238,
      "grad_norm": 0.5355867743492126,
      "learning_rate": 1.2541155342711763e-05,
      "loss": 0.0951,
      "step": 6269
    },
    {
      "epoch": 8.438761776581426,
      "grad_norm": 0.34140267968177795,
      "learning_rate": 1.2511224184375935e-05,
      "loss": 0.0917,
      "step": 6270
    },
    {
      "epoch": 8.440107671601615,
      "grad_norm": 0.31117701530456543,
      "learning_rate": 1.2481293026040108e-05,
      "loss": 0.0813,
      "step": 6271
    },
    {
      "epoch": 8.441453566621803,
      "grad_norm": 0.4199126958847046,
      "learning_rate": 1.245136186770428e-05,
      "loss": 0.0874,
      "step": 6272
    },
    {
      "epoch": 8.442799461641991,
      "grad_norm": 0.33292195200920105,
      "learning_rate": 1.2421430709368453e-05,
      "loss": 0.0765,
      "step": 6273
    },
    {
      "epoch": 8.44414535666218,
      "grad_norm": 0.5324190855026245,
      "learning_rate": 1.2391499551032626e-05,
      "loss": 0.1014,
      "step": 6274
    },
    {
      "epoch": 8.445491251682368,
      "grad_norm": 0.37215372920036316,
      "learning_rate": 1.2361568392696798e-05,
      "loss": 0.0885,
      "step": 6275
    },
    {
      "epoch": 8.446837146702558,
      "grad_norm": 0.41593605279922485,
      "learning_rate": 1.233163723436097e-05,
      "loss": 0.0862,
      "step": 6276
    },
    {
      "epoch": 8.448183041722746,
      "grad_norm": 0.3456641137599945,
      "learning_rate": 1.2301706076025142e-05,
      "loss": 0.0777,
      "step": 6277
    },
    {
      "epoch": 8.449528936742935,
      "grad_norm": 0.5316247940063477,
      "learning_rate": 1.2271774917689314e-05,
      "loss": 0.0898,
      "step": 6278
    },
    {
      "epoch": 8.450874831763123,
      "grad_norm": 0.26091185212135315,
      "learning_rate": 1.2241843759353487e-05,
      "loss": 0.0731,
      "step": 6279
    },
    {
      "epoch": 8.452220726783311,
      "grad_norm": 0.44237005710601807,
      "learning_rate": 1.2211912601017659e-05,
      "loss": 0.0972,
      "step": 6280
    },
    {
      "epoch": 8.4535666218035,
      "grad_norm": 0.27895429730415344,
      "learning_rate": 1.2181981442681832e-05,
      "loss": 0.0638,
      "step": 6281
    },
    {
      "epoch": 8.454912516823688,
      "grad_norm": 0.4182664453983307,
      "learning_rate": 1.2152050284346004e-05,
      "loss": 0.1042,
      "step": 6282
    },
    {
      "epoch": 8.456258411843876,
      "grad_norm": 0.45660391449928284,
      "learning_rate": 1.2122119126010177e-05,
      "loss": 0.0709,
      "step": 6283
    },
    {
      "epoch": 8.457604306864065,
      "grad_norm": 0.3877553939819336,
      "learning_rate": 1.209218796767435e-05,
      "loss": 0.0812,
      "step": 6284
    },
    {
      "epoch": 8.458950201884253,
      "grad_norm": 0.688754677772522,
      "learning_rate": 1.2062256809338522e-05,
      "loss": 0.1261,
      "step": 6285
    },
    {
      "epoch": 8.460296096904441,
      "grad_norm": 0.4403834640979767,
      "learning_rate": 1.2032325651002694e-05,
      "loss": 0.1218,
      "step": 6286
    },
    {
      "epoch": 8.46164199192463,
      "grad_norm": 0.3866446018218994,
      "learning_rate": 1.2002394492666867e-05,
      "loss": 0.1024,
      "step": 6287
    },
    {
      "epoch": 8.462987886944818,
      "grad_norm": 0.6456059217453003,
      "learning_rate": 1.197246333433104e-05,
      "loss": 0.1147,
      "step": 6288
    },
    {
      "epoch": 8.464333781965006,
      "grad_norm": 0.3895986080169678,
      "learning_rate": 1.1942532175995212e-05,
      "loss": 0.0906,
      "step": 6289
    },
    {
      "epoch": 8.465679676985195,
      "grad_norm": 0.3181576430797577,
      "learning_rate": 1.1912601017659383e-05,
      "loss": 0.0782,
      "step": 6290
    },
    {
      "epoch": 8.467025572005383,
      "grad_norm": 0.35778379440307617,
      "learning_rate": 1.1882669859323555e-05,
      "loss": 0.0842,
      "step": 6291
    },
    {
      "epoch": 8.468371467025571,
      "grad_norm": 0.8562071919441223,
      "learning_rate": 1.1852738700987728e-05,
      "loss": 0.1289,
      "step": 6292
    },
    {
      "epoch": 8.46971736204576,
      "grad_norm": 0.2770618498325348,
      "learning_rate": 1.18228075426519e-05,
      "loss": 0.0633,
      "step": 6293
    },
    {
      "epoch": 8.471063257065948,
      "grad_norm": 0.5143229365348816,
      "learning_rate": 1.1792876384316073e-05,
      "loss": 0.1003,
      "step": 6294
    },
    {
      "epoch": 8.472409152086136,
      "grad_norm": 0.4847199022769928,
      "learning_rate": 1.1762945225980246e-05,
      "loss": 0.0703,
      "step": 6295
    },
    {
      "epoch": 8.473755047106327,
      "grad_norm": 0.4884936809539795,
      "learning_rate": 1.1733014067644418e-05,
      "loss": 0.0885,
      "step": 6296
    },
    {
      "epoch": 8.475100942126515,
      "grad_norm": 0.47839927673339844,
      "learning_rate": 1.170308290930859e-05,
      "loss": 0.0887,
      "step": 6297
    },
    {
      "epoch": 8.476446837146703,
      "grad_norm": 0.6364726424217224,
      "learning_rate": 1.1673151750972763e-05,
      "loss": 0.1145,
      "step": 6298
    },
    {
      "epoch": 8.477792732166892,
      "grad_norm": 0.5073177218437195,
      "learning_rate": 1.1643220592636936e-05,
      "loss": 0.1102,
      "step": 6299
    },
    {
      "epoch": 8.47913862718708,
      "grad_norm": 0.6350866556167603,
      "learning_rate": 1.1613289434301108e-05,
      "loss": 0.1196,
      "step": 6300
    },
    {
      "epoch": 8.480484522207268,
      "grad_norm": 0.3551376461982727,
      "learning_rate": 1.158335827596528e-05,
      "loss": 0.0855,
      "step": 6301
    },
    {
      "epoch": 8.481830417227457,
      "grad_norm": 0.42900189757347107,
      "learning_rate": 1.1553427117629453e-05,
      "loss": 0.0977,
      "step": 6302
    },
    {
      "epoch": 8.483176312247645,
      "grad_norm": 0.444644570350647,
      "learning_rate": 1.1523495959293626e-05,
      "loss": 0.0911,
      "step": 6303
    },
    {
      "epoch": 8.484522207267833,
      "grad_norm": 0.5901516675949097,
      "learning_rate": 1.1493564800957797e-05,
      "loss": 0.1185,
      "step": 6304
    },
    {
      "epoch": 8.485868102288022,
      "grad_norm": 0.42588648200035095,
      "learning_rate": 1.146363364262197e-05,
      "loss": 0.099,
      "step": 6305
    },
    {
      "epoch": 8.48721399730821,
      "grad_norm": 0.40559494495391846,
      "learning_rate": 1.1433702484286142e-05,
      "loss": 0.0884,
      "step": 6306
    },
    {
      "epoch": 8.488559892328398,
      "grad_norm": 0.4299238324165344,
      "learning_rate": 1.1403771325950314e-05,
      "loss": 0.0834,
      "step": 6307
    },
    {
      "epoch": 8.489905787348587,
      "grad_norm": 0.4589082598686218,
      "learning_rate": 1.1373840167614487e-05,
      "loss": 0.1046,
      "step": 6308
    },
    {
      "epoch": 8.491251682368775,
      "grad_norm": 0.4099102318286896,
      "learning_rate": 1.134390900927866e-05,
      "loss": 0.0819,
      "step": 6309
    },
    {
      "epoch": 8.492597577388963,
      "grad_norm": 0.35550710558891296,
      "learning_rate": 1.1313977850942832e-05,
      "loss": 0.0742,
      "step": 6310
    },
    {
      "epoch": 8.493943472409152,
      "grad_norm": 0.2939397394657135,
      "learning_rate": 1.1284046692607004e-05,
      "loss": 0.0743,
      "step": 6311
    },
    {
      "epoch": 8.49528936742934,
      "grad_norm": 0.5169088244438171,
      "learning_rate": 1.1254115534271177e-05,
      "loss": 0.101,
      "step": 6312
    },
    {
      "epoch": 8.496635262449528,
      "grad_norm": 0.3105330169200897,
      "learning_rate": 1.122418437593535e-05,
      "loss": 0.0864,
      "step": 6313
    },
    {
      "epoch": 8.497981157469717,
      "grad_norm": 0.6132309436798096,
      "learning_rate": 1.1194253217599522e-05,
      "loss": 0.1017,
      "step": 6314
    },
    {
      "epoch": 8.499327052489905,
      "grad_norm": 0.8813265562057495,
      "learning_rate": 1.1164322059263695e-05,
      "loss": 0.0953,
      "step": 6315
    },
    {
      "epoch": 8.500672947510093,
      "grad_norm": 0.46048715710639954,
      "learning_rate": 1.1134390900927867e-05,
      "loss": 0.0791,
      "step": 6316
    },
    {
      "epoch": 8.502018842530283,
      "grad_norm": 0.34321272373199463,
      "learning_rate": 1.1104459742592038e-05,
      "loss": 0.0776,
      "step": 6317
    },
    {
      "epoch": 8.503364737550472,
      "grad_norm": 0.37244150042533875,
      "learning_rate": 1.107452858425621e-05,
      "loss": 0.0915,
      "step": 6318
    },
    {
      "epoch": 8.50471063257066,
      "grad_norm": 0.5063414573669434,
      "learning_rate": 1.1044597425920383e-05,
      "loss": 0.104,
      "step": 6319
    },
    {
      "epoch": 8.506056527590848,
      "grad_norm": 0.46370187401771545,
      "learning_rate": 1.1014666267584556e-05,
      "loss": 0.1255,
      "step": 6320
    },
    {
      "epoch": 8.507402422611037,
      "grad_norm": 0.4315643906593323,
      "learning_rate": 1.0984735109248728e-05,
      "loss": 0.0949,
      "step": 6321
    },
    {
      "epoch": 8.508748317631225,
      "grad_norm": 0.6787129640579224,
      "learning_rate": 1.09548039509129e-05,
      "loss": 0.1122,
      "step": 6322
    },
    {
      "epoch": 8.510094212651413,
      "grad_norm": 0.34786033630371094,
      "learning_rate": 1.0924872792577073e-05,
      "loss": 0.0882,
      "step": 6323
    },
    {
      "epoch": 8.511440107671602,
      "grad_norm": 0.3444077670574188,
      "learning_rate": 1.0894941634241246e-05,
      "loss": 0.0899,
      "step": 6324
    },
    {
      "epoch": 8.51278600269179,
      "grad_norm": 0.2684077322483063,
      "learning_rate": 1.0865010475905418e-05,
      "loss": 0.0657,
      "step": 6325
    },
    {
      "epoch": 8.514131897711978,
      "grad_norm": 0.39504876732826233,
      "learning_rate": 1.083507931756959e-05,
      "loss": 0.0836,
      "step": 6326
    },
    {
      "epoch": 8.515477792732167,
      "grad_norm": 0.48944535851478577,
      "learning_rate": 1.0805148159233763e-05,
      "loss": 0.0866,
      "step": 6327
    },
    {
      "epoch": 8.516823687752355,
      "grad_norm": 0.5265402793884277,
      "learning_rate": 1.0775217000897936e-05,
      "loss": 0.0816,
      "step": 6328
    },
    {
      "epoch": 8.518169582772543,
      "grad_norm": 0.36310726404190063,
      "learning_rate": 1.0745285842562108e-05,
      "loss": 0.0831,
      "step": 6329
    },
    {
      "epoch": 8.519515477792732,
      "grad_norm": 0.6633349061012268,
      "learning_rate": 1.071535468422628e-05,
      "loss": 0.0962,
      "step": 6330
    },
    {
      "epoch": 8.52086137281292,
      "grad_norm": 0.3698948919773102,
      "learning_rate": 1.0685423525890452e-05,
      "loss": 0.0864,
      "step": 6331
    },
    {
      "epoch": 8.522207267833108,
      "grad_norm": 0.6601632237434387,
      "learning_rate": 1.0655492367554624e-05,
      "loss": 0.0963,
      "step": 6332
    },
    {
      "epoch": 8.523553162853297,
      "grad_norm": 0.3258787989616394,
      "learning_rate": 1.0625561209218797e-05,
      "loss": 0.0797,
      "step": 6333
    },
    {
      "epoch": 8.524899057873485,
      "grad_norm": 0.3792743384838104,
      "learning_rate": 1.059563005088297e-05,
      "loss": 0.0819,
      "step": 6334
    },
    {
      "epoch": 8.526244952893673,
      "grad_norm": 0.3698830306529999,
      "learning_rate": 1.0565698892547142e-05,
      "loss": 0.079,
      "step": 6335
    },
    {
      "epoch": 8.527590847913864,
      "grad_norm": 0.5043533444404602,
      "learning_rate": 1.0535767734211314e-05,
      "loss": 0.1011,
      "step": 6336
    },
    {
      "epoch": 8.528936742934052,
      "grad_norm": 0.4604741036891937,
      "learning_rate": 1.0505836575875487e-05,
      "loss": 0.1015,
      "step": 6337
    },
    {
      "epoch": 8.53028263795424,
      "grad_norm": 0.35752224922180176,
      "learning_rate": 1.047590541753966e-05,
      "loss": 0.0921,
      "step": 6338
    },
    {
      "epoch": 8.531628532974429,
      "grad_norm": 0.42436447739601135,
      "learning_rate": 1.0445974259203832e-05,
      "loss": 0.0889,
      "step": 6339
    },
    {
      "epoch": 8.532974427994617,
      "grad_norm": 0.6831639409065247,
      "learning_rate": 1.0416043100868005e-05,
      "loss": 0.1056,
      "step": 6340
    },
    {
      "epoch": 8.534320323014805,
      "grad_norm": 0.3281223475933075,
      "learning_rate": 1.0386111942532177e-05,
      "loss": 0.0757,
      "step": 6341
    },
    {
      "epoch": 8.535666218034994,
      "grad_norm": 0.4723784923553467,
      "learning_rate": 1.035618078419635e-05,
      "loss": 0.1003,
      "step": 6342
    },
    {
      "epoch": 8.537012113055182,
      "grad_norm": 0.41325756907463074,
      "learning_rate": 1.032624962586052e-05,
      "loss": 0.0907,
      "step": 6343
    },
    {
      "epoch": 8.53835800807537,
      "grad_norm": 0.47556811571121216,
      "learning_rate": 1.0296318467524693e-05,
      "loss": 0.1012,
      "step": 6344
    },
    {
      "epoch": 8.539703903095559,
      "grad_norm": 0.408627450466156,
      "learning_rate": 1.0266387309188866e-05,
      "loss": 0.0988,
      "step": 6345
    },
    {
      "epoch": 8.541049798115747,
      "grad_norm": 0.4876326024532318,
      "learning_rate": 1.0236456150853038e-05,
      "loss": 0.1166,
      "step": 6346
    },
    {
      "epoch": 8.542395693135935,
      "grad_norm": 0.3479105830192566,
      "learning_rate": 1.020652499251721e-05,
      "loss": 0.0851,
      "step": 6347
    },
    {
      "epoch": 8.543741588156124,
      "grad_norm": 0.43361395597457886,
      "learning_rate": 1.0176593834181383e-05,
      "loss": 0.0908,
      "step": 6348
    },
    {
      "epoch": 8.545087483176312,
      "grad_norm": 0.24417592585086823,
      "learning_rate": 1.0146662675845556e-05,
      "loss": 0.0662,
      "step": 6349
    },
    {
      "epoch": 8.5464333781965,
      "grad_norm": 0.4478660225868225,
      "learning_rate": 1.0116731517509728e-05,
      "loss": 0.0836,
      "step": 6350
    },
    {
      "epoch": 8.547779273216689,
      "grad_norm": 0.46366438269615173,
      "learning_rate": 1.00868003591739e-05,
      "loss": 0.0844,
      "step": 6351
    },
    {
      "epoch": 8.549125168236877,
      "grad_norm": 0.517288863658905,
      "learning_rate": 1.0056869200838073e-05,
      "loss": 0.0907,
      "step": 6352
    },
    {
      "epoch": 8.550471063257065,
      "grad_norm": 0.5409461855888367,
      "learning_rate": 1.0026938042502246e-05,
      "loss": 0.1297,
      "step": 6353
    },
    {
      "epoch": 8.551816958277254,
      "grad_norm": 0.41757869720458984,
      "learning_rate": 9.997006884166418e-06,
      "loss": 0.0857,
      "step": 6354
    },
    {
      "epoch": 8.553162853297442,
      "grad_norm": 0.45432648062705994,
      "learning_rate": 9.967075725830591e-06,
      "loss": 0.1032,
      "step": 6355
    },
    {
      "epoch": 8.55450874831763,
      "grad_norm": 0.35214903950691223,
      "learning_rate": 9.937144567494762e-06,
      "loss": 0.0735,
      "step": 6356
    },
    {
      "epoch": 8.55585464333782,
      "grad_norm": 0.3201400339603424,
      "learning_rate": 9.907213409158934e-06,
      "loss": 0.0712,
      "step": 6357
    },
    {
      "epoch": 8.557200538358009,
      "grad_norm": 0.4356290102005005,
      "learning_rate": 9.877282250823107e-06,
      "loss": 0.0938,
      "step": 6358
    },
    {
      "epoch": 8.558546433378197,
      "grad_norm": 1.2813109159469604,
      "learning_rate": 9.84735109248728e-06,
      "loss": 0.1533,
      "step": 6359
    },
    {
      "epoch": 8.559892328398385,
      "grad_norm": 0.4469726085662842,
      "learning_rate": 9.817419934151452e-06,
      "loss": 0.1141,
      "step": 6360
    },
    {
      "epoch": 8.561238223418574,
      "grad_norm": 0.400776743888855,
      "learning_rate": 9.787488775815624e-06,
      "loss": 0.1015,
      "step": 6361
    },
    {
      "epoch": 8.562584118438762,
      "grad_norm": 0.3237130343914032,
      "learning_rate": 9.757557617479797e-06,
      "loss": 0.0754,
      "step": 6362
    },
    {
      "epoch": 8.56393001345895,
      "grad_norm": 0.22717763483524323,
      "learning_rate": 9.72762645914397e-06,
      "loss": 0.0467,
      "step": 6363
    },
    {
      "epoch": 8.565275908479139,
      "grad_norm": 0.4842298924922943,
      "learning_rate": 9.697695300808142e-06,
      "loss": 0.102,
      "step": 6364
    },
    {
      "epoch": 8.566621803499327,
      "grad_norm": 0.5392224788665771,
      "learning_rate": 9.667764142472315e-06,
      "loss": 0.1137,
      "step": 6365
    },
    {
      "epoch": 8.567967698519515,
      "grad_norm": 0.44907423853874207,
      "learning_rate": 9.637832984136487e-06,
      "loss": 0.1133,
      "step": 6366
    },
    {
      "epoch": 8.569313593539704,
      "grad_norm": 0.30584001541137695,
      "learning_rate": 9.60790182580066e-06,
      "loss": 0.0753,
      "step": 6367
    },
    {
      "epoch": 8.570659488559892,
      "grad_norm": 0.6075109243392944,
      "learning_rate": 9.577970667464832e-06,
      "loss": 0.1057,
      "step": 6368
    },
    {
      "epoch": 8.57200538358008,
      "grad_norm": 0.266597181558609,
      "learning_rate": 9.548039509129003e-06,
      "loss": 0.0828,
      "step": 6369
    },
    {
      "epoch": 8.573351278600269,
      "grad_norm": 0.6002157926559448,
      "learning_rate": 9.518108350793176e-06,
      "loss": 0.1364,
      "step": 6370
    },
    {
      "epoch": 8.574697173620457,
      "grad_norm": 0.4776301980018616,
      "learning_rate": 9.488177192457348e-06,
      "loss": 0.1026,
      "step": 6371
    },
    {
      "epoch": 8.576043068640645,
      "grad_norm": 0.44720837473869324,
      "learning_rate": 9.45824603412152e-06,
      "loss": 0.1001,
      "step": 6372
    },
    {
      "epoch": 8.577388963660834,
      "grad_norm": 0.5208004713058472,
      "learning_rate": 9.428314875785693e-06,
      "loss": 0.0855,
      "step": 6373
    },
    {
      "epoch": 8.578734858681022,
      "grad_norm": 0.3361027240753174,
      "learning_rate": 9.398383717449866e-06,
      "loss": 0.0855,
      "step": 6374
    },
    {
      "epoch": 8.58008075370121,
      "grad_norm": 0.2647113502025604,
      "learning_rate": 9.368452559114038e-06,
      "loss": 0.0702,
      "step": 6375
    },
    {
      "epoch": 8.5814266487214,
      "grad_norm": 0.3095676004886627,
      "learning_rate": 9.33852140077821e-06,
      "loss": 0.0804,
      "step": 6376
    },
    {
      "epoch": 8.582772543741589,
      "grad_norm": 0.44229286909103394,
      "learning_rate": 9.308590242442383e-06,
      "loss": 0.0962,
      "step": 6377
    },
    {
      "epoch": 8.584118438761777,
      "grad_norm": 0.46926528215408325,
      "learning_rate": 9.278659084106556e-06,
      "loss": 0.0836,
      "step": 6378
    },
    {
      "epoch": 8.585464333781966,
      "grad_norm": 0.43091487884521484,
      "learning_rate": 9.248727925770728e-06,
      "loss": 0.0945,
      "step": 6379
    },
    {
      "epoch": 8.586810228802154,
      "grad_norm": 0.5136915445327759,
      "learning_rate": 9.218796767434901e-06,
      "loss": 0.0999,
      "step": 6380
    },
    {
      "epoch": 8.588156123822342,
      "grad_norm": 0.4291585683822632,
      "learning_rate": 9.188865609099074e-06,
      "loss": 0.0957,
      "step": 6381
    },
    {
      "epoch": 8.58950201884253,
      "grad_norm": 0.5865280032157898,
      "learning_rate": 9.158934450763244e-06,
      "loss": 0.0855,
      "step": 6382
    },
    {
      "epoch": 8.590847913862719,
      "grad_norm": 0.23759223520755768,
      "learning_rate": 9.129003292427417e-06,
      "loss": 0.0799,
      "step": 6383
    },
    {
      "epoch": 8.592193808882907,
      "grad_norm": 0.2241140902042389,
      "learning_rate": 9.09907213409159e-06,
      "loss": 0.0668,
      "step": 6384
    },
    {
      "epoch": 8.593539703903096,
      "grad_norm": 0.38517823815345764,
      "learning_rate": 9.069140975755762e-06,
      "loss": 0.0849,
      "step": 6385
    },
    {
      "epoch": 8.594885598923284,
      "grad_norm": 0.55089271068573,
      "learning_rate": 9.039209817419935e-06,
      "loss": 0.1019,
      "step": 6386
    },
    {
      "epoch": 8.596231493943472,
      "grad_norm": 0.6697455644607544,
      "learning_rate": 9.009278659084107e-06,
      "loss": 0.113,
      "step": 6387
    },
    {
      "epoch": 8.59757738896366,
      "grad_norm": 0.445575475692749,
      "learning_rate": 8.97934750074828e-06,
      "loss": 0.095,
      "step": 6388
    },
    {
      "epoch": 8.598923283983849,
      "grad_norm": 0.39882686734199524,
      "learning_rate": 8.949416342412452e-06,
      "loss": 0.085,
      "step": 6389
    },
    {
      "epoch": 8.600269179004037,
      "grad_norm": 0.24364133179187775,
      "learning_rate": 8.919485184076625e-06,
      "loss": 0.0701,
      "step": 6390
    },
    {
      "epoch": 8.601615074024226,
      "grad_norm": 0.5875223875045776,
      "learning_rate": 8.889554025740797e-06,
      "loss": 0.1215,
      "step": 6391
    },
    {
      "epoch": 8.602960969044414,
      "grad_norm": 0.5071538686752319,
      "learning_rate": 8.85962286740497e-06,
      "loss": 0.0859,
      "step": 6392
    },
    {
      "epoch": 8.604306864064602,
      "grad_norm": 0.3164433538913727,
      "learning_rate": 8.829691709069142e-06,
      "loss": 0.0789,
      "step": 6393
    },
    {
      "epoch": 8.60565275908479,
      "grad_norm": 0.4312472939491272,
      "learning_rate": 8.799760550733315e-06,
      "loss": 0.0812,
      "step": 6394
    },
    {
      "epoch": 8.606998654104979,
      "grad_norm": 0.40825900435447693,
      "learning_rate": 8.769829392397486e-06,
      "loss": 0.0738,
      "step": 6395
    },
    {
      "epoch": 8.608344549125167,
      "grad_norm": 0.39970776438713074,
      "learning_rate": 8.739898234061658e-06,
      "loss": 0.0937,
      "step": 6396
    },
    {
      "epoch": 8.609690444145357,
      "grad_norm": 0.4994615912437439,
      "learning_rate": 8.70996707572583e-06,
      "loss": 0.1075,
      "step": 6397
    },
    {
      "epoch": 8.611036339165546,
      "grad_norm": 0.41941651701927185,
      "learning_rate": 8.680035917390003e-06,
      "loss": 0.1109,
      "step": 6398
    },
    {
      "epoch": 8.612382234185734,
      "grad_norm": 0.47192686796188354,
      "learning_rate": 8.650104759054176e-06,
      "loss": 0.1111,
      "step": 6399
    },
    {
      "epoch": 8.613728129205922,
      "grad_norm": 0.6146968007087708,
      "learning_rate": 8.620173600718348e-06,
      "loss": 0.1689,
      "step": 6400
    },
    {
      "epoch": 8.61507402422611,
      "grad_norm": 0.5652061104774475,
      "learning_rate": 8.590242442382521e-06,
      "loss": 0.1,
      "step": 6401
    },
    {
      "epoch": 8.6164199192463,
      "grad_norm": 0.5332669615745544,
      "learning_rate": 8.560311284046693e-06,
      "loss": 0.1086,
      "step": 6402
    },
    {
      "epoch": 8.617765814266487,
      "grad_norm": 0.3595525026321411,
      "learning_rate": 8.530380125710866e-06,
      "loss": 0.0838,
      "step": 6403
    },
    {
      "epoch": 8.619111709286676,
      "grad_norm": 0.5660833120346069,
      "learning_rate": 8.500448967375038e-06,
      "loss": 0.1031,
      "step": 6404
    },
    {
      "epoch": 8.620457604306864,
      "grad_norm": 0.37250396609306335,
      "learning_rate": 8.470517809039211e-06,
      "loss": 0.08,
      "step": 6405
    },
    {
      "epoch": 8.621803499327052,
      "grad_norm": 0.4452700614929199,
      "learning_rate": 8.440586650703384e-06,
      "loss": 0.0945,
      "step": 6406
    },
    {
      "epoch": 8.62314939434724,
      "grad_norm": 0.36879628896713257,
      "learning_rate": 8.410655492367556e-06,
      "loss": 0.0941,
      "step": 6407
    },
    {
      "epoch": 8.62449528936743,
      "grad_norm": 0.2926687002182007,
      "learning_rate": 8.380724334031727e-06,
      "loss": 0.0621,
      "step": 6408
    },
    {
      "epoch": 8.625841184387617,
      "grad_norm": 0.30461981892585754,
      "learning_rate": 8.3507931756959e-06,
      "loss": 0.0624,
      "step": 6409
    },
    {
      "epoch": 8.627187079407806,
      "grad_norm": 0.5657718777656555,
      "learning_rate": 8.320862017360072e-06,
      "loss": 0.1264,
      "step": 6410
    },
    {
      "epoch": 8.628532974427994,
      "grad_norm": 0.6150395274162292,
      "learning_rate": 8.290930859024245e-06,
      "loss": 0.099,
      "step": 6411
    },
    {
      "epoch": 8.629878869448182,
      "grad_norm": 0.6306320428848267,
      "learning_rate": 8.260999700688417e-06,
      "loss": 0.1035,
      "step": 6412
    },
    {
      "epoch": 8.63122476446837,
      "grad_norm": 0.38580626249313354,
      "learning_rate": 8.23106854235259e-06,
      "loss": 0.0973,
      "step": 6413
    },
    {
      "epoch": 8.63257065948856,
      "grad_norm": 0.36375102400779724,
      "learning_rate": 8.201137384016762e-06,
      "loss": 0.1016,
      "step": 6414
    },
    {
      "epoch": 8.633916554508748,
      "grad_norm": 1.0540657043457031,
      "learning_rate": 8.171206225680935e-06,
      "loss": 0.1502,
      "step": 6415
    },
    {
      "epoch": 8.635262449528938,
      "grad_norm": 0.7749653458595276,
      "learning_rate": 8.141275067345107e-06,
      "loss": 0.0909,
      "step": 6416
    },
    {
      "epoch": 8.636608344549126,
      "grad_norm": 0.4278586506843567,
      "learning_rate": 8.11134390900928e-06,
      "loss": 0.0718,
      "step": 6417
    },
    {
      "epoch": 8.637954239569314,
      "grad_norm": 0.5468183755874634,
      "learning_rate": 8.081412750673452e-06,
      "loss": 0.0951,
      "step": 6418
    },
    {
      "epoch": 8.639300134589503,
      "grad_norm": 0.4259417653083801,
      "learning_rate": 8.051481592337625e-06,
      "loss": 0.0966,
      "step": 6419
    },
    {
      "epoch": 8.640646029609691,
      "grad_norm": 0.40362879633903503,
      "learning_rate": 8.021550434001797e-06,
      "loss": 0.1004,
      "step": 6420
    },
    {
      "epoch": 8.64199192462988,
      "grad_norm": 0.39006105065345764,
      "learning_rate": 7.991619275665968e-06,
      "loss": 0.1143,
      "step": 6421
    },
    {
      "epoch": 8.643337819650068,
      "grad_norm": 0.2991968095302582,
      "learning_rate": 7.96168811733014e-06,
      "loss": 0.0715,
      "step": 6422
    },
    {
      "epoch": 8.644683714670256,
      "grad_norm": 0.33895108103752136,
      "learning_rate": 7.931756958994313e-06,
      "loss": 0.0881,
      "step": 6423
    },
    {
      "epoch": 8.646029609690444,
      "grad_norm": 0.41349780559539795,
      "learning_rate": 7.901825800658486e-06,
      "loss": 0.073,
      "step": 6424
    },
    {
      "epoch": 8.647375504710633,
      "grad_norm": 0.34149807691574097,
      "learning_rate": 7.871894642322658e-06,
      "loss": 0.0729,
      "step": 6425
    },
    {
      "epoch": 8.648721399730821,
      "grad_norm": 0.31522098183631897,
      "learning_rate": 7.841963483986831e-06,
      "loss": 0.0792,
      "step": 6426
    },
    {
      "epoch": 8.65006729475101,
      "grad_norm": 0.2573494613170624,
      "learning_rate": 7.812032325651003e-06,
      "loss": 0.0665,
      "step": 6427
    },
    {
      "epoch": 8.651413189771198,
      "grad_norm": 0.3014199733734131,
      "learning_rate": 7.782101167315176e-06,
      "loss": 0.067,
      "step": 6428
    },
    {
      "epoch": 8.652759084791386,
      "grad_norm": 0.4177635908126831,
      "learning_rate": 7.752170008979349e-06,
      "loss": 0.095,
      "step": 6429
    },
    {
      "epoch": 8.654104979811574,
      "grad_norm": 0.4759652614593506,
      "learning_rate": 7.722238850643521e-06,
      "loss": 0.1194,
      "step": 6430
    },
    {
      "epoch": 8.655450874831763,
      "grad_norm": 0.3430963456630707,
      "learning_rate": 7.692307692307694e-06,
      "loss": 0.0721,
      "step": 6431
    },
    {
      "epoch": 8.656796769851951,
      "grad_norm": 0.38170692324638367,
      "learning_rate": 7.662376533971866e-06,
      "loss": 0.0829,
      "step": 6432
    },
    {
      "epoch": 8.65814266487214,
      "grad_norm": 0.4194941520690918,
      "learning_rate": 7.632445375636039e-06,
      "loss": 0.1141,
      "step": 6433
    },
    {
      "epoch": 8.659488559892328,
      "grad_norm": 0.47498923540115356,
      "learning_rate": 7.6025142173002095e-06,
      "loss": 0.1056,
      "step": 6434
    },
    {
      "epoch": 8.660834454912516,
      "grad_norm": 0.43055275082588196,
      "learning_rate": 7.572583058964382e-06,
      "loss": 0.097,
      "step": 6435
    },
    {
      "epoch": 8.662180349932704,
      "grad_norm": 0.3597257137298584,
      "learning_rate": 7.542651900628555e-06,
      "loss": 0.0919,
      "step": 6436
    },
    {
      "epoch": 8.663526244952894,
      "grad_norm": 0.546232283115387,
      "learning_rate": 7.512720742292727e-06,
      "loss": 0.1172,
      "step": 6437
    },
    {
      "epoch": 8.664872139973083,
      "grad_norm": 0.2931325435638428,
      "learning_rate": 7.4827895839569e-06,
      "loss": 0.0811,
      "step": 6438
    },
    {
      "epoch": 8.666218034993271,
      "grad_norm": 0.49329817295074463,
      "learning_rate": 7.452858425621072e-06,
      "loss": 0.0952,
      "step": 6439
    },
    {
      "epoch": 8.66756393001346,
      "grad_norm": 0.4377882778644562,
      "learning_rate": 7.422927267285244e-06,
      "loss": 0.1291,
      "step": 6440
    },
    {
      "epoch": 8.668909825033648,
      "grad_norm": 0.34125760197639465,
      "learning_rate": 7.392996108949416e-06,
      "loss": 0.0806,
      "step": 6441
    },
    {
      "epoch": 8.670255720053836,
      "grad_norm": 0.4484395980834961,
      "learning_rate": 7.363064950613589e-06,
      "loss": 0.1046,
      "step": 6442
    },
    {
      "epoch": 8.671601615074024,
      "grad_norm": 0.3495035469532013,
      "learning_rate": 7.3331337922777615e-06,
      "loss": 0.091,
      "step": 6443
    },
    {
      "epoch": 8.672947510094213,
      "grad_norm": 0.3003675043582916,
      "learning_rate": 7.303202633941934e-06,
      "loss": 0.0608,
      "step": 6444
    },
    {
      "epoch": 8.674293405114401,
      "grad_norm": 0.3099084198474884,
      "learning_rate": 7.2732714756061066e-06,
      "loss": 0.0766,
      "step": 6445
    },
    {
      "epoch": 8.67563930013459,
      "grad_norm": 0.35387474298477173,
      "learning_rate": 7.243340317270279e-06,
      "loss": 0.0985,
      "step": 6446
    },
    {
      "epoch": 8.676985195154778,
      "grad_norm": 0.45892366766929626,
      "learning_rate": 7.213409158934451e-06,
      "loss": 0.0882,
      "step": 6447
    },
    {
      "epoch": 8.678331090174966,
      "grad_norm": 0.5878914594650269,
      "learning_rate": 7.183478000598623e-06,
      "loss": 0.1068,
      "step": 6448
    },
    {
      "epoch": 8.679676985195155,
      "grad_norm": 0.3209814429283142,
      "learning_rate": 7.153546842262796e-06,
      "loss": 0.0987,
      "step": 6449
    },
    {
      "epoch": 8.681022880215343,
      "grad_norm": 0.46176066994667053,
      "learning_rate": 7.123615683926968e-06,
      "loss": 0.0918,
      "step": 6450
    },
    {
      "epoch": 8.682368775235531,
      "grad_norm": 0.4544002413749695,
      "learning_rate": 7.09368452559114e-06,
      "loss": 0.1028,
      "step": 6451
    },
    {
      "epoch": 8.68371467025572,
      "grad_norm": 0.3134952783584595,
      "learning_rate": 7.063753367255313e-06,
      "loss": 0.0781,
      "step": 6452
    },
    {
      "epoch": 8.685060565275908,
      "grad_norm": 0.7483294606208801,
      "learning_rate": 7.033822208919485e-06,
      "loss": 0.1624,
      "step": 6453
    },
    {
      "epoch": 8.686406460296096,
      "grad_norm": 0.42404183745384216,
      "learning_rate": 7.003891050583658e-06,
      "loss": 0.0866,
      "step": 6454
    },
    {
      "epoch": 8.687752355316285,
      "grad_norm": 0.8827694654464722,
      "learning_rate": 6.97395989224783e-06,
      "loss": 0.0951,
      "step": 6455
    },
    {
      "epoch": 8.689098250336475,
      "grad_norm": 0.41326260566711426,
      "learning_rate": 6.944028733912003e-06,
      "loss": 0.0856,
      "step": 6456
    },
    {
      "epoch": 8.690444145356663,
      "grad_norm": 0.501304030418396,
      "learning_rate": 6.914097575576175e-06,
      "loss": 0.1264,
      "step": 6457
    },
    {
      "epoch": 8.691790040376851,
      "grad_norm": 0.21703287959098816,
      "learning_rate": 6.884166417240348e-06,
      "loss": 0.057,
      "step": 6458
    },
    {
      "epoch": 8.69313593539704,
      "grad_norm": 0.5508837103843689,
      "learning_rate": 6.85423525890452e-06,
      "loss": 0.1305,
      "step": 6459
    },
    {
      "epoch": 8.694481830417228,
      "grad_norm": 0.7125315070152283,
      "learning_rate": 6.824304100568692e-06,
      "loss": 0.0788,
      "step": 6460
    },
    {
      "epoch": 8.695827725437416,
      "grad_norm": 0.34960734844207764,
      "learning_rate": 6.794372942232865e-06,
      "loss": 0.0763,
      "step": 6461
    },
    {
      "epoch": 8.697173620457605,
      "grad_norm": 0.46770063042640686,
      "learning_rate": 6.764441783897036e-06,
      "loss": 0.113,
      "step": 6462
    },
    {
      "epoch": 8.698519515477793,
      "grad_norm": 0.8324911594390869,
      "learning_rate": 6.734510625561209e-06,
      "loss": 0.0943,
      "step": 6463
    },
    {
      "epoch": 8.699865410497981,
      "grad_norm": 0.306234747171402,
      "learning_rate": 6.704579467225381e-06,
      "loss": 0.0916,
      "step": 6464
    },
    {
      "epoch": 8.70121130551817,
      "grad_norm": 0.5898489952087402,
      "learning_rate": 6.674648308889554e-06,
      "loss": 0.1271,
      "step": 6465
    },
    {
      "epoch": 8.702557200538358,
      "grad_norm": 0.40503594279289246,
      "learning_rate": 6.6447171505537265e-06,
      "loss": 0.0889,
      "step": 6466
    },
    {
      "epoch": 8.703903095558546,
      "grad_norm": 0.47784802317619324,
      "learning_rate": 6.614785992217899e-06,
      "loss": 0.0759,
      "step": 6467
    },
    {
      "epoch": 8.705248990578735,
      "grad_norm": 0.459778368473053,
      "learning_rate": 6.5848548338820715e-06,
      "loss": 0.0937,
      "step": 6468
    },
    {
      "epoch": 8.706594885598923,
      "grad_norm": 0.3536530137062073,
      "learning_rate": 6.554923675546244e-06,
      "loss": 0.1103,
      "step": 6469
    },
    {
      "epoch": 8.707940780619111,
      "grad_norm": 0.5459954142570496,
      "learning_rate": 6.524992517210417e-06,
      "loss": 0.1052,
      "step": 6470
    },
    {
      "epoch": 8.7092866756393,
      "grad_norm": 0.43347999453544617,
      "learning_rate": 6.495061358874589e-06,
      "loss": 0.108,
      "step": 6471
    },
    {
      "epoch": 8.710632570659488,
      "grad_norm": 0.8504295945167542,
      "learning_rate": 6.465130200538762e-06,
      "loss": 0.108,
      "step": 6472
    },
    {
      "epoch": 8.711978465679676,
      "grad_norm": 0.4327198565006256,
      "learning_rate": 6.4351990422029325e-06,
      "loss": 0.0996,
      "step": 6473
    },
    {
      "epoch": 8.713324360699865,
      "grad_norm": 0.49126526713371277,
      "learning_rate": 6.405267883867105e-06,
      "loss": 0.0855,
      "step": 6474
    },
    {
      "epoch": 8.714670255720053,
      "grad_norm": 0.4381607472896576,
      "learning_rate": 6.375336725531278e-06,
      "loss": 0.1057,
      "step": 6475
    },
    {
      "epoch": 8.716016150740241,
      "grad_norm": 1.4068872928619385,
      "learning_rate": 6.34540556719545e-06,
      "loss": 0.128,
      "step": 6476
    },
    {
      "epoch": 8.717362045760431,
      "grad_norm": 0.571906566619873,
      "learning_rate": 6.315474408859623e-06,
      "loss": 0.1261,
      "step": 6477
    },
    {
      "epoch": 8.71870794078062,
      "grad_norm": 0.4504956007003784,
      "learning_rate": 6.285543250523795e-06,
      "loss": 0.0892,
      "step": 6478
    },
    {
      "epoch": 8.720053835800808,
      "grad_norm": 0.3252953290939331,
      "learning_rate": 6.255612092187968e-06,
      "loss": 0.0956,
      "step": 6479
    },
    {
      "epoch": 8.721399730820997,
      "grad_norm": 0.3948929011821747,
      "learning_rate": 6.22568093385214e-06,
      "loss": 0.1028,
      "step": 6480
    },
    {
      "epoch": 8.722745625841185,
      "grad_norm": 0.34346961975097656,
      "learning_rate": 6.195749775516313e-06,
      "loss": 0.0876,
      "step": 6481
    },
    {
      "epoch": 8.724091520861373,
      "grad_norm": 0.32819682359695435,
      "learning_rate": 6.165818617180485e-06,
      "loss": 0.0805,
      "step": 6482
    },
    {
      "epoch": 8.725437415881562,
      "grad_norm": 0.6051492691040039,
      "learning_rate": 6.135887458844657e-06,
      "loss": 0.1412,
      "step": 6483
    },
    {
      "epoch": 8.72678331090175,
      "grad_norm": 0.33663225173950195,
      "learning_rate": 6.1059563005088296e-06,
      "loss": 0.0745,
      "step": 6484
    },
    {
      "epoch": 8.728129205921938,
      "grad_norm": 0.3668421804904938,
      "learning_rate": 6.076025142173002e-06,
      "loss": 0.0868,
      "step": 6485
    },
    {
      "epoch": 8.729475100942127,
      "grad_norm": 0.45916876196861267,
      "learning_rate": 6.046093983837175e-06,
      "loss": 0.1177,
      "step": 6486
    },
    {
      "epoch": 8.730820995962315,
      "grad_norm": 0.46578603982925415,
      "learning_rate": 6.016162825501347e-06,
      "loss": 0.098,
      "step": 6487
    },
    {
      "epoch": 8.732166890982503,
      "grad_norm": 0.4237669110298157,
      "learning_rate": 5.98623166716552e-06,
      "loss": 0.0933,
      "step": 6488
    },
    {
      "epoch": 8.733512786002692,
      "grad_norm": 0.44092997908592224,
      "learning_rate": 5.956300508829691e-06,
      "loss": 0.098,
      "step": 6489
    },
    {
      "epoch": 8.73485868102288,
      "grad_norm": 0.5318875312805176,
      "learning_rate": 5.926369350493864e-06,
      "loss": 0.1501,
      "step": 6490
    },
    {
      "epoch": 8.736204576043068,
      "grad_norm": 0.44519540667533875,
      "learning_rate": 5.8964381921580365e-06,
      "loss": 0.1064,
      "step": 6491
    },
    {
      "epoch": 8.737550471063257,
      "grad_norm": 0.36795473098754883,
      "learning_rate": 5.866507033822209e-06,
      "loss": 0.0844,
      "step": 6492
    },
    {
      "epoch": 8.738896366083445,
      "grad_norm": 0.4087010622024536,
      "learning_rate": 5.8365758754863816e-06,
      "loss": 0.1139,
      "step": 6493
    },
    {
      "epoch": 8.740242261103633,
      "grad_norm": 0.2687150537967682,
      "learning_rate": 5.806644717150554e-06,
      "loss": 0.0651,
      "step": 6494
    },
    {
      "epoch": 8.741588156123822,
      "grad_norm": 0.5945313572883606,
      "learning_rate": 5.776713558814727e-06,
      "loss": 0.1149,
      "step": 6495
    },
    {
      "epoch": 8.742934051144012,
      "grad_norm": 0.388663113117218,
      "learning_rate": 5.746782400478898e-06,
      "loss": 0.0734,
      "step": 6496
    },
    {
      "epoch": 8.7442799461642,
      "grad_norm": 0.33748695254325867,
      "learning_rate": 5.716851242143071e-06,
      "loss": 0.0656,
      "step": 6497
    },
    {
      "epoch": 8.745625841184388,
      "grad_norm": 0.4449172914028168,
      "learning_rate": 5.686920083807243e-06,
      "loss": 0.0987,
      "step": 6498
    },
    {
      "epoch": 8.746971736204577,
      "grad_norm": 0.41422000527381897,
      "learning_rate": 5.656988925471416e-06,
      "loss": 0.074,
      "step": 6499
    },
    {
      "epoch": 8.748317631224765,
      "grad_norm": 0.46218767762184143,
      "learning_rate": 5.6270577671355885e-06,
      "loss": 0.0969,
      "step": 6500
    },
    {
      "epoch": 8.749663526244953,
      "grad_norm": 0.3805926740169525,
      "learning_rate": 5.597126608799761e-06,
      "loss": 0.092,
      "step": 6501
    },
    {
      "epoch": 8.751009421265142,
      "grad_norm": 0.42797183990478516,
      "learning_rate": 5.5671954504639335e-06,
      "loss": 0.1003,
      "step": 6502
    },
    {
      "epoch": 8.75235531628533,
      "grad_norm": 0.43087756633758545,
      "learning_rate": 5.537264292128105e-06,
      "loss": 0.1084,
      "step": 6503
    },
    {
      "epoch": 8.753701211305518,
      "grad_norm": 0.4088333547115326,
      "learning_rate": 5.507333133792278e-06,
      "loss": 0.0784,
      "step": 6504
    },
    {
      "epoch": 8.755047106325707,
      "grad_norm": 0.34980323910713196,
      "learning_rate": 5.47740197545645e-06,
      "loss": 0.1004,
      "step": 6505
    },
    {
      "epoch": 8.756393001345895,
      "grad_norm": 0.5564678907394409,
      "learning_rate": 5.447470817120623e-06,
      "loss": 0.1318,
      "step": 6506
    },
    {
      "epoch": 8.757738896366083,
      "grad_norm": 0.2721514105796814,
      "learning_rate": 5.417539658784795e-06,
      "loss": 0.0636,
      "step": 6507
    },
    {
      "epoch": 8.759084791386272,
      "grad_norm": 0.4527038633823395,
      "learning_rate": 5.387608500448968e-06,
      "loss": 0.117,
      "step": 6508
    },
    {
      "epoch": 8.76043068640646,
      "grad_norm": 0.48258674144744873,
      "learning_rate": 5.35767734211314e-06,
      "loss": 0.1114,
      "step": 6509
    },
    {
      "epoch": 8.761776581426648,
      "grad_norm": 0.37502405047416687,
      "learning_rate": 5.327746183777312e-06,
      "loss": 0.0783,
      "step": 6510
    },
    {
      "epoch": 8.763122476446837,
      "grad_norm": 0.42820972204208374,
      "learning_rate": 5.297815025441485e-06,
      "loss": 0.0811,
      "step": 6511
    },
    {
      "epoch": 8.764468371467025,
      "grad_norm": 1.3321843147277832,
      "learning_rate": 5.267883867105657e-06,
      "loss": 0.0808,
      "step": 6512
    },
    {
      "epoch": 8.765814266487213,
      "grad_norm": 0.816861093044281,
      "learning_rate": 5.23795270876983e-06,
      "loss": 0.1172,
      "step": 6513
    },
    {
      "epoch": 8.767160161507402,
      "grad_norm": 0.4824402332305908,
      "learning_rate": 5.208021550434002e-06,
      "loss": 0.0872,
      "step": 6514
    },
    {
      "epoch": 8.76850605652759,
      "grad_norm": 0.36131373047828674,
      "learning_rate": 5.178090392098175e-06,
      "loss": 0.1092,
      "step": 6515
    },
    {
      "epoch": 8.769851951547778,
      "grad_norm": 0.3825746178627014,
      "learning_rate": 5.1481592337623465e-06,
      "loss": 0.1038,
      "step": 6516
    },
    {
      "epoch": 8.771197846567969,
      "grad_norm": 0.45385098457336426,
      "learning_rate": 5.118228075426519e-06,
      "loss": 0.1292,
      "step": 6517
    },
    {
      "epoch": 8.772543741588157,
      "grad_norm": 0.30575600266456604,
      "learning_rate": 5.088296917090692e-06,
      "loss": 0.0805,
      "step": 6518
    },
    {
      "epoch": 8.773889636608345,
      "grad_norm": 0.4416300058364868,
      "learning_rate": 5.058365758754864e-06,
      "loss": 0.1138,
      "step": 6519
    },
    {
      "epoch": 8.775235531628534,
      "grad_norm": 0.3350067138671875,
      "learning_rate": 5.028434600419037e-06,
      "loss": 0.081,
      "step": 6520
    },
    {
      "epoch": 8.776581426648722,
      "grad_norm": 0.37490057945251465,
      "learning_rate": 4.998503442083209e-06,
      "loss": 0.0912,
      "step": 6521
    },
    {
      "epoch": 8.77792732166891,
      "grad_norm": 0.49096280336380005,
      "learning_rate": 4.968572283747381e-06,
      "loss": 0.0921,
      "step": 6522
    },
    {
      "epoch": 8.779273216689099,
      "grad_norm": 0.28332921862602234,
      "learning_rate": 4.9386411254115534e-06,
      "loss": 0.089,
      "step": 6523
    },
    {
      "epoch": 8.780619111709287,
      "grad_norm": 0.37033331394195557,
      "learning_rate": 4.908709967075726e-06,
      "loss": 0.0858,
      "step": 6524
    },
    {
      "epoch": 8.781965006729475,
      "grad_norm": 1.0140502452850342,
      "learning_rate": 4.8787788087398985e-06,
      "loss": 0.1374,
      "step": 6525
    },
    {
      "epoch": 8.783310901749664,
      "grad_norm": 0.28442147374153137,
      "learning_rate": 4.848847650404071e-06,
      "loss": 0.0766,
      "step": 6526
    },
    {
      "epoch": 8.784656796769852,
      "grad_norm": 0.3225930631160736,
      "learning_rate": 4.818916492068244e-06,
      "loss": 0.0834,
      "step": 6527
    },
    {
      "epoch": 8.78600269179004,
      "grad_norm": 0.40441685914993286,
      "learning_rate": 4.788985333732416e-06,
      "loss": 0.0889,
      "step": 6528
    },
    {
      "epoch": 8.787348586810229,
      "grad_norm": 0.43326693773269653,
      "learning_rate": 4.759054175396588e-06,
      "loss": 0.0815,
      "step": 6529
    },
    {
      "epoch": 8.788694481830417,
      "grad_norm": 0.6924861073493958,
      "learning_rate": 4.72912301706076e-06,
      "loss": 0.1447,
      "step": 6530
    },
    {
      "epoch": 8.790040376850605,
      "grad_norm": 0.23554298281669617,
      "learning_rate": 4.699191858724933e-06,
      "loss": 0.065,
      "step": 6531
    },
    {
      "epoch": 8.791386271870794,
      "grad_norm": 0.5344755053520203,
      "learning_rate": 4.669260700389105e-06,
      "loss": 0.0844,
      "step": 6532
    },
    {
      "epoch": 8.792732166890982,
      "grad_norm": 0.4198722541332245,
      "learning_rate": 4.639329542053278e-06,
      "loss": 0.1144,
      "step": 6533
    },
    {
      "epoch": 8.79407806191117,
      "grad_norm": 0.3462692201137543,
      "learning_rate": 4.6093983837174505e-06,
      "loss": 0.0698,
      "step": 6534
    },
    {
      "epoch": 8.795423956931359,
      "grad_norm": 0.4483768343925476,
      "learning_rate": 4.579467225381622e-06,
      "loss": 0.0987,
      "step": 6535
    },
    {
      "epoch": 8.796769851951549,
      "grad_norm": 0.47698813676834106,
      "learning_rate": 4.549536067045795e-06,
      "loss": 0.1094,
      "step": 6536
    },
    {
      "epoch": 8.798115746971735,
      "grad_norm": 0.3204665184020996,
      "learning_rate": 4.519604908709967e-06,
      "loss": 0.0723,
      "step": 6537
    },
    {
      "epoch": 8.799461641991925,
      "grad_norm": 0.4365040063858032,
      "learning_rate": 4.48967375037414e-06,
      "loss": 0.099,
      "step": 6538
    },
    {
      "epoch": 8.800807537012114,
      "grad_norm": 0.36162275075912476,
      "learning_rate": 4.459742592038312e-06,
      "loss": 0.0802,
      "step": 6539
    },
    {
      "epoch": 8.802153432032302,
      "grad_norm": 1.0473371744155884,
      "learning_rate": 4.429811433702485e-06,
      "loss": 0.1596,
      "step": 6540
    },
    {
      "epoch": 8.80349932705249,
      "grad_norm": 0.4043445289134979,
      "learning_rate": 4.399880275366657e-06,
      "loss": 0.1057,
      "step": 6541
    },
    {
      "epoch": 8.804845222072679,
      "grad_norm": 0.3772827386856079,
      "learning_rate": 4.369949117030829e-06,
      "loss": 0.0952,
      "step": 6542
    },
    {
      "epoch": 8.806191117092867,
      "grad_norm": 0.45562708377838135,
      "learning_rate": 4.340017958695002e-06,
      "loss": 0.1155,
      "step": 6543
    },
    {
      "epoch": 8.807537012113055,
      "grad_norm": 2.155271291732788,
      "learning_rate": 4.310086800359174e-06,
      "loss": 0.0879,
      "step": 6544
    },
    {
      "epoch": 8.808882907133244,
      "grad_norm": 0.43808498978614807,
      "learning_rate": 4.280155642023347e-06,
      "loss": 0.101,
      "step": 6545
    },
    {
      "epoch": 8.810228802153432,
      "grad_norm": 0.43003639578819275,
      "learning_rate": 4.250224483687519e-06,
      "loss": 0.1067,
      "step": 6546
    },
    {
      "epoch": 8.81157469717362,
      "grad_norm": 0.5646328926086426,
      "learning_rate": 4.220293325351692e-06,
      "loss": 0.1303,
      "step": 6547
    },
    {
      "epoch": 8.812920592193809,
      "grad_norm": 0.3258044123649597,
      "learning_rate": 4.1903621670158635e-06,
      "loss": 0.0777,
      "step": 6548
    },
    {
      "epoch": 8.814266487213997,
      "grad_norm": 0.3406314253807068,
      "learning_rate": 4.160431008680036e-06,
      "loss": 0.0963,
      "step": 6549
    },
    {
      "epoch": 8.815612382234185,
      "grad_norm": 0.3079814612865448,
      "learning_rate": 4.1304998503442085e-06,
      "loss": 0.0765,
      "step": 6550
    },
    {
      "epoch": 8.816958277254374,
      "grad_norm": 0.23507612943649292,
      "learning_rate": 4.100568692008381e-06,
      "loss": 0.0638,
      "step": 6551
    },
    {
      "epoch": 8.818304172274562,
      "grad_norm": 0.33824747800827026,
      "learning_rate": 4.070637533672554e-06,
      "loss": 0.0827,
      "step": 6552
    },
    {
      "epoch": 8.81965006729475,
      "grad_norm": 0.4932795464992523,
      "learning_rate": 4.040706375336726e-06,
      "loss": 0.0995,
      "step": 6553
    },
    {
      "epoch": 8.820995962314939,
      "grad_norm": 0.31038403511047363,
      "learning_rate": 4.010775217000899e-06,
      "loss": 0.0776,
      "step": 6554
    },
    {
      "epoch": 8.822341857335127,
      "grad_norm": 0.42560824751853943,
      "learning_rate": 3.98084405866507e-06,
      "loss": 0.0924,
      "step": 6555
    },
    {
      "epoch": 8.823687752355315,
      "grad_norm": 0.5073275566101074,
      "learning_rate": 3.950912900329243e-06,
      "loss": 0.0877,
      "step": 6556
    },
    {
      "epoch": 8.825033647375506,
      "grad_norm": 0.7703169584274292,
      "learning_rate": 3.9209817419934155e-06,
      "loss": 0.1545,
      "step": 6557
    },
    {
      "epoch": 8.826379542395694,
      "grad_norm": 1.2501115798950195,
      "learning_rate": 3.891050583657588e-06,
      "loss": 0.146,
      "step": 6558
    },
    {
      "epoch": 8.827725437415882,
      "grad_norm": 0.6999740600585938,
      "learning_rate": 3.8611194253217605e-06,
      "loss": 0.0988,
      "step": 6559
    },
    {
      "epoch": 8.82907133243607,
      "grad_norm": 0.36694324016571045,
      "learning_rate": 3.831188266985933e-06,
      "loss": 0.0835,
      "step": 6560
    },
    {
      "epoch": 8.830417227456259,
      "grad_norm": 0.2835048735141754,
      "learning_rate": 3.8012571086501048e-06,
      "loss": 0.0859,
      "step": 6561
    },
    {
      "epoch": 8.831763122476447,
      "grad_norm": 0.4238508343696594,
      "learning_rate": 3.7713259503142773e-06,
      "loss": 0.0898,
      "step": 6562
    },
    {
      "epoch": 8.833109017496636,
      "grad_norm": 0.4123464822769165,
      "learning_rate": 3.74139479197845e-06,
      "loss": 0.0764,
      "step": 6563
    },
    {
      "epoch": 8.834454912516824,
      "grad_norm": 0.26099541783332825,
      "learning_rate": 3.711463633642622e-06,
      "loss": 0.0641,
      "step": 6564
    },
    {
      "epoch": 8.835800807537012,
      "grad_norm": 0.38163769245147705,
      "learning_rate": 3.6815324753067945e-06,
      "loss": 0.0778,
      "step": 6565
    },
    {
      "epoch": 8.8371467025572,
      "grad_norm": 0.364351361989975,
      "learning_rate": 3.651601316970967e-06,
      "loss": 0.0896,
      "step": 6566
    },
    {
      "epoch": 8.838492597577389,
      "grad_norm": 0.5226022005081177,
      "learning_rate": 3.6216701586351396e-06,
      "loss": 0.1078,
      "step": 6567
    },
    {
      "epoch": 8.839838492597577,
      "grad_norm": 0.5195823907852173,
      "learning_rate": 3.5917390002993117e-06,
      "loss": 0.123,
      "step": 6568
    },
    {
      "epoch": 8.841184387617766,
      "grad_norm": 0.5108870267868042,
      "learning_rate": 3.561807841963484e-06,
      "loss": 0.1014,
      "step": 6569
    },
    {
      "epoch": 8.842530282637954,
      "grad_norm": 0.40356191992759705,
      "learning_rate": 3.5318766836276563e-06,
      "loss": 0.0959,
      "step": 6570
    },
    {
      "epoch": 8.843876177658142,
      "grad_norm": 0.29707640409469604,
      "learning_rate": 3.501945525291829e-06,
      "loss": 0.0766,
      "step": 6571
    },
    {
      "epoch": 8.84522207267833,
      "grad_norm": 0.39643657207489014,
      "learning_rate": 3.4720143669560014e-06,
      "loss": 0.1025,
      "step": 6572
    },
    {
      "epoch": 8.846567967698519,
      "grad_norm": 0.484673410654068,
      "learning_rate": 3.442083208620174e-06,
      "loss": 0.0832,
      "step": 6573
    },
    {
      "epoch": 8.847913862718707,
      "grad_norm": 0.3062843084335327,
      "learning_rate": 3.412152050284346e-06,
      "loss": 0.0737,
      "step": 6574
    },
    {
      "epoch": 8.849259757738896,
      "grad_norm": 0.28105631470680237,
      "learning_rate": 3.382220891948518e-06,
      "loss": 0.0741,
      "step": 6575
    },
    {
      "epoch": 8.850605652759086,
      "grad_norm": 0.4022476375102997,
      "learning_rate": 3.3522897336126907e-06,
      "loss": 0.093,
      "step": 6576
    },
    {
      "epoch": 8.851951547779272,
      "grad_norm": 0.8603629469871521,
      "learning_rate": 3.3223585752768632e-06,
      "loss": 0.0974,
      "step": 6577
    },
    {
      "epoch": 8.853297442799462,
      "grad_norm": 0.49558642506599426,
      "learning_rate": 3.2924274169410358e-06,
      "loss": 0.0924,
      "step": 6578
    },
    {
      "epoch": 8.85464333781965,
      "grad_norm": 0.4684408903121948,
      "learning_rate": 3.2624962586052083e-06,
      "loss": 0.1018,
      "step": 6579
    },
    {
      "epoch": 8.855989232839839,
      "grad_norm": 0.2969749867916107,
      "learning_rate": 3.232565100269381e-06,
      "loss": 0.0782,
      "step": 6580
    },
    {
      "epoch": 8.857335127860027,
      "grad_norm": 0.38073939085006714,
      "learning_rate": 3.2026339419335525e-06,
      "loss": 0.0948,
      "step": 6581
    },
    {
      "epoch": 8.858681022880216,
      "grad_norm": 0.46650007367134094,
      "learning_rate": 3.172702783597725e-06,
      "loss": 0.1042,
      "step": 6582
    },
    {
      "epoch": 8.860026917900404,
      "grad_norm": 0.41134998202323914,
      "learning_rate": 3.1427716252618976e-06,
      "loss": 0.1012,
      "step": 6583
    },
    {
      "epoch": 8.861372812920592,
      "grad_norm": 0.9081800580024719,
      "learning_rate": 3.11284046692607e-06,
      "loss": 0.1397,
      "step": 6584
    },
    {
      "epoch": 8.86271870794078,
      "grad_norm": 1.5516349077224731,
      "learning_rate": 3.0829093085902427e-06,
      "loss": 0.1074,
      "step": 6585
    },
    {
      "epoch": 8.864064602960969,
      "grad_norm": 0.40861406922340393,
      "learning_rate": 3.0529781502544148e-06,
      "loss": 0.079,
      "step": 6586
    },
    {
      "epoch": 8.865410497981157,
      "grad_norm": 0.39425522089004517,
      "learning_rate": 3.0230469919185873e-06,
      "loss": 0.0895,
      "step": 6587
    },
    {
      "epoch": 8.866756393001346,
      "grad_norm": 0.4306752681732178,
      "learning_rate": 2.99311583358276e-06,
      "loss": 0.0869,
      "step": 6588
    },
    {
      "epoch": 8.868102288021534,
      "grad_norm": 0.3089025318622589,
      "learning_rate": 2.963184675246932e-06,
      "loss": 0.0856,
      "step": 6589
    },
    {
      "epoch": 8.869448183041722,
      "grad_norm": 0.34597983956336975,
      "learning_rate": 2.9332535169111045e-06,
      "loss": 0.0705,
      "step": 6590
    },
    {
      "epoch": 8.87079407806191,
      "grad_norm": 0.32034826278686523,
      "learning_rate": 2.903322358575277e-06,
      "loss": 0.074,
      "step": 6591
    },
    {
      "epoch": 8.8721399730821,
      "grad_norm": 0.31861746311187744,
      "learning_rate": 2.873391200239449e-06,
      "loss": 0.0834,
      "step": 6592
    },
    {
      "epoch": 8.873485868102287,
      "grad_norm": 0.5070469975471497,
      "learning_rate": 2.8434600419036217e-06,
      "loss": 0.0814,
      "step": 6593
    },
    {
      "epoch": 8.874831763122476,
      "grad_norm": 0.4573654234409332,
      "learning_rate": 2.8135288835677942e-06,
      "loss": 0.101,
      "step": 6594
    },
    {
      "epoch": 8.876177658142664,
      "grad_norm": 0.4794843792915344,
      "learning_rate": 2.7835977252319668e-06,
      "loss": 0.078,
      "step": 6595
    },
    {
      "epoch": 8.877523553162852,
      "grad_norm": 0.3465133607387543,
      "learning_rate": 2.753666566896139e-06,
      "loss": 0.1008,
      "step": 6596
    },
    {
      "epoch": 8.878869448183043,
      "grad_norm": 0.38133129477500916,
      "learning_rate": 2.7237354085603114e-06,
      "loss": 0.0939,
      "step": 6597
    },
    {
      "epoch": 8.880215343203231,
      "grad_norm": 0.4037614166736603,
      "learning_rate": 2.693804250224484e-06,
      "loss": 0.1014,
      "step": 6598
    },
    {
      "epoch": 8.88156123822342,
      "grad_norm": 0.3560849726200104,
      "learning_rate": 2.663873091888656e-06,
      "loss": 0.0701,
      "step": 6599
    },
    {
      "epoch": 8.882907133243608,
      "grad_norm": 0.518088161945343,
      "learning_rate": 2.6339419335528286e-06,
      "loss": 0.0698,
      "step": 6600
    },
    {
      "epoch": 8.884253028263796,
      "grad_norm": 0.33958008885383606,
      "learning_rate": 2.604010775217001e-06,
      "loss": 0.0895,
      "step": 6601
    },
    {
      "epoch": 8.885598923283984,
      "grad_norm": 0.4085847735404968,
      "learning_rate": 2.5740796168811733e-06,
      "loss": 0.0873,
      "step": 6602
    },
    {
      "epoch": 8.886944818304173,
      "grad_norm": 0.7722561359405518,
      "learning_rate": 2.544148458545346e-06,
      "loss": 0.0958,
      "step": 6603
    },
    {
      "epoch": 8.888290713324361,
      "grad_norm": 0.48153501749038696,
      "learning_rate": 2.5142173002095183e-06,
      "loss": 0.1078,
      "step": 6604
    },
    {
      "epoch": 8.88963660834455,
      "grad_norm": 0.45042434334754944,
      "learning_rate": 2.4842861418736904e-06,
      "loss": 0.0975,
      "step": 6605
    },
    {
      "epoch": 8.890982503364738,
      "grad_norm": 0.9889662265777588,
      "learning_rate": 2.454354983537863e-06,
      "loss": 0.1085,
      "step": 6606
    },
    {
      "epoch": 8.892328398384926,
      "grad_norm": 0.3938044309616089,
      "learning_rate": 2.4244238252020355e-06,
      "loss": 0.092,
      "step": 6607
    },
    {
      "epoch": 8.893674293405114,
      "grad_norm": 0.544643759727478,
      "learning_rate": 2.394492666866208e-06,
      "loss": 0.091,
      "step": 6608
    },
    {
      "epoch": 8.895020188425303,
      "grad_norm": 0.526620626449585,
      "learning_rate": 2.36456150853038e-06,
      "loss": 0.0983,
      "step": 6609
    },
    {
      "epoch": 8.896366083445491,
      "grad_norm": 0.3896043300628662,
      "learning_rate": 2.3346303501945527e-06,
      "loss": 0.0875,
      "step": 6610
    },
    {
      "epoch": 8.89771197846568,
      "grad_norm": 0.4043176472187042,
      "learning_rate": 2.3046991918587252e-06,
      "loss": 0.0858,
      "step": 6611
    },
    {
      "epoch": 8.899057873485868,
      "grad_norm": 0.2693619132041931,
      "learning_rate": 2.2747680335228974e-06,
      "loss": 0.0741,
      "step": 6612
    },
    {
      "epoch": 8.900403768506056,
      "grad_norm": 0.3216535449028015,
      "learning_rate": 2.24483687518707e-06,
      "loss": 0.0747,
      "step": 6613
    },
    {
      "epoch": 8.901749663526244,
      "grad_norm": 0.6241014003753662,
      "learning_rate": 2.2149057168512424e-06,
      "loss": 0.1147,
      "step": 6614
    },
    {
      "epoch": 8.903095558546433,
      "grad_norm": 0.3751802444458008,
      "learning_rate": 2.1849745585154145e-06,
      "loss": 0.0767,
      "step": 6615
    },
    {
      "epoch": 8.904441453566621,
      "grad_norm": 0.5606765151023865,
      "learning_rate": 2.155043400179587e-06,
      "loss": 0.0944,
      "step": 6616
    },
    {
      "epoch": 8.90578734858681,
      "grad_norm": 0.42105832695961,
      "learning_rate": 2.1251122418437596e-06,
      "loss": 0.0939,
      "step": 6617
    },
    {
      "epoch": 8.907133243607,
      "grad_norm": 0.38842707872390747,
      "learning_rate": 2.0951810835079317e-06,
      "loss": 0.0938,
      "step": 6618
    },
    {
      "epoch": 8.908479138627188,
      "grad_norm": 0.47111478447914124,
      "learning_rate": 2.0652499251721043e-06,
      "loss": 0.1049,
      "step": 6619
    },
    {
      "epoch": 8.909825033647376,
      "grad_norm": 0.392467737197876,
      "learning_rate": 2.035318766836277e-06,
      "loss": 0.0945,
      "step": 6620
    },
    {
      "epoch": 8.911170928667564,
      "grad_norm": 0.35349658131599426,
      "learning_rate": 2.0053876085004493e-06,
      "loss": 0.0916,
      "step": 6621
    },
    {
      "epoch": 8.912516823687753,
      "grad_norm": 0.4978698492050171,
      "learning_rate": 1.9754564501646215e-06,
      "loss": 0.1065,
      "step": 6622
    },
    {
      "epoch": 8.913862718707941,
      "grad_norm": 1.1341997385025024,
      "learning_rate": 1.945525291828794e-06,
      "loss": 0.1147,
      "step": 6623
    },
    {
      "epoch": 8.91520861372813,
      "grad_norm": 0.4123460054397583,
      "learning_rate": 1.9155941334929665e-06,
      "loss": 0.0936,
      "step": 6624
    },
    {
      "epoch": 8.916554508748318,
      "grad_norm": 0.29557839035987854,
      "learning_rate": 1.8856629751571386e-06,
      "loss": 0.0638,
      "step": 6625
    },
    {
      "epoch": 8.917900403768506,
      "grad_norm": 1.0709837675094604,
      "learning_rate": 1.855731816821311e-06,
      "loss": 0.0862,
      "step": 6626
    },
    {
      "epoch": 8.919246298788694,
      "grad_norm": 0.34477290511131287,
      "learning_rate": 1.8258006584854835e-06,
      "loss": 0.0847,
      "step": 6627
    },
    {
      "epoch": 8.920592193808883,
      "grad_norm": 0.77606201171875,
      "learning_rate": 1.7958695001496558e-06,
      "loss": 0.1054,
      "step": 6628
    },
    {
      "epoch": 8.921938088829071,
      "grad_norm": 0.6226187348365784,
      "learning_rate": 1.7659383418138282e-06,
      "loss": 0.1203,
      "step": 6629
    },
    {
      "epoch": 8.92328398384926,
      "grad_norm": 0.4004301130771637,
      "learning_rate": 1.7360071834780007e-06,
      "loss": 0.0924,
      "step": 6630
    },
    {
      "epoch": 8.924629878869448,
      "grad_norm": 0.49481824040412903,
      "learning_rate": 1.706076025142173e-06,
      "loss": 0.1111,
      "step": 6631
    },
    {
      "epoch": 8.925975773889636,
      "grad_norm": 0.4761577248573303,
      "learning_rate": 1.6761448668063453e-06,
      "loss": 0.1069,
      "step": 6632
    },
    {
      "epoch": 8.927321668909824,
      "grad_norm": 0.7384374141693115,
      "learning_rate": 1.6462137084705179e-06,
      "loss": 0.0959,
      "step": 6633
    },
    {
      "epoch": 8.928667563930013,
      "grad_norm": 0.3842640221118927,
      "learning_rate": 1.6162825501346904e-06,
      "loss": 0.0885,
      "step": 6634
    },
    {
      "epoch": 8.930013458950201,
      "grad_norm": 0.3049706220626831,
      "learning_rate": 1.5863513917988625e-06,
      "loss": 0.0694,
      "step": 6635
    },
    {
      "epoch": 8.93135935397039,
      "grad_norm": 0.3928114175796509,
      "learning_rate": 1.556420233463035e-06,
      "loss": 0.097,
      "step": 6636
    },
    {
      "epoch": 8.93270524899058,
      "grad_norm": 0.4138313829898834,
      "learning_rate": 1.5264890751272074e-06,
      "loss": 0.0889,
      "step": 6637
    },
    {
      "epoch": 8.934051144010768,
      "grad_norm": 0.4140335023403168,
      "learning_rate": 1.49655791679138e-06,
      "loss": 0.1025,
      "step": 6638
    },
    {
      "epoch": 8.935397039030956,
      "grad_norm": 0.36156803369522095,
      "learning_rate": 1.4666267584555523e-06,
      "loss": 0.0837,
      "step": 6639
    },
    {
      "epoch": 8.936742934051145,
      "grad_norm": 0.9343655705451965,
      "learning_rate": 1.4366956001197246e-06,
      "loss": 0.0864,
      "step": 6640
    },
    {
      "epoch": 8.938088829071333,
      "grad_norm": 1.0141792297363281,
      "learning_rate": 1.4067644417838971e-06,
      "loss": 0.1115,
      "step": 6641
    },
    {
      "epoch": 8.939434724091521,
      "grad_norm": 0.6686664819717407,
      "learning_rate": 1.3768332834480694e-06,
      "loss": 0.1225,
      "step": 6642
    },
    {
      "epoch": 8.94078061911171,
      "grad_norm": 0.4611011743545532,
      "learning_rate": 1.346902125112242e-06,
      "loss": 0.1063,
      "step": 6643
    },
    {
      "epoch": 8.942126514131898,
      "grad_norm": 0.4040415287017822,
      "learning_rate": 1.3169709667764143e-06,
      "loss": 0.081,
      "step": 6644
    },
    {
      "epoch": 8.943472409152086,
      "grad_norm": 0.6649284958839417,
      "learning_rate": 1.2870398084405866e-06,
      "loss": 0.0855,
      "step": 6645
    },
    {
      "epoch": 8.944818304172275,
      "grad_norm": 0.40979278087615967,
      "learning_rate": 1.2571086501047592e-06,
      "loss": 0.1014,
      "step": 6646
    },
    {
      "epoch": 8.946164199192463,
      "grad_norm": 0.38994279503822327,
      "learning_rate": 1.2271774917689315e-06,
      "loss": 0.0841,
      "step": 6647
    },
    {
      "epoch": 8.947510094212651,
      "grad_norm": 1.079621434211731,
      "learning_rate": 1.197246333433104e-06,
      "loss": 0.1104,
      "step": 6648
    },
    {
      "epoch": 8.94885598923284,
      "grad_norm": 0.25420230627059937,
      "learning_rate": 1.1673151750972764e-06,
      "loss": 0.0651,
      "step": 6649
    },
    {
      "epoch": 8.950201884253028,
      "grad_norm": 0.42155253887176514,
      "learning_rate": 1.1373840167614487e-06,
      "loss": 0.0779,
      "step": 6650
    },
    {
      "epoch": 8.951547779273216,
      "grad_norm": 0.5932583808898926,
      "learning_rate": 1.1074528584256212e-06,
      "loss": 0.1136,
      "step": 6651
    },
    {
      "epoch": 8.952893674293405,
      "grad_norm": 0.6800202131271362,
      "learning_rate": 1.0775217000897935e-06,
      "loss": 0.1285,
      "step": 6652
    },
    {
      "epoch": 8.954239569313593,
      "grad_norm": 0.6529167890548706,
      "learning_rate": 1.0475905417539659e-06,
      "loss": 0.1258,
      "step": 6653
    },
    {
      "epoch": 8.955585464333781,
      "grad_norm": 0.3951553702354431,
      "learning_rate": 1.0176593834181384e-06,
      "loss": 0.0964,
      "step": 6654
    },
    {
      "epoch": 8.95693135935397,
      "grad_norm": 0.35931396484375,
      "learning_rate": 9.877282250823107e-07,
      "loss": 0.0871,
      "step": 6655
    },
    {
      "epoch": 8.958277254374158,
      "grad_norm": 0.2981148362159729,
      "learning_rate": 9.577970667464833e-07,
      "loss": 0.0817,
      "step": 6656
    },
    {
      "epoch": 8.959623149394346,
      "grad_norm": 0.400258868932724,
      "learning_rate": 9.278659084106555e-07,
      "loss": 0.0606,
      "step": 6657
    },
    {
      "epoch": 8.960969044414536,
      "grad_norm": 0.2466195821762085,
      "learning_rate": 8.979347500748279e-07,
      "loss": 0.0683,
      "step": 6658
    },
    {
      "epoch": 8.962314939434725,
      "grad_norm": 0.39446893334388733,
      "learning_rate": 8.680035917390003e-07,
      "loss": 0.0859,
      "step": 6659
    },
    {
      "epoch": 8.963660834454913,
      "grad_norm": 0.38262295722961426,
      "learning_rate": 8.380724334031727e-07,
      "loss": 0.0886,
      "step": 6660
    },
    {
      "epoch": 8.965006729475101,
      "grad_norm": 0.6280644536018372,
      "learning_rate": 8.081412750673452e-07,
      "loss": 0.1038,
      "step": 6661
    },
    {
      "epoch": 8.96635262449529,
      "grad_norm": 0.4166733920574188,
      "learning_rate": 7.782101167315175e-07,
      "loss": 0.1011,
      "step": 6662
    },
    {
      "epoch": 8.967698519515478,
      "grad_norm": 0.388690322637558,
      "learning_rate": 7.4827895839569e-07,
      "loss": 0.0852,
      "step": 6663
    },
    {
      "epoch": 8.969044414535666,
      "grad_norm": 0.32470694184303284,
      "learning_rate": 7.183478000598623e-07,
      "loss": 0.0734,
      "step": 6664
    },
    {
      "epoch": 8.970390309555855,
      "grad_norm": 0.4859674870967865,
      "learning_rate": 6.884166417240347e-07,
      "loss": 0.0982,
      "step": 6665
    },
    {
      "epoch": 8.971736204576043,
      "grad_norm": 0.406095027923584,
      "learning_rate": 6.584854833882072e-07,
      "loss": 0.0825,
      "step": 6666
    },
    {
      "epoch": 8.973082099596231,
      "grad_norm": 0.7346964478492737,
      "learning_rate": 6.285543250523796e-07,
      "loss": 0.1277,
      "step": 6667
    },
    {
      "epoch": 8.97442799461642,
      "grad_norm": 0.29645079374313354,
      "learning_rate": 5.98623166716552e-07,
      "loss": 0.0898,
      "step": 6668
    },
    {
      "epoch": 8.975773889636608,
      "grad_norm": 0.5099642872810364,
      "learning_rate": 5.686920083807243e-07,
      "loss": 0.1021,
      "step": 6669
    },
    {
      "epoch": 8.977119784656796,
      "grad_norm": 0.4167708456516266,
      "learning_rate": 5.387608500448968e-07,
      "loss": 0.0852,
      "step": 6670
    },
    {
      "epoch": 8.978465679676985,
      "grad_norm": 0.5722780823707581,
      "learning_rate": 5.088296917090692e-07,
      "loss": 0.0884,
      "step": 6671
    },
    {
      "epoch": 8.979811574697173,
      "grad_norm": 0.9475524425506592,
      "learning_rate": 4.788985333732416e-07,
      "loss": 0.1498,
      "step": 6672
    },
    {
      "epoch": 8.981157469717362,
      "grad_norm": 0.5979434847831726,
      "learning_rate": 4.4896737503741396e-07,
      "loss": 0.0865,
      "step": 6673
    },
    {
      "epoch": 8.98250336473755,
      "grad_norm": 0.4290965795516968,
      "learning_rate": 4.1903621670158634e-07,
      "loss": 0.0927,
      "step": 6674
    },
    {
      "epoch": 8.983849259757738,
      "grad_norm": 0.3859458863735199,
      "learning_rate": 3.8910505836575877e-07,
      "loss": 0.0716,
      "step": 6675
    },
    {
      "epoch": 8.985195154777927,
      "grad_norm": 0.3875040113925934,
      "learning_rate": 3.5917390002993115e-07,
      "loss": 0.1003,
      "step": 6676
    },
    {
      "epoch": 8.986541049798117,
      "grad_norm": 0.2943633496761322,
      "learning_rate": 3.292427416941036e-07,
      "loss": 0.0637,
      "step": 6677
    },
    {
      "epoch": 8.987886944818305,
      "grad_norm": 0.40873366594314575,
      "learning_rate": 2.99311583358276e-07,
      "loss": 0.0825,
      "step": 6678
    },
    {
      "epoch": 8.989232839838493,
      "grad_norm": 0.3435368239879608,
      "learning_rate": 2.693804250224484e-07,
      "loss": 0.089,
      "step": 6679
    },
    {
      "epoch": 8.990578734858682,
      "grad_norm": 0.41278406977653503,
      "learning_rate": 2.394492666866208e-07,
      "loss": 0.0974,
      "step": 6680
    },
    {
      "epoch": 8.99192462987887,
      "grad_norm": 0.2152508795261383,
      "learning_rate": 2.0951810835079317e-07,
      "loss": 0.0657,
      "step": 6681
    },
    {
      "epoch": 8.993270524899058,
      "grad_norm": 0.6191163063049316,
      "learning_rate": 1.7958695001496557e-07,
      "loss": 0.1192,
      "step": 6682
    },
    {
      "epoch": 8.994616419919247,
      "grad_norm": 0.42266830801963806,
      "learning_rate": 1.49655791679138e-07,
      "loss": 0.1,
      "step": 6683
    },
    {
      "epoch": 8.995962314939435,
      "grad_norm": 0.3210889995098114,
      "learning_rate": 1.197246333433104e-07,
      "loss": 0.0637,
      "step": 6684
    },
    {
      "epoch": 8.997308209959623,
      "grad_norm": 0.32451558113098145,
      "learning_rate": 8.979347500748279e-08,
      "loss": 0.0895,
      "step": 6685
    },
    {
      "epoch": 8.998654104979812,
      "grad_norm": 0.5081518888473511,
      "learning_rate": 5.98623166716552e-08,
      "loss": 0.0922,
      "step": 6686
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.4275857210159302,
      "learning_rate": 2.99311583358276e-08,
      "loss": 0.0889,
      "step": 6687
    }
  ],
  "logging_steps": 1,
  "max_steps": 6687,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.0101288121104384e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
